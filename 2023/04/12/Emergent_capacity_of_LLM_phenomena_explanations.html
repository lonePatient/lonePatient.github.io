<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大语言模型的涌现能力：现象与解释 | 闲记算法</title><meta name="keywords" content="NLP,预训练,大型语言模型,深度学习,Zero-Shot,LLM,Prompt,CoT,Instruction-Tuning,Grokking"><meta name="author" content="Weitang Liu"><meta name="copyright" content="Weitang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="注1：本文整理自我在今年3 月 11 日 “中国人工智能学会”主办的「ChatGPT 及大模型专题研讨会」上《大型语言模型的涌现能力：现象与解释》的现场分享，介绍了大语言模型中的涌现现象，以及关于涌现能力背后原因的相关猜想。感谢CSDN帮助整理的文字稿。 注2:另，有人问了，既然很多自然现象也体现出涌现能力，那么大语言模型的涌现现象需要解释吗？我个人认为是需要的。毕竟，说大语言模型的某个特殊现象属">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型的涌现能力：现象与解释">
<meta property="og:url" content="http://lonepatient.top/2023/04/12/Emergent_capacity_of_LLM_phenomena_explanations.html">
<meta property="og:site_name" content="闲记算法">
<meta property="og:description" content="注1：本文整理自我在今年3 月 11 日 “中国人工智能学会”主办的「ChatGPT 及大模型专题研讨会」上《大型语言模型的涌现能力：现象与解释》的现场分享，介绍了大语言模型中的涌现现象，以及关于涌现能力背后原因的相关猜想。感谢CSDN帮助整理的文字稿。 注2:另，有人问了，既然很多自然现象也体现出涌现能力，那么大语言模型的涌现现象需要解释吗？我个人认为是需要的。毕竟，说大语言模型的某个特殊现象属">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413215813.jpeg">
<meta property="article:published_time" content="2023-04-12T23:20:08.000Z">
<meta property="article:modified_time" content="2025-10-31T07:26:14.539Z">
<meta property="article:author" content="Weitang Liu">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="预训练">
<meta property="article:tag" content="大型语言模型">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Zero-Shot">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Prompt">
<meta property="article:tag" content="CoT">
<meta property="article:tag" content="Instruction-Tuning">
<meta property="article:tag" content="Grokking">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413215813.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://lonepatient.top/2023/04/12/Emergent_capacity_of_LLM_phenomena_explanations"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//s4.cnzz.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" data-pjax="data-pjax" src="https://s4.cnzz.com/z_stat.php?id=1273275888&amp;web_id=1273275888"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-31 07:26:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/backgroud.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><script src="https://at.alicdn.com/t/c/font_3570527_dthoqrrv2tv.css"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JwRQtLKZggvJH4sJ",ck:"JwRQtLKZggvJH4sJ"})</script><link rel="stylesheet" href="/css/universe.css"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3536467946304280" crossorigin="anonymous"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413215813.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">闲记算法</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大语言模型的涌现能力：现象与解释<a class="post-edit-link" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts/Emergent_capacity_of_LLM_phenomena_explanations.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-12T23:20:08.000Z" title="发表于 2023-04-12 23:20:08">2023-04-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T07:26:14.539Z" title="更新于 2025-10-31 07:26:14">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大型语言模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/04/12/Emergent_capacity_of_LLM_phenomena_explanations.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>注1：本文整理自我在今年3 月 11 日 “中国人工智能学会”主办的「ChatGPT 及大模型专题研讨会」上《大型语言模型的涌现能力：现象与解释》的现场分享，介绍了大语言模型中的涌现现象，以及关于涌现能力背后原因的相关猜想。感谢CSDN帮助整理的文字稿。</p>
<p>注2:另，有人问了，既然很多自然现象也体现出涌现能力，那么大语言模型的涌现现象需要解释吗？我个人认为是需要的。毕竟，说大语言模型的某个特殊现象属于“涌现现象”，也是被个别研究提出来，未有确切证明或证据，是否它和自然现象中出现的涌现现象内在机制是类似或一样的，其实可以存疑。而且我认为大模型的这个现象，背后应该有些我们可以理解的原因。如果我们不追求现象背后的解释，仅仅把目前解释不了的现象统一归类为涌现或者其它什么概念，就此了之。那么，其实我们也可以把大模型的目前理解不了的很多现象，统一归类为这是一种“神迹”，那世界上很多事情就简单多了。另另，用Grokking解释涌现现象，尽管我把它称为”用玄学解释玄学“，但是觉得还是值得深入探索的方向，也许可以把上面的说法，优化为”用含玄量较低的玄学解释另外一个含玄量较高的玄学“。</p>
<p>注3:如果仔细分析的话，大语言模型的这个所谓“涌现现象”，如果仅仅把现象归因于模型规模，目前看大概率是把问题简化了，很有可能影响因素是多样化的。如果让我归纳的话，某个任务，大语言模型是否会出现“涌现现象”这个变量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> ，很可能是以下多因素影响的某个未知函数： 模型参数规模，训练数据量大小，训练充分程度，具体任务类型<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>f</mi></mrow><annotation encoding="application/x-tex">y=f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span>(“模型参数规模”，“训练数据量大小”，“训练充分程度”，“具体任务类型”)</p>
<p>而关于此，需要进一步更深入的研究与解释。</p>
<h3 id="为什么是大语言模型？">为什么是大语言模型？</h3>
<p>随着算力的不断提升，语言模型已经从最初基于概率预测的模型发展到基于 Transformer 架构的预训练语言模型，并逐步走向大模型的时代。为什么要突出大语言模型或是在前面加个“Large”？更重要的是它的涌现能力。</p>
<p>当模型规模较小时，模型的性能和参数大致符合比例定律，即模型的性能提升和参数增长基本呈线性关系。然而，当 GPT-3/ChatGPT 这种千亿级别的大规模模型被提出后，人们发现其可以打破比例定律，实现模型能力质的飞跃。这些能力也被称为大模型的“涌现能力”（如理解人类指令等）。</p>
<h3 id="什么是大模型的涌现能力">什么是大模型的涌现能力</h3>
<p>复杂系统学科里已经对涌现现象做过很久的相关研究。那么，什么是“涌现现象”？当一个复杂系统由很多微小个体构成，这些微小个体凑到一起，相互作用，当数量足够多时，在宏观层面上展现出微观个体无法解释的特殊现象，就可以称之为“涌现现象”。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413215939.webp" alt=""></p>
<div align="center">生活中的涌现现象</div>
<p>在日常生活中也有一些涌现现象，比如雪花的形成、堵车、动物迁徙、涡流形成等。这里以雪花为例来解释：雪花的构成是水分子，水分子很小，但是大量的水分子如果在外界温度条件变化的前提下相互作用，在宏观层面就会形成一个很规律、很对称、很美丽的雪花。</p>
<p>那么问题是：超级大模型会不会出现涌现现象？显然我们很多人都知道答案，答案是会的。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220152.webp" alt=""></p>
<div align='center'>大语言模型参数增长示意图</div>
<p>我们先来看下大语言模型的规模增长情况。如果归纳下大语言模型在近两年里最大的技术进展，很有可能就是模型规模的快速增长。如今，大规模模型一般超过 100B，即千亿参数。如 Google 发布的多模态具身视觉语言模型 PaLM-E，由540B 的 PaLM 文本模型和 22B 的 VIT图像模型构成，两者集成处理多模态信息，所以它的总模型规模是 566B。</p>
<p>大语言模型规模不断增长时，对下游任务有什么影响？对于不同类型的任务，有三种不同的表现：</p>
<p>第一类任务表现出伸缩法则：这类任务一般是知识密集型任务。随着模型规模的不断增长，任务效果也持续增长，说明这类任务对大模型中知识蕴涵的数量要求较高。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220235.webp" alt=""></p>
<div align="center">伸缩法则与涌现能力</div>
<p>第二类任务表现出涌现能力：这类任务一般是由多步骤构成的复杂任务。只有当模型规模大到一定程度时，效果才会急剧增长，在模型规模小于某个临界值之前，模型基本不具备任务解决能力。这就是典型的涌现能力的体现。这类任务呈现出一种共性：大多数是由多步骤构成的复杂任务。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220334.webp" alt=""></p>
<p>第三类任务数量较少，随着模型规模增长，任务效果体现出一个U 形曲线。如上图所示，随着模型规模增长，刚开始模型效果会呈下降趋势，但当模型规模足够大时，效果反而会提升。如果对这类任务使用 思维链CoT技术，这些任务的表现就会转化成伸缩法则，效果也会随着模型规模增长而持续上升。</p>
<p>因此，模型规模增长是必然趋势，当推进大模型规模不断增长的时候，涌现能力的出现会让任务的效果更加出色。</p>
<h3 id="LLM-表现出的涌现现象">LLM 表现出的涌现现象</h3>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220358.webp" alt=""></p>
<p>目前有两大类被认为具有涌现能力的任务，第一类是 In Context Learning（“Few-Shot Prompt”），用户给出几个例子，大模型不需要调整模型参数，就能够处理好任务（参考上图给出的情感计算的例子）。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220420.webp" alt=""></p>
<p>如上图展示，利用In Context Learning，已经发现在各种类型的下游任务中，大语言模型都出现了涌现现象，体现在在模型规模不够大的时候，各种任务都处理不好，但是当跨过某个模型大小临界值的时候，大模型就突然能比较好地处理这些任务。</p>
<p>第二类具备涌现现象的技术是思维链( CoT)。CoT本质上是一种特殊的few shot prompt，就是说对于某个复杂的比如推理问题，用户把一步一步的推导过程写出来，并提供给大语言模型（如下图蓝色文字内容所示），这样大语言模型就能做一些相对复杂的推理任务。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220444.webp" alt=""></p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220524.webp" alt=""></p>
<p>从上图可以看出，无论是数学问题、符号推理问题，CoT 都具备涌现能力。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220544.webp" alt=""></p>
<p>除此之外，其他任务也有涌现能力，如上图所示的数学多位数加法、命令理解等。</p>
<h3 id="LLM-模型规模和涌现能力的关系">LLM 模型规模和涌现能力的关系</h3>
<p>可以看出，涌现能力和模型的规模大小有一定的关联关系 ，那么，我们的问题是，具体而言，两者是怎样的关系呢？</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220605.webp" alt=""></p>
<p>我们分头来看，先看下In Context Learning 的涌现能力和模型规模的关系。上图展示了对于不同类型的具体任务， In Context Learning 的涌现能力和模型规模的对照关系。</p>
<p>从图中数据可以看出，我们很难给出一个唯一的模型大小数值。不同类型的任务，在In Context Learning方面，模型多大才具备涌现能力，这跟具体的任务有一定的绑定关系。例如：图表第一行的3位数加法任务，模型只要达到 13B（130亿参数），就可以具备涌现能力，但是对倒数第二行的 Word in Context Benchmark任务而言，目前证明，只有540B 大小的模型才可以做到这点。我们只能说，就In Context Learning而言，如果模型达到 100B， 大多数任务可以具备涌现能力。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220631.webp" alt=""></p>
<p>对于CoT来说，结论也是类似的，就是说要想出现涌现能力，模型规模大小和具体任务有一定的绑定关系。</p>
<p><strong>把模型做小会影响 LLM 的涌现能力吗？</strong></p>
<p>因为对很多任务来说，只有模型规模做到比较大，才能具备涌现能力，所以我个人比较关心下列问题：我们能不能把模型做小？把模型做小是否会影响到LLM的涌现能力？这是个很有趣的问题。我们这里拿两个小模型代表来探讨这个问题。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220654.webp" alt=""></p>
<p>第一个小模型代表，是 DeepMind 2021年发表的模型 Chinchilla，这个模型目前做各种任务的效果，和 540B大小的PaLM 基本相当。Chinchilla的思路是给更多的数据，但是把模型规模做小。具体而言，它对标的是Gopher模型，Chinchilla模型大小只有 70B，是Gopher的四分之一，但是付出的代价是训练数据总量，是Gopher的四倍，所以基本思路是通过放大训练数据量，来缩小模型规模。</p>
<p>我们把Chinchilla规模做小了，问题是它还具备涌现能力吗？从上图给出的数据可以看出，起码我们可以说， Chinchilla 在自然语言处理的综合任务 MMLU 上是具备涌现能力的。如果小模型也能具备涌现能力，那么这其实侧面反映了一个问题：对于类似 GPT3 这样的模型而言，很可能它175B 这么多的模型参数，并没有被充分利用，因此，我们在以后训练模型的时候，可以考虑先增加训练数据，降低模型参数量，把模型做小，先把模型参数利用充分，在这个基础上，再继续增加数据，并推大模型规模。也即是说，目前看，我们先把模型做小，再把模型做大，看上去是可行的。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220720.webp" alt=""></p>
<p>第二个小模型代表是 Meta 发布的开源模型 LLaMA，它的做法其实很好理解，本质上就是开源的 Chinchilla，它的思路是完全遵照 Chinchilla 来做的，就是说增加训练数据，但是把模型规模做小。那么LLaMA是否具备涌现能力呢？从上图表格数据可以看出， 虽然LLaMA 在MMLU这个任务上比 Chinchilla 稍差一些，但是效果也不错。这说明LLaMA在MMLU上基本也是具备涌现能力的。</p>
<p>其实，有个工作目前还没有看到有人做，但是这个工作是很有价值的，就是充分测试当模型变得足够小（比如10B-50B规模）以后，各种任务的涌现能力是否还具备？这是个很有价值的事情，因为如果我们的结论是即使把模型规模做小，各种任务的涌现能力可以保持，那么我们就可以放心地先追求把模型做小。</p>
<h3 id="模型训练中的顿悟现象">模型训练中的顿悟现象</h3>
<p>这里介绍一个比较新的研究方向，顿悟现象，英文叫 “Grokking”。 在这里介绍模型训练过程中的顿悟，目的是希望建立起它和大模型涌现能力之间的联系，我在本文后面会尝试用顿悟现象来解释大模型的涌现能力。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220744.webp" alt=""></p>
<p>我们首先解释下什么是顿悟现象。如上图所示，对于一个<strong>训练数据较少</strong>的数学任务（通常是数字求和取余数的问题），研究人员发现一种新奇的现象。比如我们将数据集切成两块，50% 数据作为训练集（图中红线展示了随着训练过程往后走，任务指标的变化情况），50% 的数据作为验证集（图中绿线的走势展示了训练动态）。在学习数字求和取余这个任务时，它的训练动态会经历三个阶段：</p>
<ul>
<li>第一个阶段是记忆期：红线对应的训练数据指标突然走高，代表模型记住了50%的训练数据的结果，而绿线对应的验证集指标接近0，说明模型完全没有泛化能力，就是说没有学会这个任务的规律。所以这个阶段模型只是在单纯地记忆训练数据。</li>
<li>第二个阶段是平台期：这个阶段是记忆期的延续，体现为验证集合效果仍然很差，说明模型仍然没有学会规律。</li>
<li>第三个阶段是泛化期：这个阶段验证集合效果突然变好，这说明突然之间，模型学会了任务里的规律，也就是我们说的，出现了顿悟现象，突然就学明白了。</li>
</ul>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220810.webp" alt=""></p>
<p>后续研究表明：Grokking 本质上是在学习输入数字的一个好的表征。如图所示，可以看到由初始化向记忆期再到顿悟现象出现的过程，数字的表征逐步开始体现当前学习任务的任务结构。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220839.webp" alt=""></p>
<p>那么 ，我们能用Grokking 来解释大模型的涌现现象吗？目前，有部分研究暗示两者实际是存在某些关联的，但尚未有研究明确地指出两者之间的关系。两者从走势曲线看是非常接近的，但是有很大区别，因为Grokking描述的是模型训练动态中的表现，而涌现表达的是模型规模变化时的任务表现，虽然走势相近，但两者不是一回事。我认为，要想用Grokking解释涌现现象，核心是要解释清楚下列问题：为什么规模小的语言模型不会出现 Grokking ？这是个很关键的问题。因为如果规模小以及规模大的语言模型都会出现Grokking，那么说明Grokking和模型规模无关，也就不可能用来解释大模型的涌现现象。本文后面，我会给出一个自己的猜想，来建立两者之间的联系。</p>
<h3 id="LLM-涌现能力的可能原因">LLM 涌现能力的可能原因</h3>
<p>为什么随着模型增大会出现涌现现象？这里给出三种猜想。前两种是现有文献提出的，第三个是我试图采用Grokking来解释涌现现象的猜想。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220902.webp" alt=""></p>
<p><strong>猜想一：任务的评价指标不够平滑</strong></p>
<p>一种猜想是因为很多任务的评价指标不够平滑，导致我们现在看到的涌现现象。关于这一点，我们拿 Emoji_movie 任务来给出解释。Emoji_movie任务是说输入 Emoji图像，要求 LLM 给出完全正确的电影名称，只有一字不错才算正确，错一个单词都算错。 如上图所示，输入的Emoji是一个女孩的笑脸，后面跟着三张鱼类的图片，您可以猜猜这是什么电影。下面左侧的 2m代表 模型参数规模是 200 万参数，以及对应模型给出的回答。可以看出，随着模型规模不断增大至 128B 时，LLM才能完全猜对电影名称，但是在模型到了125m和4b的时候，其实模型已经慢慢开始接近正确答案了。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220924.webp" alt=""></p>
<p>如果评价指标要求很严格，要求一字不错才算对，那么Emoji_movie任务我们就会看到涌现现象的出现，如上图图左所示。但是，如果我们把问题形式换成多选题，就是给出几个候选答案，让LLM选，那么随着模型不断增大，任务效果在持续稳定变好，但涌现现象消失，如上图图右所示。这说明评价指标不够平滑，起码是一部分任务看到涌现现象的原因。</p>
<p><strong>猜想二：复杂任务</strong> <strong>vs</strong> <strong>子任务</strong></p>
<p>开始的时候我们提到过，展现出涌现现象的任务有一个共性，就是任务往往是由多个子任务构成的复杂任务。也就是说，最终任务过于复杂，如果仔细分析，可以看出它由多个子任务构成，这时候，子任务效果往往随着模型增大，符合 Scaling Law，而最终任务则体现为涌现现象。这个其实好理解，比如我们假设某个任务 T 有 5 个子任务 Sub-T 构成，每个 sub-T 随着模型增长，指标从 40% 提升到 60%，但是最终任务的指标只从 1.1% 提升到了 7%，也就是说宏观上看到了涌现现象，但是子任务效果其实是平滑增长的。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413220948.webp" alt=""></p>
<p>我们拿下国际象棋任务来作为例子，如上图所示，让语言模型预测下一步，最终评价指标是只有“将”死才算赢。如果按“将死”评估（红线），发现随着模型增大，模型在缓慢上升，符合涌现的表现。若评估LLM合法移动（绿线），而在合法的移动步骤里进行正确选择才够最后“将死”是个子任务，所以其实这是比将死简单的一个子任务。我们看合法移动随着模型规模，效果持续上升。此时，我们是看不到涌现现象的。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413221041.webp" alt=""></p>
<p>这里可以给出一个例证，如上图所示，对于CoT 任务，谷歌用 62B 和 540B 模型对LLM做错的例子进行了错误分析。对于62B做错、而540B做对的例子分析，可以看出，最多的一类错误来自于单步推理错误，这其实也能侧面说明复杂任务和子任务的关系。</p>
<p><strong>猜想三：用</strong> <strong>Grokking</strong> <strong>来解释涌现</strong></p>
<p>这里介绍我设想的如何用Grokking来解释涌现现象的一种猜想，在建立两者之间关系前，我们先来看两个已知的事实，然后在事实基础上作出推论。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413221100.webp" alt=""></p>
<p>首先，第一个事实是 Grokking 现象是描述训练数据量较少的 ML 任务的，但是任务最小训练数据量必须达到一定的量，才会出现 Grokking 现象。这里有两点，一个是本身训练数据量少，另外是最小数据量要达到临界值。只有同时满足上面两个条件，才有可能出现Grokking现象。上图的意思是：当我们只拿40%及以下比例的数据来训练模型的时候，我们看不到Grokking现象，只有记忆没有泛化，只有最小训练数据比例超过40%，才会出现模型的顿悟。这是一个已被验证的事实。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413221126.webp" alt=""></p>
<p>第二个事实是 LLM 模型规模越大，记忆数据的能力越强。关于这点目前有很多研究已经可以证明，如果简单理解的话，可以理解为：对于某个任务T，假设预训练数据里包含与任务T相关的训练数据量有100条，那么大模型可以记住其中的70条，而小模型可能只能记住30条。虽不精确，但大概是这个意思。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413221156.webp" alt=""></p>
<p>在上面的两个事实基础上，我们试图来用Grokking解释大模型的涌现现象。首先我们给出一个简单的解释，这个解释只需要利用第一个事实即可，就是说，任务的最少训练数据量需要达到临界值，才会出现Grokking。在这个事实下，对于某个任务T，尽管我们看到的预训练数据总量是巨大的，但是与T相关的训练数据其实数量很少。当我们推大模型规模的时候，往往会伴随着增加预训练数据的数据量操作，这样，当模型规模达到某个点的时候，与任务T相关的数据量，突然就达到了最小要求临界点，于是我们就看到了这个任务产生了Grokking现象。在语言模型的宏观角度，看起来就是模型达到了某个规模，突然任务T效果就开始变好，而模型规模较小的时候，因为没有达到临界值，所以一直没有Grokking现象，看起来就是语言模型没有这个能力。这是一种可以从Grokking角度解释大模型涌现现象的可能。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413221214.webp" alt=""></p>
<p>上面这个猜想其实有个约束条件，因为我们有个假设，说随着模型规模增大，训练数据量也在增大。如果这个假设不存在，也就是说，随着模型规模增大，我们固定住训练数据量不变。那么，这种情况下，怎么能用Grokking解释涌现现象呢？此时如果我们同时利用事实1和事实2，也可以给出一个解释。更具体来说，我们假设在预训练数据中，某个任务T有 100 个训练数据，当模型规模小的时可能只记得 30 个，达不到Grokking现象的临界点，而当模型规模推大时，因为模型记忆能力增强，可能就能记住其中的50个，这意味着它可能超过了Grokking的临界点，于是会出现 Grokking 里面的泛化现象。如果从这个角度看，其实我们也可以从Grokking角度来解释为何只有大模型才会具备涌现现象。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/621438653">阅读原文</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Weitang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lonepatient.top/2023/04/12/Emergent_capacity_of_LLM_phenomena_explanations.html">http://lonepatient.top/2023/04/12/Emergent_capacity_of_LLM_phenomena_explanations.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lonepatient.top" target="_blank">闲记算法</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/">预训练</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大型语言模型</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Zero-Shot/">Zero-Shot</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Prompt/">Prompt</a><a class="post-meta__tags" href="/tags/CoT/">CoT</a><a class="post-meta__tags" href="/tags/Instruction-Tuning/">Instruction-Tuning</a><a class="post-meta__tags" href="/tags/Grokking/">Grokking</a></div><div class="post_share"><div class="social-share" data-image="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413215813.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/04/18/A_Survey_of_Large_Language_Models.html"><img class="prev-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230419195449.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">A Survey of Large Language Models</div></div></a></div><div class="next-post pull-right"><a href="/2023/04/02/The-Art-of-Asking-ChatGPT-for-High-Quality-Answers-A-complete-Guide-to-Prompt-Engineering-Technique-main.html"><img class="next-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230410200141.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Prompts技巧工程完全指南</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/03/10/ChatGPT_Research_Report.html" title="ChatGPT 调研报告"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230310095306.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-10</div><div class="title">ChatGPT 调研报告</div></div></a></div><div><a href="/2022/10/30/Finetuned_Language_Models_are_Zero-Shot_Learners .html" title="Finetuned Language Models are Zero-Shot Learners"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230223213000.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-30</div><div class="title">Finetuned Language Models are Zero-Shot Learners</div></div></a></div><div><a href="/2023/06/26/Instruction-Tuning.html" title="Instruction Tuning 阶段性总结"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230627120653.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">Instruction Tuning 阶段性总结</div></div></a></div><div><a href="/2023/06/30/ReACT.html" title="ReAct：Synergizing Reasoning and Acting in Language Models"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20231009205857.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-30</div><div class="title">ReAct：Synergizing Reasoning and Acting in Language Models</div></div></a></div><div><a href="/2023/04/18/A_Survey_of_Large_Language_Models.html" title="A Survey of Large Language Models"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230419195449.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-18</div><div class="title">A Survey of Large Language Models</div></div></a></div><div><a href="/2023/03/10/Inverse-scaling-can-become-U-shaped.html" title="Inverse scaling can become U-shaped"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230418184619.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-10</div><div class="title">Inverse scaling can become U-shaped</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Weitang Liu</div><div class="author-info__description">一个致力于记录技术的博客</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lonePatient"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lonePatient" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuweitangmath@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/277974397" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有任何问题可通过留言板或者微信公众号给我留言，谢谢！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">为什么是大语言模型？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B"><span class="toc-number">2.</span> <span class="toc-text">什么是大模型的涌现能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-%E8%A1%A8%E7%8E%B0%E5%87%BA%E7%9A%84%E6%B6%8C%E7%8E%B0%E7%8E%B0%E8%B1%A1"><span class="toc-number">3.</span> <span class="toc-text">LLM 表现出的涌现现象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E5%92%8C%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">4.</span> <span class="toc-text">LLM 模型规模和涌现能力的关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%AD%E7%9A%84%E9%A1%BF%E6%82%9F%E7%8E%B0%E8%B1%A1"><span class="toc-number">5.</span> <span class="toc-text">模型训练中的顿悟现象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LLM-%E6%B6%8C%E7%8E%B0%E8%83%BD%E5%8A%9B%E7%9A%84%E5%8F%AF%E8%83%BD%E5%8E%9F%E5%9B%A0"><span class="toc-number">6.</span> <span class="toc-text">LLM 涌现能力的可能原因</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-31"/></a><div class="content"><a class="title" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31">Arxiv今日论文 | 2025-10-31</a><time datetime="2025-10-31T10:30:00.000Z" title="发表于 2025-10-31 10:30:00">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-30"/></a><div class="content"><a class="title" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30">Arxiv今日论文 | 2025-10-30</a><time datetime="2025-10-30T10:30:00.000Z" title="发表于 2025-10-30 10:30:00">2025-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-29"/></a><div class="content"><a class="title" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29">Arxiv今日论文 | 2025-10-29</a><time datetime="2025-10-29T10:30:00.000Z" title="发表于 2025-10-29 10:30:00">2025-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-28"/></a><div class="content"><a class="title" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28">Arxiv今日论文 | 2025-10-28</a><time datetime="2025-10-28T10:30:00.000Z" title="发表于 2025-10-28 10:30:00">2025-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-27"/></a><div class="content"><a class="title" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27">Arxiv今日论文 | 2025-10-27</a><time datetime="2025-10-27T10:30:00.000Z" title="发表于 2025-10-27 10:30:00">2025-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Weitang Liu</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'https://twikoo.lonepatient.top/',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.lonepatient.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo@1.4.11/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (95)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/计算机视觉/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 计算机视觉 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/知识图谱/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 知识图谱 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 深度学习 (135)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://lonepatient.top/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230219144729.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-18</span><a class="blog-slider__title" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">通向AGI之路：大型语言模型（LLM）技术精要</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/07/12/gaiic_2022_ner_top10.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20220712181905.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-07-12</span><a class="blog-slider__title" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">GAIIC2022商品标题识别二等奖获奖解决思路</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230807190424.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-01-20</span><a class="blog-slider__title" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">2025-03|高质量中文预训练模型集合</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>