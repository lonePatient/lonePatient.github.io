<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>用于大型Transformer的8-bit矩阵乘法介绍 | 闲记算法</title><meta name="keywords" content="NLP,预训练,大型语言模型,深度学习,Zero-Shot,HuggingFace,LLM,量化,8-bit,Transformer"><meta name="author" content="Weitang Liu"><meta name="copyright" content="Weitang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="语言模型正变的越来越大，PaLM有540B的参数量，OPT、GPT-3和BLOOM则大约有176B的参数量，而且我们正朝着更大的模型发展。下图是近些年语言模型的尺寸。  这些模型很难在常用设备上运行。例如，仅仅推理BLOOM-176B就需要8张A00 GPUs(每张80G显存，价格约15k美元)。而为了微调BLOOM-176B则需要72张GPU。PaLM则需要更多的资源。 这些巨型模型需要太多GP">
<meta property="og:type" content="article">
<meta property="og:title" content="用于大型Transformer的8-bit矩阵乘法介绍">
<meta property="og:url" content="http://lonepatient.top/2023/03/02/8-bit_Matrix_Multiplication_transformers_Hugging_Face_Transformers.html">
<meta property="og:site_name" content="闲记算法">
<meta property="og:description" content="语言模型正变的越来越大，PaLM有540B的参数量，OPT、GPT-3和BLOOM则大约有176B的参数量，而且我们正朝着更大的模型发展。下图是近些年语言模型的尺寸。  这些模型很难在常用设备上运行。例如，仅仅推理BLOOM-176B就需要8张A00 GPUs(每张80G显存，价格约15k美元)。而为了微调BLOOM-176B则需要72张GPU。PaLM则需要更多的资源。 这些巨型模型需要太多GP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324140620.png">
<meta property="article:published_time" content="2023-03-02T23:20:08.000Z">
<meta property="article:modified_time" content="2025-10-31T07:26:14.533Z">
<meta property="article:author" content="Weitang Liu">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="预训练">
<meta property="article:tag" content="大型语言模型">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Zero-Shot">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="量化">
<meta property="article:tag" content="8-bit">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324140620.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://lonepatient.top/2023/03/02/8-bit_Matrix_Multiplication_transformers_Hugging_Face_Transformers"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//s4.cnzz.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" data-pjax="data-pjax" src="https://s4.cnzz.com/z_stat.php?id=1273275888&amp;web_id=1273275888"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-31 07:26:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/backgroud.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><script src="https://at.alicdn.com/t/c/font_3570527_dthoqrrv2tv.css"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JwRQtLKZggvJH4sJ",ck:"JwRQtLKZggvJH4sJ"})</script><link rel="stylesheet" href="/css/universe.css"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3536467946304280" crossorigin="anonymous"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324140620.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">闲记算法</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">用于大型Transformer的8-bit矩阵乘法介绍<a class="post-edit-link" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts/8-bit_Matrix_Multiplication_transformers_Hugging_Face_Transformers.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-02T23:20:08.000Z" title="发表于 2023-03-02 23:20:08">2023-03-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T07:26:14.533Z" title="更新于 2025-10-31 07:26:14">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大型语言模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/03/02/8-bit_Matrix_Multiplication_transformers_Hugging_Face_Transformers.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>语言模型正变的越来越大，PaLM有540B的参数量，OPT、GPT-3和BLOOM则大约有176B的参数量，而且我们正朝着更大的模型发展。下图是近些年语言模型的尺寸。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324140722.png" alt=""></p>
<p>这些模型很难在常用设备上运行。例如，仅仅推理BLOOM-176B就需要8张A00 GPUs(每张80G显存，价格约15k美元)。而为了微调BLOOM-176B则需要72张GPU。PaLM则需要更多的资源。</p>
<p>这些巨型模型需要太多GPUs才能运行，因此需要寻找方法来减少资源需求并保证模型的性能。已经有各种技术用来减小模型尺寸，例如量化、蒸馏等。</p>
<p>在完成BLOOM-176B训练后，HuggingFace和BigScience逐步探索在少量GPU上运行大模型的方法。最终，设计出了Int8量化方法，该方法在不降低大模型性能的情况下，将显存占用降低了1至2倍，并且集成到了Transformers模块中。具体关于LLM.int8内容可参考余下论文：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></p>
</blockquote>
<h3 id="常用类型">常用类型</h3>
<p>浮点数在机器学习中也被称为&quot;精度&quot;。模型大小是有参数量及参数精度决定的，通常是float32、float16和bfloat16。</p>
<p>我们开始于不同浮点数的基本理解，在机器学习的背景下也被称为&quot;精度&quot;。模型的大小由其参数数量和精度决定，通常是float32、float16和bfloat16。下图：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324140828.png" alt=""></p>
<p>Float32(FP32)是标准的IEEE 32-bit浮点数表示，使用这种类型可以表示范围广泛的浮点数。在FP32中，8bits被用于&quot;指数&quot;，23bits被用于&quot;尾数&quot;, 1 bit则用于符号位。大多数的硬件都支持FP32操作和指令。</p>
<p>在Float16(FP16)数据类型中，5 bits被用作&quot;指数&quot;，10 bits用于&quot;尾数&quot;。这使得FP16数的表示范围明显小于FP32，导致有上溢和下溢的风险。例如，若你做 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi>k</mi><mo>×</mo><mn>10</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">10k \times 10k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord">10</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">10</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> ，最终得到100k。这在FP16中是不可能的，因为其最大表示为64K。因此你会得到NaN的结果，若像神经网络那样顺序执行，所有先前的工作都会被破坏。通常，loss缩放能够一定程度上克服这个问题，但并不总是有用。</p>
<p>因此，创建了一种新格式bfloat15(BF16)来避免这种问题。在BF16中，8bits被用于表示&quot;指数&quot;, 7bits被用于表示&quot;尾数&quot;。这意味着BF16能够保留和FP32相同的动态范围，但是损失了3bits的精度。BF16可以表示巨大的数，但是精度上比FP16差。</p>
<p>在Ampere架构中，NVIDIA也引入了TensorFloat-32(TF32)精度格式，其仅使用19 bits就合并了BF16的动态范围和FP16的精度。其目前仅在内部某些操作中使用。</p>
<p>在机器学习的术语中FP32被称为全精度(4 bytes)，BF16和FP16则称为半精度(2 bytes)。int8(INT8)数据类型则是由8 bits表示的数，其能够存储<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">2^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span>个不同的值([0,255]或者[-128, 127])</p>
<p>理想情况下，训练和推理应该在FP32上进行，但是其比FP16/BF16慢两倍。因此，采用一种混合精度的方法，模型权重仍然是FP32，前向和后向传播则使用FP16/BF16，从而加快训练速度。P16/BF16被用来更新FP32权重。</p>
<p>在训练过程中，模型权重以FP32存储，但在推理过程中，半精度权重能够保证与FP32权重对应的类似结果–因为只有模型在进行多个梯度更新时才需要对模型的精确参考。这意味着我们可以使用半精度权重降低一半的GPU大小来完成同样的结果。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324160054.png" alt=""></p>
<p>可以通过参数量乘以浮点数精度的大小来计算模型所占用的bytes量。例如，若模型使用bfloat16版本的BLOOM-176B模型，那么模型大小为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>176</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>9</mn></msup><mo>×</mo><mn>2</mn><mi>b</mi><mi>y</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo>=</mo><mn>352</mn><mi>G</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">176  \times 10^9 \times 2 bytes = 352GB</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">176</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">9</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">2</span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">t</span><span class="mord mathnormal">es</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">352</span><span class="mord mathnormal" style="margin-right:0.05017em;">GB</span></span></span></span>！这个量级对于适配少量GPU来说相当有挑战。</p>
<p>但是我们是否可以使用不同的数据类型以更少的存储空间来保存这些权重？一种称为量化的方法被广泛的应用于Deep Learning。</p>
<h3 id="模型量化">模型量化</h3>
<p>通过实验发现，在推理中使用2 bytes的BF16/FP16精度能够几乎达到4 bytes的FP32精度相同的效果，而且模型尺寸可以减少一半。若能够进一步削减那就太棒了，但是在更低的精度上推理质量开始急剧下降。为了解决这个问题，我们引入了8 bits量化。该方法使用四分之一的精度，这样仅需要1/4的模型尺寸！但是，其不是通过丢掉另外一半bits来实现的。</p>
<p>量化基本上是从一种数据类型&quot;舍入&quot;为另一种数据类型来完成的。例如，若一个数量类型范围0…9，另一个范围则是0…4。那么第一个数据类型中的&quot;4&quot;将会被舍入为第二种数据类型中的&quot;2&quot;。然而，若第一种数据类型中的&quot;3&quot;，其会位于第二种数据类型的1和2之间，然后通常会被舍入为&quot;2&quot;。也就是说第一种数据类型中的&quot;4&quot;和&quot;3&quot;都会对应第二种数据类型中的&quot;2&quot;。这表明量化是可能带来信息丢失的噪音过程，一种有损压缩。</p>
<p>有两种常见的8-bit量化技术：zero-point量化和absolute maximum(absmax)量化。zero-point量化和absmax量化会将浮点数值映射至更加紧凑的int8(1 byte)值。这些方法首先会将输入按照量化常数进行缩放，从而实现规范化。</p>
<p>举例来说，在zero-point量化中，若范围是 -1.0…1.0， 并希望量化至范围  -127…127 。那么应该按照因子127进行缩放，然后四舍五入至8-bit精度。为了还原原始值，需要将int8的值除以量化因子127。例如，0.3被缩放为<br>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.3</mn><mo>×</mo><mn>127</mn><mo>=</mo><mn>38.1</mn></mrow><annotation encoding="application/x-tex">0.3 \times 127 = 38.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">0.3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">127</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">38.1</span></span></span></span> ，然后四舍五入为38。若要恢复，则38/127=0.2992。在这个例子中量化误差为0.008。随着这些微小的误差在模型各个层中传播，会逐步积累和增长并导致性能下降。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324161317.png" alt=""></p>
<p>再来看看absmax量化的细节。为了在absmax量化中完成fp16和int8的映射，需要先除以张量中的绝对最大值(令整个张量介于-1至1之间)，然后在乘以目标数据类型的总范围。</p>
<p>例如，在一个向量上应用absmax量化，该向量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1.2</mn><mo separator="true">,</mo><mo>−</mo><mn>0.5</mn><mo separator="true">,</mo><mo>−</mo><mn>4.3</mn><mo separator="true">,</mo><mn>1.2</mn><mo separator="true">,</mo><mo>−</mo><mn>3.1</mn><mo separator="true">,</mo><mn>0.8</mn><mo separator="true">,</mo><mn>2.4</mn><mo separator="true">,</mo><mn>5.4</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">1.2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">0.5</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">4.3</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1.2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">3.1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">0.8</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">2.4</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">5.4</span><span class="mclose">]</span></span></span></span>，从向量中选择最大值，即5.4。而int8的范围为[-127,127]，所以量化过程为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mi>v</mi><mn>5.3</mn></mfrac><mo>×</mo><mn>127</mn><mo>=</mo><mi>v</mi><mo>×</mo><mfrac><mn>127</mn><mn>5.4</mn></mfrac><mo>≈</mo><mi>v</mi><mo>×</mo><mn>23.5</mn></mrow><annotation encoding="application/x-tex">\frac{v}{5.3} \times 127 = v \times \frac{127}{5.4} \approx v \times 23.5
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.7936em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5.3</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">127</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5.4</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">127</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">23.5</span></span></span></span></span></p>
<p>即整个向量乘以缩放因子23.5。最终得到的量化后向量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>28</mn><mo separator="true">,</mo><mo>−</mo><mn>12</mn><mo separator="true">,</mo><mo>−</mo><mn>101</mn><mo separator="true">,</mo><mn>28</mn><mo separator="true">,</mo><mo>−</mo><mn>73</mn><mo separator="true">,</mo><mn>19</mn><mo separator="true">,</mo><mn>56</mn><mo separator="true">,</mo><mn>127</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[28, -12, -101, 28, -73, 19, 56, 127]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">28</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">12</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">101</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">28</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord">73</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">19</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">56</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">127</span><span class="mclose">]</span></span></span></span>。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324161925.gif" alt=""></p>
<p>为了还原原始值，可以使用全精度的int8数除以量化因子23.5。但是由于四舍五入的原因，会丢失一些精度。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324162335.png" alt=""></p>
<p>对于一个无符号的int8，我们将减去最小值，然后按绝对最大值进行缩放。这接近于零点量化的做法。它类似于最小-最大缩放，但后者保持了数值缩放的方式，使数值 &quot;0 &quot;总是由一个整数表示，没有任何量化误差。</p>
<p>这些技巧能够以多种方式组合。例如，当涉及矩阵乘法时，ow-wise或者vector-wise量化可以使得结果更加准确。以矩阵乘法 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>×</mo><mi>B</mi><mo>=</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">A \times B = C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>为例，相对于使用每个张量的绝对最大值来规范张量，vector-vise量化则会寻找矩阵A每行的绝对最大值和矩阵B每列的绝对最大值。然后通过除以这些绝对最大值向量来规范化矩阵A和B。然后执行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>×</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A \times B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 来得到C。为了最终返回FP16精度的值，通过计算A和B绝对最大值向量的外积来反规范化。</p>
<p>这些技术虽然能够量化模型，但是在较大模型上会带来性能下降。Hugging Face Transformers和Accelerate库集成了一种称为LLM.int8()的8-bit量化算法，能够在176B参数量模型上使用且不降低模型效果。</p>
<h3 id="int8">int8</h3>
<p>理解Transformer中与规模相关的涌现特性对于理解为什么传统量化方式在大模型中失败至关重要。性能的下降是由异常特征值导致的，会在后面解释这一情况。</p>
<p>LLM.int8()算法本质上可以由三个步骤来完成矩阵乘法：</p>
<ul>
<li>对输入的hidden states逐列的提取异常值(即大于某个阈值的值)；</li>
<li>分别对FP16中的异常值和INT8中的非异常值执行矩阵乘法；</li>
<li>对非异常的结果进行反量化，并将两者结果合并来获得最终的FP16结果；</li>
</ul>
<p>三个步骤如下图所示：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324141959.gif" alt=""></p>
<h3 id="异常值特征">异常值特征</h3>
<p>在整个分布之外的值，称为异常值。异常值检测被广泛使用，而拥有特征分布的先验知识有助于异常值检测任务。</p>
<p>具体来说，我们观察到经典的量化算法在超过6B参数量的transformer模型上失效了。虽然在较小的模型上也能观测到较大的异常值特征。但是，我们观察到一个参数量的阈值，transformer中的异常值会系统性的出现在每个层中。</p>
<p>由于8-bit精度的局限性，因此仅使用几个特别大的值来量化向量将导致非常差的结果。此外，transformer架构的内在特征就是将所有的元素连接在一起，这将导致错误跨越多层传播并被加剧。因此，开发出了混合精度分解来实现这种极端异常值的量化。</p>
<h3 id="MatMul内部">MatMul内部</h3>
<p>一旦得到hidden state，使用自定义阈值来抽取异常值并分解矩阵为上述两部分。我们发现使用6作为阈值进行抽取可以完整的恢复推理性能。异常值部分以fp16实现，所以是经典的矩阵乘法；而8-bit则是通过vector-wise量化将模型权重和hidden state量化至8-bit的精度。即hidden-state使用row-wise量化，模型权重使用column-wise量化。经过这个步骤后，再将结果反量化并以半精度返回。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324142054.png" alt=""></p>
<h3 id="零退化意味着什么">零退化意味着什么</h3>
<p>如何评估性能下降？8-bit模型到底损失了多少性能？这里在8-bit模型和native模型上运行了常见的基准，分别针对OPT-175B和BLOOM-176B。</p>
<p>对于OPT-175B</p>
<table>
<thead>
<tr>
<th>benchmarks</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>difference - value</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>metric</td>
<td>value - int8</td>
<td>value - fp16</td>
<td>std err - fp16</td>
<td>-</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc_norm</td>
<td>0.7849</td>
<td>0.7849</td>
<td>0.0041</td>
<td>0</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc</td>
<td>0.5921</td>
<td>0.5931</td>
<td>0.0049</td>
<td>0.001</td>
</tr>
<tr>
<td>piqa</td>
<td>acc</td>
<td>0.7965</td>
<td>0.7959</td>
<td>0.0094</td>
<td>0.0006</td>
</tr>
<tr>
<td>piqa</td>
<td>acc_norm</td>
<td>0.8101</td>
<td>0.8107</td>
<td>0.0091</td>
<td>0.0006</td>
</tr>
<tr>
<td>lambada</td>
<td>ppl</td>
<td>3.0142</td>
<td>3.0152</td>
<td>0.0552</td>
<td>0.001</td>
</tr>
<tr>
<td>lambada</td>
<td>acc</td>
<td>0.7464</td>
<td>0.7466</td>
<td>0.0061</td>
<td>0.0002</td>
</tr>
<tr>
<td>winogrande</td>
<td>acc</td>
<td>0.7174</td>
<td>0.7245</td>
<td>0.0125</td>
<td>0.0071</td>
</tr>
</tbody>
</table>
<p>对于BLOOM-176B</p>
<table>
<thead>
<tr>
<th>benchmarks</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>-</th>
<th>difference - value</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>metric</td>
<td>value - int8</td>
<td>value - bf16</td>
<td>std err - bf16</td>
<td>-</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc_norm</td>
<td>0.7274</td>
<td>0.7303</td>
<td>0.0044</td>
<td>0.0029</td>
</tr>
<tr>
<td>hellaswag</td>
<td>acc</td>
<td>0.5563</td>
<td>0.5584</td>
<td>0.005</td>
<td>0.0021</td>
</tr>
<tr>
<td>piqa</td>
<td>acc</td>
<td>0.7835</td>
<td>0.7884</td>
<td>0.0095</td>
<td>0.0049</td>
</tr>
<tr>
<td>piqa</td>
<td>acc_norm</td>
<td>0.7922</td>
<td>0.7911</td>
<td>0.0095</td>
<td>0.0011</td>
</tr>
<tr>
<td>lambada</td>
<td>ppl</td>
<td>3.9191</td>
<td>3.931</td>
<td>0.0846</td>
<td>0.0119</td>
</tr>
<tr>
<td>lambada</td>
<td>acc</td>
<td>0.6808</td>
<td>0.6718</td>
<td>0.0065</td>
<td>0.009</td>
</tr>
<tr>
<td>winogrande</td>
<td>acc</td>
<td>0.7048</td>
<td>0.7048</td>
<td>0.0128</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>可以看到这些模型的性能下降为0，因为这些指标的绝对差值小于标准误差。</p>
<h3 id="比native模型更快？">比native模型更快？</h3>
<p>LLM.int8()方法的主要目标在不降低性能的情况下，使得大模型更容易被使用。但是，如果该方法非常的慢则就不实用了。所以，我们对多个模型的生成速度进行了基准测试。实验发现使用LLM.int8()的BLOOM-176B要比fp16版本慢15%至23%，这是一个可以接受的范围。但是较小的模型下降会更多。开发人员正在逐步优化这个问题。</p>
<table>
<thead>
<tr>
<th>Precision</th>
<th>Number of parameters</th>
<th>Hardware</th>
<th>Time per token in milliseconds for Batch Size 1</th>
<th>Time per token in milliseconds for Batch Size 8</th>
<th>Time per token in milliseconds for Batch Size 32</th>
</tr>
</thead>
<tbody>
<tr>
<td>bf16</td>
<td>176B</td>
<td>8xA100 80GB</td>
<td>239</td>
<td>32</td>
<td>9.9</td>
</tr>
<tr>
<td>int8</td>
<td>176B</td>
<td>4xA100 80GB</td>
<td>282</td>
<td>37.5</td>
<td>10.2</td>
</tr>
<tr>
<td>bf16</td>
<td>176B</td>
<td>14xA100 40GB</td>
<td>285</td>
<td>36.5</td>
<td>10.4</td>
</tr>
<tr>
<td>int8</td>
<td>176B</td>
<td>5xA100 40GB</td>
<td>367</td>
<td>46.4</td>
<td>oom</td>
</tr>
<tr>
<td>fp16</td>
<td>11B</td>
<td>2xT4 15GB</td>
<td>11.7</td>
<td>1.7</td>
<td>0.5</td>
</tr>
<tr>
<td>int8</td>
<td>11B</td>
<td>1xT4 15GB</td>
<td>43.5</td>
<td>5.3</td>
<td>1.3</td>
</tr>
<tr>
<td>fp32</td>
<td>3B</td>
<td>2xT4 15GB</td>
<td>45</td>
<td>7.2</td>
<td>3.1</td>
</tr>
<tr>
<td>int8</td>
<td>3B</td>
<td>1xT4 15GB</td>
<td>312</td>
<td>39.1</td>
<td>10.2</td>
</tr>
</tbody>
</table>
<h3 id="Transformers集成">Transformers集成</h3>
<p>本文重点描述的模块是Linear8bitLt，你可以直接从bitsandbytes库中引入。其来自于经典的torch.nn模块，并使用下面的代码来轻易的使用和部署。</p>
<p>下面是一个使用bitsandbytes将一个小模型转换为int8类型。</p>
<p>正确的引入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> bitsandbytes <span class="keyword">as</span> bnb</span><br><span class="line"><span class="keyword">from</span> bnb.nn <span class="keyword">import</span> Linear8bitLt</span><br></pre></td></tr></table></figure>
<p>定义一个fp16的模型。注意，你可以将任何精度的checkpoint或模型转换为8位（FP16、BF16或FP32），但目前，模型的输入必须是FP16，我们的Int8模块才能工作。所以我们在这里把我们的模型当作FP16模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先定义一个fp16的模型</span></span><br><span class="line">fp16_model = nn.Sequential(nn.Linear(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                           nn.Linear(<span class="number">64</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure>
<p>假设该模型已经完成训练，保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[... train the model ...]</span><br><span class="line">torch.save(fp16_model.state_dict(), <span class="string">&quot;model.pt&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>现在再定义一个int8模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int8_model = nn.Sequential(</span><br><span class="line">    Linear8bitLt(<span class="number">64</span>, <span class="number">64</span>, has_fp16_weights=<span class="literal">False</span>),</span><br><span class="line">    Linear8bitLt(<span class="number">64</span>, <span class="number">64</span>, has_fp16_weights=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>添加参数has_fp16_weights很重要。默认值为True，其被用于Int8/FP16混合精度训练。然而，这里关注的是推理，所以将其设置为False。</p>
<p>现在将fp16的模型加载至int8模型中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int8_model.load_state_dict(torch.load(<span class="string">&quot;model.pt&quot;</span>))</span><br><span class="line"><span class="comment"># print(int8_model[0].weight)</span></span><br><span class="line">int8_model = int8_model.to(<span class="string">&quot;cuda:0&quot;</span>) <span class="comment"># 执行该代码时会进行量化</span></span><br><span class="line"><span class="comment"># print(int8_model[0].weight)</span></span><br></pre></td></tr></table></figure>
<p>首先我们查看量化之前的模型参数，通过print(int8_model[0].weight)如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int8_model[0].weight</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ 0.0031, -0.0438,  0.0494,  ..., -0.0046, -0.0410,  0.0436],</span><br><span class="line">        [-0.1013,  0.0394,  0.0787,  ...,  0.0986,  0.0595,  0.0162],</span><br><span class="line">        [-0.0859, -0.1227, -0.1209,  ...,  0.1158,  0.0186, -0.0530],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0804,  0.0725,  0.0638,  ..., -0.0487, -0.0524, -0.1076],</span><br><span class="line">        [-0.0200, -0.0406,  0.0663,  ...,  0.0123,  0.0551, -0.0121],</span><br><span class="line">        [-0.0041,  0.0865, -0.0013,  ..., -0.0427, -0.0764,  0.1189]],</span><br><span class="line">       dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<p>接着运行上述第二行代码之后，通过输出print(int8_model[0].weight)可以看到模型被量化为Int8类型:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int8_model[0].weight</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[   3,  -47,   54,  ...,   -5,  -44,   47],</span><br><span class="line">        [-104,   40,   81,  ...,  101,   61,   17],</span><br><span class="line">        [ -89, -127, -125,  ...,  120,   19,  -55],</span><br><span class="line">        ...,</span><br><span class="line">        [  82,   74,   65,  ...,  -49,  -53, -109],</span><br><span class="line">        [ -21,  -42,   68,  ...,   13,   57,  -12],</span><br><span class="line">        [  -4,   88,   -1,  ...,  -43,  -78,  121]],</span><br><span class="line">        device=&#x27;cuda:0&#x27;, dtype=torch.int8, requires_grad=True)</span><br></pre></td></tr></table></figure>
<p>正如我们在前几节解释量化时看到的那样，权重值是 “截断的”。另外，这些值似乎分布在[-127, 127]之间。你可能也想知道如何检索FP16的权重，以便在FP16中执行离群的MatMul？你可以简单地做：</p>
<p>那么怎么还原为FP16权重呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(int8_model[<span class="number">0</span>].weight.CB * int8_model[<span class="number">0</span>].weight.SCB) / <span class="number">127</span></span><br></pre></td></tr></table></figure>
<p>可以得到:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0028, -0.0459,  0.0522,  ..., -0.0049, -0.0428,  0.0462],</span><br><span class="line">        [-0.0960,  0.0391,  0.0782,  ...,  0.0994,  0.0593,  0.0167],</span><br><span class="line">        [-0.0822, -0.1240, -0.1207,  ...,  0.1181,  0.0185, -0.0541],</span><br><span class="line">        ...,</span><br><span class="line">        [ 0.0757,  0.0723,  0.0628,  ..., -0.0482, -0.0516, -0.1072],</span><br><span class="line">        [-0.0194, -0.0410,  0.0657,  ...,  0.0128,  0.0554, -0.0118],</span><br><span class="line">        [-0.0037,  0.0859, -0.0010,  ..., -0.0423, -0.0759,  0.1190]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure>
<p>使用int8模型进行推理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_ = torch.randn((<span class="number">1</span>,<span class="number">64</span>), dtype=torch.float16)</span><br><span class="line">hidden_states = int8_model(input_.to(torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="accelerate">accelerate</h3>
<p>当使用大模型时，acceleate库包含了有用的程序。init_empty_weights方法特别有用，因为任何模型(无论大小)都可以作为上下文管理器使用此方法进行初始化，而无需为模型权重分配任何内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> init_empty_weights</span><br><span class="line">​</span><br><span class="line"><span class="keyword">with</span> init_empty_weights():</span><br><span class="line">    model = nn.Sequential(*[nn.Linear(<span class="number">100000</span>, <span class="number">100000</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>)])</span><br></pre></td></tr></table></figure>
<p>这个初始化的模型会被放置至Pytorch的元设备上，其是一种不用分配存储空间来表示shape和dtype的潜在机制。</p>
<p>起初，该函数在.from_pretrained函数中被调用，并将所有参数重写为torch.nn.Parameter。但是，这不符合我们的需求，因为希望在Linear8bitLt模块中保留Int8Params类。因此我们将</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">module._parameters[name] = nn.Parameter(module._parameters[name].to(torch.device(<span class="string">&quot;meta&quot;</span>)))</span><br></pre></td></tr></table></figure>
<p>修改为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">param_cls = <span class="built_in">type</span>(module._parameters[name])</span><br><span class="line">kwargs = module._parameters[name].__dict__</span><br><span class="line">module._parameters[name] = param_cls(module._parameters[name].to(torch.device(<span class="string">&quot;meta&quot;</span>)), **kwargs)</span><br></pre></td></tr></table></figure>
<p>通过这个修改，我们可以通过自定义函数在没有任何内存消耗的情况下，利用这个上下文管理器将所有的nn.Linear替换为bnb.nn.Linear8bitLt。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">replace_8bit_linear</span>(<span class="params">model, threshold=<span class="number">6.0</span>, module_to_not_convert=<span class="string">&quot;lm_head&quot;</span></span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_children():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(<span class="built_in">list</span>(module.children())) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 递归</span></span><br><span class="line">            replace_8bit_linear(module, threshold, module_to_not_convert)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear) <span class="keyword">and</span> name != module_to_not_convert:</span><br><span class="line">            <span class="keyword">with</span> init_empty_weights():</span><br><span class="line">                model._modules[name] = bnb.nn.Linear8bitLt(</span><br><span class="line">                    module.in_features,</span><br><span class="line">                    module.out_features,</span><br><span class="line">                    module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>,</span><br><span class="line">                    has_fp16_weights=<span class="literal">False</span>,</span><br><span class="line">                    threshold=threshold,</span><br><span class="line">                )</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>该函数会递归的将元设备上的所有nn.Linear替换为Linear8bitLt模块。属性has_fp16_weights必须被设置为False，以便加载int8权重和量化信息。</p>
<h3 id="如何在transformers中使用">如何在transformers中使用</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">inference</span>(<span class="params">payload, model, tokenizer</span>):</span><br><span class="line">    input_ids = tokenizer(payload, return_tensors=<span class="string">&quot;pt&quot;</span>).input_ids.to(model.device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;输入:\n    <span class="subst">&#123;payload&#125;</span>&quot;</span>)</span><br><span class="line">    logits = model.generate(input_ids, num_beams=<span class="number">1</span>, max_new_tokens=<span class="number">128</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;生成:\n    <span class="subst">&#123;tokenizer.decode(logits[<span class="number">0</span>].tolist()[<span class="built_in">len</span>(input_ids[<span class="number">0</span>]):])&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;bigscience/bloomz-7b1-mt&quot;</span></span><br><span class="line">payload = <span class="string">&quot;一个传奇的开端，一个不灭的神话，这不仅仅是一部电影，而是作为一个走进新时代的标签，永远彪炳史册。你认为这句话的立场是赞扬、中立还是批评？&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="string">&quot;auto&quot;</span>, load_in_8bit=<span class="literal">True</span>)</span><br><span class="line">model_native = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="string">&quot;auto&quot;</span>)</span><br><span class="line"><span class="comment"># 比较推理结果</span></span><br><span class="line">inference(payload, model_8bit, tokenizer)</span><br><span class="line">inference(payload, model_native, tokenizer)</span><br><span class="line"><span class="comment"># 计算显存节约程度</span></span><br><span class="line">mem_fp16 = model_native.get_memory_footprint()</span><br><span class="line">mem_int8 = model_8bit.get_memory_footprint()</span><br><span class="line"><span class="built_in">print</span>(mem_fp16/mem_int8)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/blog/hf-bitsandbytes-integration">原文地址</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Weitang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lonepatient.top/2023/03/02/8-bit_Matrix_Multiplication_transformers_Hugging_Face_Transformers.html">http://lonepatient.top/2023/03/02/8-bit_Matrix_Multiplication_transformers_Hugging_Face_Transformers.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lonepatient.top" target="_blank">闲记算法</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/">预训练</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大型语言模型</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Zero-Shot/">Zero-Shot</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/%E9%87%8F%E5%8C%96/">量化</a><a class="post-meta__tags" href="/tags/8-bit/">8-bit</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230324140620.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/03/08/Large_Language_Models_Are_Human-Level_Prompt_Engineers.html"><img class="prev-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230312165646.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Large Language Models Are Human-Level Prompt Engineers</div></div></a></div><div class="next-post pull-right"><a href="/2023/03/01/lora.html"><img class="next-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230318170051.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LoRA：Low-Rank Adaptation of Large Language Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/04/18/A_Survey_of_Large_Language_Models.html" title="A Survey of Large Language Models"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230419195449.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-18</div><div class="title">A Survey of Large Language Models</div></div></a></div><div><a href="/2023/03/23/Big_Language_Model_BLOOM_Reasoning_Tool_Test.html" title="大语言模型BLOOM推理工具测试"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230323163716.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-23</div><div class="title">大语言模型BLOOM推理工具测试</div></div></a></div><div><a href="/2023/03/10/ChatGPT_Research_Report.html" title="ChatGPT 调研报告"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230310095306.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-10</div><div class="title">ChatGPT 调研报告</div></div></a></div><div><a href="/2023/04/12/Emergent_capacity_of_LLM_phenomena_explanations.html" title="大语言模型的涌现能力：现象与解释"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230413215813.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-12</div><div class="title">大语言模型的涌现能力：现象与解释</div></div></a></div><div><a href="/2022/10/30/Finetuned_Language_Models_are_Zero-Shot_Learners .html" title="Finetuned Language Models are Zero-Shot Learners"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230223213000.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-30</div><div class="title">Finetuned Language Models are Zero-Shot Learners</div></div></a></div><div><a href="/2023/06/26/Instruction-Tuning.html" title="Instruction Tuning 阶段性总结"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230627120653.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-26</div><div class="title">Instruction Tuning 阶段性总结</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Weitang Liu</div><div class="author-info__description">一个致力于记录技术的博客</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lonePatient"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lonePatient" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuweitangmath@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/277974397" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有任何问题可通过留言板或者微信公众号给我留言，谢谢！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">常用类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">模型量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#int8"><span class="toc-number">3.</span> <span class="toc-text">int8</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E5%80%BC%E7%89%B9%E5%BE%81"><span class="toc-number">4.</span> <span class="toc-text">异常值特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MatMul%E5%86%85%E9%83%A8"><span class="toc-number">5.</span> <span class="toc-text">MatMul内部</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%B6%E9%80%80%E5%8C%96%E6%84%8F%E5%91%B3%E7%9D%80%E4%BB%80%E4%B9%88"><span class="toc-number">6.</span> <span class="toc-text">零退化意味着什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AF%94native%E6%A8%A1%E5%9E%8B%E6%9B%B4%E5%BF%AB%EF%BC%9F"><span class="toc-number">7.</span> <span class="toc-text">比native模型更快？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformers%E9%9B%86%E6%88%90"><span class="toc-number">8.</span> <span class="toc-text">Transformers集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#accelerate"><span class="toc-number">9.</span> <span class="toc-text">accelerate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%9C%A8transformers%E4%B8%AD%E4%BD%BF%E7%94%A8"><span class="toc-number">10.</span> <span class="toc-text">如何在transformers中使用</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-31"/></a><div class="content"><a class="title" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31">Arxiv今日论文 | 2025-10-31</a><time datetime="2025-10-31T10:30:00.000Z" title="发表于 2025-10-31 10:30:00">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-30"/></a><div class="content"><a class="title" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30">Arxiv今日论文 | 2025-10-30</a><time datetime="2025-10-30T10:30:00.000Z" title="发表于 2025-10-30 10:30:00">2025-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-29"/></a><div class="content"><a class="title" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29">Arxiv今日论文 | 2025-10-29</a><time datetime="2025-10-29T10:30:00.000Z" title="发表于 2025-10-29 10:30:00">2025-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-28"/></a><div class="content"><a class="title" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28">Arxiv今日论文 | 2025-10-28</a><time datetime="2025-10-28T10:30:00.000Z" title="发表于 2025-10-28 10:30:00">2025-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-27"/></a><div class="content"><a class="title" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27">Arxiv今日论文 | 2025-10-27</a><time datetime="2025-10-27T10:30:00.000Z" title="发表于 2025-10-27 10:30:00">2025-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Weitang Liu</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'https://twikoo.lonepatient.top/',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.lonepatient.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo@1.4.11/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (95)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/计算机视觉/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 计算机视觉 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/知识图谱/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 知识图谱 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 深度学习 (135)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://lonepatient.top/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230219144729.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-18</span><a class="blog-slider__title" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">通向AGI之路：大型语言模型（LLM）技术精要</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/07/12/gaiic_2022_ner_top10.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20220712181905.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-07-12</span><a class="blog-slider__title" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">GAIIC2022商品标题识别二等奖获奖解决思路</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230807190424.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-01-20</span><a class="blog-slider__title" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">2025-03|高质量中文预训练模型集合</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>