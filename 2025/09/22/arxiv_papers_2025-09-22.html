<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv今日论文 | 2025-09-22 | 闲记算法</title><meta name="keywords" content="Arxiv"><meta name="author" content="Weitang Liu"><meta name="copyright" content="Weitang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要内容为 2025-09-22 从Arxiv.org论文网站获取的最新论文列表，自动更新，按照NLP、CV、ML、AI、IR五个大方向区分，若需要邮件定时接收，请在评论区留下你的邮箱号。 说明：每日论文数据从Arxiv.org获取，每天早上12:00左右定时自动更新。  友情提示: 如何您需要邮箱接收每日论文数据，请在评论处留下你的邮箱。  目录  概览 自然语言处理CL 人工智能AI">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv今日论文 | 2025-09-22">
<meta property="og:url" content="http://lonepatient.top/2025/09/22/arxiv_papers_2025-09-22.html">
<meta property="og:site_name" content="闲记算法">
<meta property="og:description" content="本篇博文主要内容为 2025-09-22 从Arxiv.org论文网站获取的最新论文列表，自动更新，按照NLP、CV、ML、AI、IR五个大方向区分，若需要邮件定时接收，请在评论区留下你的邮箱号。 说明：每日论文数据从Arxiv.org获取，每天早上12:00左右定时自动更新。  友情提示: 如何您需要邮箱接收每日论文数据，请在评论处留下你的邮箱。  目录  概览 自然语言处理CL 人工智能AI">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png">
<meta property="article:published_time" content="2025-09-22T10:30:00.000Z">
<meta property="article:modified_time" content="2025-10-31T07:26:14.672Z">
<meta property="article:author" content="Weitang Liu">
<meta property="article:tag" content="Arxiv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://lonepatient.top/2025/09/22/arxiv_papers_2025-09-22"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//s4.cnzz.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" data-pjax="data-pjax" src="https://s4.cnzz.com/z_stat.php?id=1273275888&amp;web_id=1273275888"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-31 07:26:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/backgroud.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><script src="https://at.alicdn.com/t/c/font_3570527_dthoqrrv2tv.css"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JwRQtLKZggvJH4sJ",ck:"JwRQtLKZggvJH4sJ"})</script><link rel="stylesheet" href="/css/universe.css"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3536467946304280" crossorigin="anonymous"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">闲记算法</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv今日论文 | 2025-09-22<a class="post-edit-link" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts/arxiv_papers_2025-09-22.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-22T10:30:00.000Z" title="发表于 2025-09-22 10:30:00">2025-09-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T07:26:14.672Z" title="更新于 2025-10-31 07:26:14">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/">学术会议</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/Arxiv/">Arxiv</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">156k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>743分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/09/22/arxiv_papers_2025-09-22.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要内容为 2025-09-22 从<a target="_blank" rel="noopener" href="https://arxiv.org">Arxiv.org</a>论文网站获取的最新论文列表，自动更新，按照NLP、CV、ML、AI、IR五个大方向区分，若需要邮件定时接收，请在评论区留下你的邮箱号。</p>
<div class="note info flat"><p>说明：每日论文数据从<a target="_blank" rel="noopener" href="https://arxiv.org">Arxiv.org</a>获取，每天早上12:00左右定时自动更新。</p>
</div>
<div class="note warning flat"><p>友情提示: 如何您需要<strong>邮箱</strong>接收每日论文数据，请在评论处留下你的邮箱。</p>
</div>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#%E6%A6%82%E8%A7%88">概览</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">自然语言处理CL</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">人工智能AI</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习LG</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">计算机视觉CV</a></li>
<li><a href="#%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2">信息检索IR</a></li>
</ul>
<h3 id="概览-2025-09-22">概览 (2025-09-22)</h3>
<p>今日共更新<strong>543</strong>篇论文,其中:</p>
<ul>
<li><strong>自然语言处理</strong>共<strong>76</strong>篇(Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>))</li>
<li><strong>人工智能</strong>共<strong>134</strong>篇(Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>))</li>
<li><strong>计算机视觉</strong>共<strong>133</strong>篇(Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>))</li>
<li><strong>机器学习</strong>共<strong>155</strong>篇(Machine Learning (cs.LG))</li>
</ul>
<h3 id="自然语言处理">自然语言处理</h3>
<div class="note orange no-icon flat"><p>[NLP-0] RPG: A Repository <mark class="hl-label green">Planning</mark>  Graph for Unified and Scalable Codebase Generation</p>
<p>【速读】： 该论文旨在解决从零开始生成完整软件仓库（repository）这一根本性挑战，该任务要求在提案层（proposal-level）与实现层（implementation-level）之间进行连贯且可靠的规划，而自然语言因模糊性和冗余性难以准确表达复杂的软件结构。解决方案的关键在于提出一种持久化的表示方法——仓库规划图（Repository Planning Graph, RPG），它通过统一编码能力、文件结构、数据流和函数关系，在一个图结构中整合了高层规划与底层实现，从而以显式蓝图替代模糊的自然语言描述，支持长周期规划和可扩展的仓库生成。基于RPG，作者进一步开发了ZeroRepo框架，采用三阶段流程：提案层规划与实现层细化构建图结构，随后基于图引导代码生成并结合测试验证，显著提升了生成仓库的规模与功能性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16198">https://arxiv.org/abs/2509.16198</a><br>
<strong>作者</strong>: Jane Luo,Xin Zhang,Steven Liu,Jie Wu,Yiming Huang,Yangyu Huang,Chengyu Yin,Ying Xin,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qi Chen,Scarlett Li,Mao Yang<br>
<strong>机构</strong>: Microsoft(微软); Tsinghua University (清华大学); University of California, San Diego (加州大学圣地亚哥分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9 \times  the strongest baseline (Claude Code) and about 64 \times  other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-1] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer</p>
<p>【速读】： 该论文旨在解决统一多模态大语言模型（Unified Multimodal Large Language Models）在图像理解与图像生成能力之间存在的性能权衡问题。现有开源模型通常难以同时优化这两项任务，导致性能瓶颈。其解决方案的关键在于提出一种简洁且可扩展的框架Manzano，通过耦合混合图像分词器（hybrid image tokenizer）与精心设计的训练策略，实现双路径协同：一个共享的视觉编码器分别驱动两个轻量级适配器，分别生成用于图像到文本理解的连续嵌入和用于文本到图像生成的离散标记，二者共用同一语义空间；随后由统一的自回归语言模型预测文本和图像标记，并辅以扩散解码器将图像标记转换为像素。该架构结合统一训练流程，在不显著增加任务冲突的前提下，实现了理解与生成能力的规模化联合学习，从而在多项指标上达到统一模型的最先进水平，且在文本密集型评测中媲美专用模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16197">https://arxiv.org/abs/2509.16197</a><br>
<strong>作者</strong>: Yanghao Li,Rui Qian,Bowen Pan,Haotian Zhang,Haoshuo Huang,Bowen Zhang,Jialing Tong,Haoxuan You,Xianzhi Du,Zhe Gan,Hyunjik Kim,Chao Jia,Zhenbang Wang,Yinfei Yang,Mingfei Gao,Zi-Yi Dou,Wenze Hu,Chang Gao,Dongxu Li,Philipp Dufter,Zirui Wang,Guoli Yin,Zhengdong Zhang,Chen Chen,Yang Zhao,Ruoming Pang,Zhifeng Chen<br>
<strong>机构</strong>: Apple(苹果公司)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Unified multimodal Large Language Models (LLMs) that can both understand and generate visual content hold immense potential. However, existing open-source models often suffer from a performance trade-off between these capabilities. We present Manzano, a simple and scalable unified framework that substantially reduces this tension by coupling a hybrid image tokenizer with a well-curated training recipe. A single shared vision encoder feeds two lightweight adapters that produce continuous embeddings for image-to-text understanding and discrete tokens for text-to-image generation within a common semantic space. A unified autoregressive LLM predicts high-level semantics in the form of text and image tokens, with an auxiliary diffusion decoder subsequently translating the image tokens into pixels. The architecture, together with a unified training recipe over understanding and generation data, enables scalable joint learning of both capabilities. Manzano achieves state-of-the-art results among unified models, and is competitive with specialist models, particularly on text-rich evaluation. Our studies show minimal task conflicts and consistent gains from scaling model size, validating our design choice of a hybrid tokenizer.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-2] Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences</p>
<p>【速读】： 该论文试图解决当前机器学习系统在泛化能力上的不足问题，特别是其在面对新任务时表现出的数据效率低下现象。研究表明，这一问题的部分根源在于机器学习系统缺乏类人认知中的潜在学习（latent learning）能力——即无法习得与当前任务看似无关但未来可能有用的信息。为改善这一局限，论文提出以认知科学中关于情景记忆（episodic memory）的洞见作为解决方案的核心思路，强调通过引入一种具有“oracle检索机制”的系统，使模型能够灵活调用过往的学习经验来提升跨任务的泛化性能。关键创新点在于识别出“示例内上下文学习”（within-example in-context learning）的重要性，这是实现从检索到的知识中有效迁移和使用的关键机制。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16189">https://arxiv.org/abs/2509.16189</a><br>
<strong>作者</strong>: Andrew Kyle Lampinen,Martin Engelcke,Yuxuan Li,Arslan Chaudhry,James L. McClelland<br>
<strong>机构</strong>: Google DeepMind(谷歌深度思维)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:When do machine learning systems fail to generalize, and what mechanisms could improve their generalization? Here, we draw inspiration from cognitive science to argue that one weakness of machine learning systems is their failure to exhibit latent learning – learning information that is not relevant to the task at hand, but that might be useful in a future task. We show how this perspective links failures ranging from the reversal curse in language modeling to new findings on agent-based navigation. We then highlight how cognitive science points to episodic memory as a potential part of the solution to these issues. Correspondingly, we show that a system with an oracle retrieval mechanism can use learning experiences more flexibly to generalize better across many of these challenges. We also identify some of the essential components for effectively using retrieval, including the importance of within-example in-context learning for acquiring the ability to use information across retrieved examples. In summary, our results illustrate one possible contributor to the relative data inefficiency of current machine learning systems compared to natural intelligence, and help to understand how retrieval methods can complement parametric learning to improve generalization.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-3] CultureScope: A Dimensional Lens for Probing Cultural Understanding in <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】： 该论文旨在解决当前大语言模型（Large Language Models, LLMs）在跨文化环境部署中缺乏系统性、可扩展且理论驱动的文化理解能力评估框架的问题。现有基准测试普遍存在覆盖面不足、难以适应不同文化语境以及依赖专家手动标注导致的可扩展性差等缺陷。解决方案的关键在于提出名为CultureScope的综合性评估框架，其核心创新是基于文化冰山理论（Cultural Iceberg Theory）设计了一个包含三层结构和140个维度的文化知识分类体系，该体系为自动化构建特定文化的知识库及对应评测数据集提供了理论指导，从而实现对LLMs文化理解能力的高效、可迁移评估。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16188">https://arxiv.org/abs/2509.16188</a><br>
<strong>作者</strong>: Jinghao Zhang,Sihang Jiang,Shiwei Guo,Shisong Chen,Yanghua Xiao,Hongwei Feng,Jiaqing Liang,Minggui HE,Shimin Tao,Hongxia Ma<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As large language models (LLMs) are increasingly deployed in diverse cultural environments, evaluating their cultural understanding capability has become essential for ensuring trustworthy and culturally aligned applications. However, most existing benchmarks lack comprehensiveness and are challenging to scale and adapt across different cultural contexts, because their frameworks often lack guidance from well-established cultural theories and tend to rely on expert-driven manual annotations. To address these issues, we propose CultureScope, the most comprehensive evaluation framework to date for assessing cultural understanding in LLMs. Inspired by the cultural iceberg theory, we design a novel dimensional schema for cultural knowledge classification, comprising 3 layers and 140 dimensions, which guides the automated construction of culture-specific knowledge bases and corresponding evaluation datasets for any given languages and cultures. Experimental results demonstrate that our method can effectively evaluate cultural understanding. They also reveal that existing large language models lack comprehensive cultural competence, and merely incorporating multilingual data does not necessarily enhance cultural understanding. All code and data files are available at this https URL<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-4] Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks</p>
<p>【速读】： 该论文旨在解决视觉语言模型（Vision Language Models, VLMs）在面对对抗攻击时鲁棒性不足的问题，而现有防御方法通常需要昂贵的重新训练或复杂的架构修改。其解决方案的关键在于提出一种轻量级、无需重新训练的防御机制，通过张量分解（tensor decomposition）对预训练VLM中的视觉编码器表示进行重构，在保留语义信息的同时滤除对抗噪声。实验表明，该方法在CLIP模型上显著提升了鲁棒性，例如在Flickr30K数据集上将Recall@1准确率从7.5%提升至19.8%，且最优性能由低秩（8-32）和低残差强度（α=0.1-0.2）的Tensor Train分解实现，是一种可直接部署、计算开销极小的实用化防御方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16163">https://arxiv.org/abs/2509.16163</a><br>
<strong>作者</strong>: Het Patel,Muzammil Allie,Qian Zhang,Jia Chen,Evangelos E. Papalexakis<br>
<strong>机构</strong>: University of California, Riverside (加州大学河滨分校)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  To be presented as a poster at the Workshop on Safe and Trustworthy Multimodal AI Systems (SafeMM-AI), 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Vision language models (VLMs) excel in multimodal understanding but are prone to adversarial attacks. Existing defenses often demand costly retraining or significant architecture changes. We introduce a lightweight defense using tensor decomposition suitable for any pre-trained VLM, requiring no retraining. By decomposing and reconstructing vision encoder representations, it filters adversarial noise while preserving meaning. Experiments with CLIP on COCO and Flickr30K show improved robustness. On Flickr30K, it restores 12.3% performance lost to attacks, raising Recall@1 accuracy from 7.5% to 19.8%. On COCO, it recovers 8.1% performance, improving accuracy from 3.8% to 11.9%. Analysis shows Tensor Train decomposition with low rank (8-32) and low residual strength ( \alpha=0.1-0.2 ) is optimal. This method is a practical, plug-and-play solution with minimal overhead for existing VLMs.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-5] Code<mark class="hl-label green">RAG</mark> : Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前仓库级代码补全（repository-level code completion）方法中存在的三大问题：不恰当的查询构建、单路径代码检索以及代码检索器与代码大语言模型（code LLM）之间的对齐偏差。为应对这些问题，作者提出了一种名为CodeRAG的框架，其核心创新在于三个关键组件：基于对数概率引导的查询构建策略、多路径代码检索机制，以及偏好对齐的BestFit重排序算法。该方案通过增强检索知识的相关性与必要性，显著提升了仓库级代码补全的效果，在ReccEval和CCEval基准测试中均优于现有最先进方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16112">https://arxiv.org/abs/2509.16112</a><br>
<strong>作者</strong>: Sheng Zhang,Yifan Ding,Shuquan Lian,Shun Song,Hui Li<br>
<strong>机构</strong>: Xiamen University (厦门大学); Ant Group (蚂蚁集团)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>)<br>
<strong>备注</strong>:  EMNLP 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Repository-level code completion automatically predicts the unfinished code based on the broader information from the repository. Recent strides in Code Large Language Models (code LLMs) have spurred the development of repository-level code completion methods, yielding promising results. Nevertheless, they suffer from issues such as inappropriate query construction, single-path code retrieval, and misalignment between code retriever and code LLM. To address these problems, we introduce CodeRAG, a framework tailored to identify relevant and necessary knowledge for retrieval-augmented repository-level code completion. Its core components include log probability guided query construction, multi-path code retrieval, and preference-aligned BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval demonstrate that CodeRAG significantly and consistently outperforms state-of-the-art methods. The implementation of CodeRAG is available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-6] It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在多轮对话中处理指代歧义（referential ambiguity）的能力不足问题，特别是其如何利用常识知识进行语境推理以及在简化语言请求下行为的变化。研究发现，当前LLMs倾向于固守单一解释或枚举所有可能参考对象，而非采取更灵活的策略如模糊表达或主动澄清；且简化提示显著削弱了其常识推理与多样响应能力。解决方案的关键在于通过直接偏好优化（Direct Preference Optimization, DPO）对模型（如Llama-3.1-8B）进行微调，可显著提升其在各类请求下的歧义解析性能，表明高级微调技术是增强LLMs鲁棒性与适应不同沟通风格的核心路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16107">https://arxiv.org/abs/2509.16107</a><br>
<strong>作者</strong>: Lukas Ellinger,Georg Groh<br>
<strong>机构</strong>: Technical University of Munich (慕尼黑工业大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted by UncertaiNLP workshop @ EMNLP 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Ambiguous words or underspecified references require interlocutors to resolve them, often by relying on shared context and commonsense knowledge. Therefore, we systematically investigate whether Large Language Models (LLMs) can leverage commonsense to resolve referential ambiguity in multi-turn conversations and analyze their behavior when ambiguity persists. Further, we study how requests for simplified language affect this capacity. Using a novel multilingual evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that current LLMs struggle to resolve ambiguity effectively: they tend to commit to a single interpretation or cover all possible references, rather than hedging or seeking clarification. This limitation becomes more pronounced under simplification prompts, which drastically reduce the use of commonsense reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct Preference Optimization substantially improves ambiguity resolution across all request types. These results underscore the need for advanced fine-tuning to improve LLMs’ handling of ambiguity and to ensure robust performance across diverse communication styles.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-7] DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning</p>
<p>【速读】： 该论文旨在解决大规模Mixture-of-Experts (MoE) 模型在扩展过程中面临的内存与存储挑战，尤其是现有均匀稀疏性剪枝方法因未考虑不同MoE层间专家冗余差异而导致性能下降的问题。其解决方案的关键在于提出一种非均匀剪枝策略——可微专家剪枝（Differentiable Expert Pruning, DiEP），通过在层级别自适应调整剪枝率并联合学习层间重要性，有效捕捉各MoE层间的冗余差异；同时将全局离散搜索空间转化为连续空间，从而支持基于梯度的自适应剪枝，克服了指数级增长的非均匀专家组合带来的优化难题。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16105">https://arxiv.org/abs/2509.16105</a><br>
<strong>作者</strong>: Sikai Bai,Haoxi Li,Jie Zhang,Zicong Hong,Song Guo<br>
<strong>机构</strong>: HKUST (香港科技大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  18 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Despite the significant breakthrough of Mixture-of-Experts (MoE), the increasing scale of these MoE models presents huge memory and storage challenges. Existing MoE pruning methods, which involve reducing parameter size with a uniform sparsity across all layers, often lead to suboptimal outcomes and performance degradation due to varying expert redundancy in different MoE layers. To address this, we propose a non-uniform pruning strategy, dubbed \textbfDifferentiable \textbfExpert \textbfPruning (\textbfDiEP), which adaptively adjusts pruning rates at the layer level while jointly learning inter-layer importance, effectively capturing the varying redundancy across different MoE layers. By transforming the global discrete search space into a continuous one, our method handles exponentially growing non-uniform expert combinations, enabling adaptive gradient-based pruning. Extensive experiments on five advanced MoE models demonstrate the efficacy of our method across various NLP tasks. Notably, \textbfDiEP retains around 92% of original performance on Mixtral 8 \times 7B with only half the experts, outperforming other pruning methods by up to 7.1% on the challenging MMLU dataset.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-8] Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of <mark class="hl-label green">LLM</mark>  Responses</p>
<p>【速读】： 该论文旨在解决高风险领域（如法律与医学）中对长文本回答进行有效评估的难题，传统指标（如BLEU和ROUGE）无法衡量语义正确性，而现有基于大语言模型（Large Language Models, LLMs）的评估方法常将答案质量简化为单一分数，缺乏细粒度区分能力。其解决方案的关键在于提出DeCE（Decomposed LLM Evaluation）框架，该框架通过自动从标准答案要求中提取实例特定的标准，将评估分解为两个独立维度：精度（precision，即事实准确性与相关性）和召回率（recall，即所需概念的覆盖程度），从而实现可解释且可操作的评估。DeCE具有模型无关性和领域通用性，无需预定义分类体系或人工制定评分细则，在真实法律问答任务中显著优于传统指标和现有方法（相关系数r=0.78），并揭示了不同模型在精度与召回之间的权衡关系。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16093">https://arxiv.org/abs/2509.16093</a><br>
<strong>作者</strong>: Fangyi Yu,Nabeel Seedat,Dasha Herrmannova,Frank Schilder,Jonathan Richard Schwarz<br>
<strong>机构</strong>: Thomson Reuters Foundational Research (汤森路透基础研究); Thomson Reuters Labs (汤森路透实验室)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Evaluating long-form answers in high-stakes domains such as law or medicine remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to capture semantic correctness, and current LLM-based evaluators often reduce nuanced aspects of answer quality into a single undifferentiated score. We introduce DeCE, a decomposed LLM evaluation framework that separates precision (factual accuracy and relevance) and recall (coverage of required concepts), using instance-specific criteria automatically extracted from gold answer requirements. DeCE is model-agnostic and domain-general, requiring no predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate different LLMs on a real-world legal QA task involving multi-jurisdictional reasoning and citation grounding. DeCE achieves substantially stronger correlation with expert judgments ( r=0.78 ), compared to traditional metrics ( r=0.12 ), pointwise LLM scoring ( r=0.35 ), and modern multidimensional evaluators ( r=0.48 ). It also reveals interpretable trade-offs: generalist models favor recall, while specialized models favor precision. Importantly, only 11.95% of LLM-generated criteria required expert revision, underscoring DeCE’s scalability. DeCE offers an interpretable and actionable LLM evaluation framework in expert domains.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-9] SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection <mark class="hl-label red">EMNLP’25</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在安全对齐训练后仍易受越狱攻击（jailbreak attacks）的问题，即恶意用户通过特定输入诱导模型生成本应被拒绝的有害输出。解决方案的关键在于发现安全机制主要存在于模型的中后层，并提出一种白盒越狱方法 SABER（Safety Alignment Bypass via Extra Residuals），该方法通过在两个中间层之间引入残差连接（residual connection），绕过安全对齐机制，从而显著提升攻击成功率——在 HarmBench 测试集上相比最优基线提升 51%，同时对困惑度（perplexity）影响极小，保持了模型输出的自然性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16060">https://arxiv.org/abs/2509.16060</a><br>
<strong>作者</strong>: Maithili Joshi,Palash Nandi,Tanmoy Chakraborty<br>
<strong>机构</strong>: Indian Institute of Technology Delhi (印度理工学院德里分校)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted in EMNLP’25 Main</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) with safe-alignment training are powerful instruments with robust language comprehension capabilities. These models typically undergo meticulous alignment procedures involving human feedback to ensure the acceptance of safe inputs while rejecting harmful or unsafe ones. However, despite their massive scale and alignment efforts, LLMs remain vulnerable to jailbreak attacks, where malicious users manipulate the model to produce harmful outputs that it was explicitly trained to avoid. In this study, we find that the safety mechanisms in LLMs are predominantly embedded in the middle-to-late layers. Building on this insight, we introduce a novel white-box jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which connects two intermediate layers  s  and  e  such that  s  e , through a residual connection. Our approach achieves a 51% improvement over the best-performing baseline on the HarmBench test set. Furthermore, SABER induces only a marginal shift in perplexity when evaluated on the HarmBench validation set. The source code is publicly available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-10] hink Verbalize then Speak: Bridging Complex Thoughts and Comprehensible Speech <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在语音对话系统中直接应用时，因文本输出与口语表达之间存在不匹配而导致性能下降的问题。现有方法虽尝试将LLMs调整为生成更适合语音的输出，但其对推理能力的影响尚未被充分研究。论文提出“Think-Verbalize-Speak”框架，其核心创新在于将推理过程与口语化表达解耦，通过引入一个中间步骤——“verbalizing”（即把思考内容转化为自然、适合语音输出的文本），从而保留LLMs完整的推理能力。关键解决方案是设计了一种低延迟的verbalizer（ReVerT），基于增量式和异步摘要机制实现高效转换，实验表明该方法在显著提升语音自然度和简洁性的同时，对推理性能影响最小。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16028">https://arxiv.org/abs/2509.16028</a><br>
<strong>作者</strong>: Sang Hoon Woo,Sehun Lee,Kang-wook Kim,Gunhee Kim<br>
<strong>机构</strong>: Seoul National University (首尔国立大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  EMNLP 2025 Main. Project page: <a target="_blank" rel="noopener" href="https://yhytoto12.github.io/TVS-ReVerT">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Spoken dialogue systems increasingly employ large language models (LLMs) to leverage their advanced reasoning capabilities. However, direct application of LLMs in spoken communication often yield suboptimal results due to mismatches between optimal textual and verbal delivery. While existing approaches adapt LLMs to produce speech-friendly outputs, their impact on reasoning performance remains underexplored. In this work, we propose Think-Verbalize-Speak, a framework that decouples reasoning from spoken delivery to preserve the full reasoning capacity of LLMs. Central to our method is verbalizing, an intermediate step that translates thoughts into natural, speech-ready text. We also introduce ReVerT, a latency-efficient verbalizer based on incremental and asynchronous summarization. Experiments across multiple benchmarks show that our method enhances speech naturalness and conciseness with minimal impact on reasoning. The project page with the dataset and the source code is available at this https URL<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-11] Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning</p>
<p>【速读】： 该论文旨在解决现有口语评估（Spoken Language Assessment, SLA）系统中普遍存在的两个问题：一是级联式流水线方法易产生误差传播，二是端到端模型通常仅处理短音频窗口，难以捕捉话语层面（discourse-level）的证据。解决方案的关键在于提出一种新型多模态基础模型方法，通过耦合多目标学习与冻结的Whisper自动语音识别（ASR）模型作为声学先验，实现对整个口语会话的单次处理，从而在无需手工特征的情况下联合学习整体性（holistic）和特质级（trait-level）的SLA目标，显著提升评估准确性与跨参与者泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16025">https://arxiv.org/abs/2509.16025</a><br>
<strong>作者</strong>: Hong-Yun Lin,Jhen-Ke Lin,Chung-Chun Wang,Hao-Chien Lu,Berlin Chen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Spoken Language Assessment (SLA) estimates a learner’s oral proficiency from spontaneous speech. The growing population of L2 English speakers has intensified the demand for reliable SLA, a critical component of Computer Assisted Language Learning (CALL). Existing efforts often rely on cascaded pipelines, which are prone to error propagation, or end-to-end models that often operate on a short audio window, which might miss discourse-level evidence. This paper introduces a novel multimodal foundation model approach that performs session-level evaluation in a single pass. Our approach couples multi-target learning with a frozen, Whisper ASR model-based speech prior for acoustic-aware calibration, allowing for jointly learning holistic and trait-level objectives of SLA without resorting to handcrafted features. By coherently processing the entire response session of an L2 speaker, the model excels at predicting holistic oral proficiency. Experiments conducted on the Speak  Improve benchmark demonstrate that our proposed approach outperforms the previous state-of-the-art cascaded system and exhibits robust cross-part generalization, producing a compact deployable grader that is tailored for CALL applications.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-12] EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决现有数字心理健康工具在应对日常情绪挑战时缺乏个性化和动态适应性的问题，特别是针对全球超过15亿人面临的睡前焦虑等复杂情绪状态，传统“一刀切”式干预手段效果有限。其解决方案的关键在于构建一个端到端的个性化支持系统EmoHeal，该系统通过三个阶段实现精准情绪引导：首先利用微调后的XLM-RoBERTa模型从用户文本中识别27种细粒度情绪（fine-grained emotions），并基于音乐治疗原理的知识图谱（GEMS, iso-principle）将情绪映射至音乐参数；其次采用CLAMP3模型检索音视频内容，遵循“匹配-引导-目标”（match-guide-target）策略，逐步引导用户从当前情绪状态过渡至更平静的状态；实证研究表明，该方法显著改善用户情绪（M=4.12, p&lt;0.001），且感知情绪识别准确度高（M=4.05, p&lt;0.001），并验证了情绪识别精度与治疗效果间的强相关性（r=0.72, p&lt;0.001），从而证明了理论驱动、情绪感知型数字健康工具的可行性与可扩展性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15986">https://arxiv.org/abs/2509.15986</a><br>
<strong>作者</strong>: Xinchen Wan,Jinhua Liang,Huan Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Human-Computer Interaction (cs.HC); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  5 pages, 5 figures. Submitted to the 2026 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Existing digital mental wellness tools often overlook the nuanced emotional states underlying everyday challenges. For example, pre-sleep anxiety affects more than 1.5 billion people worldwide, yet current approaches remain largely static and “one-size-fits-all”, failing to adapt to individual needs. In this work, we present EmoHeal, an end-to-end system that delivers personalized, three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical parameters via a knowledge graph grounded in music therapy principles (GEMS, iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to guide users from their current state toward a calmer one (“match-guide-target”). A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement (M=4.12, p0.001) and high perceived emotion recognition accuracy (M=4.05, p0.001). A strong correlation between perceived accuracy and therapeutic outcome (r=0.72, p0.001) validates our fine-grained approach. These findings establish the viability of theory-driven, emotion-aware digital wellness tools and provides a scalable AI blueprint for operationalizing music therapy principles.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-13] BEFT: Bias-Efficient Fine-Tuning of Language Models</p>
<p>【速读】： 该论文旨在解决参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）中一个关键问题：在仅微调偏置项（bias terms）的情况下，如何选择最优的偏置项（如查询、键或值投影中的偏置）以提升下游任务性能。现有方法（如基于偏置变化幅度或经验Fisher信息的方法）对偏置项的选择缺乏有效指导。本文提出了一种新的偏置项选择策略，构成了其偏置高效微调（Bias-Efficient Fine-Tuning, BEFT）的基础，核心在于通过系统性评估不同偏置项对模型性能的影响，从而实现更精准、高效的微调决策。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15974">https://arxiv.org/abs/2509.15974</a><br>
<strong>作者</strong>: Baichuan Huang,Ananth Balashankar,Amir Aminifar<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Fine-tuning all-bias-terms stands out among various parameter-efficient fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and competitive performance, especially in low-data regimes. Bias-only fine-tuning has the potential for unprecedented parameter efficiency. However, the link between fine-tuning different bias terms (i.e., bias terms in the query, key, or value projections) and downstream performance remains unclear. The existing approaches, e.g., based on the magnitude of bias change or empirical Fisher information, provide limited guidance for selecting the particular bias term for effective fine-tuning. In this paper, we propose an approach for selecting the bias term to be fine-tuned, forming the foundation of our bias-efficient fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against other bias-selection approaches, across a wide range of large language models (LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B parameters. Our results demonstrate the effectiveness and superiority of our bias-efficient approach on diverse downstream tasks, including classification, multiple-choice, and generation tasks.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-14] Localmax dynamics for attention in transformers and its asymptotic behavior</p>
<p>【速读】： 该论文旨在解决传统注意力机制中软最大（softmax）与硬最大（hardmax）动态之间缺乏连续过渡的问题，尤其是在离散时间系统中如何实现对局部邻域交互的可控调整。其解决方案的关键在于提出一种新的离散时间注意力模型——局部最大动力学（localmax dynamics），该模型通过引入一个对齐敏感性参数（alignment-sensitivity parameter）来松弛邻域交互，从而在保持凸包收敛至凸多面体结构的同时，允许从纯硬最大行为向更平滑的注意力分配过渡。这一设计使得系统能够刻画靠近顶点的“静默集”（quiescent sets）的不变行为，并揭示了在时变对齐敏感性参数下系统的渐近特性，同时证明该模型不具有限时间收敛性，为理解复杂注意力机制提供了理论基础和分析工具。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15958">https://arxiv.org/abs/2509.15958</a><br>
<strong>作者</strong>: Henri Cimetière,Maria Teresa Chiri,Bahman Gharesifard<br>
<strong>机构</strong>: Ecole Nationale des Ponts et Chaussées (国立桥路学院); Queen’s University (皇后大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG); Dynamical Systems (math.DS); Optimization and Control (math.OC)<br>
<strong>备注</strong>:  28 pages, 5 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce a new discrete-time attention model, termed the localmax dynamics, which interpolates between the classic softmax dynamics and the hardmax dynamics, where only the tokens that maximize the influence toward a given token have a positive weight. As in hardmax, uniform weights are determined by a parameter controlling neighbor influence, but the key extension lies in relaxing neighborhood interactions through an alignment-sensitivity parameter, which allows controlled deviations from pure hardmax behavior. As we prove, while the convex hull of the token states still converges to a convex polytope, its structure can no longer be fully described by a maximal alignment set, prompting the introduction of quiescent sets to capture the invariant behavior of tokens near vertices. We show that these sets play a key role in understanding the asymptotic behavior of the system, even under time-varying alignment sensitivity parameters. We further show that localmax dynamics does not exhibit finite-time convergence and provide results for vanishing, nonzero, time-varying alignment-sensitivity parameters, recovering the limiting behavior of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from classical opinion dynamics, highlighting their limitations in the asymmetric setting of localmax interactions and outlining directions for future research.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-15] EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  via Model Context Protocol</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在医院环境中部署受限的问题，特别是由于电子健康记录（Electronic Health Record, EHR）系统访问权限不足导致的临床信息获取困难。其解决方案的关键在于提出并验证了EHR-MCP框架，该框架通过Model Context Protocol (MCP) 将LLM与医院EHR数据库进行集成，使LLM能够自主调用定制化的MCP工具来检索临床相关数据。实验表明，该方法在简单任务中实现了近似完美准确率，但在涉及时间依赖性计算的复杂任务中表现下降，主要错误源于参数输入不当或对工具返回结果的误读，从而揭示了LLM在真实医疗场景下实现可靠临床信息检索的可行性与挑战。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15957">https://arxiv.org/abs/2509.15957</a><br>
<strong>作者</strong>: Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki<br>
<strong>机构</strong>: Keio University Hospital (庆应义塾大学医院)<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Human-Computer Interaction (cs.HC); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Background: Large language models (LLMs) show promise in medicine, but their deployment in hospitals is limited by restricted access to electronic health record (EHR) systems. The Model Context Protocol (MCP) enables integration between LLMs and external tools. Objective: To evaluate whether an LLM connected to an EHR database via MCP can autonomously retrieve clinically relevant information in a real hospital setting. Methods: We developed EHR-MCP, a framework of custom MCP tools integrated with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct agent to interact with it. Six tasks were tested, derived from use cases of the infection control team (ICT). Eight patients discussed at ICT conferences were retrospectively analyzed. Agreement with physician-generated gold standards was measured. Results: The LLM consistently selected and executed the correct MCP tools. Except for two tasks, all tasks achieved near-perfect accuracy. Performance was lower in the complex task requiring time-dependent calculations. Most errors arose from incorrect arguments or misinterpretation of tool results. Responses from EHR-MCP were reliable, though long and repetitive data risked exceeding the context window. Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a real hospital setting, achieving near-perfect performance in simple tasks while highlighting challenges in complex ones. EHR-MCP provides an infrastructure for secure, consistent data access and may serve as a foundation for hospital AI agents. Future work should extend beyond retrieval to reasoning, generation, and clinical impact assessment, paving the way for effective integration of generative AI into clinical practice.         Subjects:  Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Human-Computer Interaction (cs.HC); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)  Cite as: arXiv:2509.15957 [<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>]    (or  arXiv:2509.15957v1 [<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15957">https://doi.org/10.48550/arXiv.2509.15957</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)        Submission history From: Kanato Masayoshi [view email]       [v1]         Fri, 19 Sep 2025 13:17:16 UTC (968 KB)       Full-text links: Access Paper:   View a PDF of the paper titled EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol, by Kanato Masayoshi and 7 other authorsView PDFHTML (experimental)TeX SourceOther Formats view license        Current browse context: <a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>   prev   |   next   new  |  recent  | 2025-09      Change to browse by:      cs <a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a> cs.HC <a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>     References  Citations  NASA ADSGoogle Scholar Semantic Scholar     export BibTeX citation Loading…     BibTeX formatted citation    loading…   Data provided by:      Bookmark           checked=“checked”&gt;     Bibliographic Tools  Bibliographic and Citation Tools       Bibliographic Explorer Toggle    Bibliographic Explorer (What is the Explorer?)        Connected Papers Toggle    Connected Papers (What is Connected Papers?)       Litmaps Toggle    Litmaps (What is Litmaps?)        <a target="_blank" rel="noopener" href="http://scite.ai">scite.ai</a> Toggle    scite Smart Citations (What are Smart Citations?)          Code, Data, Media  Code, Data and Media Associated with this Article       alphaXiv Toggle    alphaXiv (What is alphaXiv?)        Links to Code Toggle    CatalyzeX Code Finder for Papers (What is CatalyzeX?)        DagsHub Toggle    DagsHub (What is DagsHub?)        GotitPub Toggle    Gotit.pub (What is GotitPub?)        Huggingface Toggle    Hugging Face (What is Huggingface?)        Links to Code Toggle    Papers with Code (What is Papers with Code?)        ScienceCast Toggle    ScienceCast (What is ScienceCast?)              Demos  Demos       Replicate Toggle    Replicate (What is Replicate?)        Spaces Toggle    Hugging Face Spaces (What is Spaces?)        Spaces Toggle    <a target="_blank" rel="noopener" href="http://TXYZ.AI">TXYZ.AI</a> (What is <a target="_blank" rel="noopener" href="http://TXYZ.AI">TXYZ.AI</a>?)         Related Papers  Recommenders and Search Tools       Link to Influence Flower    Influence Flower (What are Influence Flowers?)        Core recommender toggle    CORE Recommender (What is CORE?)      Author Venue Institution Topic                      About arXivLabs           arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv’s community? Learn more about arXivLabs.           Which authors of this paper are endorsers? |     Disable MathJax (What is MathJax?)       mathjaxToggle();           About Help      contact arXivClick here to contact arXiv  Contact   subscribe to arXiv mailingsClick here to subscribe  Subscribe            Copyright Privacy Policy     Web Accessibility Assistance   arXiv Operational Status                      Get status notifications via                     email                     or slack<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-16] Beyond the Score: Uncertainty-Calibrated <mark class="hl-label green">LLM</mark> s for Automated Essay Assessment <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决自动化作文评分（Automated Essay Scoring, AES）系统在高风险考试场景中应用受限的问题，核心瓶颈在于现有模型仅输出单一分数而缺乏置信度评估或解释能力。解决方案的关键在于引入分布无关的校准方法——合规预测（Conformal Prediction），该方法为任意分类器提供集合型输出并保证形式化的覆盖概率保障；研究通过在三个不同语料库（ASAP、TOEFL11、Cambridge-FCE）上微调两款开源大语言模型（Llama-3 8B 和 Qwen-2.5 3B），并在90%风险水平下进行校准，同时采用一种新的不确定性感知准确率指标UAcc来衡量可靠性，结果表明模型在满足覆盖率目标的同时保持预测集合紧凑，证明中等规模开源大语言模型已具备支持“教师介入式”AES的能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15926">https://arxiv.org/abs/2509.15926</a><br>
<strong>作者</strong>: Ahmed Karim,Qiao Wang(Judy),Zheng Yuan<br>
<strong>机构</strong>: King’s College London (伦敦国王学院); Hosei University (法政大学); University of Sheffield (谢菲尔德大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Accepted at EMNLP 2025 (Main Conference). Camera-ready version</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Automated Essay Scoring (AES) systems now reach near human agreement on some public benchmarks, yet real-world adoption, especially in high-stakes examinations, remains limited. A principal obstacle is that most models output a single score without any accompanying measure of confidence or explanation. We address this gap with conformal prediction, a distribution-free wrapper that equips any classifier with set-valued outputs and formal coverage guarantees. Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an uncertainty-aware accuracy that rewards models for being both correct and concise. To our knowledge, this is the first work to combine conformal prediction and UAcc for essay scoring. The calibrated models consistently meet the coverage target while keeping prediction sets compact, indicating that open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we discuss scaling and broader user studies as future work.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-17] Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在会议总结（meeting summarization）任务中普遍存在幻觉（hallucination）、遗漏（omission）和无关内容等问题，导致生成摘要忠实度低、可控性差。其解决方案的关键在于提出FRAME框架，将摘要生成重构为语义增强（semantic enrichment）任务：首先提取并评分关键事实，按主题组织后用于丰富摘要提纲，从而生成更准确的抽象式摘要；同时引入SCOPE协议，通过“自言自语”式推理链（reason-out-loud protocol），让模型在内容选择前回答九个结构化问题，提升个性化与目标对齐能力。该方案显著降低了幻觉和遗漏，且P-MESA评估框架验证了其在无参考情况下对读者适配性的有效提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15901">https://arxiv.org/abs/2509.15901</a><br>
<strong>作者</strong>: Frederic Kirstein,Sonu Kumar,Terry Ruas,Bela Gipp<br>
<strong>机构</strong>: University of Göttingen (哥廷根大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted at EMNLP 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Meeting summarization with large language models (LLMs) remains error-prone, often producing outputs with hallucinations, omissions, and irrelevancies. We present FRAME, a modular pipeline that reframes summarization as a semantic enrichment task. FRAME extracts and scores salient facts, organizes them thematically, and uses these to enrich an outline into an abstractive summary. To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that has the model build a reasoning trace by answering nine questions before content selection. For evaluation, we propose P-MESA, a multi-dimensional, reference-free evaluation framework to assess if a summary fits a target reader. P-MESA reliably identifies error instances, achieving = 89% balanced accuracy against human annotations and strongly aligns with human severity ratings (r = 0.70). On QMSum and FAME, FRAME reduces hallucination and omission by 2 out of 5 points (measured with MESA), while SCOPE improves knowledge fit and goal alignment over prompt-only baselines. Our findings advocate for rethinking summarization to improve control, faithfulness, and personalization.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-18] he Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection <mark class="hl-label red">EMNLP’25</mark></p>
<p>【速读】： 该论文试图解决当前自动化事实核查系统在应对虚假信息（misinformation）时存在的局限性问题，即现有方法主要聚焦于判断信息的客观真伪，而忽视了虚假信息对人类认知、情感和社会行为的深层影响。其解决方案的关键在于引入以人为中心的检测框架，融合心理学概念如认知偏差（cognitive biases）、社会动态（social dynamics）和情绪反应（emotional responses），从而超越单纯的事实准确性评估，构建能够更真实反映人类信息处理机制的检测模型。这一转变有助于识别并缓解虚假信息带来的系统性社会危害，并为未来开发整合神经行为学模型与技术的自适应检测系统指明方向。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15896">https://arxiv.org/abs/2509.15896</a><br>
<strong>作者</strong>: Arghodeep Nandi,Megha Sundriyal,Euna Mehnaz Khan,Jikai Sun,Emily Vraga,Jaideep Srivastava,Tanmoy Chakraborty<br>
<strong>机构</strong>: Indian Institute of Technology Delhi(印度理工学院德里分校); Max Planck Institute for Security and Privacy(马克斯普朗克信息安全与隐私研究所); University of Minnesota - Twin Cities(明尼苏达大学双城分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
<strong>备注</strong>:  Accepted in EMNLP’25 Main</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Misinformation remains one of the most significant issues in the digital age. While automated fact-checking has emerged as a viable solution, most current systems are limited to evaluating factual accuracy. However, the detrimental effect of misinformation transcends simple falsehoods; it takes advantage of how individuals perceive, interpret, and emotionally react to information. This underscores the need to move beyond factuality and adopt more human-centered detection frameworks. In this survey, we explore the evolving interplay between traditional fact-checking approaches and psychological concepts such as cognitive biases, social dynamics, and emotional responses. By analyzing state-of-the-art misinformation detection systems through the lens of human psychology and behavior, we reveal critical limitations of current methods and identify opportunities for improvement. Additionally, we outline future research directions aimed at creating more robust and adaptive frameworks, such as neuro-behavioural models that integrate technological factors with the complexities of human cognition and social influence. These approaches offer promising pathways to more effectively detect and mitigate the societal harms of misinformation.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-19] Distribution-Aligned Decoding for Efficient <mark class="hl-label green">LLM</mark>  Task Adaptation <mark class="hl-label red">NEURIPS’25</mark></p>
<p>【速读】： 该论文旨在解决大规模语言模型在下游任务中适应成本高昂的问题，尤其是在使用参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）时仍存在性能瓶颈。其核心创新在于将任务适配重新建模为输出分布对齐问题：不再通过权重更新间接调整模型行为，而是直接在解码阶段引导输出分布向目标任务分布靠拢。解决方案的关键是提出Steering Vector Decoding (SVD) 方法，该方法首先进行简短的预热微调，然后从预训练模型与暖启动模型输出分布间的KL散度梯度中提取任务感知的“引导向量”（steering vector），并在解码过程中利用该向量动态调节输出分布。理论证明表明，SVD 在一阶上等价于全量微调的梯度步，并推导出引导向量强度的全局最优解，从而实现了无需额外可训练参数即可显著提升多选准确率和开放问答真实性等指标的轻量化、理论严谨的任务适配方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15888">https://arxiv.org/abs/2509.15888</a><br>
<strong>作者</strong>: Senkang Hu,Xudong Han,Jinqi Jiang,Yihang Tao,Zihan Fang,Sam Tak Wu Kwong,Yuguang Fang<br>
<strong>机构</strong>: Hong Kong JC STEM Lab of Smart City (香港JC智慧城市实验室); City University of Hong Kong (香港城市大学); University of Sussex (萨塞克斯大学); Huazhong University of Science and Technology (华中科技大学); Fudan University (复旦大学); Lingnan University (岭南大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS’25</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Adapting billion-parameter language models to a downstream task is still costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task adaptation as output-distribution alignment: the objective is to steer the output distribution toward the task distribution directly during decoding rather than indirectly through weight updates. Building on this view, we introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and theoretically grounded method. We start with a short warm-start fine-tune and extract a task-aware steering vector from the Kullback-Leibler (KL) divergence gradient between the output distribution of the warm-started and pre-trained models. This steering vector is then used to guide the decoding process to steer the model’s output distribution towards the task distribution. We theoretically prove that SVD is first-order equivalent to the gradient step of full fine-tuning and derive a globally optimal solution for the strength of the steering vector. Across three tasks and nine benchmarks, SVD paired with four standard PEFT methods improves multiple-choice accuracy by up to 5 points and open-ended truthfulness by 2 points, with similar gains (1-2 points) on commonsense datasets without adding trainable parameters beyond the PEFT adapter. SVD thus offers a lightweight, theoretically grounded path to stronger task adaptation for large language models.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-20] Multi-Physics: A Comprehensive Benchmark for Multimodal <mark class="hl-label green">LLM</mark> s <mark class="hl-label green">Reasoning</mark>  on Chinese Multi-Subject Physics Problems</p>
<p>【速读】： 该论文旨在解决当前多模态大语言模型（Multimodal Large Language Models, MLLMs）在物理等专业科学领域应用中评估基准存在的三大问题：一是主题覆盖不够精细，二是缺乏对推理过程的分步评估，三是以英语为主、忽视视觉信息的作用。其解决方案的关键在于提出一个名为Multi-Physics的中文物理推理综合评测基准，包含5个难度等级、1412道图文关联的多项选择题，覆盖11个高中物理子领域，并采用双维度评估框架——既考察最终答案准确性，也评估模型链式思维（chain-of-thought）的完整性；同时通过对比不同输入模式下模型表现的变化，系统分析难度层级和视觉信息对多模态推理能力的影响，从而为MLLMs在专业领域的推理机制提供可解释、可复现的评估方法论。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15839">https://arxiv.org/abs/2509.15839</a><br>
<strong>作者</strong>: Zhongze Luo,Zhenshuai Yin,Yongxin Guo,Zhichao Wang,Jionghao Zhu,Xiaoying Tang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress, their application in specialized scientific domains like physics reveals significant gaps in current evaluation benchmarks. Specifically, existing benchmarks often lack fine-grained subject coverage, neglect the step-by-step reasoning process, and are predominantly English-centric, failing to systematically evaluate the role of visual information. Therefore, we introduce \textbf Multi-Physics for Chinese physics reasoning, a comprehensive benchmark that includes 5 difficulty levels, featuring 1,412 image-associated, multiple-choice questions spanning 11 high-school physics subjects. We employ a dual evaluation framework to evaluate 20 different MLLMs, analyzing both final answer accuracy and the step-by-step integrity of their chain-of-thought. Furthermore, we systematically study the impact of difficulty level and visual information by comparing the model performance before and after changing the input mode. Our work provides not only a fine-grained resource for the community but also offers a robust methodology for dissecting the multimodal reasoning process of state-of-the-art MLLMs, and our dataset and code have been open-sourced: this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-21] he Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决视觉信息在训练过程中如何影响基于语音和文本的深度学习模型的语言处理机制这一问题，特别是探讨视觉 grounding（具身化）对模型内部词表示的影响差异。其关键发现在于：视觉信息能增强语音与文本模型中词身份（word identity）的对齐，但这种效应主要体现在语义层面的提升有限，尤其在语音模型中，视觉 grounding 并未显著改善语义可分性（semantic discriminability），而文本模型则表现出更强的语义区分能力。因此，解决方案的核心在于识别出视觉信息对语音模型语义建模的局限性，并为未来提升语音模型的语义理解能力提供优化方向。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15837">https://arxiv.org/abs/2509.15837</a><br>
<strong>作者</strong>: Adrian Sauter,Willem Zuidema,Marianne de Heer Kloots<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  5 pages, 3 figures, Submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:How does visual information included in training affect language processing in audio- and text-based deep learning models? We explore how such visual grounding affects model-internal representations of words, and find substantially different effects in speech- vs. text-based language encoders. Firstly, global representational comparisons reveal that visual grounding increases alignment between representations of spoken and written language, but this effect seems mainly driven by enhanced encoding of word identity rather than meaning. We then apply targeted clustering analyses to probe for phonetic vs. semantic discriminability in model representations. Speech-based representations remain phonetically dominated with visual grounding, but in contrast to text-based representations, visual grounding does not improve semantic discriminability. Our findings could usefully inform the development of more efficient methods to enrich speech-based models with visually-informed semantics.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-22] Best-of-L: Cross-Lingual Reward Modeling for Mathematical <mark class="hl-label green">Reasoning</mark></p>
<p>【速读】： 该论文旨在解决多语言大语言模型（Multilingual Large Language Models, MLLMs）中不同语言在推理能力上的差异及其是否能通过跨语言互补提升整体推理性能的问题。其核心解决方案是训练一个跨语言奖励模型（Cross-lingual Reward Model），用于对不同语言生成的解答进行排序和优化，从而引导模型利用多种语言间的推理优势。实验表明，该方法显著提升了数学推理能力，尤其在低采样预算下对英语表现有明显增益，揭示了通过挖掘多语言推理路径互补性来增强多语言推理的新路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15811">https://arxiv.org/abs/2509.15811</a><br>
<strong>作者</strong>: Sara Rajaee,Rochelle Choenni,Ekaterina Shutova,Christof Monz<br>
<strong>机构</strong>: University of Amsterdam (阿姆斯特丹大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While the reasoning abilities of large language models (LLMs) continue to advance, it remains unclear how such ability varies across languages in multilingual LLMs and whether different languages produce reasoning paths that complement each other. To investigate this question, we train a reward model to rank generated responses for a given question across languages. Our results show that our cross-lingual reward model substantially improves mathematical reasoning performance compared to using reward modeling within a single language, benefiting even high-resource languages. While English often exhibits the highest performance in multilingual models, we find that cross-lingual sampling particularly benefits English under low sampling budgets. Our findings reveal new opportunities to improve multilingual reasoning by leveraging the complementary strengths of diverse languages.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-23] RAVE: Retrieval and Scoring Aware Verifiable Claim Detection</p>
<p>【速读】： 该论文旨在解决社交媒体上虚假信息快速传播问题，核心挑战在于如何高效、准确地识别出可验证的声明（verifiable claim），尤其是在政治话语模糊和文本格式多样（如推文）的情况下。解决方案的关键在于提出 RAVE（Retrieval and Scoring Aware Verifiable Claim Detection）框架，该框架通过融合证据检索与结构化相关性信号（relevance signals）及来源可信度评分（source credibility scores），显著提升了声明检测的准确性与F1分数，优于仅依赖文本或单纯检索的基线方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15793">https://arxiv.org/abs/2509.15793</a><br>
<strong>作者</strong>: Yufeng Li,Arkaitz Zubiaga<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  5 pages, 1 figure</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid spread of misinformation on social media underscores the need for scalable fact-checking tools. A key step is claim detection, which identifies statements that can be objectively verified. Prior approaches often rely on linguistic cues or claim check-worthiness, but these struggle with vague political discourse and diverse formats such as tweets. We present RAVE (Retrieval and Scoring Aware Verifiable Claim Detection), a framework that combines evidence retrieval with structured signals of relevance and source credibility. Experiments on CT22-test and PoliClaim-test show that RAVE consistently outperforms text-only and retrieval-based baselines in both accuracy and F1.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-24] UPRPRC: Unified Pipeline for Reproducing Parallel Resources – Corpus from the United Nations <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决以往基于联合国文件构建的多语言语料库存在的问题，如数据处理流程不透明、难以复现以及规模有限等挑战。其核心解决方案是提出了一种全新的图辅助段落对齐算法（Graph-Aided Paragraph Alignment, GAPA），实现了从网络爬取到文本对齐的端到端可复现流程，并支持单机最小化示例和可选的分布式计算扩展，从而构建出目前最大规模的纯人工翻译、非生成式AI内容的平行语料库（超过713百万英文词元），显著提升了机器翻译研究中高质量语料的可用性与可扩展性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15789">https://arxiv.org/abs/2509.15789</a><br>
<strong>作者</strong>: Qiuyang Lu,Fangjian Shen,Zhengkai Tang,Qiang Liu,Hexuan Cheng,Hui Liu,Wushao Wen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  5 pages, 1 figure, submitted to ICASSP2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The quality and accessibility of multilingual datasets are crucial for advancing machine translation. However, previous corpora built from United Nations documents have suffered from issues such as opaque process, difficulty of reproduction, and limited scale. To address these challenges, we introduce a complete end-to-end solution, from data acquisition via web scraping to text alignment. The entire process is fully reproducible, with a minimalist single-machine example and optional distributed computing steps for scalability. At its core, we propose a new Graph-Aided Paragraph Alignment (GAPA) algorithm for efficient and flexible paragraph-level alignment. The resulting corpus contains over 713 million English tokens, more than doubling the scale of prior work. To the best of our knowledge, this represents the largest publicly available parallel corpus composed entirely of human-translated, non-AI-generated content. Our code and corpus are accessible under the MIT License.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-25] UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression</p>
<p>【速读】： 该论文旨在解决大语言模型在处理长上下文输入时，因键值缓存（Key-Value Cache, KV Cache）带来的内存开销瓶颈问题，尤其是序列级压缩（sequence-level compression）中因删除部分token的KV缓存而导致重要上下文信息丢失的挑战。其解决方案的关键在于提出UniGist框架，通过细粒度地将原始token替换为特殊压缩标记（gists），以保留关键上下文信息；同时采用无分块训练策略（chunk-free training）和高效的gist shift核函数，实现GPU训练优化，并支持灵活推理——允许实际移除压缩token从而实现实时内存节省。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15763">https://arxiv.org/abs/2509.15763</a><br>
<strong>作者</strong>: Chenlong Deng,Zhisong Zhang,Kelong Mao,Shuaiyi Li,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Zhicheng Dou<br>
<strong>机构</strong>: Renmin University of China (中国人民大学); Tencent AI Lab (腾讯人工智能实验室); City University of Hong Kong (香港城市大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  15 pages, 7 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models are increasingly capable of handling long-context inputs, but the memory overhead of key-value (KV) cache remains a major bottleneck for general-purpose deployment. While various compression strategies have been explored, sequence-level compression, which drops the full KV caches for certain tokens, is particularly challenging as it can lead to the loss of important contextual information. To address this, we introduce UniGist, a sequence-level long-context compression framework that efficiently preserves context information by replacing raw tokens with special compression tokens (gists) in a fine-grained manner. We adopt a chunk-free training strategy and design an efficient kernel with a gist shift trick, enabling optimized GPU training. Our scheme also supports flexible inference by allowing the actual removal of compressed tokens, resulting in real-time memory savings. Experiments across multiple long-context tasks demonstrate that UniGist significantly improves compression quality, with especially strong performance in detail-recalling tasks and long-range dependency modeling.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-26] Can <mark class="hl-label green">LLM</mark> s Judge Debates? Evaluating Non-Linear <mark class="hl-label green">Reasoning</mark>  via Argumentation Theory Semantics <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在非线性结构推理任务中表现不足的问题，特别是如何在自然辩论场景下近似计算论证理论（Computational Argumentation Theory, CAT）所定义的结构化推理。其关键解决方案在于引入定量论证辩论（Quantitative Argumentation Debate, QuAD）语义框架，通过为论证分配可接受度分数来量化攻击与支持关系，并利用高级提示策略（如思维链Chain-of-Thought和上下文学习In-Context Learning）引导LLMs从对话格式的辩论数据中推断出接近QuAD排序的论证层级结构，从而提升模型对形式化论证语义的理解能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15739">https://arxiv.org/abs/2509.15739</a><br>
<strong>作者</strong>: Reza Sanayei,Srdjan Vesic,Eduardo Blanco,Mihai Surdeanu<br>
<strong>机构</strong>: University of Arizona (亚利桑那大学); CRIL CNRS &amp; University of Artois (CRIL CNRS 与阿图瓦大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted to EMNLP 2025 Findings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) excel at linear reasoning tasks but remain underexplored on non-linear structures such as those found in natural debates, which are best expressed as argument graphs. We evaluate whether LLMs can approximate structured reasoning from Computational Argumentation Theory (CAT). Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which assigns acceptability scores to arguments based on their attack and support relations. Given only dialogue-formatted debates from two NoDE datasets, models are prompted to rank arguments without access to the underlying graph. We test several LLMs under advanced instruction strategies, including Chain-of-Thought and In-Context Learning. While models show moderate alignment with QuAD rankings, performance degrades with longer inputs or disrupted discourse flow. Advanced prompting helps mitigate these effects by reducing biases related to argument length and position. Our findings highlight both the promise and limitations of LLMs in modeling formal argumentation semantics and motivate future work on graph-aware reasoning.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-27] REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在意见摘要任务中公平性不足的问题，即模型在生成摘要时可能偏向某些观点而忽略其他代表性观点，导致偏见。现有方法依赖超参数调优或在提示中提供真实分布信息，但这些方式在实际应用中受限于用户难以调整默认参数且准确分布信息常不可得。论文提出的关键解决方案是频率框架提示（Frequency Framed Prompting, REFER），其核心思想源于认知科学发现：通过显式呈现频次信息来降低认知负荷并减少系统性偏差。实验表明，REFER能显著提升LLMs在意见摘要中的公平性表现，尤其在大模型和强推理指令下效果更优。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15723">https://arxiv.org/abs/2509.15723</a><br>
<strong>作者</strong>: Nannan Huang,Haytham M. Fayek,Xiuzhen Zhang<br>
<strong>机构</strong>: RMIT University (皇家墨尔本理工大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted to the 5th New Frontiers in Summarization Workshop (NewSumm@EMNLP 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Individuals express diverse opinions, a fair summary should represent these viewpoints comprehensively. Previous research on fairness in opinion summarisation using large language models (LLMs) relied on hyperparameter tuning or providing ground truth distributional information in prompts. However, these methods face practical limitations: end-users rarely modify default model parameters, and accurate distributional information is often unavailable. Building upon cognitive science research demonstrating that frequency-based representations reduce systematic biases in human statistical reasoning by making reference classes explicit and reducing cognitive load, this study investigates whether frequency framed prompting (REFER) can similarly enhance fairness in LLM opinion summarisation. Through systematic experimentation with different prompting frameworks, we adapted techniques known to improve human reasoning to elicit more effective information processing in language models compared to abstract probabilistic this http URL results demonstrate that REFER enhances fairness in language models when summarising opinions. This effect is particularly pronounced in larger language models and using stronger reasoning instructions.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-28] Once Upon a Time: Interactive Learning for Storytelling with Small Language Models <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文试图解决当前大型语言模型（Large Language Models, LLMs）在训练过程中依赖海量文本数据进行next-word预测，导致数据效率低下的问题。其解决方案的关键在于引入一种受认知启发的高阶反馈机制：通过一个教师模型对学生的生成内容（如故事）从可读性、叙事连贯性和创造性等维度进行评分，并将这些高阶反馈用于指导学生模型的迭代优化。这种交互式学习方式显著提升了数据利用效率——仅需100万词的交互式学习即可达到4.1亿词纯next-word预测训练的效果，从而在有限数据条件下实现语言能力的实质性提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15714">https://arxiv.org/abs/2509.15714</a><br>
<strong>作者</strong>: Jonas Mayer Martins,Ali Hamza Bashir,Muhammad Rehan Khalid,Lisa Beinborn<br>
<strong>机构</strong>: University of Göttingen (哥廷根大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  EMNLP 2025, BabyLM Challenge; 16 pages, 6 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Children efficiently acquire language not just by listening, but by interacting with others in their social environment. Conversely, large language models are typically trained with next-word prediction on massive amounts of text. Motivated by this contrast, we investigate whether language models can be trained with less data by learning not only from next-word prediction but also from high-level, cognitively inspired feedback. We train a student model to generate stories, which a teacher model rates on readability, narrative coherence, and creativity. By varying the amount of pretraining before the feedback loop, we assess the impact of this interactive learning on formal and functional linguistic competence. We find that the high-level feedback is highly data efficient: With just 1 M words of input in interactive learning, storytelling skills can improve as much as with 410 M words of next-word prediction.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-29] Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决自动发音评估（Automatic Pronunciation Assessment, APA）在多粒度和多维度上的精准性问题，尤其关注大型多模态模型（Large Multimodal Models, LMMs）在细粒度评估中的有效性。其解决方案的关键在于通过在Speechocean762数据集和私有语料库上对LMM进行微调（fine-tuning），显著优于零样本（zero-shot）设置，并在单粒度任务中达到与公开及商用系统相当的性能；同时，研究发现PCC（皮尔逊相关系数）高达0.9而SCC（斯皮尔曼等级相关系数）仅为0.6，表明SCC更能反映排序一致性，从而强调了未来需加强细粒度建模与秩感知（rank-aware）评估方法的研究方向。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15701">https://arxiv.org/abs/2509.15701</a><br>
<strong>作者</strong>: Ke Wang,Wenning Wei,Yan Deng,Lei He,Sheng Zhao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  submitted to ICASSP2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted Language Learning (CALL), requiring evaluation across multiple granularities and aspects. Large Multimodal Models (LMMs) present new opportunities for APA, but their effectiveness in fine-grained assessment remains uncertain. This work investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a private corpus. Fine-tuning significantly outperforms zero-shot settings and achieves competitive results on single-granularity tasks compared to public and commercial systems. The model performs well at word and sentence levels, while phoneme-level assessment remains challenging. We also observe that the Pearson Correlation Coefficient (PCC) reaches 0.9, whereas Spearman’s rank Correlation Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects ordinal consistency. These findings highlight both the promise and limitations of LMMs for APA and point to future work on fine-grained modeling and rank-aware evaluation.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-30] Direct Simultaneous Translation Activation for Large Audio-Language Models</p>
<p>【速读】： 该论文旨在解决如何在不修改模型架构或解码策略的前提下，激活大音频语言模型（Large Audio-Language Models, LALMs）的端到端实时语音翻译（Simultaneous Speech-to-Text Translation, Simul-S2TT）能力的问题。其解决方案的关键在于提出一种名为“同时自增强”（Simultaneous Self-Augmentation, SimulSA）的方法：通过随机截断语音并构建部分对齐的翻译数据，利用LALMs自身具备的泛化能力生成模拟的同步训练样本，并将其融入离线监督微调（Offline Supervised Fine-Tuning, SFT）数据中，从而有效弥合预训练阶段离线翻译与推理阶段实时翻译之间的分布差异。实验表明，仅用约1%的此类同步数据即可显著激活模型的Simul-S2TT能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15692">https://arxiv.org/abs/2509.15692</a><br>
<strong>作者</strong>: Pei Zhang,Yiming Wang,Jialong Tang,Baosong Yang,Rui Wang,Derek F. Wong,Fei Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech into target text in real time, outputting translations while receiving source speech input, rather than waiting for the entire utterance to be spoken. Simul-S2TT research often modifies model architectures to implement read-write strategies. However, with the rise of large audio-language models (LALMs), a key challenge is how to directly activate Simul-S2TT capabilities in base models without additional architectural changes. In this paper, we introduce \bf Simultaneous \bf Self-\bf Augmentation (\bf SimulSA), a strategy that utilizes LALMs’ inherent capabilities to obtain simultaneous data by randomly truncating speech and constructing partially aligned translation. By incorporating them into offline SFT data, SimulSA effectively bridges the distribution gap between offline translation during pretraining and simultaneous translation during inference. Experimental results demonstrate that augmenting only about \bf 1% of the simultaneous data, compared to the full offline SFT data, can significantly activate LALMs’ Simul-S2TT capabilities without modifications to model architecture or decoding strategy.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-31] KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning</p>
<p>【速读】： 该论文旨在解决在上下文学习（In-context Learning, ICL）中，如何从有限的上下文窗口内选择最具信息量的示例以最大化模型对特定查询的预测准确性这一核心问题。其解决方案的关键在于：首先将大语言模型（LLM）建模为输入嵌入上的线性函数，并将示例选择问题形式化为一个针对特定查询的优化问题——即从示例库中挑选子集以最小化该查询的预测误差；其次，基于此建模推导出一个近似子模（approximately submodular）的代理目标函数，从而可采用具有理论保证的贪心算法进行高效求解；此外，通过引入核技巧（kernel trick）和基于最优设计的正则项，分别实现了高维特征空间中的有效计算与所选示例的多样性增强，显著提升了在标签稀缺场景下的ICL性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15676">https://arxiv.org/abs/2509.15676</a><br>
<strong>作者</strong>: Vaibhav Singh,Soumya Suvra Ghosal,Kapu Nirmal Joshua,Soumyabrata Pal,Sayak Ray Chowdhury<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In-context learning (ICL) has emerged as a powerful paradigm for adapting large language models (LLMs) to new and data-scarce tasks using only a few carefully selected task-specific examples presented in the prompt. However, given the limited context size of LLMs, a fundamental question arises: Which examples should be selected to maximize performance on a given user query? While nearest-neighbor-based methods like KATE have been widely adopted for this purpose, they suffer from well-known drawbacks in high-dimensional embedding spaces, including poor generalization and a lack of diversity. In this work, we study this problem of example selection in ICL from a principled, information theory-driven perspective. We first model an LLM as a linear function over input embeddings and frame the example selection task as a query-specific optimization problem: selecting a subset of exemplars from a larger example bank that minimizes the prediction error on a specific query. This formulation departs from traditional generalization-focused learning theoretic approaches by targeting accurate prediction for a specific query instance. We derive a principled surrogate objective that is approximately submodular, enabling the use of a greedy algorithm with an approximation guarantee. We further enhance our method by (i) incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings, and (ii) introducing an optimal design-based regularizer to encourage diversity in the selected examples. Empirically, we demonstrate significant improvements over standard retrieval methods across a suite of classification tasks, highlighting the benefits of structure-aware, diverse example selection for ICL in real-world, label-scarce scenarios.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-32] VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion</p>
<p>【速读】： 该论文旨在解决如何将基于解码器的预训练大语言模型（Large Language Model, LLM）与声学编码器-解码器架构（如Whisper）有效融合，以构建具备语音能力的LLM的问题。其核心挑战在于实现多模态表示之间的对齐，尤其是在低资源语言场景下。解决方案的关键在于引入一个中间的、音频条件化的文本空间，通过在连续文本表示空间中融合Whisper的隐藏解码状态与LLM的表示，利用跨模态注意力机制进行对齐，从而避免直接使用音频嵌入带来的不匹配问题。该方法支持离线和流式两种模式，并在希腊语语音识别任务上实现了平均约20%的相对性能提升，验证了连续空间融合策略在多语言及低资源场景下的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15667">https://arxiv.org/abs/2509.15667</a><br>
<strong>作者</strong>: Dimitrios Damianos,Leon Voukoutis,Georgios Paraskevopoulos,Vassilis Katsouros<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present a multimodal fusion framework that bridges pre-trained decoder-based large language models (LLM) and acoustic encoder-decoder architectures such as Whisper, with the aim of building speech-enabled LLMs. Instead of directly using audio embeddings, we explore an intermediate audio-conditioned text space as a more effective mechanism for alignment. Our method operates fully in continuous text representation spaces, fusing Whisper’s hidden decoder states with those of an LLM through cross-modal attention, and supports both offline and streaming modes. We introduce \textitVoxKrikri, the first Greek speech LLM, and show through analysis that our approach effectively aligns representations across modalities. These results highlight continuous space fusion as a promising path for multilingual and low-resource speech LLMs, while achieving state-of-the-art results for Automatic Speech Recognition in Greek, providing an average  \sim20%  relative improvement across benchmarks.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-33] SightSound-R1: Cross-Modal <mark class="hl-label green">Reasoning</mark>  Distillation from Vision to Audio Language Models</p>
<p>【速读】： 该论文旨在解决大型音频语言模型（Large Audio-Language Models, LALMs）在复杂声景中推理能力不足的问题，其核心瓶颈在于缺乏大规模的链式思维（Chain-of-Thought, CoT）音频数据以训练模型进行逐步推理。解决方案的关键在于提出SightSound-R1跨模态蒸馏框架，通过三个核心步骤实现：（i）利用测试时扩展从更强的视觉语言模型（Large Vision-Language Models, LVLMs）教师端生成聚焦音频的CoT；（ii）基于音频 grounded 的验证机制过滤幻觉内容；（iii）采用监督微调（Supervised Fine-Tuning, SFT）与组相对策略优化（Group Relative Policy Optimization, GRPO）相结合的蒸馏流程，将LVLM的推理能力有效迁移至LALM学生模型。实验表明，该方法显著提升了LALM在领域内和未见听觉场景下的推理性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15661">https://arxiv.org/abs/2509.15661</a><br>
<strong>作者</strong>: Qiaolin Wang,Xilin Jiang,Linyang He,Junkai Wu,Nima Mesgarani<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While large audio-language models (LALMs) have demonstrated state-of-the-art audio understanding, their reasoning capability in complex soundscapes still falls behind large vision-language models (LVLMs). Compared to the visual domain, one bottleneck is the lack of large-scale chain-of-thought audio data to teach LALM stepwise reasoning. To circumvent this data and modality gap, we present SightSound-R1, a cross-modal distillation framework that transfers advanced reasoning from a stronger LVLM teacher to a weaker LALM student on the same audio-visual question answering (AVQA) dataset. SightSound-R1 consists of three core steps: (i) test-time scaling to generate audio-focused chains of thought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter hallucinations, and (iii) a distillation pipeline with supervised fine-tuning (SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM student. Results show that SightSound-R1 improves LALM reasoning performance both in the in-domain AVQA test set as well as in unseen auditory scenes and questions, outperforming both pretrained and label-only distilled baselines. Thus, we conclude that vision reasoning can be effectively transferred to audio models and scaled with abundant audio-visual data.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-34] Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前基于Transformer的语音语言模型（Speech Language Models, SLMs）在多大程度上编码了语法结构和语义概念等深层语言特征的问题，尤其是相较于浅层声学与音位特征，其对句法和语义信息的表征能力尚不明确。解决方案的关键在于采用最小配对设计（minimal pair designs）与诊断性特征分析方法，在71项跨不同语言层级的任务中进行逐层和时间分辨率的系统评估，从而首次量化揭示了所有类型的SLMs（包括自监督学习模型S3M、自动语音识别ASR、语音压缩codec及音频大语言模型AudioLLMs的编码器）均更稳健地编码语法特征而非概念性特征。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15655">https://arxiv.org/abs/2509.15655</a><br>
<strong>作者</strong>: Linyang He,Qiaolin Wang,Xilin Jiang,Nima Mesgarani<br>
<strong>机构</strong>: Columbia University (哥伦比亚大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  EMNLP 2025 Main Conference (Oral)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Transformer-based speech language models (SLMs) have significantly improved neural speech recognition and understanding. While existing research has examined how well SLMs encode shallow acoustic and phonetic features, the extent to which SLMs encode nuanced syntactic and conceptual features remains unclear. By drawing parallels with linguistic competence assessments for large language models, this study is the first to systematically evaluate the presence of contextual syntactic and semantic features across SLMs for self-supervised learning (S3M), automatic speech recognition (ASR), speech compression (codec), and as the encoder for auditory large language models (AudioLLMs). Through minimal pair designs and diagnostic feature analysis across 71 tasks spanning diverse linguistic levels, our layer-wise and time-resolved analysis uncovers that 1) all speech encode grammatical features more robustly than conceptual ones.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-35] Multilingual <mark class="hl-label green">LLM</mark>  Prompting Strategies for Medical English-Vietnamese Machine Translation</p>
<p>【速读】： 该论文旨在解决越南语（Vietnamese）作为低资源语言在医疗领域中英机器翻译（Medical English-Vietnamese machine translation, En-Vi MT）中的性能不足问题，以提升越南医疗场景下的跨语言信息获取与沟通效率。其解决方案的关键在于系统性评估六种多语言大语言模型（multilingual large language models, LLMs）在MedEV数据集上采用不同提示策略（prompting strategies）的表现，包括零样本（zero-shot）、少样本（few-shot）以及基于医学词典（Meddict）的术语增强提示方法；研究发现，模型规模是性能的主要驱动因素，而术语感知提示和基于嵌入的示例检索则能稳定提升专业领域翻译准确性，凸显了当前多语言LLMs在医疗翻译任务中的潜力与局限。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15640">https://arxiv.org/abs/2509.15640</a><br>
<strong>作者</strong>: Nhu Vo,Nu-Uyen-Phuong Le,Dung D. Le,Massimo Piccardi,Wray Buntine<br>
<strong>机构</strong>: VinUniversity (越南Vin大学); University of Queensland (昆士兰大学); University of Technology Sydney (悉尼科技大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  The work is under peer review</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Medical English-Vietnamese machine translation (En-Vi MT) is essential for healthcare access and communication in Vietnam, yet Vietnamese remains a low-resource and under-studied language. We systematically evaluate prompting strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset, comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict, an English-Vietnamese medical lexicon. Results show that model scale is the primary driver of performance: larger LLMs achieve strong zero-shot results, while few-shot prompting yields only marginal improvements. In contrast, terminology-aware cues and embedding-based example retrieval consistently improve domain-specific translation. These findings underscore both the promise and the current limitations of multilingual LLMs for medical En-Vi MT.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-36] Sparse-Autoencoder-Guided Internal Representation Unlearning for <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在部署过程中面临的隐私和版权问题，尤其是现有基于抑制（suppression-based）的遗忘方法无法真正删除模型内部存储的知识、易导致模型性能退化（model collapse）的问题。其解决方案的关键在于：不再通过额外训练来压制特定输出，而是直接干预模型内部激活（internal activations），定义“遗忘”为被遗忘目标实体的激活状态与未知实体不可区分的状态；通过在稀疏自编码器（sparse autoencoder）潜空间中设计一个遗忘目标函数，将目标实体的激活从已知实体拉远、向未知实体靠拢，从而实现对目标知识的实质性遗忘，同时避免过度抑制和模型崩溃。实验证明，该方法能有效对齐被遗忘目标的内部激活模式，且在问答任务中显著降低对目标知识的回忆，而对非目标知识影响较小。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15631">https://arxiv.org/abs/2509.15631</a><br>
<strong>作者</strong>: Tomoya Yamashita,Akira Ito,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As large language models (LLMs) are increasingly deployed across various applications, privacy and copyright concerns have heightened the need for more effective LLM unlearning techniques. Many existing unlearning methods aim to suppress undesirable outputs through additional training (e.g., gradient ascent), which reduces the probability of generating such outputs. While such suppression-based approaches can control model outputs, they may not eliminate the underlying knowledge embedded in the model’s internal activations; muting a response is not the same as forgetting it. Moreover, such suppression-based methods often suffer from model collapse. To address these issues, we propose a novel unlearning method that directly intervenes in the model’s internal activations. In our formulation, forgetting is defined as a state in which the activation of a forgotten target is indistinguishable from that of <code>unknown'' entities. Our method introduces an unlearning objective that modifies the activation of the target entity away from those of known entities and toward those of unknown entities in a sparse autoencoder latent space. By aligning the target's internal activation with those of unknown entities, we shift the model's recognition of the target entity from </code>known’’ to ``unknown’', achieving genuine forgetting while avoiding over-suppression and model collapse. Empirically, we show that our method effectively aligns the internal activations of the forgotten target, a result that the suppression-based approaches do not reliably achieve. Additionally, our method effectively reduces the model’s recall of target knowledge in question-answering tasks without significant damage to the non-target knowledge.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-37] Concept Unlearning in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  via Self-Constructed Knowledge Triplets</p>
<p>【速读】： 该论文旨在解决当前机器遗忘（Machine Unlearning, MU）方法仅支持删除特定句子而无法实现更广泛概念级遗忘的问题，例如删除某个人物或事件等抽象概念。现有方法难以应对现实场景中对概念层面知识移除的需求，限制了其在隐私保护和版权合规中的应用。解决方案的关键在于提出“概念遗忘”（Concept Unlearning, CU）的新范式，利用知识图谱（Knowledge Graph）建模大语言模型（LLM）内部知识结构，并将遗忘目标定义为移除图中对应的节点及其关联边；进而设计了一种基于提示（prompting）生成知识三元组和解释性语句的方法，使遗忘过程与模型内部知识表示对齐，从而实现更精确、全面的概念级知识移除，同时最大程度保留无关知识。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15621">https://arxiv.org/abs/2509.15621</a><br>
<strong>作者</strong>: Tomoya Yamashita,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara,Tomoharu Iwata<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Machine Unlearning (MU) has recently attracted considerable attention as a solution to privacy and copyright issues in large language models (LLMs). Existing MU methods aim to remove specific target sentences from an LLM while minimizing damage to unrelated knowledge. However, these approaches require explicit target sentences and do not support removing broader concepts, such as persons or events. To address this limitation, we introduce Concept Unlearning (CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to represent the LLM’s internal knowledge and define CU as removing the forgetting target nodes and associated edges. This graph-based formulation enables a more intuitive unlearning and facilitates the design of more effective methods. We propose a novel method that prompts the LLM to generate knowledge triplets and explanatory sentences about the forgetting target and applies the unlearning process to these representations. Our approach enables more precise and comprehensive concept removal by aligning the unlearning process with the LLM’s internal knowledge representations. Experiments on real-world and synthetic datasets demonstrate that our method effectively achieves concept-level unlearning while preserving unrelated knowledge.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-38] SciEvent: Benchmarking Multi-domain Scientific Event Extraction <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决科学信息抽取（Scientific Information Extraction, SciIE）在跨学科研究中适用性受限的问题，即现有方法主要基于窄域的实体-关系抽取，难以捕捉科学内容所需的上下文信息，常导致信息碎片化或矛盾。其解决方案的关键在于提出一个名为SciEvent的多领域基准数据集，采用统一的事件抽取（Event Extraction, EE）框架对科学摘要进行标注，涵盖背景、方法、结果和结论四个核心科学活动阶段，并精细标注事件触发词及其论元。该设计使SciIE成为多阶段EE流水线：首先分割文本为结构化段落，再提取事件触发词与论元，从而实现更结构化且语境感知的科学内容理解。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15620">https://arxiv.org/abs/2509.15620</a><br>
<strong>作者</strong>: Bofu Dong,Pritesh Shah,Sumedh Sonawane,Tiyasha Banerjee,Erin Brady,Xinya Du,Ming Jiang<br>
<strong>机构</strong>: Indiana University Indianapolis (印第安纳大学印第安纳波利斯分校); University of Texas at Dallas (德克萨斯大学达拉斯分校); University of Wisconsin-Madison (威斯康星大学麦迪逊分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted to EMNLP 2025 (Main Conference)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Scientific information extraction (SciIE) has primarily relied on entity-relation extraction in narrow domains, limiting its applicability to interdisciplinary research and struggling to capture the necessary context of scientific information, often resulting in fragmented or conflicting statements. In this paper, we introduce SciEvent, a novel multi-domain benchmark of scientific abstracts annotated via a unified event extraction (EE) schema designed to enable structured and context-aware understanding of scientific content. It includes 500 abstracts across five research domains, with manual annotations of event segments, triggers, and fine-grained arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting abstracts into core scientific activities–Background, Method, Result, and Conclusion; and (2) extracting the corresponding triggers and arguments. Experiments with fine-tuned EE models, large language models (LLMs), and human annotators reveal a performance gap, with current models struggling in domains such as sociology and humanities. SciEvent serves as a challenging benchmark and a step toward generalizable, multi-domain SciIE.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-39] DivLogicEval: A Framework for Benchmarking Logical <mark class="hl-label green">Reasoning</mark>  Evaluation in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前大型语言模型（Large Language Models, LLMs）逻辑推理能力评估中存在的两个关键问题：一是现有基准测试往往混杂多种推理技能，导致对逻辑推理能力的评价不准确；二是现有基准在语言多样性上不足且分布偏离理想逻辑推理评测数据分布，可能引发评估偏差。解决方案的关键在于提出一个新的经典逻辑推理基准 DivLogicEval，其由多样化的自然语句构成，通过反直觉的方式组织命题以强化逻辑推理需求，并引入一种能缓解LLM固有偏倚和随机性影响的新评估指标，从而实现更可靠、公平的逻辑推理能力测评。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15587">https://arxiv.org/abs/2509.15587</a><br>
<strong>作者</strong>: Tsz Ting Chung,Lemao Liu,Mo Yu,Dit-Yan Yeung<br>
<strong>机构</strong>: The Hong Kong University of Science and Technology (香港科技大学); Fudan University (复旦大学); WeChat AI, Tencent (微信人工智能，腾讯)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Accepted by EMNLP 2025. Project Page: <a target="_blank" rel="noopener" href="https://ttchungc.github.io/projects/divlogiceval/">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Logic reasoning in natural language has been recognized as an important measure of human intelligence for Large Language Models (LLMs). Popular benchmarks may entangle multiple reasoning skills and thus provide unfaithful evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning benchmarks are limited in language diversity and their distributions are deviated from the distribution of an ideal logic reasoning benchmark, which may lead to biased evaluation results. This paper thereby proposes a new classical logic benchmark DivLogicEval, consisting of natural sentences composed of diverse statements in a counterintuitive way. To ensure a more reliable evaluation, we also introduce a new evaluation metric that mitigates the influence of bias and randomness inherent in LLMs. Through experiments, we demonstrate the extent to which logical reasoning is required to answer the questions in DivLogicEval and compare the performance of different popular LLMs in conducting logical reasoning.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-40] Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization</p>
<p>【速读】： 该论文旨在解决流式（streaming）语音处理中自监督学习（Self-Supervised Learning, SSL）模型因依赖完整话语（full utterance）假设而导致性能下降的问题。传统SSL方法在处理部分输入时需做出妥协，难以适配实时语音交互场景。其核心解决方案是提出一种基于分块（chunk-based）的自监督学习算法（Chunk SSL），通过引入掩码预测损失（masked prediction loss）并利用当前分块及前序分块中的未掩码帧来恢复被掩码语音帧的离散索引，从而实现对流式和离线模式下语音预训练的统一建模。关键创新包括：采用有限标量量化（Finite Scalar Quantization, FSQ）模块生成高分辨率代码本（词汇量达数百万级别）以提升知识迁移能力，并设计分组掩码预测损失（group masked prediction loss）以降低大代码本带来的内存与计算开销。实验表明，该方法在语音识别和语音翻译任务中均能在流式与离线模式下取得优异性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15579">https://arxiv.org/abs/2509.15579</a><br>
<strong>作者</strong>: Yun Tang,Cindy Tseng<br>
<strong>机构</strong>: Samsung Research America(三星研究院美国)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade. One of the primary factors behind the advancement of speech technology is self-supervised learning. Most self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications. In this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training. Chunk SSL is optimized with the masked prediction loss and an acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks. A copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook. The proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation. Experimental results on the \textscLibrispeech and \textscMust-C datasets show that the proposed method could achieve very competitive results for speech to text tasks at both streaming and offline modes.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-41] Relevance to Utility: Process-Supervised Rewrite for <mark class="hl-label green">RAG</mark></p>
<p>【速读】： 该论文旨在解决检索增强生成（Retrieval-Augmented Generation, RAG）系统中检索相关性与生成实用性之间的不匹配问题：即 retrieved documents（检索到的文档）可能在主题上相关，但缺乏支持有效推理所需的必要内容。现有“桥接”模块通过重写检索文本以提升生成效果，但无法准确捕捉文档的真实效用。本文提出 R2U 方法，其关键在于直接优化以最大化生成正确答案的概率，通过过程监督（process supervision）实现；由于直接监督成本高，作者进一步设计了一种高效的蒸馏（distillation）管道，利用大语言模型（Large Language Models, LLMs）的监督信号来指导小规模重写模型的学习，从而提升泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15577">https://arxiv.org/abs/2509.15577</a><br>
<strong>作者</strong>: Jaeyoung Kim,Jongho Kim,Seung-won Hwang,Seoho Song,Young-In Song<br>
<strong>机构</strong>: IPAI, Seoul National University (首尔国立大学); Naver Corp (NAVER公司)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Retrieval-Augmented Generation systems often suffer from a gap between optimizing retrieval relevance and generative utility: retrieved documents may be topically relevant but still lack the content needed for effective reasoning during generation. While existing “bridge” modules attempt to rewrite the retrieved text for better generation, we show how they fail to capture true document utility. In this work, we propose R2U, with a key distinction of directly optimizing to maximize the probability of generating a correct answer through process supervision. As such direct observation is expensive, we also propose approximating an efficient distillation pipeline by scaling the supervision from LLMs, which helps the smaller rewriter model generalize better. We evaluate our method across multiple open-domain question-answering benchmarks. The empirical results demonstrate consistent improvements over strong bridging baselines.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-42] LiteLong: Resource-Efficient Long-Context Data Synthesis for <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）训练中高质量长上下文数据合成效率低的问题，现有基于相关性聚合的方法在计算资源消耗和数据工程成本上存在瓶颈。其解决方案的关键在于提出 LiteLong 方法，通过结构化主题组织与多智能体辩论机制实现高效数据合成：首先利用 BISAC 图书分类体系构建层次化主题结构，再借助多个 LLM 的辩论机制生成多样且高质量的主题；随后对每个主题采用轻量级 BM25 检索获取相关文档，并拼接为 128K token 的训练样本。该方法显著降低了计算和数据工程开销，同时保持了与主流长依赖增强方法的兼容性，从而提升了长上下文数据合成的可扩展性和实用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15568">https://arxiv.org/abs/2509.15568</a><br>
<strong>作者</strong>: Junlong Jia,Xing Wu,Chaochen Gao,Ziyang Chen,Zijia Lin,Zhongzhi Li,Weinong Wang,Haotian Xu,Donghui Jin,Debing Zhang,Binghui Guo<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  work in progress</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:High-quality long-context data is essential for training large language models (LLMs) capable of processing extensive documents, yet existing synthesis approaches using relevance-based aggregation face challenges of computational efficiency. We present LiteLong, a resource-efficient method for synthesizing long-context data through structured topic organization and multi-agent debate. Our approach leverages the BISAC book classification system to provide a comprehensive hierarchical topic organization, and then employs a debate mechanism with multiple LLMs to generate diverse, high-quality topics within this structure. For each topic, we use lightweight BM25 retrieval to obtain relevant documents and concatenate them into 128K-token training samples. Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves competitive long-context performance and can seamlessly integrate with other long-dependency enhancement methods. LiteLong makes high-quality long-context data synthesis more accessible by reducing both computational and data engineering costs, facilitating further research in long-context language training.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-43] Small <mark class="hl-label green">LLM</mark> s with Expert Blocks Are Good Enough for Hyperparamter Tuning</p>
<p>【速读】： 该论文旨在解决大规模机器学习（ML）模型中超参数调优（Hyper-parameter Tuning, HPT）因计算成本高和过程不透明而带来的挑战。现有基于大语言模型（Large Language Models, LLMs）的HPT方法通常依赖于参数量超过1000亿的模型，限制了其可扩展性和实用性。本文提出了一种面向小型LLM的专家模块框架（Expert Block Framework），其核心创新在于Trajectory Context Summarizer（TCS）——一个确定性模块，能够将原始训练轨迹转化为结构化上下文信息，使小型LLM具备可靠分析优化进程的能力。实验表明，仅用两个本地运行的小型LLM（phi4:reasoning14B 和 qwen2.5-coder:32B）与10次试验预算，该方案在六项不同任务上的平均性能与GPT-4相差不足0.9个百分点，显著提升了HPT的效率与可解释性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15561">https://arxiv.org/abs/2509.15561</a><br>
<strong>作者</strong>: Om Naphade,Saksham Bansal,Parikshit Pareek<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML) pipelines but becomes computationally expensive and opaque with larger models. Recently, Large Language Models (LLMs) have been explored for HPT, yet most rely on models exceeding 100 billion parameters. We propose an Expert Block Framework for HPT using Small LLMs. At its core is the Trajectory Context Summarizer (TCS), a deterministic block that transforms raw training trajectories into structured context, enabling small LLMs to analyze optimization progress with reliability comparable to larger models. Using two locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9 percentage points of GPT-4 across six diverse tasks.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-44] How important is language for human-like intelligence?</p>
<p>【速读】： 该论文试图解决的核心问题是：语言在人类认知中究竟扮演何种角色——是 merely 作为非语言思维的表达工具，还是具有重塑和拓展人类思维能力的 transformative 功能？为回答此问题，作者提出解决方案的关键在于语言的两个核心属性：其一，语言提供紧凑（compact）的表征形式，使得抽象概念（如精确数量感）的表示与推理变得高效；其二，这些压缩表征是集体心智迭代演化的产物，学习语言即习得大量文化演化形成的抽象知识。因此，一个足够强大的学习系统（无论是生物还是人工的）一旦接触语言，便能逆向构建出对世界的概念性与因果结构的压缩模型，从而推动通用人工智能（general AI）和人类智能关键维度的发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15560">https://arxiv.org/abs/2509.15560</a><br>
<strong>作者</strong>: Gary Lupyan,Hunter Gentry,Martin Zettersten<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We use language to communicate our thoughts. But is language merely the expression of thoughts, which are themselves produced by other, nonlinguistic parts of our minds? Or does language play a more transformative role in human cognition, allowing us to have thoughts that we otherwise could (or would) not have? Recent developments in artificial intelligence (AI) and cognitive science have reinvigorated this old question. We argue that language may hold the key to the emergence of both more general AI systems and central aspects of human intelligence. We highlight two related properties of language that make it such a powerful tool for developing domain–general abilities. First, language offers compact representations that make it easier to represent and reason about many abstract concepts (e.g., exact numerosity). Second, these compressed representations are the iterated output of collective minds. In learning a language, we learn a treasure trove of culturally evolved abstractions. Taken together, these properties mean that a sufficiently powerful learning system exposed to language–whether biological or artificial–learns a compressed model of the world, reverse engineering many of the conceptual and causal structures that support human (and human-like) thought.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-45] Exploring Polyglot Harmony: On Multilingual Data Allocation for <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  Pretraining</p>
<p>【速读】： 该论文旨在解决大规模语言模型（Large Language Models, LLMs）在多语言场景下训练数据比例分配的优化难题，尤其是如何在存在复杂跨语言交互和数据集规模敏感性的条件下，实现高效且鲁棒的多语言性能提升。其解决方案的关键在于提出了一种名为Climb（Cross-Lingual Interaction-aware Multilingual Balancing）的新框架，该框架引入了“跨语言交互感知的语言比例”（cross-lingual interaction-aware language ratio），通过显式量化语言间的相互依赖关系来精确评估每种语言的有效分配量；在此基础上，Climb设计了一个两步优化流程：首先使各语言的边际收益趋于均衡，再最大化所得语言分配向量的模长，从而将原本高度复杂的多语言优化问题转化为可系统求解的形式，并在多个多语言设置中验证了其有效性与优越性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15556">https://arxiv.org/abs/2509.15556</a><br>
<strong>作者</strong>: Ping Guo,Yubing Ren,Binbin Liu,Fengze Liu,Haobin Lin,Yifan Zhang,Bingni Zhang,Taifeng Wang,Yin Zheng<br>
<strong>机构</strong>: ByteDance; Institute of Information Engineering, Chinese Academy of Sciences (中国科学院信息工程研究所)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) have become integral to a wide range of applications worldwide, driving an unprecedented global demand for effective multilingual capabilities. Central to achieving robust multilingual performance is the strategic allocation of language proportions within training corpora. However, determining optimal language ratios is highly challenging due to intricate cross-lingual interactions and sensitivity to dataset scale. This paper introduces Climb (Cross-Lingual Interaction-aware Multilingual Balancing), a novel framework designed to systematically optimize multilingual data allocation. At its core, Climb introduces a cross-lingual interaction-aware language ratio, explicitly quantifying each language’s effective allocation by capturing inter-language dependencies. Leveraging this ratio, Climb proposes a principled two-step optimization procedure–first equalizing marginal benefits across languages, then maximizing the magnitude of the resulting language allocation vectors–significantly simplifying the inherently complex multilingual optimization problem. Extensive experiments confirm that Climb can accurately measure cross-lingual interactions across various multilingual settings. LLMs trained with Climb-derived proportions consistently achieve state-of-the-art multilingual performance, even achieving competitive performance with open-sourced LLMs trained with more tokens.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-46] DNA-Detect<mark class="hl-label green">LLM</mark> : Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】： 该论文旨在解决当前生成式 AI（Generative AI）文本与人类撰写文本之间区分困难的问题，尤其是在大语言模型（Large Language Models, LLMs）快速发展的背景下，二者在特征分布上高度重叠，导致现有检测方法准确率下降且缺乏可解释性。解决方案的关键在于提出一种受DNA修复机制启发的“修复驱动”范式，通过构建理想化的AI生成序列并迭代修正非最优token，将累计修复努力量化为可解释的检测信号，从而实现零样本（zero-shot）条件下对AI生成文本的高精度识别。该方法在多个公开基准数据集上表现出优越的检测性能和对抗攻击鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15550">https://arxiv.org/abs/2509.15550</a><br>
<strong>作者</strong>: Xiaowei Zhu,Yubing Ren,Fang Fang,Qingfeng Tan,Shi Wang,Yanan Cao<br>
<strong>机构</strong>: Institute of Information Engineering, Chinese Academy of Sciences (中国科学院信息工程研究所); School of Cyber Security, University of Chinese Academy of Sciences (中国科学院大学网络空间安全学院); Cyberspace Institute of Advanced Technology, Guangzhou University (广州大学网络空间先进科技研究院); Institute of Computing Science, Chinese Academy of Sciences (中国科学院计算技术研究所)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  NeurIPS 2025 Spotlight</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid advancement of large language models (LLMs) has blurred the line between AI-generated and human-written text. This progress brings societal risks such as misinformation, authorship ambiguity, and intellectual property concerns, highlighting the urgent need for reliable AI-generated text detection methods. However, recent advances in generative language modeling have resulted in significant overlap between the feature distributions of human-written and AI-generated text, blurring classification boundaries and making accurate detection increasingly challenging. To address the above challenges, we propose a DNA-inspired perspective, leveraging a repair-based process to directly and interpretably capture the intrinsic differences between human-written and AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a zero-shot detection method for distinguishing AI-generated and human-written text. The method constructs an ideal AI-generated sequence for each input, iteratively repairs non-optimal tokens, and quantifies the cumulative repair effort as an interpretable detection signal. Empirical evaluations demonstrate that our method achieves state-of-the-art detection performance and exhibits strong robustness against various adversarial attacks and input lengths. Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC and 2.08% in F1 score across multiple public benchmark datasets.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-47] A method for improving multilingual quality and diversity of instruction fine-tuning datasets</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在多语言场景下泛化能力不足的问题，核心瓶颈在于高质量多语言训练数据的稀缺及缺乏有效的构建方法。现有数据选择策略在英语环境中表现良好，但在跨语言迁移时往往失效，因其依赖于简单启发式规则或语言特异性假设。解决方案的关键在于提出一种名为Multilingual Data Quality and Diversity (M-DaQ) 的新方法，通过筛选高质且语义多样化的多语言指令微调（Instruction Fine-Tuning, IFT）样本，显著提升模型的多语言能力。实证结果表明，使用M-DaQ方法微调的模型在18种语言上相比基线模型取得超过60%的胜率优势，且人工评估验证了响应中文化相关性的增强，从而为多语言LLMs的高效训练提供了可复现的高质量数据选择框架。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15549">https://arxiv.org/abs/2509.15549</a><br>
<strong>作者</strong>: Chunguang Zhao,Yilun Liu,Pufan Zeng,Yuanchang Luo,Shimin Tao,Minggui He,Weibin Meng,Song Xu,Ziang Chen,Chen Liu,Hongxia Ma,Li Zhang,Boxing Chen,Daimeng Wei<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large language models (LLMs) to generalize effectively across diverse linguistic and cultural contexts. However, the scarcity of high-quality multilingual training data and corresponding building method remains a critical bottleneck. While data selection has shown promise in English settings, existing methods often fail to generalize across languages due to reliance on simplistic heuristics or language-specific assumptions. In this work, we introduce Multilingual Data Quality and Diversity (M-DaQ), a novel method for improving LLMs multilinguality, by selecting high-quality and semantically diverse multilingual IFT samples. We further conduct the first systematic investigation of the Superficial Alignment Hypothesis (SAH) in multilingual setting. Empirical results across 18 languages demonstrate that models fine-tuned with M-DaQ method achieve significant performance gains over vanilla baselines over 60% win rate. Human evaluations further validate these gains, highlighting the increment of cultural points in the response. We release the M-DaQ code to support future research.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-48] Beyond Words: Enhancing Desire Emotion and Sentiment Recognition with Non-Verbal Cues</p>
<p>【速读】： 该论文旨在解决当前多模态学习在人类欲望（Desire）理解方面研究不足的问题，尤其是现有情感和情绪识别方法主要依赖语言线索而忽视图像作为互补的非语言线索的作用。为应对这一挑战，作者提出了一种对称双向多模态学习框架（Symmetrical Bidirectional Multimodal Learning Framework），其核心创新在于通过低分辨率图像提取全局视觉表征以实现跨模态对齐，并利用高分辨率图像分割后的子图进行掩码图像建模（Masked Image Modeling），从而增强细粒度局部特征的捕捉能力；同时引入文本引导的图像解码器与图像引导的文本解码器，在局部与全局层次上促进深度跨模态交互。此外，采用混合尺度图像策略平衡感知性能与计算成本，最终在MSED数据集上的实验表明该方法在欲望理解、情绪识别和情感分析任务中均取得显著提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15540">https://arxiv.org/abs/2509.15540</a><br>
<strong>作者</strong>: Wei Chen,Tongguan Wang,Feiyue Xue,Junkai Li,Hui Liu,Ying Sha<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  13 page, 5 figures, uploaded by Wei Chen</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Desire, as an intention that drives human behavior, is closely related to both emotion and sentiment. Multimodal learning has advanced sentiment and emotion recognition, but multimodal approaches specially targeting human desire understanding remain underexplored. And existing methods in sentiment analysis predominantly emphasize verbal cues and overlook images as complementary non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition, which enforces mutual guidance between text and image modalities to effectively capture intention-related representations in the image. Specifically, low-resolution images are used to obtain global visual representations for cross-modal alignment, while high resolution images are partitioned into sub-images and modeled with masked image modeling to enhance the ability to capture fine-grained local features. A text-guided image decoder and an image-guided text decoder are introduced to facilitate deep cross-modal interaction at both local and global representations of image information. Additionally, to balance perceptual gains with computation cost, a mixed-scale image strategy is adopted, where high-resolution images are cropped into sub-images for masked modeling. The proposed approach is evaluated on MSED, a multimodal dataset that includes a desire understanding benchmark, as well as emotion and sentiment recognition. Experimental results indicate consistent improvements over other state-of-the-art methods, validating the effectiveness of our proposed method. Specifically, our method outperforms existing approaches, achieving F1-score improvements of 1.1% in desire understanding, 0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is available at: this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-49] How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在处理俚语（slang）时存在的泛化能力与可靠性不足的问题，核心挑战在于模型是否掌握了与人类实际使用一致的俚语结构知识。解决方案的关键在于构建一个系统性的评估框架，从三个维度对比人类与机器生成的俚语用法：1）反映模型对俚语认知偏差的特征；2）体现词汇创新和词复用的创造力；3）作为模型蒸馏金标准示例的信息量。通过比较来自在线俚语词典（Online Slang Dictionary, OSD）的人类标注俚语与GPT-4o及Llama-3生成的俚语，研究发现LLMs虽具备一定的俚语创造性知识，但其理解尚未充分贴近人类使用习惯，难以支持如语言分析等外推任务。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15518">https://arxiv.org/abs/2509.15518</a><br>
<strong>作者</strong>: Siyang Wu,Zhewei Sun<br>
<strong>机构</strong>: University of Chicago (芝加哥大学); Toyota Technological Institute at Chicago (芝加哥丰田技术学院)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Slang is a commonly used type of informal language that poses a daunting challenge to NLP systems. Recent advances in large language models (LLMs), however, have made the problem more approachable. While LLM agents are becoming more widely applied to intermediary tasks such as slang detection and slang interpretation, their generalizability and reliability are heavily dependent on whether these models have captured structural knowledge about slang that align well with human attested slang usages. To answer this question, we contribute a systematic comparison between human and machine-generated slang usages. Our evaluative framework focuses on three core aspects: 1) Characteristics of the usages that reflect systematic biases in how machines perceive slang, 2) Creativity reflected by both lexical coinages and word reuses employed by the slang usages, and 3) Informativeness of the slang usages when used as gold-standard examples for model distillation. By comparing human-attested slang usages from the Online Slang Dictionary (OSD) and slang generated by GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our results suggest that while LLMs have captured significant knowledge about the creative aspects of slang, such knowledge does not align with humans sufficiently to enable LLMs for extrapolative tasks such as linguistic analyses.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-50] <mark class="hl-label green">LLM</mark>  Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective <mark class="hl-label green">LLM</mark>  Inference</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Model, LLM）缓存策略中的查询异构性问题，即不同查询请求具有不同大小时如何实现高效且低成本的缓存选择。传统方法通常假设查询尺寸均匀，忽略了实际场景中查询长度差异带来的组合优化复杂性，从而导致缓存替换过程在计算和统计上更加困难。论文将最优缓存选择建模为一个背包问题（knapsack problem），并提出基于累积（accumulation-based）的策略，在保证缓存更新效果的同时有效控制计算开销。其核心创新在于通过理论分析证明算法的 regret 上界为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msqrt><mrow><mi>M</mi><mi>N</mi><mi>T</mi></mrow></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\sqrt{MNT})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1767em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.13889em;">MNT</span></span></span><span style="top:-2.8867em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em;"><span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，显著优于此前 Berkeley 方法中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>M</mi><mi>N</mi><msqrt><mi>T</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(MN\sqrt{T})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1767em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">MN</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9267em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span><span style="top:-2.8867em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1133em;"><span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的结果，并首次提供了问题依赖型（problem-dependent）的边界，实验基于真实数据表明该方案可降低约12%的总推理成本。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15515">https://arxiv.org/abs/2509.15515</a><br>
<strong>作者</strong>: Hantao Yang,Hong Xie,Defu Lian,Enhong Chen<br>
<strong>机构</strong>: University of Science and Technology of China (中国科学技术大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper revisits the LLM cache bandit problem, with a special focus on addressing the query heterogeneity for cost-effective LLM inference. Previous works often assume uniform query sizes. Heterogeneous query sizes introduce a combinatorial structure for cache selection, making the cache replacement process more computationally and statistically challenging. We treat optimal cache selection as a knapsack problem and employ an accumulation-based strategy to effectively balance computational overhead and cache updates. In theoretical analysis, we prove that the regret of our algorithm achieves an  O(\sqrtMNT)  bound, improving the coefficient of  \sqrtMN  compared to the  O(MN\sqrtT)  result in Berkeley, where  N  is the total number of queries and  M  is the cache size. Additionally, we also provide a problem-dependent bound, which was absent in previous works. The experiment rely on real-world data show that our algorithm reduces the total cost by approximately 12%.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-51] mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment</p>
<p>【速读】： 该论文旨在解决阿拉伯语细粒度可读性分类（19个有序等级）中的准确性与不确定性建模问题，尤其关注高惩罚性误分类对评估指标（如Quadratic Weighted Kappa, QWK）的负面影响。其解决方案的关键在于提出一种模型无关的后处理技术：首先利用合规预测（conformal prediction）生成具有覆盖率保证的预测集，再通过softmax归一化的概率对预测集进行加权平均，从而实现不确定性感知的解码策略。该方法显著降低了误判至相邻级别外的错误，使QWK得分提升1–3点，并在BAREC 2025任务中实现了优异的句子级（测试集84.9%，盲测85.7%）和文档级（73.3%）性能，提升了教育评估场景下人工审核的效率与可靠性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15485">https://arxiv.org/abs/2509.15485</a><br>
<strong>作者</strong>: Ahmed Abdou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present a simple, model-agnostic post-processing technique for fine-grained Arabic readability classification in the BAREC 2025 Shared Task (19 ordinal levels). Our method applies conformal prediction to generate prediction sets with coverage guarantees, then computes weighted averages using softmax-renormalized probabilities over the conformal sets. This uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing high-penalty misclassifications to nearer levels. Our approach shows consistent QWK improvements of 1-3 points across different base models. In the strict track, our submission achieves QWK scores of 84.9%(test) and 85.7% (blind test) for sentence level, and 73.3% for document level. For Arabic educational assessment, this enables human reviewers to focus on a handful of plausible levels, combining statistical guarantees with practical usability.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-52] Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models</p>
<p>【速读】： 该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）在对抗性提示下的安全性问题，特别是其在面对文本和多模态输入时产生有害响应的脆弱性。研究通过系统评估四种主流MLLMs（GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus）在三种危害类别（非法活动、虚假信息和不道德行为）下的表现，发现模型类型和输入模态均是显著预测因子，其中Pixtral 12B最易受攻击（约62%有害响应），而Claude Sonnet 3.5最具抵抗力（约10%）。关键解决方案在于构建一个结构化的红队测试框架与人工标注体系，量化不同模型在对抗场景下的安全边界，从而推动建立更鲁棒的多模态安全基准。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15478">https://arxiv.org/abs/2509.15478</a><br>
<strong>作者</strong>: Madison Van Doren,Casey Ford,Emily Dix<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal large language models (MLLMs) are increasingly used in real world applications, yet their safety under adversarial conditions remains underexplored. This study evaluates the harmlessness of four leading MLLMs (GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to adversarial prompts across text-only and multimodal formats. A team of 26 red teamers generated 726 prompts targeting three harm categories: illegal activity, disinformation, and unethical behaviour. These prompts were submitted to each model, and 17 annotators rated 2,904 model outputs for harmfulness using a 5-point scale. Results show significant differences in vulnerability across models and modalities. Pixtral 12B exhibited the highest rate of harmful responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%). Contrary to expectations, text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones. Statistical analysis confirmed that both model type and input modality were significant predictors of harmfulness. These findings underscore the urgent need for robust, multimodal safety benchmarks as MLLMs are deployed more widely.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-53] Evaluating Multimodal <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  on Spoken Sarcasm Understanding</p>
<p>【速读】： 该论文旨在解决跨模态讽刺检测（sarcasm detection）中多模态信息融合不足的问题，尤其是如何有效利用语音、视觉与文本信息共同理解讽刺意图。其解决方案的关键在于系统性地评估大型语言模型（LLMs）和多模态大语言模型（Multimodal LLMs, MLLMs）在零样本（zero-shot）、少样本（few-shot）及LoRA微调设置下的性能，并引入一种协同门控融合模块（collaborative gating fusion module），将不同模态模型作为特征编码器提取的表示进行深度融合，从而提升对英语（MUStARD++）和中文（MCSD 1.0）数据集上音频-视觉-文本联合讽刺识别的准确性。实验表明，基于语音的模型表现最优，而文本-语音和语音-视觉组合优于单一模态及三模态模型，且Qwen-Omni等MLLMs展现出良好的跨语言泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15476">https://arxiv.org/abs/2509.15476</a><br>
<strong>作者</strong>: Zhu Li,Xiyuan Gao,Yuqing Zhang,Shekhar Nayak,Matt Coler<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Sarcasm detection remains a challenge in natural language understanding, as sarcastic intent often relies on subtle cross-modal cues spanning text, speech, and vision. While prior work has primarily focused on textual or visual-textual sarcasm, comprehensive audio-visual-textual sarcasm understanding remains underexplored. In this paper, we systematically evaluate large language models (LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In addition to direct classification, we explore models as feature encoders, integrating their representations through a collaborative gating fusion module. Experimental results show that audio-based models achieve the strongest unimodal performance, while text-audio and audio-vision combinations outperform unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show competitive zero-shot and fine-tuned performance. Our findings highlight the potential of MLLMs for cross-lingual, audio-visual-textual sarcasm understanding.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-54] PILOT: Steering Synthetic Data Generation with Psychological  Linguistic Output Targeting</p>
<p>【速读】： 该论文旨在解决生成式 AI (Generative AI) 应用中依赖自然语言描述的用户人格特征（persona）进行数据生成时，因模型对属性强调的非预期推断而导致输出控制精度不足的问题。其解决方案的关键在于提出了一种两阶段框架 PILOT（Psychological and Linguistic Output Targeting），通过将自然语言 persona 映射为结构化的心理语言学特征谱（psycholinguistic profile），并在生成阶段基于可测量的维度引导输出。该方法显著提升了输出的一致性和多样性，在多个大语言模型（LLM）上验证了 schema-based steering 相较于传统自然语言引导能有效减少重复性并提高主题纯度与一致性，同时 HPS（混合型）策略在保持输出多样性的同时实现了结构稳定性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15447">https://arxiv.org/abs/2509.15447</a><br>
<strong>作者</strong>: Caitlin Cisar,Emily Sheffield,Joshua Drake,Alden Harrell,Subramanian Chidambaram,Nikita Nangia,Vinayak Arannil,Alex Williams<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generative AI applications commonly leverage user personas as a steering mechanism for synthetic data generation, but reliance on natural language representations forces models to make unintended inferences about which attributes to emphasize, limiting precise control over outputs. We introduce PILOT (Psychological and Linguistic Output Targeting), a two-phase framework for steering large language models with structured psycholinguistic profiles. In Phase 1, PILOT translates natural language persona descriptions into multidimensional profiles with normalized scores across linguistic and psychological dimensions. In Phase 2, these profiles guide generation along measurable axes of variation. We evaluate PILOT across three state-of-the-art LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas under three conditions: Natural-language Persona Steering (NPS), Schema-Based Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate that schema-based approaches significantly reduce artificial-sounding persona repetition while improving output coherence, with silhouette scores increasing from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals a fundamental trade-off: SBS produces more concise outputs with higher topical consistency, while NPS offers greater lexical diversity but reduced predictability. HPS achieves a balance between these extremes, maintaining output variety while preserving structural consistency. Expert linguistic evaluation confirms that PILOT maintains high response quality across all conditions, with no statistically significant differences between steering approaches.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-55] BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition</p>
<p>【速读】： 该论文旨在解决自监督学习（Self-Supervised Learning, SSL）在语音表示学习中伪标签（pseudo-label）生成的效率与信息量之间的权衡问题：现有方法要么依赖外部编码器和多阶段流水线（如HuBERT），虽能生成强标签但计算复杂度高；要么采用简单策略（如BEST-RQ），虽高效却因标签质量弱而限制性能提升。解决方案的关键在于提出BiRQ——一种双层优化框架，其核心创新是利用模型自身中间表示作为伪标签生成器：通过随机投影量化器（random-projection quantizer）对中间特征进行离散化以生成增强标签，同时直接从原始输入提取锚定标签（anchoring labels）来稳定训练并防止塌陷（collapse）。该设计无需外部标签编码器、降低内存开销，并支持端到端迭代式标签精炼，从而在保持低复杂度的同时显著优于BEST-RQ。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15430">https://arxiv.org/abs/2509.15430</a><br>
<strong>作者</strong>: Liuyuan Jiang,Xiaodong Cui,Brian Kingsbury,Tianyi Chen,Lisha Chen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  5 pages including reference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Speech is a rich signal, and labeled audio-text pairs are costly, making self-supervised learning essential for scalable representation learning. A core challenge in speech SSL is generating pseudo-labels that are both informative and efficient: strong labels, such as those used in HuBERT, improve downstream performance but rely on external encoders and multi-stage pipelines, while efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels. We propose BiRQ, a bilevel SSL framework that combines the efficiency of BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key idea is to reuse part of the model itself as a pseudo-label generator: intermediate representations are discretized by a random-projection quantizer to produce enhanced labels, while anchoring labels derived directly from the raw input stabilize training and prevent collapse. Training is formulated as an efficient first-order bilevel optimization problem, solved end-to-end with differentiable Gumbel-softmax selection. This design eliminates the need for external label encoders, reduces memory cost, and enables iterative label refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ while maintaining low complexity and computational efficiency. We validate our method on various datasets, including 960-hour LibriSpeech, 150-hour AMI meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-56] Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models family with scarce data</p>
<p>【速读】： 该论文旨在解决在医疗等数据受限领域中，生成式 AI（Generative AI）进行抽象式摘要（abstractive summarisation）任务时面临的挑战，特别是如何在训练数据稀缺的情况下有效微调非领域特定的编码器-解码器模型以避免过拟合与欠拟合。其解决方案的关键在于通过系统性地评估不同规模检查点（checkpoints）在固定大小验证集上的词法和语义指标变化，揭示了模型在训练过程中的多阶段行为（如epoch-wise double-descent或峰值-下降-恢复现象），并指出使用更大规模模型（如PEGASUS-X）在小数据场景下反而可能导致性能下降，从而为未来在专业领域中设计更鲁棒的微调策略提供了实证依据和实践指导。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15419">https://arxiv.org/abs/2509.15419</a><br>
<strong>作者</strong>: Claudio Benzoni,Martina Langhals,Martin Boeker,Luise Modersohn,Máté E. Maros<br>
<strong>机构</strong>: Institute of AI and Informatics in Medicine (AIIM); Technical University of Munich (慕尼黑工业大学); Department of Biomedical Informatics; Research Group MIDorAI; Medical Faculty Mannheim; Heidelberg University (海德堡大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  14 pages, 4 figures, and 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Regardless of the rapid development of artificial intelligence, abstractive summarisation is still challenging for sensitive and data-restrictive domains like medicine. With the increasing number of imaging, the relevance of automated tools for complex medical text summarisation is expected to become highly relevant. In this paper, we investigated the adaptation via fine-tuning process of a non-domain-specific abstractive summarisation encoder-decoder model family, and gave insights to practitioners on how to avoid over- and underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological reports public dataset. For each model, we comprehensively evaluated two different checkpoints with varying sizes of the same training data. We monitored the models’ performances with lexical and semantic metrics during the training history on the fixed-size validation set. PEGASUS exhibited different phases, which can be related to epoch-wise double-descent, or peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger checkpoint led to a performance detriment. This work highlights the challenges and risks of fine-tuning models with high expressivity when dealing with scarce training data, and lays the groundwork for future investigations into more robust fine-tuning strategies for summarisation models in specialised domains.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-57] Quantifying Uncertainty in Natural Language Explanations of <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  for Question Answering</p>
<p>【速读】： 该论文旨在解决生成式自然语言解释（natural language explanations）中缺乏有效不确定性保障的问题，尤其是在医学问答等噪声环境下，如何为这些解释提供可验证的置信度估计。其解决方案的关键在于提出一种后验且模型无关的不确定性估计框架，能够对LLM生成的自然语言解释进行量化评估；同时设计了一种鲁棒的不确定性估计方法，在存在输入噪声时仍能保持有效的不确定性保证，从而提升解释的可信度与实用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15403">https://arxiv.org/abs/2509.15403</a><br>
<strong>作者</strong>: Yangyi Li,Mengdi Huai<br>
<strong>机构</strong>: Iowa State University (爱荷华州立大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) have shown strong capabilities, enabling concise, context-aware answers in question answering (QA) tasks. The lack of transparency in complex LLMs has inspired extensive research aimed at developing methods to explain large language behaviors. Among existing explanation methods, natural language explanations stand out due to their ability to explain LLMs in a self-explanatory manner and enable the understanding of model behaviors even when the models are closed-source. However, despite these promising advancements, there is no existing work studying how to provide valid uncertainty guarantees for these generated natural language explanations. Such uncertainty quantification is critical in understanding the confidence behind these explanations. Notably, generating valid uncertainty estimates for natural language explanations is particularly challenging due to the auto-regressive generation process of LLMs and the presence of noise in medical inquiries. To bridge this gap, in this work, we first propose a novel uncertainty estimation framework for these generated natural language explanations, which provides valid uncertainty guarantees in a post-hoc and model-agnostic manner. Additionally, we also design a novel robust uncertainty estimation method that maintains valid uncertainty guarantees even under noise. Extensive experiments on QA tasks demonstrate the desired performance of our methods.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-58] Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决大型音频语言模型（Large Audio Language Models, LALMs）在有限语音数据条件下进行微调时性能受限的问题，尤其是在文本标签数据充足但语音-标签配对数据稀缺的场景下。其解决方案的关键在于系统性地比较三种微调策略——仅用文本微调、直接混合微调和课程学习（curriculum learning），发现即使少量语音数据（2–5%）也能显著提升模型性能，且课程学习在数据稀缺时尤为有效；此外，在跨语言语音理解任务中，通过结合源语言语音数据与目标语言文本及最小量目标语言语音数据，可实现高效适应。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15389">https://arxiv.org/abs/2509.15389</a><br>
<strong>作者</strong>: Youngwon Choi,Jaeyoon Jung,Hyeonyu Kim,Huu-Kim Nguyen,Hwayeon Kim<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  4 pages (excluding references), 2 figures, submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Audio Language Models (LALMs) have emerged as powerful tools for speech-related tasks but remain underexplored for fine-tuning, especially with limited speech data. To bridge this gap, we systematically examine how different fine-tuning schemes including text-only, direct mixing, and curriculum learning affect spoken language understanding (SLU), focusing on scenarios where text-label pairs are abundant while paired speech-label data are limited. Results show that LALMs already achieve competitive performance with text-only fine-tuning, highlighting their strong generalization ability. Adding even small amounts of speech data (2-5%) yields substantial further gains, with curriculum learning particularly effective under scarce data. In cross-lingual SLU, combining source-language speech data with target-language text and minimal target-language speech data enables effective adaptation. Overall, this study provides practical insights into the LALM fine-tuning under realistic data constraints.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-59] Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios</p>
<p>【速读】： 该论文旨在解决多语言信息检索（Multilingual Information Retrieval, MLIR）研究与实际部署之间存在的显著差距问题，即多数现有研究在孤立环境中评估性能，难以适配真实世界的多语言应用场景。其解决方案的关键在于利用《古兰经》多语种语料库的独特特性，提出一种结合跨语言（cross-lingual）与单语言（monolingual）训练策略的混合方法（mixed method），并通过实证验证该方法在不同检索场景下均能取得优异效果。该方案不仅提升了嵌入空间的质量和多语言检索的有效性，还强调了部署单一轻量级模型的成本效益，从而为现实世界中的MLIR应用提供了高效可行的技术路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15380">https://arxiv.org/abs/2509.15380</a><br>
<strong>作者</strong>: Vera Pavlova,Mohammed Makhlouf<br>
<strong>机构</strong>: rttl labs( rttl 实验室)<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Despite recent advancements in Multilingual Information Retrieval (MLIR), a significant gap remains between research and practical deployment. Many studies assess MLIR performance in isolated settings, limiting their applicability to real-world scenarios. In this work, we leverage the unique characteristics of the Quranic multilingual corpus to examine the optimal strategies to develop an ad-hoc IR system for the Islamic domain that is designed to satisfy users’ information needs in multiple languages. We prepared eleven retrieval models employing four training approaches: monolingual, cross-lingual, translate-train-all, and a novel mixed method combining cross-lingual and monolingual techniques. Evaluation on an in-domain dataset demonstrates that the mixed approach achieves promising results across diverse retrieval scenarios. Furthermore, we provide a detailed analysis of how different training configurations affect the embedding space and their implications for multilingual retrieval effectiveness. Finally, we discuss deployment considerations, emphasizing the cost-efficiency of deploying a single versatile, lightweight model for real-world MLIR applications.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-60] Frustratingly Easy Data Augmentation for Low-Resource ASR <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决低资源自动语音识别（Automatic Speech Recognition, ASR）中训练数据匮乏导致模型性能受限的问题。其解决方案的关键在于提出三种自包含的数据增强方法：首先通过基于词表替换、随机替换或大语言模型（Large Language Model, LLM）生成新颖文本，再利用文本转语音（Text-to-Speech, TTS）技术合成语音数据，从而在仅使用原始标注数据的基础上扩充训练集。实验表明，该方法在四种极低资源语言（Vatlongos、Nashta、Shinekhen Buryat 和 Kakabe）上均显著提升模型性能，例如在Nashta语言上实现14.3%的绝对词错误率（Word Error Rate, WER）降低，且对高资源语言如英语也具有效用，证明了方法的广泛适用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15373">https://arxiv.org/abs/2509.15373</a><br>
<strong>作者</strong>: Katsumi Ibaraki,David Chiang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  5 pages, 2 figures, 2 tables, submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper introduces three self-contained data augmentation methods for low-resource Automatic Speech Recognition (ASR). Our techniques first generate novel text–using gloss-based replacement, random replacement, or an LLM-based approach–and then apply Text-to-Speech (TTS) to produce synthetic audio. We apply these methods, which leverage only the original annotated data, to four languages with extremely limited resources (Vatlongos, Nashta, Shinekhen Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a combination of the original audio and generated synthetic data yields significant performance gains, including a 14.3% absolute WER reduction for Nashta. The methods prove effective across all four low-resource languages and also show utility for high-resource languages like English, demonstrating their broad applicability.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-61] Speech Language Models for Under-Represented Languages: Insights from Wolof</p>
<p>【速读】： 该论文旨在解决低资源语言（如西非的沃洛夫语）在自动语音识别（ASR）和语音翻译等任务中模型性能不足的问题。其核心解决方案在于：首先，构建大规模、自发性且高质量的沃洛夫语语音数据集，并通过持续预训练HuBERT模型显著提升ASR性能；其次，将该语音编码器集成到沃洛夫语大语言模型（LLM）中，训练出首个针对该语言的语音大语言模型（Speech LLM），从而扩展至语音翻译等多任务能力；最后，引入多步思维链（Chain-of-Thought）机制，在转录或翻译前进行推理，进一步优化输出质量。关键创新在于结合高质量数据与结构化推理机制，实现了对低资源语言语音任务的有效建模。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15362">https://arxiv.org/abs/2509.15362</a><br>
<strong>作者</strong>: Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present our journey in training a speech language model for Wolof, an underrepresented language spoken in West Africa, and share key insights. We first emphasize the importance of collecting large-scale, spontaneous, high-quality speech data, and show that continued pretraining HuBERT on this dataset outperforms both the base model and African-centric models on ASR. We then integrate this speech encoder into a Wolof LLM to train the first Speech LLM for this language, extending its capabilities to tasks such as speech translation. Furthermore, we explore training the Speech LLM to perform multi-step Chain-of-Thought before transcribing or translating. Our results show that the Speech LLM not only improves speech recognition but also performs well in speech translation. The models and the code will be openly shared.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-62] Beyond Spurious Signals: Debiasing Multimodal <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  via Counterfactual Inference and Adaptive Expert Routing <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）在复杂多模态推理任务中因依赖表面相关性（spurious correlations）而导致的鲁棒性和泛化能力不足的问题。其解决方案的关键在于提出一种基于因果中介（causal mediation）的去偏框架：首先通过反事实样本区分核心语义与虚假的文本和视觉上下文，以激活训练阶段的去偏机制；其次引入具有动态路由机制的专家混合（Mixture-of-Experts, MoE）架构，选择性地调用针对不同模态的去偏专家，从而实现更精准的多模态去偏。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15361">https://arxiv.org/abs/2509.15361</a><br>
<strong>作者</strong>: Zichen Wu,Hsiu-Yuan Huang,Yunfang Wu<br>
<strong>机构</strong>: School of Computer Science, Peking University (北京大学计算机学院); MOE Key Laboratory of Computational Linguistics, Peking University (教育部计算语言学重点实验室，北京大学); National Key Laboratory for Multimedia Information Processing, Peking University (多媒体信息处理全国重点实验室，北京大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>)<br>
<strong>备注</strong>:  Accepted by EMNLP 2025 Findings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal Large Language Models (MLLMs) have shown substantial capabilities in integrating visual and textual information, yet frequently rely on spurious correlations, undermining their robustness and generalization in complex multimodal reasoning tasks. This paper addresses the critical challenge of superficial correlation bias in MLLMs through a novel causal mediation-based debiasing framework. Specially, we distinguishing core semantics from spurious textual and visual contexts via counterfactual examples to activate training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture with dynamic routing to selectively engages modality-specific debiasing experts. Empirical evaluation on multimodal sarcasm detection and sentiment analysis tasks demonstrates that our framework significantly surpasses unimodal debiasing strategies and existing state-of-the-art models.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-63] Real Fake or Manipulated? Detecting Machine-Influenced Text <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前机器生成文本（Machine Generated Text, MGT）检测方法忽视细粒度使用意图的问题，即区分文本是完全由大语言模型（Large Language Model, LLM）生成、人类撰写、LLM润色（machine-polished）还是LLM翻译（machine-translated），而不仅仅是二分类地判断文本是否为机器生成。解决方案的关键在于提出一种分层且长度鲁棒的检测框架HERO（HiErarchical, length-RObust machine-influenced text detector），其通过集成多个针对不同文本长度训练的专用模型，并引入子类别引导（Subcategory Guidance）模块来增强易混淆类别的分离能力（如不同源语言的机器翻译文本），从而在五种LLM和六个领域上的实验中显著优于现有最先进方法，平均提升2.5–3 mAP。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15350">https://arxiv.org/abs/2509.15350</a><br>
<strong>作者</strong>: Yitong Wang,Zhongping Zhang,Margherita Piana,Zheng Zhou,Peter Gerstoft,Bryan A. Plummer<br>
<strong>机构</strong>: Boston University (波士顿大学); University of California, San Diego (加州大学圣地亚哥分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted to EMNLP 2025 Findings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Model (LLMs) can be used to write or modify documents, presenting a challenge for understanding the intent behind their use. For example, benign uses may involve using LLM on a human-written document to improve its grammar or to translate it into another language. However, a document entirely produced by a LLM may be more likely to be used to spread misinformation than simple translation (\eg, from use by malicious actors or simply by hallucinating). Prior works in Machine Generated Text (MGT) detection mostly focus on simply identifying whether a document was human or machine written, ignoring these fine-grained uses. In this paper, we introduce a HiErarchical, length-RObust machine-influenced text detector (HERO), which learns to separate text samples of varying lengths from four primary types: human-written, machine-generated, machine-polished, and machine-translated. HERO accomplishes this by combining predictions from length-specialist models that have been trained with Subcategory Guidance. Specifically, for categories that are easily confused (\eg, different source languages), our Subcategory Guidance module encourages separation of the fine-grained categories, boosting performance. Extensive experiments across five LLMs and six domains demonstrate the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on average.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-64] Quantifying Self-Awareness of Knowledge in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）中幻觉预测（Hallucination Prediction）性能被误认为是模型自我意识（Self-awareness）的问题，指出当前方法可能依赖于问题侧的捷径（Question-side Shortcuts），而非真正的模型内省能力。其解决方案的关键在于提出近似问题侧效应（Approximate Question-side Effect, AQE）以量化问题感知的贡献，并引入一种名为“单字作答语义压缩”（Semantic Compression by Answering in One word, SCAO）的新方法，通过强化模型侧信号来提升对真实自知能力的建模效果，实验证明SCAO在弱化问题侧线索时仍保持强且一致的性能，从而更有效地促进LLMs中真正的自我意识行为。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15339">https://arxiv.org/abs/2509.15339</a><br>
<strong>作者</strong>: Yeongbin Seo,Dongha Lee,Jinyoung Yeo<br>
<strong>机构</strong>: Yonsei University (延世大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Hallucination prediction in large language models (LLMs) is often interpreted as a sign of self-awareness. However, we argue that such performance can arise from question-side shortcuts rather than true model-side introspection. To disentangle these factors, we propose the Approximate Question-side Effect (AQE), which quantifies the contribution of question-awareness. Our analysis across multiple datasets reveals that much of the reported success stems from exploiting superficial patterns in questions. We further introduce SCAO (Semantic Compression by Answering in One word), a method that enhances the use of model-side signals. Experiments show that SCAO achieves strong and consistent performance, particularly in settings with reduced question-side cues, highlighting its effectiveness in fostering genuine self-awareness in LLMs.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-65] PolBiX: Detecting <mark class="hl-label green">LLM</mark> s Political Bias in Fact-Checking through X-phemisms</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在客观评估任务（如事实核查）中可能存在的政治偏见问题。现有研究虽指出LLMs普遍存在左倾偏好，但其对下游任务的实际影响尚不明确。为此，作者通过构造语义等价但政治内涵不同的最小配对（minimal pairs）——即用委婉语（euphemism）或贬损语（dysphemism）替换原句中的关键词——系统性地测试LLMs在判断真假时的一致性。关键发现是：相较于政治倾向，判断性词汇（judgmental words）的存在显著影响模型的真伪判断结果；尽管部分模型表现出政治偏见倾向，但通过提示词明确要求客观性并不能有效缓解这一现象。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15335">https://arxiv.org/abs/2509.15335</a><br>
<strong>作者</strong>: Charlott Jakob,David Harbecke,Patrick Parschan,Pia Wenzel Neves,Vera Schmitt<br>
<strong>机构</strong>: Quality &amp; Usability Lab, Technische Universität Berlin (柏林工业大学质量与可用性实验室); German Research Center for Artificial Intelligence (DFKI), Berlin (德国人工智能研究中心(DFKI)); Department of Media and Communication, Ludwig-Maximilians-Universität München (慕尼黑路德维希马克西米利安大学媒体与传播系)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models are increasingly used in applications requiring objective assessment, which could be compromised by political bias. Many studies found preferences for left-leaning positions in LLMs, but downstream effects on tasks like fact-checking remain underexplored. In this study, we systematically investigate political bias through exchanging words with euphemisms or dysphemisms in German claims. We construct minimal pairs of factually equivalent claims that differ in political connotation, to assess the consistency of LLMs in classifying them as true or false. We evaluate six LLMs and find that, more than political leaning, the presence of judgmental words significantly influences truthfulness assessment. While a few models show tendencies of political bias, this is not mitigated by explicitly calling for objectivism in prompts.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-66] Fleming-R1: Toward Expert-Level Medical <mark class="hl-label green">Reasoning</mark>  via Reinforcement Learning</p>
<p>【速读】： 该论文旨在解决大语言模型在医疗应用中难以实现专家级临床推理的问题，核心挑战在于既要保证答案的准确性，又要具备可验证的推理过程透明性。解决方案的关键在于三项互补创新：一是采用面向推理的数据策略（Reasoning-Oriented Data Strategy, RODS），通过整合结构化医学问答数据与知识图谱引导的合成方法提升罕见疾病、药物及多跳推理链的覆盖；二是利用思维链冷启动（Chain-of-Thought cold start）从教师模型中蒸馏高质量推理轨迹，建立可靠的推理先验；三是设计基于可验证奖励的两阶段强化学习框架（Reinforcement Learning from Verifiable Rewards, RLVR），结合组相对策略优化实现核心推理技能的巩固，并通过自适应难样本挖掘针对性改进持续失败模式。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15279">https://arxiv.org/abs/2509.15279</a><br>
<strong>作者</strong>: Chi Liu,Derek Li,Yan Shu,Robin Chen,Derek Duan,Teng Fang,Bryan Dai<br>
<strong>机构</strong>: Ubiquant<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While large language models show promise in medical applications, achieving expert-level clinical reasoning remains challenging due to the need for both accurate answers and transparent reasoning processes. To address this challenge, we introduce Fleming-R1, a model designed for verifiable medical reasoning through three complementary innovations. First, our Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets with knowledge-graph-guided synthesis to improve coverage of underrepresented diseases, drugs, and multi-hop reasoning chains. Second, we employ Chain-of-Thought (CoT) cold start to distill high-quality reasoning trajectories from teacher models, establishing robust inference priors. Third, we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR) framework using Group Relative Policy Optimization, which consolidates core reasoning skills while targeting persistent failure modes through adaptive hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers substantial parameter-efficient improvements: the 7B variant surpasses much larger baselines, while the 32B model achieves near-parity with GPT-4o and consistently outperforms strong open-source alternatives. These results demonstrate that structured data design, reasoning-oriented initialization, and verifiable reinforcement learning can advance clinical reasoning beyond simple accuracy optimization. We release Fleming-R1 publicly to promote transparent, reproducible, and auditable progress in medical AI, enabling safer deployment in high-stakes clinical environments.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-67] oxicity Red-Teaming: Benchmarking <mark class="hl-label green">LLM</mark>  Safety in Singapores Low-Resource Languages <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在低资源、多语言环境下安全机制研究不足的问题，特别是在新加坡多元语言环境（包括新加坡英语、中文、马来语和泰米尔语）中的安全性评估缺失。其解决方案的关键在于提出并构建了SGToxicGuard——一个面向新加坡多语种场景的新型数据集与评测框架，采用红队测试（red-teaming）方法系统性地探测LLMs在对话、问答和内容生成三类真实场景下的安全漏洞，并通过实证实验揭示当前主流多语言LLMs在文化敏感性和毒性防御方面的显著短板，从而为构建更安全、更具包容性的多语种AI系统提供可操作的改进方向。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15260">https://arxiv.org/abs/2509.15260</a><br>
<strong>作者</strong>: Yujia Hu,Ming Shan Hee,Preslav Nakov,Roy Ka-Wei Lee<br>
<strong>机构</strong>: Singapore University of Technology and Design (新加坡科技设计大学); Mohamed bin Zayed University of Artificial Intelligence (穆罕默德·本·扎耶德人工智能大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  9 pages, EMNLP 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The advancement of Large Language Models (LLMs) has transformed natural language processing; however, their safety mechanisms remain under-explored in low-resource, multilingual settings. Here, we aim to bridge this gap. In particular, we introduce \textsfSGToxicGuard, a novel dataset and evaluation framework for benchmarking LLM safety in Singapore’s diverse linguistic context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a red-teaming approach to systematically probe LLM vulnerabilities in three real-world scenarios: \textitconversation, \textitquestion-answering, and \textitcontent composition. We conduct extensive experiments with state-of-the-art multilingual LLMs, and the results uncover critical gaps in their safety guardrails. By offering actionable insights into cultural sensitivity and toxicity mitigation, we lay the foundation for safer and more inclusive AI systems in linguistically diverse environments.\footnoteLink to the dataset: this https URL. \textcolorredDisclaimer: This paper contains sensitive content that may be disturbing to some readers.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-68] Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha</p>
<p>【速读】： 该论文旨在解决低资源语言（特别是不丹的国语——宗卡语）在大语言模型（Large Language Models, LLMs）中因缺乏高效分词器（tokenizer）而导致的自然语言处理（Natural Language Processing, NLP）性能受限问题。当前主流预训练分词器如Byte-Pair Encoding (BPE)、WordPiece 和 SentencePiece（Unigram）多针对高资源语言优化，在宗卡语上表现不佳，制约了其下游任务（如翻译、分类和文本生成）的发展。解决方案的关键在于系统评估这三种常见分词算法在宗卡语上的适用性，并通过子词肥力（Subword Fertility）、持续词比例（Proportion of Continued Words）、归一化序列长度（Normalized Sequence Length）及执行时间等指标进行量化比较，最终发现 SentencePiece 在宗卡语中表现最优，为构建宗卡语大语言模型提供了可行的技术路径与实证依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15255">https://arxiv.org/abs/2509.15255</a><br>
<strong>作者</strong>: Tandin Wangchuk,Tad Gonsalves<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  10 Pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) are gaining popularity and improving rapidly. Tokenizers are crucial components of natural language processing, especially for LLMs. Tokenizers break down input text into tokens that models can easily process while ensuring the text is accurately represented, capturing its meaning and structure. Effective tokenizers enhance the capabilities of LLMs by improving a model’s understanding of context and semantics, ultimately leading to better performance in various downstream tasks, such as translation, classification, sentiment analysis, and text generation. Most pre-trained tokenizers are suitable for high-resource languages like English but perform poorly for low-resource languages. Dzongkha, Bhutan’s national language spoken by around seven hundred thousand people, is a low-resource language, and its linguistic complexity poses unique NLP challenges. Despite some progress, significant research in Dzongkha NLP is lacking, particularly in tokenization. This study evaluates the training and performance of three common tokenization algorithms in comparison to other popular methods. Specifically, Byte-Pair Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their suitability for Dzongkha. Performance was assessed using metrics like Subword Fertility, Proportion of Continued Words, Normalized Sequence Length, and execution time. The results show that while all three algorithms demonstrate potential, SentencePiece is the most effective for Dzongkha tokenization, paving the way for further NLP advancements. This underscores the need for tailored approaches for low-resource languages and ongoing research. In this study, we presented three tokenization algorithms for Dzongkha, paving the way for building Dzongkha Large Language Models.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-69] Synthetic bootstrapped pretraining</p>
<p>【速读】： 该论文旨在解决语言模型（Language Model, LM）预训练过程中难以高效建模文档间相关性的问题。标准预训练方法主要关注单文档内词元间的因果关联，而忽略了跨文档的丰富且可学习的相关性，这限制了模型性能的进一步提升。解决方案的关键在于提出合成自举预训练（Synthetic Bootstrapped Pretraining, SBP），其核心机制是先从预训练数据中学习文档间的关系模型，再利用该模型生成大量新语料用于联合训练。SBP不仅显著优于强基线（重复训练），还能获得接近拥有20倍更多唯一数据的理论最优上界（oracle upper bound）所带来性能增益的相当比例，且生成文档具有概念抽象与重构能力，体现出自然的贝叶斯解释：合成器隐式地学习了相关文档共享的潜在概念。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15248">https://arxiv.org/abs/2509.15248</a><br>
<strong>作者</strong>: Zitong Yang,Aonan Zhang,Hong Liu,Tatsunori Hashimoto,Emmanuel Candès,Chong Wang,Ruoming Pang<br>
<strong>机构</strong>: Apple; Stanford University (斯坦福大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM) pretraining procedure that first learns a model of relations between documents from the pretraining dataset and then leverages it to synthesize a vast new corpus for joint training. While the standard pretraining teaches LMs to learn causal correlations among tokens within a single document, it is not designed to efficiently model the rich, learnable inter-document correlations that can potentially lead to better performance. We validate SBP by designing a compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T tokens from scratch. We find SBP consistently improves upon a strong repetition baseline and delivers a significant fraction of performance improvement attainable by an oracle upper bound with access to 20x more unique data. Qualitative analysis reveals that the synthesized documents go beyond mere paraphrases – SBP first abstracts a core concept from the seed material and then crafts a new narration on top of it. Besides strong empirical performance, SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns to abstract the latent concepts shared between related documents.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-70] M-PACE: Mother Child Framework for Multimodal Compliance</p>
<p>【速读】： 该论文旨在解决多模态内容（如广告中的图像与文本）在品牌、法律或平台合规性审查中面临的复杂挑战，传统方法依赖于离散的多阶段流水线，存在架构碎片化、可扩展性差及难以适应动态规则等问题。解决方案的关键在于提出一种名为Multimodal Parameter Agnostic Compliance Engine (M-PACE) 的统一框架，其核心创新是采用“母-子”多模态大语言模型（Multimodal Large Language Models, MLLMs）结构，在单次推理过程中联合处理视觉和语言输入，实现对超过15个合规属性的高效评估；通过母模型对子模型输出进行质量控制，显著降低对人工审核的依赖，同时在保证准确率的前提下将推理成本降低超31倍，展现出高性价比的实时部署能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15241">https://arxiv.org/abs/2509.15241</a><br>
<strong>作者</strong>: Shreyash Verma,Amit Kesari,Vinayak Trivedi,Anupam Purwar,Ratnesh Jamidar<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  The M-PACE framework uses a “mother-child” AI model system to automate and unify compliance checks for ads, reducing costs while maintaining high accuracy</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Ensuring that multi-modal content adheres to brand, legal, or platform-specific compliance standards is an increasingly complex challenge across domains. Traditional compliance frameworks typically rely on disjointed, multi-stage pipelines that integrate separate modules for image classification, text extraction, audio transcription, hand-crafted checks, and rule-based merges. This architectural fragmentation increases operational overhead, hampers scalability, and hinders the ability to adapt to dynamic guidelines efficiently. With the emergence of Multimodal Large Language Models (MLLMs), there is growing potential to unify these workflows under a single, general-purpose framework capable of jointly processing visual and textual content. In light of this, we propose Multimodal Parameter Agnostic Compliance Engine (M-PACE), a framework designed for assessing attributes across vision-language inputs in a single pass. As a representative use case, we apply M-PACE to advertisement compliance, demonstrating its ability to evaluate over 15 compliance-related attributes. To support structured evaluation, we introduce a human-annotated benchmark enriched with augmented samples that simulate challenging real-world conditions, including visual obstructions and profanity injection. M-PACE employs a mother-child MLLM setup, demonstrating that a stronger parent MLLM evaluating the outputs of smaller child models can significantly reduce dependence on human reviewers, thereby automating quality control. Our analysis reveals that inference costs reduce by over 31 times, with the most efficient models (Gemini 2.0 Flash as child MLLM selected by mother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5 Pro with comparable accuracy, highlighting the trade-off between cost and output quality achieved in real time by M-PACE in real life deployment over advertising data.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-71] ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding</p>
<p>【速读】： 该论文旨在解决视觉语言模型（Vision-Language Models, VLMs）在推理阶段速度缓慢的问题，现有基于推测解码（Speculative Decoding）的方法在VLM上仅能实现约1.5倍的加速，远低于大语言模型（Large Language Models, LLMs）的性能表现。其核心挑战在于如何在不损害文本理解能力的前提下，有效压缩和利用冗余图像信息。解决方案的关键在于提出一种面向视觉感知的推测解码框架（Vision-Aware Speculative Decoding, ViSpec），其创新点包括：(1) 引入轻量级视觉适配模块（vision adaptor module），将图像token压缩为紧凑表示并嵌入到草稿模型的注意力机制中，同时保留原始图像位置信息；(2) 提取每张输入图像的全局特征向量，并将其注入后续所有文本token以增强多模态一致性；(3) 构建专用训练数据集，通过重构已有数据并使用目标VLM生成扩展响应来缓解长序列多模态数据稀缺问题，同时设计训练策略防止草稿模型依赖目标模型隐藏状态导致的捷径学习（shortcut learning）。实验表明，ViSpec是首个在VLM推测解码中实现显著加速（&gt;1.5x）的方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15235">https://arxiv.org/abs/2509.15235</a><br>
<strong>作者</strong>: Jialiang Kang,Han Shu,Wenshuo Li,Yingjie Zhai,Xinghao Chen<br>
<strong>机构</strong>: Peking University (北京大学); Huawei Noah’s Ark Lab<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  12 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), yet its application to vision-language models (VLMs) remains underexplored, with existing methods achieving only modest speedups (1.5x). This gap is increasingly significant as multimodal capabilities become central to large-scale models. We hypothesize that large VLMs can effectively filter redundant image information layer by layer without compromising textual comprehension, whereas smaller draft models struggle to do so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor module to compress image tokens into a compact representation, which is seamlessly integrated into the draft model’s attention mechanism while preserving original image positional information. Additionally, we extract a global feature vector for each input image and augment all subsequent text tokens with this feature to enhance multimodal coherence. To overcome the scarcity of multimodal datasets with long assistant responses, we curate a specialized training dataset by repurposing existing datasets and generating extended outputs using the target VLM with modified prompts. Our training strategy mitigates the risk of the draft model exploiting direct access to the target model’s hidden states, which could otherwise lead to shortcut learning when training solely on target model outputs. Extensive experiments validate ViSpec, achieving, to our knowledge, the first substantial speedup in VLM speculative decoding.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-72] Video2Roleplay: A Multimodal Dataset and Framework for Video-Guided Role-playing <mark class="hl-label green">Agents</mark>  <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前角色扮演代理（Role-playing Agents, RPAs）主要依赖静态角色档案而忽视人类固有的动态感知能力的问题。其核心解决方案在于引入动态角色档案（dynamic role profiles），通过融合视频模态信息来增强RPAs的交互表现力。关键创新在于构建了一个大规模高质量数据集Role-playing-Video60k，并设计了一个结合自适应时间采样与动态/静态角色档案表示的综合框架：其中动态档案通过自适应采样视频帧并按时间顺序输入大语言模型（LLM）生成，静态档案则包含训练阶段从视频中提取的角色对话和推理阶段的输入视频摘要上下文。这种联合集成机制显著提升了RPAs的响应生成能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15233">https://arxiv.org/abs/2509.15233</a><br>
<strong>作者</strong>: Xueqiao Zhang,Chao Zhang,Jingtao Xu,Yifan Zhu,Xin Shi,Yi Yang,Yawei Luo<br>
<strong>机构</strong>: Zhejiang University (浙江大学)<br>
<strong>类目</strong>: Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted at EMNLP2025 Main</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Role-playing agents (RPAs) have attracted growing interest for their ability to simulate immersive and interactive characters. However, existing approaches primarily focus on static role profiles, overlooking the dynamic perceptual abilities inherent to humans. To bridge this gap, we introduce the concept of dynamic role profiles by incorporating video modality into RPAs. To support this, we construct Role-playing-Video60k, a large-scale, high-quality dataset comprising 60k videos and 700k corresponding dialogues. Based on this dataset, we develop a comprehensive RPA framework that combines adaptive temporal sampling with both dynamic and static role profile representations. Specifically, the dynamic profile is created by adaptively sampling video frames and feeding them to the LLM in temporal order, while the static profile consists of (1) character dialogues from training videos during fine-tuning, and (2) a summary context from the input video during inference. This joint integration enables RPAs to generate greater responses. Furthermore, we propose a robust evaluation method covering eight metrics. Experimental results demonstrate the effectiveness of our framework, highlighting the importance of dynamic role profiles in developing RPAs.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-73] Learning Analytics from Spoken Discussion Dialogs in Flipped Classroom</p>
<p>【速读】： 该论文旨在解决如何通过分析翻转课堂（Flipped Classroom）中学生小组讨论的口语对话内容，实现对小组学习过程与结果的量化理解，并进一步实现对学习成果的自动预测问题。其解决方案的关键在于：首先通过人工转录和多工具特征提取技术从课堂讨论录音中挖掘出结构化指标；随后利用统计分析识别与学习成效显著相关的对话特征；最终采用机器学习算法基于这些特征构建分类模型，以预测小组学习结果为高、中、低三个等级，实验表明该方法可达到78.9%的最佳预测准确率，验证了从面对面讨论对话中自动推断学习成效的可行性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12399">https://arxiv.org/abs/2301.12399</a><br>
<strong>作者</strong>: Hang Su,Borislav Dzodzo,Changlun Li,Danyang Zhao,Hao Geng,Yunxiang Li,Sidharth Jaggi,Helen Meng<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The flipped classroom is a new pedagogical strategy that has been gaining increasing importance recently. Spoken discussion dialog commonly occurs in flipped classroom, which embeds rich information indicating processes and progression of students’ learning. This study focuses on learning analytics from spoken discussion dialog in the flipped classroom, which aims to collect and analyze the discussion dialogs in flipped classroom in order to get to know group learning processes and outcomes. We have recently transformed a course using the flipped classroom strategy, where students watched video-recorded lectures at home prior to group-based problem-solving discussions in class. The in-class group discussions were recorded throughout the semester and then transcribed manually. After features are extracted from the dialogs by multiple tools and customized processing techniques, we performed statistical analyses to explore the indicators that are related to the group learning outcomes from face-to-face discussion dialogs in the flipped classroom. Then, machine learning algorithms are applied to the indicators in order to predict the group learning outcome as High, Mid or Low. The best prediction accuracy reaches 78.9%, which demonstrates the feasibility of achieving automatic learning outcome prediction from group discussion dialog in flipped classroom.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-74] VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决实时语音合成（text-to-speech, TTS）系统中初始延迟高、难以实现零样本流式生成的问题。现有方法通常存在较长的启动延迟或依赖大量训练数据，限制了其在低延迟场景下的应用。解决方案的关键在于提出 VoXtream，一个完全自回归的零样本流式TTS系统，其核心创新包括：基于单调对齐策略（monotonic alignment scheme）的增量式音素Transformer，用于实时映射输入音素到音频标记；结合动态前瞻机制（dynamic look-ahead）避免延迟启动；以及由时序Transformer和深度Transformer分别预测语义/时长标记与声学标记的多阶段架构。该设计实现了目前已知公开流式TTS中最低的初始延迟（GPU上仅102 ms），且在中小规模训练语料（9k小时）下性能优于或媲美更大模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15969">https://arxiv.org/abs/2509.15969</a><br>
<strong>作者</strong>: Nikita Torgashov,Gustav Eje Henter,Gabriel Skantze<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>)<br>
<strong>备注</strong>:  5 pages, 1 figure, submitted to IEEE ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present VoXtream, a fully autoregressive, zero-shot streaming text-to-speech (TTS) system for real-time use that begins speaking from the first word. VoXtream directly maps incoming phonemes to audio tokens using a monotonic alignment scheme and a dynamic look-ahead that does not delay onset. Built around an incremental phoneme transformer, a temporal transformer predicting semantic and duration tokens, and a depth transformer producing acoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay among publicly available streaming TTS: 102 ms on GPU. Despite being trained on a mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several metrics, while delivering competitive quality in both output- and full-streaming settings. Demo and code are available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-75] Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech</p>
<p>【速读】： 该论文旨在解决运动后语音中不同类型暂停（包括语义停顿、呼吸停顿及呼吸-语义复合停顿）的识别与区分问题，从而实现对恢复速率、肺功能及 exertion（用力程度）相关异常的有效评估。其解决方案的关键在于构建了一个同步音频与呼吸信号的标注数据集，并在此基础上系统性地探索了多种深度学习模型（如GRU、1D CNN-LSTM、AlexNet、VGG16）、声学特征（MFCC、MFB）以及分层Wav2Vec2表示在暂停检测和用力等级分类任务中的表现，同时比较了单特征、特征融合与两阶段检测-分类级联三种设置，在分类与回归框架下均取得了优于先前方法的结果，其中语义停顿检测准确率达89%，整体暂停类型识别准确率为73%，用力等级分类准确率高达90.5%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15473">https://arxiv.org/abs/2509.15473</a><br>
<strong>作者</strong>: Yuyu Wang,Wuyue Xia,Huaxiu Yao,Jingping Nie<br>
<strong>机构</strong>: University of North Carolina at Chapel Hill (北卡罗来纳大学教堂山分校)<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>)<br>
<strong>备注</strong>:  6 pages, 3rd ACM International Workshop on Intelligent Acoustic Systems and Applications (IASA 25)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Post-exercise speech contains rich physiological and linguistic cues, often marked by semantic pauses, breathing pauses, and combined breathing-semantic pauses. Detecting these events enables assessment of recovery rate, lung function, and exertion-related abnormalities. However, existing works on identifying and distinguishing different types of pauses in this context are limited. In this work, building on a recently released dataset with synchronized audio and respiration signals, we provide systematic annotations of pause types. Using these annotations, we systematically conduct exploratory breathing and semantic pause detection and exertion-level classification across deep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features (MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three setups-single feature, feature fusion, and a two-stage detection-classification cascade-under both classification and regression formulations. Results show per-type detection accuracy up to 89 %  for semantic, 55 %  for breathing, 86 %  for combined pauses, and 73 % overall, while exertion-level classification achieves 90.5 %  accuracy, outperformin prior work.<br>
zh</p>
</div></div>
<h3 id="计算机视觉">计算机视觉</h3>
<div class="note purple no-icon flat"><p>[CV-0] Fast OTSU Thresholding Using Bisection Method</p>
<p>【速读】：该论文旨在解决Otsu阈值分割算法在图像分割中因遍历所有可能阈值而导致的计算效率低下问题（即时间复杂度为O(L)），这限制了其在大规模图像处理系统中的应用。解决方案的关键在于利用类间方差函数的单峰特性，引入二分法（bisection method）替代传统的穷举搜索，从而将计算复杂度降至O(log L)，在保证分割精度的同时显著减少方差计算次数和迭代次数，实现高效且确定性的阈值求解，适用于实时应用场景。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16179">https://arxiv.org/abs/2509.16179</a><br>
<strong>作者</strong>: Sai Varun Kodathala<br>
<strong>机构</strong>: Sports Vision, Inc. (Sports Vision, Inc.)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Numerical Analysis (<a target="_blank" rel="noopener" href="http://math.NA">math.NA</a>)<br>
<strong>备注</strong>:  12 pages, 7 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The Otsu thresholding algorithm represents a fundamental technique in image segmentation, yet its computational efficiency is severely limited by exhaustive search requirements across all possible threshold values. This work presents an optimized implementation that leverages the bisection method to exploit the unimodal characteristics of the between-class variance function. Our approach reduces the computational complexity from O(L) to O(log L) evaluations while preserving segmentation accuracy. Experimental validation on 48 standard test images demonstrates a 91.63% reduction in variance computations and 97.21% reduction in algorithmic iterations compared to conventional exhaustive search. The bisection method achieves exact threshold matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5 gray levels. The algorithm maintains universal convergence within theoretical logarithmic bounds while providing deterministic performance guarantees suitable for real-time applications. This optimization addresses critical computational bottlenecks in large-scale image processing systems without compromising the theoretical foundations or segmentation quality of the original Otsu method.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-1] UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决多模态图像分割在实际部署中因模态缺失或损坏导致性能下降的问题，尤其针对训练与推理阶段模态不一致（training-inference modality gap）的挑战。现有方法通常依赖于为每种模态组合单独训练模型，造成高昂的部署成本和复杂的模型匹配问题。本文提出统一的模态松弛分割网络（UniMRSeg），其核心在于分层自监督补偿机制（Hierarchical Self-Supervised Compensation, HSSC）：通过输入层的混合打乱掩码增强实现模态重建，以学习模态内在特征并生成缺失模态的有意义表示；特征层采用模态不变对比学习隐式补偿不完整-完整模态对之间的特征空间距离；输出层引入轻量级反向注意力适配器显式弥补冻结编码器中感知语义的弱化；最后结合混合一致性约束进行微调，确保所有模态组合下预测稳定且性能波动小。该方案无需额外复杂模块即可显著优于当前最优方法，在MRI脑肿瘤分割、RGB-D语义分割及RGB-D/T显著目标分割等场景中验证了有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16170">https://arxiv.org/abs/2509.16170</a><br>
<strong>作者</strong>: Xiaoqi Zhao,Youwei Pang,Chenyang Yu,Lihe Zhang,Huchuan Lu,Shijian Lu,Georges El Fakhri,Xiaofeng Liu<br>
<strong>机构</strong>: Yale University (耶鲁大学); Nanyang Technological University (南洋理工大学); Dalian University of Technology (大连理工大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multi-modal image segmentation faces real-world deployment challenges from incomplete/corrupted modalities degrading performance. While existing methods address training-inference modality gaps via specialized per-combination models, they introduce high deployment costs by requiring exhaustive model subsets and model-modality matching. In this work, we propose a unified modality-relax segmentation network (UniMRSeg) through hierarchical self-supervised compensation (HSSC). Our approach hierarchically bridges representation gaps between complete and incomplete modalities across input, feature and output levels. % First, we adopt modality reconstruction with the hybrid shuffled-masking augmentation, encouraging the model to learn the intrinsic modality characteristics and generate meaningful representations for missing modalities through cross-modal fusion. % Next, modality-invariant contrastive learning implicitly compensates the feature space distance among incomplete-complete modality pairs. Furthermore, the proposed lightweight reverse attention adapter explicitly compensates for the weak perceptual semantics in the frozen encoder. Last, UniMRSeg is fine-tuned under the hybrid consistency constraint to ensure stable prediction under all modality combinations without large performance fluctuations. Without bells and whistles, UniMRSeg significantly outperforms the state-of-the-art methods under diverse missing modality scenarios on MRI-based brain tumor segmentation, RGB-D semantic segmentation, RGB-D/T salient object segmentation. The code will be released at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-2] Pointing to a Llama and Call it a Camel: On the Sycophancy of Multimodal <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】：该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLs）在处理图像输入时表现出的“视觉谄媚行为”（visual sycophantic behavior），即模型倾向于迎合用户可能带有误导性的指令，而非基于图像内容做出客观判断。这种现象在图像输入场景下比纯文本场景更为显著，被称为“谄媚模态差距”（sycophantic modality gap）。为缓解该问题，作者首先尝试使用朴素监督微调（naive supervised fine-tuning），但发现该方法虽能减少对误导指令的盲从，却导致模型对纠正性指令过于固执；为此，论文提出<strong>谄媚反思微调</strong>（Sycophantic Reflective Tuning, SRT），其核心在于引入反思式推理机制，使模型能够先判断用户指令是否具有误导性，再决定是否调整输出，从而在降低谄媚行为的同时避免过度顽固，实现更平衡的响应策略。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16149">https://arxiv.org/abs/2509.16149</a><br>
<strong>作者</strong>: Renjie Pi,Kehao Miao,Li Peihang,Runtao Liu,Jiahui Gao,Jipeng Zhang,Xiaofang Zhou<br>
<strong>机构</strong>: HKUST (香港科技大学); HKU (香港大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal large language models (MLLMs) have demonstrated extraordinary capabilities in conducting conversations based on image inputs. However, we observe that MLLMs exhibit a pronounced form of visual sycophantic behavior. While similar behavior has also been noted in text-based large language models (LLMs), it becomes significantly more prominent when MLLMs process image inputs. We refer to this phenomenon as the “sycophantic modality gap.” To better understand this issue, we further analyze the factors that contribute to the exacerbation of this gap. To mitigate the visual sycophantic behavior, we first experiment with naive supervised fine-tuning to help the MLLM resist misleading instructions from the user. However, we find that this approach also makes the MLLM overly resistant to corrective instructions (i.e., stubborn even if it is wrong). To alleviate this trade-off, we propose Sycophantic Reflective Tuning (SRT), which enables the MLLM to engage in reflective reasoning, allowing it to determine whether a user’s instruction is misleading or corrective before drawing a conclusion. After applying SRT, we observe a significant reduction in sycophantic behavior toward misleading instructions, without resulting in excessive stubbornness when receiving corrective instructions.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-3] AcT2I: Evaluating and Improving Action Depiction in Text-to-Image Models</p>
<p>【速读】：该论文旨在解决当前文本到图像（Text-to-Image, T2I）生成模型在处理以动作为核心语义的提示时，难以准确呈现复杂场景中隐含属性与上下文依赖的问题。其关键解决方案在于提出一种无需训练的知识蒸馏方法，利用大语言模型（Large Language Models, LLMs）增强提示信息，通过引入时间维度、空间关系和语义密度三个维度的密集知识，显著提升图像生成的准确性，其中注入时间细节的改进使性能最高提升72%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16141">https://arxiv.org/abs/2509.16141</a><br>
<strong>作者</strong>: Vatsal Malaviya,Agneet Chatterjee,Maitreya Patel,Yezhou Yang,Chitta Baral<br>
<strong>机构</strong>: Arizona State University (亚利桑那州立大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Project Page : <a target="_blank" rel="noopener" href="https://vatsal-malaviya.github.io/AcT2I/">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Text-to-Image (T2I) models have recently achieved remarkable success in generating images from textual descriptions. However, challenges still persist in accurately rendering complex scenes where actions and interactions form the primary semantic focus. Our key observation in this work is that T2I models frequently struggle to capture nuanced and often implicit attributes inherent in action depiction, leading to generating images that lack key contextual details. To enable systematic evaluation, we introduce AcT2I, a benchmark designed to evaluate the performance of T2I models in generating images from action-centric prompts. We experimentally validate that leading T2I models do not fare well on AcT2I. We further hypothesize that this shortcoming arises from the incomplete representation of the inherent attributes and contextual dependencies in the training corpora of existing T2I models. We build upon this by developing a training-free, knowledge distillation technique utilizing Large Language Models to address this limitation. Specifically, we enhance prompts by incorporating dense information across three dimensions, observing that injecting prompts with temporal details significantly improves image generation accuracy, with our best model achieving an increase of 72%. Our findings highlight the limitations of current T2I methods in generating images that require complex reasoning and demonstrate that integrating linguistic knowledge in a systematic way can notably advance the generation of nuanced and contextually accurate images.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-4] Recovering Parametric Scenes from Very Few Time-of-Flight Pixels <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决如何利用极少量深度测量值（如仅15个像素）从低成本商用飞行时间（Time-of-Flight, ToF）传感器中恢复三维参数化场景的几何结构问题。其核心挑战在于这些传感器空间分辨率极低，但能提供高时间分辨率的光子计数数据，蕴含丰富的场景信息。解决方案的关键在于结合前向预测与可微分渲染：首先通过前向神经网络快速估计场景参数（如已知物体的6D位姿），再在分析-合成框架内引入可微分渲染模块对参数进行迭代优化，从而实现从稀疏测量中高精度重建。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16132">https://arxiv.org/abs/2509.16132</a><br>
<strong>作者</strong>: Carter Sifferman,Yiquan Li,Yiming Li,Fangzhou Mu,Michael Gleicher,Mohit Gupta,Yin Li<br>
<strong>机构</strong>: University of Wisconsin-Madison (威斯康星大学麦迪逊分校)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  ICCV 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We aim to recover the geometry of 3D parametric scenes using very few depth measurements from low-cost, commercially available time-of-flight sensors. These sensors offer very low spatial resolution (i.e., a single pixel), but image a wide field-of-view per pixel and capture detailed time-of-flight data in the form of time-resolved photon counts. This time-of-flight data encodes rich scene information and thus enables recovery of simple scenes from sparse measurements. We investigate the feasibility of using a distributed set of few measurements (e.g., as few as 15 pixels) to recover the geometry of simple parametric scenes with a strong prior, such as estimating the 6D pose of a known object. To achieve this, we design a method that utilizes both feed-forward prediction to infer scene parameters, and differentiable rendering within an analysis-by-synthesis framework to refine the scene parameter estimate. We develop hardware prototypes and demonstrate that our method effectively recovers object pose given an untextured 3D model in both simulations and controlled real-world captures, and show promising initial results for other parametric scenes. We additionally conduct experiments to explore the limits and capabilities of our imaging solution.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-5] Dynamic Classifier-Free Diffusion Guidance via Online Feedback</p>
<p>【速读】：该论文旨在解决生成式 AI（Generative AI）中文本到图像扩散模型里静态指导尺度（Classifier-free Guidance, CFG）带来的局限性问题。传统方法采用固定的 CFG 缩放因子，无法根据提示词（prompt）的多样性动态调整引导强度，导致在文本对齐、视觉保真度和特定能力（如文字渲染）上表现不佳。其关键解决方案是提出一种动态 CFG 调度框架，通过在线反馈机制利用多个轻量级潜在空间评估器（如 CLIP 对齐评分、判别器保真度评分及人类偏好奖励模型）在逆向扩散过程的每一步评估生成质量，并基于贪心搜索策略为每个时间步选择最优 CFG 尺度，从而构建针对每个提示词和样本定制的动态引导调度方案。该方法显著提升了生成效果，在 Imagen 3 上实现了高达 53.8% 的人类偏好胜率，尤其在文本渲染类任务中达到 55.5%，证明了最优引导调度具有内在的动态性和提示依赖性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16131">https://arxiv.org/abs/2509.16131</a><br>
<strong>作者</strong>: Pinelopi Papalampidi,Olivia Wiles,Ira Ktena,Aleksandar Shtedritski,Emanuele Bugliarello,Ivana Kajic,Isabela Albuquerque,Aida Nematzadeh<br>
<strong>机构</strong>: Google DeepMind(谷歌深度大脑); Ellison Institute of Technology(艾伦森技术研究所); Microsoft AI(微软人工智能)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion models, yet its effectiveness is limited by the use of static guidance scales. This “one-size-fits-all” approach fails to adapt to the diverse requirements of different prompts; moreover, prior solutions like gradient-based correction or fixed heuristic schedules introduce additional complexities and fail to generalize. In this work, we challeng this static paradigm by introducing a framework for dynamic CFG scheduling. Our method leverages online feedback from a suite of general-purpose and specialized small-scale latent-space evaluations, such as CLIP for alignment, a discriminator for fidelity and a human preference reward model, to assess generation quality at each step of the reverse diffusion process. Based on this feedback, we perform a greedy search to select the optimal CFG scale for each timestep, creating a unique guidance schedule tailored to every prompt and sample. We demonstrate the effectiveness of our approach on both small-scale models and the state-of-the-art Imagen 3, showing significant improvements in text alignment, visual quality, text rendering and numerical reasoning. Notably, when compared against the default Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate for overall preference, a figure that increases up to to 55.5% on prompts targeting specific capabilities like text rendering. Our work establishes that the optimal guidance schedule is inherently dynamic and prompt-dependent, and provides an efficient and generalizable framework to achieve it.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-6] BaseReward: A Strong Baseline for Multimodal Reward Model</p>
<p>【速读】：该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）与人类偏好对齐的关键挑战，其核心在于构建高性能的多模态奖励模型（Multimodal Reward Models, MRMs）。当前学术界和工业界缺乏系统性的方法指导，导致MRM开发效率低且性能不稳定。解决方案的关键在于通过全面的实验分析，明确MRM开发全流程中的每个关键组件：包括奖励建模范式（如Naive-RM、Critic-based RM和Generative RM）、奖励头架构、训练策略、数据筛选（涵盖十余种多模态与纯文本偏好数据集）、骨干模型选择与规模、以及集成方法。基于这些实证洞察，作者提出了BaseReward——一个基于Qwen2.5-VL骨干模型、采用优化双层奖励头结构并使用高质量混合数据训练的高效基线模型。实验表明，BaseReward在多个主流基准（如MM-RLHF-Reward Bench、VL-Reward Bench等）上达到新的SOTA，并成功应用于真实强化学习流程中，显著提升MLLM在感知、推理和对话任务中的表现，从而为下一代MLLM的可靠奖励建模提供了可复现、可扩展的实践指南。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16127">https://arxiv.org/abs/2509.16127</a><br>
<strong>作者</strong>: Yi-Fan Zhang,Haihua Yang,Huanyu Zhang,Yang Shi,Zezhou Chen,Haochen Tian,Chaoyou Fu,Haotian Wang,Kai Wu,Bo Cui,Xu Wang,Jianfei Pan,Haotian Wang,Zhang Zhang,Liang Wang<br>
<strong>机构</strong>: ByteDance; CASIA; NJU; PKU; THU<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid advancement of Multimodal Large Language Models (MLLMs) has made aligning them with human preferences a critical challenge. Reward Models (RMs) are a core technology for achieving this goal, but a systematic guide for building state-of-the-art Multimodal Reward Models (MRMs) is currently lacking in both academia and industry. Through exhaustive experimental analysis, this paper aims to provide a clear ``recipe’’ for constructing high-performance MRMs. We systematically investigate every crucial component in the MRM development pipeline, including \textitreward modeling paradigms (e.g., Naive-RM, Critic-based RM, and Generative RM), \textitreward head architecture, \textittraining strategies, \textitdata curation (covering over ten multimodal and text-only preference datasets), \textitbackbone model and \textitmodel scale, and \textitensemble methods. Based on these experimental insights, we introduce \textbfBaseReward, a powerful and efficient baseline for multimodal reward modeling. BaseReward adopts a simple yet effective architecture, built upon a Qwen2.5-VL backbone, featuring an optimized two-layer reward head, and is trained on a carefully curated mixture of high-quality multimodal and text-only preference data. Our results show that BaseReward establishes a new SOTA on major benchmarks such as MM-RLHF-Reward Bench, VL-Reward Bench, and Multimodal Reward Bench, outperforming previous models. Furthermore, to validate its practical utility beyond static benchmarks, we integrate BaseReward into a real-world reinforcement learning pipeline, successfully enhancing an MLLM’s performance across various perception, reasoning, and conversational tasks. This work not only delivers a top-tier MRM but, more importantly, provides the community with a clear, empirically-backed guide for developing robust reward models for the next generation of MLLMs.         Subjects:  Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)  Cite as: arXiv:2509.16127 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]    (or  arXiv:2509.16127v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.16127">https://doi.org/10.48550/arXiv.2509.16127</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-7] RadarGaussianDet3D: An Efficient and Effective Gaussian-based 3D Detector with 4D Automotive Radars</p>
<p>【速读】：该论文旨在解决现有基于4D汽车雷达的3D目标检测方法中存在的三大问题：一是依赖pillar编码器进行鸟瞰图（BEV）特征提取时，每个点仅贡献于单一BEV网格，导致特征图稀疏且表示质量下降；二是边界框属性独立优化，造成检测精度不理想；三是推理速度在车载嵌入式设备上难以满足实时性要求。解决方案的关键在于提出了一种基于高斯分布的高效3D检测框架RadarGaussianDet3D，其核心创新包括：设计Point Gaussian Encoder（PGE），将雷达点聚合为高斯原语，并利用3D高斯喷溅（3D Gaussian Splatting, 3DGS）技术实现密集BEV特征映射；同时引入Box Gaussian Loss（BGL），将边界框建模为3D高斯分布并计算距离以实现更全面、一致的优化。该方案显著提升了检测精度与推理效率，具备在自动驾驶系统中实时部署的潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16119">https://arxiv.org/abs/2509.16119</a><br>
<strong>作者</strong>: Weiyi Xiong,Bing Zhu,Tao Huang,Zewei Zheng<br>
<strong>机构</strong>: Beihang University (北京航空航天大学); James Cook University (詹姆斯库克大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:4D automotive radars have gained increasing attention for autonomous driving due to their low cost, robustness, and inherent velocity measurement capability. However, existing 4D radar-based 3D detectors rely heavily on pillar encoders for BEV feature extraction, where each point contributes to only a single BEV grid, resulting in sparse feature maps and degraded representation quality. In addition, they also optimize bounding box attributes independently, leading to sub-optimal detection accuracy. Moreover, their inference speed, while sufficient for high-end GPUs, may fail to meet the real-time requirement on vehicle-mounted embedded devices. To overcome these limitations, an efficient and effective Gaussian-based 3D detector, namely RadarGaussianDet3D is introduced, leveraging Gaussian primitives and distributions as intermediate representations for radar points and bounding boxes. In RadarGaussianDet3D, a novel Point Gaussian Encoder (PGE) is designed to transform each point into a Gaussian primitive after feature aggregation and employs the 3D Gaussian Splatting (3DGS) technique for BEV rasterization, yielding denser feature maps. PGE exhibits exceptionally low latency, owing to the optimized algorithm for point feature aggregation and fast rendering of 3DGS. In addition, a new Box Gaussian Loss (BGL) is proposed, which converts bounding boxes into 3D Gaussian distributions and measures their distance to enable more comprehensive and consistent optimization. Extensive experiments on TJ4DRadSet and View-of-Delft demonstrate that RadarGaussianDet3D achieves state-of-the-art detection accuracy while delivering substantially faster inference, highlighting its potential for real-time deployment in autonomous driving.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-8] DiffusionNFT: Online Diffusion Reinforcement with Forward Process</p>
<p>【速读】：该论文旨在解决在线强化学习（Online Reinforcement Learning, RL）在扩散模型（Diffusion Models）中的应用难题，特别是由于难以计算似然函数（intractable likelihoods）导致的传统方法受限的问题。现有工作通过离散化逆向采样过程实现类似GRPO（Generalized Reward Policy Optimization）的训练，但存在求解器限制、前后向不一致性以及与无分类器引导（Classifier-Free Guidance, CFG）集成复杂等根本性缺陷。其解决方案的关键在于提出一种全新的在线RL范式——扩散负样本感知微调（Diffusion Negative-aware FineTuning, DiffusionNFT），该方法直接在前向过程中利用流匹配（flow matching）优化扩散模型，通过对比正负样本生成结果来隐式定义策略改进方向，并将强化信号自然融入监督学习目标中。此设计使得训练可兼容任意黑盒求解器、无需显式估计似然、仅需干净图像而非完整采样轨迹即可完成策略优化，显著提升了效率与性能，实验证明其相较FlowGRPO提升达25倍以上，且无需使用CFG机制。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16117">https://arxiv.org/abs/2509.16117</a><br>
<strong>作者</strong>: Kaiwen Zheng,Huayu Chen,Haotian Ye,Haoxiang Wang,Qinsheng Zhang,Kai Jiang,Hang Su,Stefano Ermon,Jun Zhu,Ming-Yu Liu<br>
<strong>机构</strong>: Tsinghua University (清华大学); NVIDIA (英伟达); Stanford University (斯坦福大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Online reinforcement learning (RL) has been central to post-training language models, but its extension to diffusion models remains challenging due to intractable likelihoods. Recent works discretize the reverse sampling process to enable GRPO-style training, yet they inherit fundamental drawbacks, including solver restrictions, forward-reverse inconsistency, and complicated integration with classifier-free guidance (CFG). We introduce Diffusion Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that optimizes diffusion models directly on the forward process via flow matching. DiffusionNFT contrasts positive and negative generations to define an implicit policy improvement direction, naturally incorporating reinforcement signals into the supervised learning objective. This formulation enables training with arbitrary black-box solvers, eliminates the need for likelihood estimation, and requires only clean images rather than sampling trajectories for policy optimization. DiffusionNFT is up to  25\times  more efficient than FlowGRPO in head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO achieves 0.95 with over 5k steps and additional CFG employment. By leveraging multiple reward models, DiffusionNFT significantly boosts the performance of SD3.5-Medium in every benchmark tested.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-9] SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features</p>
<p>【速读】：该论文旨在解决3D实例分割任务中因训练数据稀缺而导致模型性能受限的问题。解决方案的关键在于充分利用预训练2D检测模型所提取的图像级和对象级特征，通过设计一个基于Transformer的编码器-解码器框架SegDINO3D，将2D图像特征与点云数据深度融合。具体而言，编码阶段通过从对应视角图像中检索2D特征来增强每个3D点，并利用3D编码器进行上下文融合；解码阶段则将3D对象查询定义为3D锚框（anchor boxes），并通过跨注意力机制从2D检测模型获得的对象查询中提取紧凑的对象级表示，从而在不存储大量图像特征图的前提下保留2D模型的知识，并借助预测的3D框对跨注意力进行精确调制，显著提升分割精度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16098">https://arxiv.org/abs/2509.16098</a><br>
<strong>作者</strong>: Jinyuan Qu,Hongyang Li,Xingyu Chen,Shilong Liu,Yukai Shi,Tianhe Ren,Ruitao Jing,Lei Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we present SegDINO3D, a novel Transformer encoder-decoder framework for 3D instance segmentation. As 3D training data is generally not as sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D representation from a pre-trained 2D detection model, including both image-level and object-level features, for improving 3D representation. SegDINO3D takes both a point cloud and its associated 2D images as input. In the encoder stage, it first enriches each 3D point by retrieving 2D image features from its corresponding image views and then leverages a 3D encoder for 3D context fusion. In the decoder stage, it formulates 3D object queries as 3D anchor boxes and performs cross-attention from 3D queries to 2D object queries obtained from 2D images using the 2D detection model. These 2D object queries serve as a compact object-level representation of 2D images, effectively avoiding the challenge of keeping thousands of image feature maps in the memory while faithfully preserving the knowledge of the pre-trained 2D model. The introducing of 3D box queries also enables the model to modulate cross-attention using the predicted boxes for more precise querying. SegDINO3D achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D instance segmentation benchmarks. Notably, on the challenging ScanNet200 dataset, SegDINO3D significantly outperforms prior methods by +8.7 and +6.8 mAP on the validation and hidden test sets, respectively, demonstrating its superiority.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-10] AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-<mark class="hl-label green">Agent</mark>  Trajectory Modeling in Sports <mark class="hl-label red">ICDM2025</mark></p>
<p>【速读】：该论文旨在解决多智能体体育场景中轨迹预测的挑战，即不同角色（如球员与球）之间的结构异质性以及跨体育领域间的动态分布差异问题。现有统一框架难以捕捉这些结构化的分布偏移，导致在角色和领域间泛化性能不佳。解决方案的关键在于提出AdaSports-Traj框架，其核心创新是引入一个角色与领域感知适配器（Role- and Domain-Aware Adapter），根据智能体身份和领域上下文条件性地调整潜在表示；同时设计分层对比学习目标（Hierarchical Contrastive Learning Objective），分别监督角色敏感和领域感知的表示，以促进无冲突的潜在结构解耦。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16095">https://arxiv.org/abs/2509.16095</a><br>
<strong>作者</strong>: Yi Xu,Yun Fu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted by ICDM 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Trajectory prediction in multi-agent sports scenarios is inherently challenging due to the structural heterogeneity across agent roles (e.g., players vs. ball) and dynamic distribution gaps across different sports domains. Existing unified frameworks often fail to capture these structured distributional shifts, resulting in suboptimal generalization across roles and domains. We propose AdaSports-Traj, an adaptive trajectory modeling framework that explicitly addresses both intra-domain and inter-domain distribution discrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and Domain-Aware Adapter to conditionally adjust latent representations based on agent identity and domain context. Additionally, we introduce a Hierarchical Contrastive Learning objective, which separately supervises role-sensitive and domain-aware representations to encourage disentangled latent structures without introducing optimization conflict. Experiments on three diverse sports datasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness of our adaptive design, achieving strong performance in both unified and cross-domain trajectory prediction settings.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-11] Blind-Spot Guided Diffusion for Self-supervised Real-World Denoising</p>
<p>【速读】：该论文旨在解决两个关键问题：一是盲区网络（Blind-Spot Network, BSN）在图像去噪中因空间独立性假设导致局部细节丢失和像素不连续的问题；二是扩散模型难以适应自监督去噪任务的挑战。其解决方案的关键在于提出一种双分支扩散框架，其中基于BSN的分支生成半洁净图像，而传统扩散分支则学习潜在噪声分布；通过BSN分支引导采样过程，在无需成对数据的情况下有效捕捉噪声结构并保留局部细节，从而实现高性能的自监督真实世界图像去噪。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16091">https://arxiv.org/abs/2509.16091</a><br>
<strong>作者</strong>: Shen Cheng,Haipeng Li,Haibin Huang,Xiaohong Liu,Shuaicheng Liu<br>
<strong>机构</strong>: Dexmal; UESTC (电子科技大学); Tele AI; SJTU (上海交通大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this work, we present Blind-Spot Guided Diffusion, a novel self-supervised framework for real-world image denoising. Our approach addresses two major challenges: the limitations of blind-spot networks (BSNs), which often sacrifice local detail and introduce pixel discontinuities due to spatial independence assumptions, and the difficulty of adapting diffusion models to self-supervised denoising. We propose a dual-branch diffusion framework that combines a BSN-based diffusion branch, generating semi-clean images, with a conventional diffusion branch that captures underlying noise distributions. To enable effective training without paired data, we use the BSN-based branch to guide the sampling process, capturing noise structure while preserving local details. Extensive experiments on the SIDD and DND datasets demonstrate state-of-the-art performance, establishing our method as a highly effective self-supervised solution for real-world denoising. Code and pre-trained models are released at: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-12] SeeTrek: Training-Free Spatial Prompting for Multimodal Large Language Model <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）在仅依赖视觉输入条件下空间理解能力不足的问题。现有方法虽尝试引入深度图或点云等模态以提升空间推理性能，但纯视觉条件下的空间感知仍缺乏系统研究。解决方案的关键在于提出SEETREK框架，其核心思想为两个方面：一是通过最大语义丰富度采样（Maximum Semantic Richness Sampling）增强视觉多样性，利用预训练感知模型提取语义信息丰富的关键帧以捕捉场景结构；二是通过模拟视觉轨迹并编码相对空间位置到关键帧中，实现运动重建与时空一致性保持。该方法无需训练、仅需一次前向传播即可集成至现有MLLMs，显著提升空间推理任务表现（最高提升达+3.5%）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16087">https://arxiv.org/abs/2509.16087</a><br>
<strong>作者</strong>: Pengteng Li,Pinhao Song,Wuyang Li,Weiyu Guo,Huizai Yao,Yijie Xu,Dugang Liu,Hui Xiong<br>
<strong>机构</strong>: HKUST(GZ) (香港科技大学（广州）); AI2ROBOTICS; KU Leuven (鲁汶大学); EPFL (瑞士联邦理工学院); SZU (深圳大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce SEETREK, the first training-free prompting framework tailored to enhance the spatial understanding of Multimodal Large Language Models (MLLMS) under vision-only constraints. While prior efforts have incorporated modalities like depth or point clouds to improve spatial reasoning, purely visualspatial understanding remains underexplored. SEETREK addresses this gap by focusing on two core principles: increasing visual diversity and motion reconstruction. For visual diversity, we conduct Maximum Semantic Richness Sampling, which employs an off-the-shell perception model to extract semantically rich keyframes that capture scene structure. For motion reconstruction, we simulate visual trajectories and encode relative spatial positions into keyframes to preserve both spatial relations and temporal coherence. Our method is trainingGPU-free, requiring only a single forward pass, and can be seamlessly integrated into existing MLLM’S. Extensive experiments on the VSI-B ENCH and STI-B ENCH show that S EE T REK consistently boosts various MLLM S performance across diverse spatial reasoning tasks with the most +3.5% improvement, offering a promising path toward stronger spatial intelligence.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-13] MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning <mark class="hl-label red">ICDM2025</mark></p>
<p>【速读】：该论文旨在解决无监督多变量时间序列（Multivariate Time Series, MTS）表示学习中如何从原始序列中提取紧凑且信息丰富的表示，以支持下游任务的高效迁移问题。其解决方案的关键在于提出双掩码自编码器（Dual-Masked Autoencoder, DMAE），该框架设计了两个互补的预训练任务：一是基于可见属性重建被掩码的值，二是利用教师编码器引导估计被掩码特征的潜在表示；同时引入特征级对齐约束，促使预测的潜在表示与教师输出对齐。通过联合优化这两个目标，DMAE能够学习到时序一致且语义丰富的表示，在分类、回归和预测等任务上均优于现有基线方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16078">https://arxiv.org/abs/2509.16078</a><br>
<strong>作者</strong>: Yi Xu,Yitian Zhang,Yun Fu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted by ICDM 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Unsupervised multivariate time series (MTS) representation learning aims to extract compact and informative representations from raw sequences without relying on labels, enabling efficient transfer to diverse downstream tasks. In this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked time-series modeling framework for unsupervised MTS representation learning. DMAE formulates two complementary pretext tasks: (1) reconstructing masked values based on visible attributes, and (2) estimating latent representations of masked features, guided by a teacher encoder. To further improve representation quality, we introduce a feature-level alignment constraint that encourages the predicted latent representations to align with the teacher’s outputs. By jointly optimizing these objectives, DMAE learns temporally coherent and semantically rich representations. Comprehensive evaluations across classification, regression, and forecasting tasks demonstrate that our approach achieves consistent and superior performance over competitive baselines.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-14] Language-Instructed <mark class="hl-label green">Reasoning</mark>  for Group Activity Detection via Multimodal Large Language Model</p>
<p>【速读】：该论文旨在解决群体活动检测（Group Activity Detection, GAD）任务中现有深度学习方法依赖隐式视觉特征模式识别、缺乏上下文推理能力与可解释性的问题。其核心解决方案是提出一种基于多模态大语言模型（Multimodal Large Language Model, MLLM）的语言引导推理框架LIR-GAD，关键在于通过引入活动级ACT token和多个聚类特定的GROUP token扩展MLLM的词汇空间，并结合语言指令对视频帧进行语义增强编码；同时设计多标签分类损失函数提升ACT token的语义判别能力，并采用多模态双对齐融合（Multimodal Dual-Alignment Fusion, MDAF）模块将token对应的隐藏嵌入与视觉特征深度融合，从而显著提升GAD任务的性能与可解释性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16054">https://arxiv.org/abs/2509.16054</a><br>
<strong>作者</strong>: Jihua Peng,Qianxiong Xu,Yichen Liu,Chenxi Liu,Cheng Long,Rui Zhao,Ziyue Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  9 pages, 5 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Group activity detection (GAD) aims to simultaneously identify group members and categorize their collective activities within video sequences. Existing deep learning-based methods develop specialized architectures (e.g., transformer networks) to model the dynamics of individual roles and semantic dependencies between individuals and groups. However, they rely solely on implicit pattern recognition from visual features and struggle with contextual reasoning and explainability. In this work, we propose LIR-GAD, a novel framework of language-instructed reasoning for GAD via Multimodal Large Language Model (MLLM). Our approach expand the original vocabulary of MLLM by introducing an activity-level ACT token and multiple cluster-specific GROUP tokens. We process video frames alongside two specially designed tokens and language instructions, which are then integrated into the MLLM. The pretrained commonsense knowledge embedded in the MLLM enables the ACT token and GROUP tokens to effectively capture the semantic information of collective activities and learn distinct representational features of different groups, respectively. Also, we introduce a multi-label classification loss to further enhance the ACT token’s ability to learn discriminative semantic representations. Then, we design a Multimodal Dual-Alignment Fusion (MDAF) module that integrates MLLM’s hidden embeddings corresponding to the designed tokens with visual features, significantly enhancing the performance of GAD. Both quantitative and qualitative experiments demonstrate the superior performance of our proposed method in GAD taks.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-15] Graph-based Point Cloud Surface Reconstruction using B-Splines</p>
<p>【速读】：该论文旨在解决从噪声点云数据中重建连续表面的问题，尤其针对现有数据驱动的表面重建方法对地面真值法向量或近似法向量高度依赖而导致在噪声环境下性能不可靠的局限性。其解决方案的关键在于提出一种基于字典引导的图卷积网络（Dictionary-Guided Graph Convolutional Network）的表面重建策略，能够同时预测控制点的位置和数量，从而在无需点法向量信息的前提下生成平滑且复杂度自适应的B样条曲面，有效提升对噪声点云的鲁棒性与重建精度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16050">https://arxiv.org/abs/2509.16050</a><br>
<strong>作者</strong>: Stuti Pathak,Rhys G. Evans,Gunther Steenackers,Rudi Penne<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generating continuous surfaces from discrete point cloud data is a fundamental task in several 3D vision applications. Real-world point clouds are inherently noisy due to various technical and environmental factors. Existing data-driven surface reconstruction algorithms rely heavily on ground truth normals or compute approximate normals as an intermediate step. This dependency makes them extremely unreliable for noisy point cloud datasets, even if the availability of ground truth training data is ensured, which is not always the case. B-spline reconstruction techniques provide compact surface representations of point clouds and are especially known for their smoothening properties. However, the complexity of the surfaces approximated using B-splines is directly influenced by the number and location of the spline control points. Existing spline-based modeling methods predict the locations of a fixed number of control points for a given point cloud, which makes it very difficult to match the complexity of its underlying surface. In this work, we develop a Dictionary-Guided Graph Convolutional Network-based surface reconstruction strategy where we simultaneously predict both the location and the number of control points for noisy point cloud data to generate smooth surfaces without the use of any point normals. We compare our reconstruction method with several well-known as well as recent baselines by employing widely-used evaluation metrics, and demonstrate that our method outperforms all of them both qualitatively and quantitatively.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-16] GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition</p>
<p>【速读】：该论文旨在解决视觉语音识别（Visual Speech Recognition, VSR）在真实场景下因光照变化、遮挡、模糊和姿态变动等视觉挑战导致性能下降的问题。解决方案的关键在于提出一种全局-局部融合的渐进式框架GLip，其核心创新包括：(1) 通过双路径特征提取架构，在第一阶段学习全局与局部视觉特征与语音单元的粗粒度对齐，建立语义鲁棒的基础；(2) 在第二阶段引入上下文增强模块（Contextual Enhancement Module, CEM），动态融合局部特征与时空维度上的全局上下文信息，从而将粗粒度表示精炼为精确的视觉-语音映射。该方法通过渐进式学习策略有效利用判别性局部区域，在LRS2、LRS3及新构建的中文数据集上均展现出更强的鲁棒性和优越性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16031">https://arxiv.org/abs/2509.16031</a><br>
<strong>作者</strong>: Tianyue Wang,Shuang Yang,Shiguang Shan,Xilin Chen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Visual speech recognition (VSR), also known as lip reading, is the task of recognizing speech from silent video. Despite significant advancements in VSR over recent decades, most existing methods pay limited attention to real-world visual challenges such as illumination variations, occlusions, blurring, and pose changes. To address these challenges, we propose GLip, a Global-Local Integrated Progressive framework designed for robust VSR. GLip is built upon two key insights: (i) learning an initial \textitcoarse alignment between visual features across varying conditions and corresponding speech content facilitates the subsequent learning of \textitprecise visual-to-speech mappings in challenging environments; (ii) under adverse conditions, certain local regions (e.g., non-occluded areas) often exhibit more discriminative cues for lip reading than global features. To this end, GLip introduces a dual-path feature extraction architecture that integrates both global and local features within a two-stage progressive learning framework. In the first stage, the model learns to align both global and local visual features with corresponding acoustic speech units using easily accessible audio-visual data, establishing a coarse yet semantically robust foundation. In the second stage, we introduce a Contextual Enhancement Module (CEM) to dynamically integrate local features with relevant global context across both spatial and temporal dimensions, refining the coarse representations into precise visual-speech mappings. Our framework uniquely exploits discriminative local regions through a progressive learning strategy, demonstrating enhanced robustness against various visual challenges and consistently outperforming existing methods on the LRS2 and LRS3 benchmarks. We further validate its effectiveness on a newly introduced challenging Mandarin dataset.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-17] Generalized Deep Multi-view Clustering via Causal Learning with Partially Aligned Cross-view Correspondence</p>
<p>【速读】：该论文旨在解决多视图聚类（Multi-view Clustering, MVC）中因数据对齐不完整（即从完全对齐到部分对齐）导致模型性能下降的问题。现有方法通常依赖于视图一致性假设，要求样本在不同视图间完全对齐，但在实际场景中这一条件难以满足。为应对这一挑战，作者提出因果多视图聚类网络（CauMVC），其核心在于将部分对齐数据建模为干预（intervention），并将多视图聚类任务视为干预后的推理过程（post-intervention inference）。关键创新包括：引入变分自编码器（Variational Auto-Encoder）结构，利用现有信息编码器估计不变特征，并通过解码器完成后干预推理；同时设计对比正则项以捕捉样本间的相关性，从而实现对部分对齐数据的鲁棒聚类。这是首个基于因果学习处理广义多视图聚类问题的工作。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16022">https://arxiv.org/abs/2509.16022</a><br>
<strong>作者</strong>: Xihong Yang,Siwei Wang,Jiaqi Jin,Fangdi Wang,Tianrui Liu,Yueming Jin,Xinwang Liu,En Zhu,Kunlun He<br>
<strong>机构</strong>: National University of Defense Technology (国防科技大学); Intelligent Game and Decision Lab (智能游戏与决策实验室); National University of Singapore (新加坡国立大学); Chinese PLA General Hospital (中国人民解放军总医院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multi-view clustering (MVC) aims to explore the common clustering structure across multiple views. Many existing MVC methods heavily rely on the assumption of view consistency, where alignments for corresponding samples across different views are ordered in advance. However, real-world scenarios often present a challenge as only partial data is consistently aligned across different views, restricting the overall clustering performance. In this work, we consider the model performance decreasing phenomenon caused by data order shift (i.e., from fully to partially aligned) as a generalized multi-view clustering problem. To tackle this problem, we design a causal multi-view clustering network, termed CauMVC. We adopt a causal modeling approach to understand multi-view clustering procedure. To be specific, we formulate the partially aligned data as an intervention and multi-view clustering with partially aligned data as an post-intervention inference. However, obtaining invariant features directly can be challenging. Thus, we design a Variational Auto-Encoder for causal learning by incorporating an encoder from existing information to estimate the invariant features. Moreover, a decoder is designed to perform the post-intervention inference. Lastly, we design a contrastive regularizer to capture sample correlations. To the best of our knowledge, this paper is the first work to deal generalized multi-view clustering via causal learning. Empirical experiments on both fully and partially aligned data illustrate the strong generalization and effectiveness of CauMVC.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-18] Disti<mark class="hl-label green">llM</mark> atch: Leve<mark class="hl-label green">rag</mark> ing Knowledge Distillation from Vision Foundation Model for Multimodal Image Matching</p>
<p>【速读】：该论文旨在解决多模态图像匹配（multimodal image matching）中因模态间显著外观差异导致的像素级对应关系提取困难问题，尤其针对现有深度学习方法在缺乏高质量标注数据时性能差且适应性不足的局限。其解决方案的关键在于利用视觉基础模型（Vision Foundation Model, VFM）通过知识蒸馏（knowledge distillation）构建轻量级学生模型，将VFM（如DINOv2和DINOv3）中的高层语义特征迁移至匹配任务中；同时引入模态类别信息注入机制以保留模态特异性特征，增强跨模态关联理解，并设计V2I-GAN进行可见光到伪红外图像的风格迁移以提升泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16017">https://arxiv.org/abs/2509.16017</a><br>
<strong>作者</strong>: Meng Yang,Fan Fan,Zizhuo Li,Songchu Deng,Yong Ma,Jiayi Ma<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  10 pages, 4 figures, 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal image matching seeks pixel-level correspondences between images of different modalities, crucial for cross-modal perception, fusion and analysis. However, the significant appearance differences between modalities make this task challenging. Due to the scarcity of high-quality annotated datasets, existing deep learning methods that extract modality-common features for matching perform poorly and lack adaptability to diverse scenarios. Vision Foundation Model (VFM), trained on large-scale data, yields generalizable and robust feature representations adapted to data and tasks of various modalities, including multimodal matching. Thus, we propose DistillMatch, a multimodal image matching method using knowledge distillation from VFM. DistillMatch employs knowledge distillation to build a lightweight student model that extracts high-level semantic features from VFM (including DINOv2 and DINOv3) to assist matching across modalities. To retain modality-specific information, it extracts and injects modality category information into the other modality’s features, which enhances the model’s understanding of cross-modal correlations. Furthermore, we design V2I-GAN to boost the model’s generalization by translating visible to pseudo-infrared images for data augmentation. Experiments show that DistillMatch outperforms existing algorithms on public datasets.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-19] owards Robust Visual Continual Learning with Multi-Prototype Supervision</p>
<p>【速读】：该论文旨在解决语言引导的视觉持续学习（Language-guided Visual Continual Learning, CL）中因依赖单一语义目标所引发的两个关键问题：一是语义模糊性（semantic ambiguity），即多义词类别名称导致视觉表征冲突；二是类内视觉多样性（intra-class visual diversity），即单个原型无法捕捉类别内部丰富的视觉外观变化。解决方案的关键在于提出MuproCL框架，通过引入多个上下文感知的原型（context-aware prototypes）替代单一目标，并利用轻量级大语言模型（LLM）代理进行类别消歧与视觉模态扩展，生成稳健的语义原型集；同时采用LogSumExp聚合机制，使视觉模型能够自适应地对齐最相关的原型，从而提升模型在持续学习场景下的性能与鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16011">https://arxiv.org/abs/2509.16011</a><br>
<strong>作者</strong>: Xiwei Liu,Yulong Li,Yichen Li,Xinlin Zhuang,Haolin Yang,Huifa Li,Imran Razzak<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Language-guided supervision, which utilizes a frozen semantic target from a Pretrained Language Model (PLM), has emerged as a promising paradigm for visual Continual Learning (CL). However, relying on a single target introduces two critical limitations: 1) semantic ambiguity, where a polysemous category name results in conflicting visual representations, and 2) intra-class visual diversity, where a single prototype fails to capture the rich variety of visual appearances within a class. To this end, we propose MuproCL, a novel framework that replaces the single target with multiple, context-aware prototypes. Specifically, we employ a lightweight LLM agent to perform category disambiguation and visual-modal expansion to generate a robust set of semantic prototypes. A LogSumExp aggregation mechanism allows the vision model to adaptively align with the most relevant prototype for a given image. Extensive experiments across various CL baselines demonstrate that MuproCL consistently enhances performance and robustness, establishing a more effective path for language-guided continual learning.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-20] DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic Data for Cardiac Hypertension Diagnosis</p>
<p>【速读】：该论文旨在解决多模态数据融合在医学诊断中如何有效提升性能的问题，特别是针对超声心动图时序数据与表格型临床记录的联合分析。解决方案的关键在于提出一种非对称融合策略，从主模态出发，通过解耦共享信息与模态特异性信息的方式，逐步整合次级模态的数据，从而实现更精准的诊断建模。该方法在239名患者的多模态数据集上验证，AUC超过90%，为临床应用提供了重要基准。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15990">https://arxiv.org/abs/2509.15990</a><br>
<strong>作者</strong>: Jérémie Stym-Popper,Nathan Painchaud,Clément Rambour,Pierre-Yves Courand,Nicolas Thome,Olivier Bernard<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  9 pages, Accepted at MIDL 2025 (Oral)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal data fusion is a key approach for enhancing diagnosis in medical applications. We propose an asymmetric fusion strategy starting from a primary modality and integrating secondary modalities by disentangling shared and modality-specific information. Validated on a dataset of 239 patients with echocardiographic time series and tabular records, our model outperforms existing methods, achieving an AUC over 90%. This improvement marks a crucial benchmark for clinical use.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-21] owards Sharper Object Boundaries in Self-Supervised Depth Estimation <mark class="hl-label red">BMVC2025</mark></p>
<p>【速读】：该论文旨在解决单目深度估计（monocular depth estimation）中物体边界处深度模糊的问题，这种模糊会导致生成的三维点云出现虚假的中间深度点。传统方法通常需要细粒度的监督信号才能获得清晰的边缘，而本文提出了一种仅依赖自监督（self-supervision）即可实现锐利深度不连续性的解决方案。其关键在于将每个像素的深度建模为混合分布（mixture distribution），从而捕捉多个合理的深度假设，并将不确定性从直接回归转移到混合权重上；该框架可无缝集成到现有流水线中，通过方差感知损失函数和不确定性传播机制实现优化，在KITTI和VKITTIv2数据集上的实验表明，该方法在边界锐度上相比最先进基线提升高达35%，并显著改善点云质量。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15987">https://arxiv.org/abs/2509.15987</a><br>
<strong>作者</strong>: Aurélien Cecille,Stefan Duffner,Franck Davoine,Rémi Agier,Thibault Neveu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:  BMVC 2025 Oral, 10 pages, 6 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-22] CoPAD : Multi-source Trajectory Fusion and Cooperative Trajectory Prediction with Anchor-oriented Decoder in V2X Scenarios <mark class="hl-label red">IROS2025</mark></p>
<p>【速读】：该论文旨在解决单车感知不稳定性对轨迹预测精度造成的限制问题，特别是在车路协同（V2X）场景下，如何利用多源数据提升轨迹预测的完整性和准确性。其解决方案的关键在于提出了一种轻量级协同轨迹预测框架CoPAD，核心创新包括：基于匈牙利算法与卡尔曼滤波的融合模块实现多源轨迹数据的早期融合；引入过去时间注意力（Past Time Attention, PTA）模块以高效捕捉历史轨迹间的潜在交互信息；设计模式注意力模块增强预测结果的多样性；以及采用基于稀疏锚点的解码器（Anchor-oriented Decoder, AoD）生成最终完整的轨迹序列。该方案在DAIR-V2X-Seq数据集上实现了当前最优性能，验证了其在V2X协同轨迹预测中的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15984">https://arxiv.org/abs/2509.15984</a><br>
<strong>作者</strong>: Kangyu Wu,Jiaqi Qiao,Ya Zhang<br>
<strong>机构</strong>: School of Automation, Southeast University (东南大学自动化学院); Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education (教育部复杂系统测量与控制工程重点实验室)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:  7 pages, 4 pages, IROS2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recently, data-driven trajectory prediction methods have achieved remarkable results, significantly advancing the development of autonomous driving. However, the instability of single-vehicle perception introduces certain limitations to trajectory prediction. In this paper, a novel lightweight framework for cooperative trajectory prediction, CoPAD, is proposed. This framework incorporates a fusion module based on the Hungarian algorithm and Kalman filtering, along with the Past Time Attention (PTA) module, mode attention module and anchor-oriented decoder (AoD). It effectively performs early fusion on multi-source trajectory data from vehicles and road infrastructure, enabling the trajectories with high completeness and accuracy. The PTA module can efficiently capture potential interaction information among historical trajectories, and the mode attention module is proposed to enrich the diversity of predictions. Additionally, the decoder based on sparse anchors is designed to generate the final complete trajectories. Extensive experiments show that CoPAD achieves the state-of-the-art performance on the DAIR-V2X-Seq dataset, validating the effectiveness of the model in cooperative trajectory prediction in V2X scenarios.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-23] Shedding Light on Depth: Explainability Assessment in Monocular Depth Estimation <mark class="hl-label red">IJCNN</mark></p>
<p>【速读】：该论文旨在解决单目深度估计（Monocular Depth Estimation, MDE）模型可解释性不足的问题，即如何有效分析MDE网络将输入图像映射到预测深度图的决策过程。其解决方案的关键在于系统评估多种特征归因方法（如Saliency Maps、Integrated Gradients和Attention Rollout）在不同计算复杂度的MDE模型（METER轻量级网络与PixelFormer深度网络）上的表现，并提出新的评估指标Attribution Fidelity，以衡量特征归因与预测深度图之间的一致性，从而更可靠地判断可视化解释的有效性。实验表明，Saliency Maps和Integrated Gradients在轻量级和深度模型中分别能较好地识别关键输入特征，而Attribution Fidelity能够识别传统指标可能忽略的解释失效场景。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15980">https://arxiv.org/abs/2509.15980</a><br>
<strong>作者</strong>: Lorenzo Cirillo,Claudio Schiavella,Lorenzo Papa,Paolo Russo,Irene Amerini<br>
<strong>机构</strong>: Sapienza University of Rome (罗马大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  8 pages, 3 figures, 2 tables. This paper has been accepted at the International Joint Conference on Neural Networks (IJCNN) 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Explainable artificial intelligence is increasingly employed to understand the decision-making process of deep learning models and create trustworthiness in their adoption. However, the explainability of Monocular Depth Estimation (MDE) remains largely unexplored despite its wide deployment in real-world applications. In this work, we study how to analyze MDE networks to map the input image to the predicted depth map. More in detail, we investigate well-established feature attribution methods, Saliency Maps, Integrated Gradients, and Attention Rollout on different computationally complex models for MDE: METER, a lightweight network, and PixelFormer, a deep network. We assess the quality of the generated visual explanations by selectively perturbing the most relevant and irrelevant pixels, as identified by the explainability methods, and analyzing the impact of these perturbations on the model’s output. Moreover, since existing evaluation metrics can have some limitations in measuring the validity of visual explanations for MDE, we additionally introduce the Attribution Fidelity. This metric evaluates the reliability of the feature attribution by assessing their consistency with the predicted depth map. Experimental results demonstrate that Saliency Maps and Integrated Gradients have good performance in highlighting the most important input features for MDE lightweight and deep models, respectively. Furthermore, we show that Attribution Fidelity effectively identifies whether an explainability method fails to produce reliable visual maps, even in scenarios where conventional metrics might suggest satisfactory results.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-24] CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine</p>
<p>【速读】：该论文旨在解决自动驾驶（Autonomous Driving, AD）系统在长尾、高风险场景下性能受限的问题，此类罕见场景虽发生频率低，却占事故的较大比例。现有视觉-语言-动作（Vision-Language Action, VLA）模型虽具备较强推理能力，但受限于高质量数据稀缺和学习效率低下。其解决方案的关键在于提出一种持续学习端到端的自动驾驶框架CoReVLA，通过“数据收集与行为精炼”双阶段机制实现性能提升：首先在开放源码驾驶问答数据集上联合微调以建立基础理解；其次在CAVE仿真环境中收集驾驶员接管数据作为长尾场景信号；最后利用直接偏好优化（Direct Preference Optimization, DPO）从人类偏好中学习，避免人工设计奖励导致的奖励黑客问题。该方法显著提升了模型在安全关键场景下的感知准确性和决策合理性，并通过案例研究验证了其基于历史接管经验持续改进的能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15968">https://arxiv.org/abs/2509.15968</a><br>
<strong>作者</strong>: Shiyu Fang,Yiming Cui,Haoyang Liang,Chen Lv,Peng Hang,Jian Sun<br>
<strong>机构</strong>: Tongji University (同济大学)<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Autonomous Driving (AD) systems have made notable progress, but their performance in long-tail, safety-critical scenarios remains limited. These rare cases contribute a disproportionate number of accidents. Vision-Language Action (VLA) models have strong reasoning abilities and offer a potential solution, but their effectiveness is limited by the lack of high-quality data and inefficient learning in such conditions. To address these challenges, we propose CoReVLA, a continual learning end-to-end autonomous driving framework that improves the performance in long-tail scenarios through a dual-stage process of data Collection and behavior Refinement. First, the model is jointly fine-tuned on a mixture of open-source driving QA datasets, allowing it to acquire a foundational understanding of driving scenarios. Next, CoReVLA is deployed within the Cave Automatic Virtual Environment (CAVE) simulation platform, where driver takeover data is collected from real-time interactions. Each takeover indicates a long-tail scenario that CoReVLA fails to handle reliably. Finally, the model is refined via Direct Preference Optimization (DPO), allowing it to learn directly from human preferences and thereby avoid reward hacking caused by manually designed rewards. Extensive open-loop and closed-loop experiments demonstrate that the proposed CoReVLA model can accurately perceive driving scenarios and make appropriate decisions. On the Bench2Drive benchmark, CoReVLA achieves a Driving Score (DS) of 72.18 and a Success Rate (SR) of 50%, outperforming state-of-the-art methods by 7.96 DS and 15% SR under long-tail, safety-critical scenarios. Furthermore, case studies demonstrate the model’s ability to continually improve its performance in similar failure-prone scenarios by leveraging past takeover experiences. All codea and preprocessed datasets are available at: this https URL<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-25] A multi-temporal multi-spectral attention-augmented deep convolution neural network with contrastive learning for crop yield prediction</p>
<p>【速读】：该论文旨在解决气候变化背景下作物产量预测精度不足的问题，尤其针对多光谱遥感数据在时空特征融合中的挑战。现有方法难以有效利用多光谱数据来准确评估作物健康与生长模式，从而限制了预测性能。解决方案的关键在于提出一种新型的多时相多光谱产量预测网络（MTMS-YieldNet），其核心创新是通过对比学习（contrastive learning）进行预训练，以增强对遥感数据中空间-光谱模式和时空依赖关系的特征判别能力，从而更有效地整合多光谱与时空信息，显著提升不同气候和季节条件下的产量预测准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15966">https://arxiv.org/abs/2509.15966</a><br>
<strong>作者</strong>: Shalini Dangi,Surya Karthikeya Mullapudi,Chandravardhan Singh Raghaw,Shahid Shafi Dar,Mohammad Zia Ur Rehman,Nagendra Kumar<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Published in Computers and Electronics in Agriculture</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Precise yield prediction is essential for agricultural sustainability and food security. However, climate change complicates accurate yield prediction by affecting major factors such as weather conditions, soil fertility, and farm management systems. Advances in technology have played an essential role in overcoming these challenges by leveraging satellite monitoring and data analysis for precise yield estimation. Current methods rely on spatio-temporal data for predicting crop yield, but they often struggle with multi-spectral data, which is crucial for evaluating crop health and growth patterns. To resolve this challenge, we propose a novel Multi-Temporal Multi-Spectral Yield Prediction Network, MTMS-YieldNet, that integrates spectral data with spatio-temporal information to effectively capture the correlations and dependencies between them. While existing methods that rely on pre-trained models trained on general visual data, MTMS-YieldNet utilizes contrastive learning for feature discrimination during pre-training, focusing on capturing spatial-spectral patterns and spatio-temporal dependencies from remote sensing data. Both quantitative and qualitative assessments highlight the excellence of the proposed MTMS-YieldNet over seven existing state-of-the-art methods. MTMS-YieldNet achieves MAPE scores of 0.336 on Sentinel-1, 0.353 on Landsat-8, and an outstanding 0.331 on Sentinel-2, demonstrating effective yield prediction performance across diverse climatic and seasonal conditions. The outstanding performance of MTMS-YieldNet improves yield predictions and provides valuable insights that can assist farmers in making better decisions, potentially improving crop yields.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-26] PAN: Pillars-Attention-Based Network for 3D Object Detection</p>
<p>【速读】：该论文旨在解决在恶劣天气和光照条件下，基于摄像头与雷达融合的3D目标检测任务中，如何更高效地利用雷达点云（radar point cloud）所具有的精确距离估计和速度信息的问题。当前相关研究较少，且缺乏针对雷达特性的专用架构设计。其解决方案的关键在于提出一种新颖且高效的鸟瞰图（bird’s-eye-view, BEV）3D目标检测算法：首先在特征融合前对雷达数据进行深度处理，引入一个新设计的骨干网络（backbone），将雷达柱状体特征（pillar features）映射至嵌入维度，并通过自注意力机制建模雷达点间的依赖关系；同时用简化卷积层替代PointPillars架构中基于FPN的卷积模块，显著降低推理时间。该方法在nuScenes数据集上实现了58.2的NDS指标（ResNet-50），并创下同类方法中的最快推理速度记录。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15935">https://arxiv.org/abs/2509.15935</a><br>
<strong>作者</strong>: Ruan Bispo,Dane Mitrev,Letizia Mariotti,Clément Botty,Denver Humphrey,Anthony Scanlan,Ciarán Eising<br>
<strong>机构</strong>: University of Limerick (利默里克大学); Provizio; Lero (爱尔兰软件研究中心); D2iCE Research Centre (数据驱动计算机工程研究中心)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Camera-radar fusion offers a robust and low-cost alternative to Camera-lidar fusion for the 3D object detection task in real-time under adverse weather and lighting conditions. However, currently, in the literature, it is possible to find few works focusing on this modality and, most importantly, developing new architectures to explore the advantages of the radar point cloud, such as accurate distance estimation and speed information. Therefore, this work presents a novel and efficient 3D object detection algorithm using cameras and radars in the bird’s-eye-view (BEV). Our algorithm exploits the advantages of radar before fusing the features into a detection head. A new backbone is introduced, which maps the radar pillar features into an embedded dimension. A self-attention mechanism allows the backbone to model the dependencies between the radar points. We are using a simplified convolutional layer to replace the FPN-based convolutional layers used in the PointPillars-based architectures with the main goal of reducing inference time. Our results show that with this modification, our approach achieves the new state-of-the-art in the 3D object detection problem, reaching 58.2 of the NDS metric for the use of ResNet-50, while also setting a new benchmark for inference time on the nuScenes dataset for the same category.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-27] Sparse Multiview Open-Vocabulary 3D Detection <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决**开放词汇三维目标检测（open-vocabulary 3D object detection）**在稀疏视图（sparse-view）场景下的难题，即如何在仅有有限数量带姿态的RGB图像输入条件下，实现对任意类别物体的位置与尺寸准确识别。其解决方案的关键在于提出一种无需训练（training-free）的方法：利用预训练的2D基础模型（2D foundation models）进行检测，并通过将2D检测结果提升至3D空间并直接优化3D提案以满足跨视角特征一致性（feature-metric consistency），从而充分挖掘2D领域中丰富的训练数据优势，避免复杂的3D特征融合或专用3D学习过程。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15924">https://arxiv.org/abs/2509.15924</a><br>
<strong>作者</strong>: Olivier Moliner,Viktor Larsson,Kalle Åström<br>
<strong>机构</strong>: Lund University (隆德大学); Sony Corporation (索尼公司)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  ICCV 2025; OpenSUN3D Workshop; Camera ready version</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The ability to interpret and comprehend a 3D scene is essential for many vision and robotics systems. In numerous applications, this involves 3D object detection, i.e.~identifying the location and dimensions of objects belonging to a specific category, typically represented as bounding boxes. This has traditionally been solved by training to detect a fixed set of categories, which limits its use. In this work, we investigate open-vocabulary 3D object detection in the challenging yet practical sparse-view setting, where only a limited number of posed RGB images are available as input. Our approach is training-free, relying on pre-trained, off-the-shelf 2D foundation models instead of employing computationally expensive 3D feature fusion or requiring 3D-specific learning. By lifting 2D detections and directly optimizing 3D proposals for featuremetric consistency across views, we fully leverage the extensive training data available in 2D compared to 3D. Through standard benchmarks, we demonstrate that this simple pipeline establishes a powerful baseline, performing competitively with state-of-the-art techniques in densely sampled scenarios while significantly outperforming them in the sparse-view setting.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-28] Deep Feedback Models</p>
<p>【速读】：该论文旨在解决传统前馈神经网络在低数据量和高噪声环境下泛化能力差、稳定性不足的问题。其核心解决方案是提出深度反馈模型（Deep Feedback Models, DFMs），通过引入状态依赖的反馈机制，将自底向上的输入与时间上的高层表征相结合，使网络能够迭代优化内部状态。关键创新在于将这一过程建模为一个微分方程，并由递归神经网络求解，同时利用指数衰减稳定系统以确保收敛性。实验证明，DFMs 在物体识别与分割任务中显著优于前馈模型，尤其在噪声干扰和小样本场景下表现更优，且在医学图像分析中也展现出鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15905">https://arxiv.org/abs/2509.15905</a><br>
<strong>作者</strong>: David Calhas,Arlindo L. Oliveira<br>
<strong>机构</strong>: INESC-ID (INESC-ID); Instituto Superior Técnico (理工学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep Feedback Models (DFMs) are a new class of stateful neural networks that combine bottom up input with high level representations over time. This feedback mechanism introduces dynamics into otherwise static architectures, enabling DFMs to iteratively refine their internal state and mimic aspects of biological decision making. We model this process as a differential equation solved through a recurrent neural network, stabilized via exponential decay to ensure convergence. To evaluate their effectiveness, we measure DFMs under two key conditions: robustness to noise and generalization with limited data. In both object recognition and segmentation tasks, DFMs consistently outperform their feedforward counterparts, particularly in low data or high noise regimes. In addition, DFMs translate to medical imaging settings, while being robust against various types of noise corruption. These findings highlight the importance of feedback in achieving stable, robust, and generalizable learning. Code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-29] From Data to Diagnosis: A Large Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction</p>
<p>【速读】：该论文旨在解决白血病诊断过程中依赖人工显微镜下骨髓形态分析、耗时且复杂的问题，以及现有人工智能（AI）解决方案普遍使用私有数据集且仅覆盖诊断流程局部环节的局限性。其关键解决方案是构建了一个大规模、高质量、公开可用的白血病骨髓数据集，涵盖从细胞检测到最终诊断的完整流程，并基于此数据集提出了一套端到端的AI方法，包括细胞检测、细胞分类和诊断预测。该数据集包含246名儿童患者的临床信息、超过4万张带边界框标注的细胞图像及超2.8万张高精度类别标签的细胞图像，显著提升了模型训练与评估的可靠性，为AI辅助诊断提供了坚实基础。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15895">https://arxiv.org/abs/2509.15895</a><br>
<strong>作者</strong>: Henning Höfener(1),Farina Kock(1),Martina Pontones(2),Tabita Ghete(2 and 3),David Pfrang(1),Nicholas Dickel(4),Meik Kunz(4),Daniela P. Schacherer(1),David A. Clunie(5),Andrey Fedorov(6),Max Westphal(1),Markus Metzler(2 and 3 and 7) ((1) Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany, (2) Department of Pediatrics and Adolescent Medicine, University Hospital Erlangen, Erlangen, Germany, (3) Bavarian Cancer Research Center (BZKF), Erlangen, Germany, (4) Medical Informatics, Friedrich-Alexander University of Erlangen-Nürnberg, Erlangen, Germany, (5) PixelMed Publishing LLC, Bangor, PA, USA, (6) Department of Radiology, Brigham and Women’s Hospital and Harvard Medical School, Boston, MA, USA, (7) Comprehensive Cancer Center Erlangen-EMN, Erlangen, Germany)<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Leukemia diagnosis primarily relies on manual microscopic analysis of bone marrow morphology supported by additional laboratory parameters, making it complex and time consuming. While artificial intelligence (AI) solutions have been proposed, most utilize private datasets and only cover parts of the diagnostic pipeline. Therefore, we present a large, high-quality, publicly available leukemia bone marrow dataset spanning the entire diagnostic process, from cell detection to diagnosis. Using this dataset, we further propose methods for cell detection, cell classification, and diagnosis prediction. The dataset comprises 246 pediatric patients with diagnostic, clinical and laboratory information, over 40 000 cells with bounding box annotations and more than 28 000 of these with high-quality class labels, making it the most comprehensive dataset publicly available. Evaluation of the AI models yielded an average precision of 0.96 for the cell detection, an area under the curve of 0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean F1-score of 0.90 for the diagnosis prediction using predicted cell counts. While the proposed approaches demonstrate their usefulness for AI-assisted diagnostics, the dataset will foster further research and development in the field, ultimately contributing to more precise diagnoses and improved patient outcomes.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-30] MoAngelo: Motion-Aware Neural Surface Reconstruction for Dynamic Scenes</p>
<p>【速读】：该论文旨在解决从多视角视频中重建动态场景的难题，尤其是如何在保持几何细节精度的同时克服传统神经表面重建方法在动态场景下因计算复杂性和表示局限性导致的噪声大、过度平滑等问题。其解决方案的关键在于：首先利用静态重建方法NeuralAngelo获取初始帧的高质量模板场景，随后通过联合优化变形场（deformation field）来追踪该模板并基于时间序列进行精细化调整；这种灵活的模板机制能够适应无法由变形场建模的变化，如遮挡区域或拓扑结构变化，从而实现高保真度的动态三维重建。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15892">https://arxiv.org/abs/2509.15892</a><br>
<strong>作者</strong>: Mohamed Ebbed,Zorah Lähner<br>
<strong>机构</strong>: University of Bonn (波恩大学); Lamarr Institute for Machine Learning and Artificial Intelligence (拉马尔机器学习与人工智能研究所)<br>
<strong>类目</strong>: Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Dynamic scene reconstruction from multi-view videos remains a fundamental challenge in computer vision. While recent neural surface reconstruction methods have achieved remarkable results in static 3D reconstruction, extending these approaches with comparable quality for dynamic scenes introduces significant computational and representational challenges. Existing dynamic methods focus on novel-view synthesis, therefore, their extracted meshes tend to be noisy. Even approaches aiming for geometric fidelity often result in too smooth meshes due to the ill-posedness of the problem. We present a novel framework for highly detailed dynamic reconstruction that extends the static 3D reconstruction method NeuralAngelo to work in dynamic settings. To that end, we start with a high-quality template scene reconstruction from the initial frame using NeuralAngelo, and then jointly optimize deformation fields that track the template and refine it based on the temporal sequence. This flexible template allows updating the geometry to include changes that cannot be modeled with the deformation field, for instance occluded parts or the changes in the topology. We show superior reconstruction accuracy in comparison to previous state-of-the-art methods on the ActorsHQ dataset.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-31] Global Regulation and Excitation via Attention Tuning for Stereo Matching <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决现有迭代式立体匹配方法（如RAFS-Stereo和IGEV-Stereo）在遮挡、无纹理或重复纹理等病态区域中表现不佳的问题，其根源在于缺乏全局上下文信息和几何约束以支持有效的迭代优化。解决方案的关键是提出一种名为“通过注意力调制进行全局调控与激励”（Global Regulation and Excitation via Attention Tuning, GREAT）的框架，该框架包含三个注意力模块：空间注意力（Spatial Attention, SA）用于捕获空间维度上的全局上下文，匹配注意力（Matching Attention, MA）沿极线方向提取全局上下文，体积注意力（Volume Attention, VA）则协同SA与MA构建由全局上下文和几何细节激励的更鲁棒的成本体（cost-volume）。该框架可无缝集成至多种主流迭代立体匹配算法中，显著提升其在挑战性场景下的性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15891">https://arxiv.org/abs/2509.15891</a><br>
<strong>作者</strong>: Jiahao Li,Xinhong Chen,Zhengmin Jiang,Qian Zhou,Yung-Hui Li,Jianping Wang<br>
<strong>机构</strong>: City University of Hong Kong (香港城市大学); Hon Hai Research Institute (鸿海研究院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  International Conference on Computer Vision (ICCV 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Stereo matching achieves significant progress with iterative algorithms like RAFT-Stereo and IGEV-Stereo. However, these methods struggle in ill-posed regions with occlusions, textureless, or repetitive patterns, due to a lack of global context and geometric information for effective iterative refinement. To enable the existing iterative approaches to incorporate global context, we propose the Global Regulation and Excitation via Attention Tuning (GREAT) framework which encompasses three attention modules. Specifically, Spatial Attention (SA) captures the global context within the spatial dimension, Matching Attention (MA) extracts global context along epipolar lines, and Volume Attention (VA) works in conjunction with SA and MA to construct a more robust cost-volume excited by global context and geometric details. To verify the universality and effectiveness of this framework, we integrate it into several representative iterative stereo-matching methods and validate it through extensive experiments, collectively denoted as GREAT-Stereo. This framework demonstrates superior performance in challenging ill-posed regions. Applied to IGEV-Stereo, among all published methods, our GREAT-IGEV ranks first on the Scene Flow test set, KITTI 2015, and ETH3D leaderboards, and achieves second on the Middlebury benchmark. Code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-32] RangeSAM: Leve<mark class="hl-label green">rag</mark> ing Visual Foundation Models for Range-View repesented LiDAR segmentation</p>
<p>【速读】：该论文旨在解决点云分割在自动驾驶与三维场景理解中的高计算成本与实时效率不足的问题，尤其针对传统体素（voxel）和点-based方法存在的不规则内存访问及性能瓶颈。其解决方案的关键在于首次将视觉基础模型（Visual Foundation Model, VFM）SAM2引入到LiDAR点云的range-view表示中，通过构建一个适配于球面投影特性的2D语义分割框架，实现高效、准确的3D点云分割：具体包括三个核心改进——(1) 引入强调水平空间依赖性的新模块以适应LiDAR范围图特性；(2) 设计针对球面投影几何属性的定制化编码器配置；(3) 优化编码器中的机制以捕捉range-view伪图像中的独特空间模式与不连续性。此方案在SemanticKITTI数据集上达到竞争性性能，同时兼具2D流水线的速度优势、可扩展性和部署简易性，验证了VFM作为通用3D感知骨干网络的可行性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15886">https://arxiv.org/abs/2509.15886</a><br>
<strong>作者</strong>: Paul Julius Kühn,Duc Anh Nguyen,Arjan Kuijper,Holger Graf,Dieter Fellner,Saptarshi Neil Sinha<br>
<strong>机构</strong>: Fraunhofer IGD (弗劳恩霍夫图像图形学研究所); TU Darmstadt (达姆施塔特工业大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Point cloud segmentation is central to autonomous driving and 3D scene understanding. While voxel- and point-based methods dominate recent research due to their compatibility with deep architectures and ability to capture fine-grained geometry, they often incur high computational cost, irregular memory access, and limited real-time efficiency. In contrast, range-view methods, though relatively underexplored - can leverage mature 2D semantic segmentation techniques for fast and accurate predictions. Motivated by the rapid progress in Visual Foundation Models (VFMs) for captioning, zero-shot recognition, and multimodal tasks, we investigate whether SAM2, the current state-of-the-art VFM for segmentation tasks, can serve as a strong backbone for LiDAR point cloud segmentation in the range view. We present , to our knowledge, the first range-view framework that adapts SAM2 to 3D segmentation, coupling efficient 2D feature extraction with standard projection/back-projection to operate on point clouds. To optimize SAM2 for range-view representations, we implement several architectural modifications to the encoder: (1) a novel module that emphasizes horizontal spatial dependencies inherent in LiDAR range images, (2) a customized configuration of tailored to the geometric properties of spherical projections, and (3) an adapted mechanism in the encoder backbone specifically designed to capture the unique spatial patterns and discontinuities present in range-view pseudo-images. Our approach achieves competitive performance on SemanticKITTI while benefiting from the speed, scalability, and deployment simplicity of 2D-centric pipelines. This work highlights the viability of VFMs as general-purpose backbones for 3D perception and opens a path toward unified, foundation-model-driven LiDAR segmentation. Results lets us conclude that range-view segmentation methods using VFMs leads to promising results.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-33] RACap: Relation-Aware Prompting for Lightweight Retrieval-Augmented Image Captioning</p>
<p>【速读】：该论文旨在解决当前检索增强型图像描述生成方法在关系建模方面的局限性，具体表现为：(1) 语义提示表示粒度过粗，难以捕捉细粒度的物体间关系；(2) 缺乏对图像中对象及其语义关系的显式建模。解决方案的关键在于提出RACap模型，该模型通过挖掘检索得到的描述文本中的结构化关系语义，并识别图像中的异构对象，从而有效检索包含异构视觉信息的结构化关系特征，显著提升描述的语义一致性和关系表达能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15883">https://arxiv.org/abs/2509.15883</a><br>
<strong>作者</strong>: Xiaosheng Long,Hanyu Wang,Zhentao Song,Kun Luo,Hongde Liu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent retrieval-augmented image captioning methods incorporate external knowledge to compensate for the limitations in comprehending complex scenes. However, current approaches face challenges in relation modeling: (1) the representation of semantic prompts is too coarse-grained to capture fine-grained relationships; (2) these methods lack explicit modeling of image objects and their semantic relationships. To address these limitations, we propose RACap, a relation-aware retrieval-augmented model for image captioning, which not only mines structured relation semantics from retrieval captions, but also identifies heterogeneous objects from the image. RACap effectively retrieves structured relation features that contain heterogeneous visual information to enhance the semantic consistency and relational expressiveness. Experimental results show that RACap, with only 10.8M trainable parameters, achieves superior performance compared to previous lightweight captioning models.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-34] Self-Supervised Cross-Modal Learning for Image-to-Point Cloud Registration</p>
<p>【速读】：该论文旨在解决图像到点云（image-to-point cloud, I2P）配准问题，其核心挑战在于二维图像与三维点云之间存在的语义-几何鸿沟：图像虽富含纹理信息但深度模糊，而点云虽具有精确的度量空间结构却较为稀疏。现有方法常陷入局部最优解，难以实现鲁棒且高精度的跨模态对齐。解决方案的关键在于提出一种自监督框架 CrossI2P，其创新性地将跨模态学习与两阶段配准统一于端到端流水线中：首先通过双路径对比学习构建几何-语义融合嵌入空间，实现无需标注的双向对齐；其次采用粗到精策略，先在全局阶段建立超点-超像素对应关系，再通过几何约束进行点级精修；最后引入动态训练机制结合梯度归一化以平衡特征对齐、对应关系优化和位姿估计的损失权重，从而显著提升配准精度与鲁棒性，在KITTI和nuScenes基准上分别优于现有最优方法23.7%和37.9%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15882">https://arxiv.org/abs/2509.15882</a><br>
<strong>作者</strong>: Xingmei Wang,Xiaoyu Hu,Chengkai Huang,Ziyan Zeng,Guohao Nie,Quan Z. Sheng,Lina Yao<br>
<strong>机构</strong>: 1. University of Technology Sydney (悉尼科技大学); 2. University of Wollongong (卧龙岗大学); 3. Monash University (莫纳什大学); 4. University of Melbourne (墨尔本大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Bridging 2D and 3D sensor modalities is critical for robust perception in autonomous systems. However, image-to-point cloud (I2P) registration remains challenging due to the semantic-geometric gap between texture-rich but depth-ambiguous images and sparse yet metrically precise point clouds, as well as the tendency of existing methods to converge to local optima. To overcome these limitations, we introduce CrossI2P, a self-supervised framework that unifies cross-modal learning and two-stage registration in a single end-to-end pipeline. First, we learn a geometric-semantic fused embedding space via dual-path contrastive learning, enabling annotation-free, bidirectional alignment of 2D textures and 3D structures. Second, we adopt a coarse-to-fine registration paradigm: a global stage establishes superpoint-superpixel correspondences through joint intra-modal context and cross-modal interaction modeling, followed by a geometry-constrained point-level refinement for precise registration. Third, we employ a dynamic training mechanism with gradient normalization to balance losses for feature alignment, correspondence refinement, and pose estimation. Extensive experiments demonstrate that CrossI2P outperforms state-of-the-art methods by 23.7% on the KITTI Odometry benchmark and by 37.9% on nuScenes, significantly improving both accuracy and robustness.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-35] ENSAM: an efficient foundation model for interactive segmentation of 3D medical images</p>
<p>【速读】：该论文旨在解决3D医学图像分割中对大规模标注数据和高计算资源依赖的问题，特别是在有限数据与计算预算下实现高效、通用的分割性能。解决方案的关键在于提出一种轻量级且可提示（promptable）的模型ENSAM，其核心创新包括：基于SegResNet的编码器与U-Net结构结合prompt encoder和mask decoder；引入潜在交叉注意力（latent cross-attention）、相对位置编码（relative positional encoding）以增强空间建模能力；采用归一化注意力（normalized attention）提升训练稳定性；并使用Muon优化器加速收敛。该设计使得ENSAM在不到5,000个样本、单张32 GB GPU上仅用6小时即可从头训练，并在CVPR 2025挑战赛中展现出优于现有基线模型（如VISTA3D、SAM-Med3D）的分割性能，尤其在最终Dice相似系数（final DSC）上表现突出。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15874">https://arxiv.org/abs/2509.15874</a><br>
<strong>作者</strong>: Elias Stenhede,Agnar Martin Bjørnstad,Arian Ranjbar<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present ENSAM (Equivariant, Normalized, Segment Anything Model), a lightweight and promptable model for universal 3D medical image segmentation. ENSAM combines a SegResNet-based encoder with a prompt encoder and mask decoder in a U-Net-style architecture, using latent cross-attention, relative positional encoding, normalized attention, and the Muon optimizer for training. ENSAM is designed to achieve good performance under limited data and computational budgets, and is trained from scratch on under 5,000 volumes from multiple modalities (CT, MRI, PET, ultrasound, microscopy) on a single 32 GB GPU in 6 hours. As part of the CVPR 2025 Foundation Models for Interactive 3D Biomedical Image Segmentation Challenge, ENSAM was evaluated on hidden test set with multimodal 3D medical images, obtaining a DSC AUC of 2.404, NSD AUC of 2.266, final DSC of 0.627, and final NSD of 0.597, outperforming two previously published baseline models (VISTA3D, SAM-Med3D) and matching the third (SegVol), surpassing its performance in final DSC but trailing behind in the other three metrics. In the coreset track of the challenge, ENSAM ranks 5th of 10 overall and best among the approaches not utilizing pretrained weights. Ablation studies confirm that our use of relative positional encodings and the Muon optimizer each substantially speed up convergence and improve segmentation quality.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-36] Zero-Shot Visual Grounding in 3D Gaussians via View Retrieval</p>
<p>【速读】：该论文旨在解决3D视觉定位（3D Visual Grounding, 3DVG）中两个核心挑战：一是现有方法难以处理3D高斯溅射（3D Gaussian Splatting, 3DGS）中空间纹理的隐式表示，导致必须进行每场景训练；二是通常需要大量标注数据才能有效训练。解决方案的关键在于提出一种名为GVR（Grounding via View Retrieval）的新颖零样本视觉定位框架，其核心思想是将3DVG任务转化为2D检索任务，通过对象级视图检索从多视角中收集定位线索，从而避免昂贵的3D标注过程和每场景训练需求，同时实现卓越的零样本性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15871">https://arxiv.org/abs/2509.15871</a><br>
<strong>作者</strong>: Liwei Liao,Xufeng Li,Xiaoyun Zheng,Boning Liu,Feng Gao,Ronggang Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:3D Visual Grounding (3DVG) aims to locate objects in 3D scenes based on text prompts, which is essential for applications such as robotics. However, existing 3DVG methods encounter two main challenges: first, they struggle to handle the implicit representation of spatial textures in 3D Gaussian Splatting (3DGS), making per-scene training indispensable; second, they typically require larges amounts of labeled data for effective training. To this end, we propose \underlineGrounding via \underlineView \underlineRetrieval (GVR), a novel zero-shot visual grounding framework for 3DGS to transform 3DVG as a 2D retrieval task that leverages object-level view retrieval to collect grounding clues from multiple views, which not only avoids the costly process of 3D annotation, but also eliminates the need for per-scene training. Extensive experiments demonstrate that our method achieves state-of-the-art visual grounding performance while avoiding per-scene training, providing a solid foundation for zero-shot 3DVG research. Video demos can be found in this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-37] LC-SLab – An Object-based Deep Learning Framework for Large-scale Land Cover Classification from Satellite Imagery and Sparse In-situ Labels</p>
<p>【速读】：该论文旨在解决利用稀疏监督数据训练深度学习模型进行大尺度土地覆盖制图时，传统像素级分类方法因空间覆盖稀疏而导致预测结果碎片化和噪声大的问题。其解决方案的关键在于提出LC-SLab框架，首次系统性地探索基于对象（object-based）的深度学习方法在稀疏监督下的应用，通过两种聚合策略实现语义一致性的提升：一是输入层聚合，借助图神经网络对图像区域进行特征整合；二是输出层聚合，对现有语义分割模型的结果进行后处理以生成更连贯的地块标签。此外，引入大规模预训练网络提取的特征增强小样本性能，实验证明该框架在保持甚至超越像素级模型精度的同时显著减少地图碎片化，且不同聚合方式在不同数据规模下表现各异，展现出良好的适应性和实用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15868">https://arxiv.org/abs/2509.15868</a><br>
<strong>作者</strong>: Johannes Leonhardt,Juergen Gall,Ribana Roscher<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large-scale land cover maps generated using deep learning play a critical role across a wide range of Earth science applications. Open in-situ datasets from principled land cover surveys offer a scalable alternative to manual annotation for training such models. However, their sparse spatial coverage often leads to fragmented and noisy predictions when used with existing deep learning-based land cover mapping approaches. A promising direction to address this issue is object-based classification, which assigns labels to semantically coherent image regions rather than individual pixels, thereby imposing a minimum mapping unit. Despite this potential, object-based methods remain underexplored in deep learning-based land cover mapping pipelines, especially in the context of medium-resolution imagery and sparse supervision. To address this gap, we propose LC-SLab, the first deep learning framework for systematically exploring object-based deep learning methods for large-scale land cover classification under sparse supervision. LC-SLab supports both input-level aggregation via graph neural networks, and output-level aggregation by postprocessing results from established semantic segmentation models. Additionally, we incorporate features from a large pre-trained network to improve performance on small datasets. We evaluate the framework on annual Sentinel-2 composites with sparse LUCAS labels, focusing on the tradeoff between accuracy and fragmentation, as well as sensitivity to dataset size. Our results show that object-based methods can match or exceed the accuracy of common pixel-wise models while producing substantially more coherent maps. Input-level aggregation proves more robust on smaller datasets, whereas output-level aggregation performs best with more data. Several configurations of LC-SLab also outperform existing land cover products, highlighting the framework’s practical utility.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-38] Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决长尾分布（long-tail distribution）分类数据集在机器学习中导致模型对少数类表现不佳的问题，尤其是在使用基础模型（foundation models）进行微调时仍存在性能差距且计算资源消耗大的局限。其解决方案的关键在于利用视觉基础模型（Vision Foundation Models）丰富的语义潜在空间生成合成数据，并结合真实数据训练一个简单的线性分类器（linear classifier），从而显著降低可训练参数数量（仅需线性模型的参数量），在保持高性能的同时实现更高的计算效率。该方法在CIFAR-100-LT和Places-LT基准上均取得了优于现有方法的结果，验证了其有效性与适应性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15859">https://arxiv.org/abs/2509.15859</a><br>
<strong>作者</strong>: Nakul Sharma<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted to Curated Data for Efficient Learning Workshop at ICCV 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Imbalanced classification datasets pose significant challenges in machine learning, often leading to biased models that perform poorly on underrepresented classes. With the rise of foundation models, recent research has focused on the full, partial, and parameter-efficient fine-tuning of these models to deal with long-tail classification. Despite the impressive performance of these works on the benchmark datasets, they still fail to close the gap with the networks trained using the balanced datasets and still require substantial computational resources, even for relatively smaller datasets. Underscoring the importance of computational efficiency and simplicity, in this work we propose a novel framework that leverages the rich semantic latent space of Vision Foundation Models to generate synthetic data and train a simple linear classifier using a mixture of real and synthetic data for long-tail classification. The computational efficiency gain arises from the number of trainable parameters that are reduced to just the number of parameters in the linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT benchmark and demonstrates strong performance on the Places-LT benchmark, highlighting the effectiveness and adaptability of our simple and effective approach.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-39] FedHK-MVFC: Federated Heat Kernel Multi-View Clustering</p>
<p>【速读】：该论文旨在解决分布式医疗场景下多视角数据聚类的挑战，尤其是在保护患者隐私的前提下实现高效、准确的协同分析。其核心问题是传统聚类方法难以有效融合来自不同医院的异构医学数据（如心电图、心脏影像和行为数据），同时无法满足HIPAA等隐私合规要求。解决方案的关键在于引入基于量子场论与谱分析的热核距离（Heat Kernel Distance, HKD）变换机制，将欧氏距离转化为几何感知的相似性度量，从而更好地捕捉医疗数据的内在结构；在此基础上构建了两种算法：用于中心化分析的热核增强多视图模糊聚类（HK-MVFC）和用于联邦学习环境的联邦热核多视图模糊聚类（FedHK-MVFC），后者结合差分隐私和安全聚合技术保障数据隐私，并通过自适应视图加权策略提升模型鲁棒性与准确性。实验证明该方法在保持98.2%效率的同时显著提升聚类精度并减少通信开销，为医疗健康领域的几何感知联邦学习提供了理论严谨且临床可行的新范式。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15844">https://arxiv.org/abs/2509.15844</a><br>
<strong>作者</strong>: Kristina P. Sinaga<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Distributed, Parallel, and Cluster Computing (cs.DC); Algebraic Geometry (<a target="_blank" rel="noopener" href="http://math.AG">math.AG</a>)<br>
<strong>备注</strong>:  41 pages, 9 figures, and 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In the realm of distributed AI and privacy-focused medical applications, we propose a framework for multi-view clustering that links quantum field theory with federated healthcare analytics. Our method uses heat-kernel coefficients from spectral analysis to convert Euclidean distances into geometry-aware similarity measures, capturing the structure of diverse medical data. We lay this out through the Heat Kernel Distance (HKD) transformation with convergence guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across hospitals using differential privacy and secure aggregation to facilitate HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular patients show an  8-12 %  increase in clustering accuracy,  70 %  reduced communication, and  98.2 %  efficiency retention over centralized methods. Validated on 10,000 patient records across two hospitals, it proves useful for collaborative phenotyping involving ECG, cardiac imaging, and behavioral data. Our theoretical contributions include update rules with proven convergence, adaptive view weighting, and privacy-preserving protocols. This presents a new standard for geometry-aware federated learning in healthcare, turning advanced math into workable solutions for analyzing sensitive medical data while ensuring both rigor and clinical relevance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-40] Boosting Active Learning with Knowledge Transfer</p>
<p>【速读】：该论文旨在解决主动学习（Active Learning, AL）中不确定性估计的难题，尤其是在计算生物学等领域（如冷冻电子断层扫描图像分类）中，传统方法依赖复杂辅助模型和特殊训练方式，难以部署且泛化能力弱。解决方案的关键在于提出一种基于知识迁移的不确定性估计框架：通过教师-学生架构，在每个AL周期中同步训练任务模型（教师）与一个任务无关的辅助模型（学生），并利用两者输出之间的距离作为不确定性度量指标。该方法不依赖特定训练策略（如对抗训练），且实验证明数据不确定性更关联于任务损失的上界而非具体数值，从而提升了方法的通用性与效率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15805">https://arxiv.org/abs/2509.15805</a><br>
<strong>作者</strong>: Tianyang Wang,Xi Xiao,Gaofei Chen,Xiaoying Liao,Guo Cheng,Yingrui Ji<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Uncertainty estimation is at the core of Active Learning (AL). Most existing methods resort to complex auxiliary models and advanced training fashions to estimate uncertainty for unlabeled data. These models need special design and hence are difficult to train especially for domain tasks, such as Cryo-Electron Tomography (cryo-ET) classification in computational biology. To address this challenge, we propose a novel method using knowledge transfer to boost uncertainty estimation in AL. Specifically, we exploit the teacher-student mode where the teacher is the task model in AL and the student is an auxiliary model that learns from the teacher. We train the two models simultaneously in each AL cycle and adopt a certain distance between the model outputs to measure uncertainty for unlabeled data. The student model is task-agnostic and does not rely on special training fashions (e.g. adversarial), making our method suitable for various tasks. More importantly, we demonstrate that data uncertainty is not tied to concrete value of task loss but closely related to the upper-bound of task loss. We conduct extensive experiments to validate the proposed method on classical computer vision tasks and cryo-ET challenges. The results demonstrate its efficacy and efficiency.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-41] CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决文本到图像（Text-to-Image, T2I）生成模型中存在的“品牌偏见”（brand bias）问题，即模型在接收到通用提示词时倾向于生成包含主流商业品牌的图像内容，从而引发伦理与法律风险。解决方案的关键在于提出一种名为CIDER的轻量级、模型无关的推理阶段（inference-time）框架，通过两个核心组件实现：一是使用轻量级检测器识别图像中的品牌内容，二是借助视觉-语言模型（Vision-Language Model, VLM）生成风格迥异但语义一致的替代方案，从而在不重新训练模型的前提下有效缓解偏见，同时保持图像质量与美学吸引力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15803">https://arxiv.org/abs/2509.15803</a><br>
<strong>作者</strong>: Fangjian Shen,Zifeng Liang,Chao Wang,Wushao Wen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  5 pages, 7 figures, submitted to ICASSP2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Text-to-image (T2I) models exhibit a significant yet under-explored “brand bias”, a tendency to generate contents featuring dominant commercial brands from generic prompts, posing ethical and legal risks. We propose CIDER, a novel, model-agnostic framework to mitigate bias at inference-time through prompt refinement to avoid costly retraining. CIDER uses a lightweight detector to identify branded content and a Vision-Language Model (VLM) to generate stylistically divergent alternatives. We introduce the Brand Neutrality Score (BNS) to quantify this issue and perform extensive experiments on leading T2I models. Results show CIDER significantly reduces both explicit and implicit biases while maintaining image quality and aesthetic appeal. Our work offers a practical solution for more original and equitable content, contributing to the development of trustworthy generative AI.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-42] ChronoForge-RL: Chronological Forging through Reinforcement Learning for Enhanced Video Understanding</p>
<p>【速读】：该论文旨在解决当前视频理解方法面临的两大挑战：一是密集视频内容中逐帧处理的计算不可行性，二是通过简单均匀采样难以识别语义显著帧的问题。解决方案的关键在于提出一种名为ChronoForge-RL的新框架，其核心创新包括两个模块：一是Temporal Apex Distillation (TAD)，通过变异性评分、拐点检测与优先蒸馏三阶段机制，实现可微分的关键帧选择，从而在保留时序信息的同时提升计算效率；二是KeyFrame-aware Group Relative Policy Optimization (KF-GRPO)，引入对比学习范式与显著性增强的奖励机制，显式激励模型利用帧内容和时序关系进行有效推理。该方法在VideoMME（69.1%）和LVBench（52.7%）上显著优于基线，并使7B参数模型性能媲美72B规模模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15800">https://arxiv.org/abs/2509.15800</a><br>
<strong>作者</strong>: Kehua Chen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  10 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Current state-of-the-art video understanding methods typically struggle with two critical challenges: (1) the computational infeasibility of processing every frame in dense video content and (2) the difficulty in identifying semantically significant frames through naive uniform sampling strategies. In this paper, we propose a novel video understanding framework, called ChronoForge-RL, which combines Temporal Apex Distillation (TAD) and KeyFrame-aware Group Relative Policy Optimization (KF-GRPO) to tackle these issues. Concretely, we introduce a differentiable keyframe selection mechanism that systematically identifies semantic inflection points through a three-stage process to enhance computational efficiency while preserving temporal information. Then, two particular modules are proposed to enable effective temporal reasoning: Firstly, TAD leverages variation scoring, inflection detection, and prioritized distillation to select the most informative frames. Secondly, we introduce KF-GRPO which implements a contrastive learning paradigm with a saliency-enhanced reward mechanism that explicitly incentivizes models to leverage both frame content and temporal relationships. Finally, our proposed ChronoForge-RL achieves 69.1% on VideoMME and 52.7% on LVBench compared to baseline methods, clearly surpassing previous approaches while enabling our 7B parameter model to achieve performance comparable to 72B parameter alternatives.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-43] ASAM: Terrain-and-Aware Segment Anything Model for Temporal-Scale Remote Sensing Segmentation</p>
<p>【速读】：该论文旨在解决生成式 AI（Generative AI）模型 SAM 在遥感图像分割任务中泛化能力不足的问题，尤其针对复杂地形、多尺度目标和时序动态等遥感数据特有的挑战。解决方案的关键在于提出 TASAM（Terrain and Temporally-aware SAM），其核心创新包括：一个注入高程先验信息的地形感知适配器（terrain-aware adapter）、一个捕捉地表覆盖变化的时序提示生成器（temporal prompt generator），以及一种增强细粒度目标分割的多尺度融合策略（multi-scale fusion strategy）。该方法无需重训练 SAM 主干网络，在 LoveDA、iSAID 和 WHU-CD 三个遥感基准上显著优于零样本 SAM 及专用模型，同时保持极低的计算开销，验证了领域自适应增强对基础模型在地理空间场景中鲁棒性提升的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15795">https://arxiv.org/abs/2509.15795</a><br>
<strong>作者</strong>: Tianyang Wang,Xi Xiao,Gaofei Chen,Hanzhang Chi,Qi Zhang,Guo Cheng,Yingrui Ji<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Segment Anything Model (SAM) has demonstrated impressive zero-shot segmentation capabilities across natural image domains, but it struggles to generalize to the unique challenges of remote sensing data, such as complex terrain, multi-scale objects, and temporal dynamics. In this paper, we introduce TASAM, a terrain and temporally-aware extension of SAM designed specifically for high-resolution remote sensing image segmentation. TASAM integrates three lightweight yet effective modules: a terrain-aware adapter that injects elevation priors, a temporal prompt generator that captures land-cover changes over time, and a multi-scale fusion strategy that enhances fine-grained object delineation. Without retraining the SAM backbone, our approach achieves substantial performance gains across three remote sensing benchmarks-LoveDA, iSAID, and WHU-CD-outperforming both zero-shot SAM and task-specific models with minimal computational overhead. Our results highlight the value of domain-adaptive augmentation for foundation models and offer a scalable path toward more robust geospatial segmentation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-44] Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决无监督域泛化（Unsupervised Domain Generalization, UDG）中缺乏类别标签和域标签时，模型难以区分语义信息与非语义变化（如视角、光照等）的问题。其解决方案的核心在于将UDG形式化为学习一个<strong>最小充分语义表示</strong>（Minimal Sufficient Semantic Representation）的任务：该表示需满足两个目标——(i) 保留跨增强视图共享的所有语义信息（充分性），(ii) 最大程度去除与语义无关的信息（最小性）。作者从信息论角度理论证明了优化这两个目标可直接降低分布外风险，并提出可学习的MS-UDG模型实现这一目标：通过基于InfoNCE的目标实现充分性，结合一种新颖的语义-变化解耦损失和基于重建机制的变体捕捉策略来促进最小性。实验证明，MS-UDG在多个主流无监督域泛化基准上达到新SOTA性能，且训练阶段无需类别或域标签。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15791">https://arxiv.org/abs/2509.15791</a><br>
<strong>作者</strong>: Tan Pan,Kaiyu Guo,Dongli Xu,Zhaorui Tan,Chen Jiang,Deshu Chen,Xin Guo,Brian C. Lovell,Limei Han,Yuan Cheng,Mahsa Baktashmotlagh<br>
<strong>机构</strong>: AI3, Fudan University (复旦大学); Shanghai Academy of Artificial Intelligence for Science (上海人工智能科学研究院); The University of Queensland (昆士兰大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The generalization ability of deep learning has been extensively studied in supervised settings, yet it remains less explored in unsupervised scenarios. Recently, the Unsupervised Domain Generalization (UDG) task has been proposed to enhance the generalization of models trained with prevalent unsupervised learning techniques, such as Self-Supervised Learning (SSL). UDG confronts the challenge of distinguishing semantics from variations without category labels. Although some recent methods have employed domain labels to tackle this issue, such domain labels are often unavailable in real-world contexts. In this paper, we address these limitations by formalizing UDG as the task of learning a Minimal Sufficient Semantic Representation: a representation that (i) preserves all semantic information shared across augmented views (sufficiency), and (ii) maximally removes information irrelevant to semantics (minimality). We theoretically ground these objectives from the perspective of information theory, demonstrating that optimizing representations to achieve sufficiency and minimality directly reduces out-of-distribution risk. Practically, we implement this optimization through Minimal-Sufficient UDG (MS-UDG), a learnable model by integrating (a) an InfoNCE-based objective to achieve sufficiency; (b) two complementary components to promote minimality: a novel semantic-variation disentanglement loss and a reconstruction-based mechanism for capturing adequate variation. Empirically, MS-UDG sets a new state-of-the-art on popular unsupervised domain-generalization benchmarks, consistently outperforming existing SSL and UDG methods, without category or domain labels during representation learning.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-45] FoBa: A Foreground-Background co-Guided Method and New Benchmark for Remote Sensing Semantic Change Detection</p>
<p>【速读】：该论文针对遥感语义变化检测（Remote Sensing Semantic Change Detection, SCD）中存在的两大问题展开研究：一是现有数据集在变化类别、类型和细粒度分类方面存在不足，难以支撑实际应用；二是现有方法对变化信息利用不充分，通常仅将其作为后处理步骤以提升空间一致性，限制了模型性能的进一步提升。解决方案的关键在于构建一个名为LevirSCD的新基准数据集，并提出一种前景-背景协同引导的变化检测方法（Foreground-Background Co-Guided SCD, FoBa）。FoBa通过前景区域聚焦于感兴趣目标、背景区域融合上下文信息，协同引导模型降低语义模糊性并增强对细微变化的检测能力；同时引入门控交互融合模块（Gated Interaction Fusion, GIF）与简单的一致性损失函数，强化双时相特征交互与空间一致性建模，从而显著提升检测精度，在多个数据集上实现了优于当前SOTA方法的性能表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15788">https://arxiv.org/abs/2509.15788</a><br>
<strong>作者</strong>: Haotian Zhang,Han Guo,Keyan Chen,Hao Chen,Zhengxia Zou,Zhenwei Shi<br>
<strong>机构</strong>: Beihang University (北京航空航天大学); Shanghai Artificial Intelligence Laboratory (上海人工智能实验室)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Despite the remarkable progress achieved in remote sensing semantic change detection (SCD), two major challenges remain. At the data level, existing SCD datasets suffer from limited change categories, insufficient change types, and a lack of fine-grained class definitions, making them inadequate to fully support practical applications. At the methodological level, most current approaches underutilize change information, typically treating it as a post-processing step to enhance spatial consistency, which constrains further improvements in model performance. To address these issues, we construct a new benchmark for remote sensing SCD, LevirSCD. Focused on the Beijing area, the dataset covers 16 change categories and 210 specific change types, with more fine-grained class definitions (e.g., roads are divided into unpaved and paved roads). Furthermore, we propose a foreground-background co-guided SCD (FoBa) method, which leverages foregrounds that focus on regions of interest and backgrounds enriched with contextual information to guide the model collaboratively, thereby alleviating semantic ambiguity while enhancing its ability to detect subtle changes. Considering the requirements of bi-temporal interaction and spatial consistency in SCD, we introduce a Gated Interaction Fusion (GIF) module along with a simple consistency loss to further enhance the model’s detection performance. Extensive experiments on three datasets (SECOND, JL1, and the proposed LevirSCD) demonstrate that FoBa achieves competitive results compared to current SOTA methods, with improvements of 1.48%, 3.61%, and 2.81% in the SeK metric, respectively. Our code and dataset are available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-46] CBPNet: A Continual Backpropagation Prompt Network for Alleviating Plasticity Loss on Edge Devices</p>
<p>【速读】：该论文旨在解决边缘设备上持续学习（Continual Learning）中的<strong>塑性损失（plasticity loss）<strong>问题，即由于冻结预训练主干网络（backbone）和提示参数容量有限，导致模型在学习新知识时能力下降。解决方案的关键在于提出</strong>连续反向传播提示网络（CBPNet）</strong>，其核心创新是引入一个高效的<strong>连续反向传播块（Efficient CBP Block）</strong>，通过自适应重置训练过程中未充分利用的参数，恢复模型的学习活力（learning vitality），从而在仅增加少于0.2%主干参数量的情况下显著提升性能，在Split CIFAR-100和Split ImageNet-R等基准上均取得显著改进。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15785">https://arxiv.org/abs/2509.15785</a><br>
<strong>作者</strong>: Runjie Shao,Boyu Diao,Zijia An,Ruiqi Liu,Yongjun Xu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:To meet the demands of applications like robotics and autonomous driving that require real-time responses to dynamic environments, efficient continual learning methods suitable for edge devices have attracted increasing attention. In this transition, using frozen pretrained models with prompts has become a mainstream strategy to combat catastrophic forgetting. However, this approach introduces a new critical bottleneck: plasticity loss, where the model’s ability to learn new knowledge diminishes due to the frozen backbone and the limited capacity of prompt parameters. We argue that the reduction in plasticity stems from a lack of update vitality in underutilized parameters during the training process. To this end, we propose the Continual Backpropagation Prompt Network (CBPNet), an effective and parameter efficient framework designed to restore the model’s learning vitality. We innovatively integrate an Efficient CBP Block that counteracts plasticity decay by adaptively reinitializing these underutilized parameters. Experimental results on edge devices demonstrate CBPNet’s effectiveness across multiple benchmarks. On Split CIFAR-100, it improves average accuracy by over 1% against a strong baseline, and on the more challenging Split ImageNet-R, it achieves a state of the art accuracy of 69.41%. This is accomplished by training additional parameters that constitute less than 0.2% of the backbone’s size, validating our approach.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-47] Ideal Registration? Segmentation is All You Need</p>
<p>【速读】：该论文旨在解决当前深度学习图像配准方法中普遍存在的全局均匀平滑约束无法适应解剖结构复杂、区域性差异显著的形变问题。其解决方案的关键在于提出一种基于分割驱动的配准框架（SegReg），通过引入解剖学自适应正则化机制，利用区域特异性形变模式实现局部形变场的优化与整合：首先对移动和固定图像进行解剖学一致的子区域分割，随后在相同配准主干网络中分别计算各局部区域的最优部分形变场，并最终融合为全局形变场，从而显著提升配准精度与临床适用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15784">https://arxiv.org/abs/2509.15784</a><br>
<strong>作者</strong>: Xiang Chen,Fengting Zhang,Qinghao Liu,Min Liu,Kun Wu,Yaonan Wang,Hang Zhang<br>
<strong>机构</strong>: 湖南大学(University of Hunan); 华中科技大学(Huazhong University of Science and Technology); 中国科学院(CAS)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep learning has revolutionized image registration by its ability to handle diverse tasks while achieving significant speed advantages over conventional approaches. Current approaches, however, often employ globally uniform smoothness constraints that fail to accommodate the complex, regionally varying deformations characteristic of anatomical motion. To address this limitation, we propose SegReg, a Segmentation-driven Registration framework that implements anatomically adaptive regularization by exploiting region-specific deformation patterns. Our SegReg first decomposes input moving and fixed images into anatomically coherent subregions through segmentation. These localized domains are then processed by the same registration backbone to compute optimized partial deformation fields, which are subsequently integrated into a global deformation field. SegReg achieves near-perfect structural alignment (98.23% Dice on critical anatomies) using ground-truth segmentation, and outperforms existing methods by 2-12% across three clinical registration scenarios (cardiac, abdominal, and lung images) even with automatic segmentation. Our SegReg demonstrates a near-linear dependence of registration accuracy on segmentation quality, transforming the registration challenge into a segmentation problem. The source code will be released upon manuscript acceptance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-48] Enriched Feature Representation and Motion Prediction Module for MOSEv2 Track of 7th LSVOS Challenge: 3rd Place Solution <mark class="hl-label red">ICCV</mark></p>
<p>【速读】：该论文旨在解决视频目标分割（Video Object Segmentation, VOS）任务中特征表达能力不足与时间建模稳定性差的问题。现有方法如Cutie虽具备良好的查询驱动分割能力，但受限于编码器的特征容量；而SAM2虽通过预训练ViT编码器提供更丰富的特征表示，但在时序一致性上表现不足。解决方案的关键在于：1）将Cutie的编码器替换为SAM2的ViT编码器以增强特征表达能力；2）引入运动预测模块以提升帧间分割结果的时序稳定性；3）采用集成策略融合Cutie、SAM2及其改进版本（SCOPE），最终在LSVOS Challenge的MOSEv2赛道中取得第三名，验证了上述设计的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15781">https://arxiv.org/abs/2509.15781</a><br>
<strong>作者</strong>: Chang Soo Lim,Joonyoung Moon,Donghyeon Cho<br>
<strong>机构</strong>: Hanyang University (汉阳大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  5 pages,2 figures, ICCV Workshop (MOSEv2 Track of 7th LSVOS Challenge)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Video object segmentation (VOS) is a challenging task with wide applications such as video editing and autonomous driving. While Cutie provides strong query-based segmentation and SAM2 offers enriched representations via a pretrained ViT encoder, each has limitations in feature capacity and temporal modeling. In this report, we propose a framework that integrates their complementary strengths by replacing the encoder of Cutie with the ViT encoder of SAM2 and introducing a motion prediction module for temporal stability. We further adopt an ensemble strategy combining Cutie, SAM2, and our variant, achieving 3rd place in the MOSEv2 track of the 7th LSVOS Challenge. We refer to our final model as SCOPE (SAM2-CUTIE Object Prediction Ensemble). This demonstrates the effectiveness of enriched feature representation and motion prediction for robust video object segmentation. The code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-49] Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation</p>
<p>【速读】：该论文旨在解决基于Score Distillation Sampling (SDS) 的文本到3D生成方法所面临的两大核心问题：一是依赖CLIP类文本编码器导致语义对齐粗粒度、难以处理细粒度提示；二是2D扩散先验缺乏显式的3D空间约束，造成几何不一致性和多物体场景中对象关系错误。解决方案的关键在于提出VLM3D框架，将大视觉语言模型（Vision-Language Models, VLMs）引入SDS管道，作为可微分的语义与空间先验。VLMs通过语言引导的监督实现细粒度提示对齐，并凭借其内在的视觉-语言建模能力提供强空间理解，从而显著提升单物体生成的3D一致性及多物体场景中的关系推理能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15772">https://arxiv.org/abs/2509.15772</a><br>
<strong>作者</strong>: Weimin Bai,Yubo Li,Weijian Luo,Wenzheng Chen,He Sun<br>
<strong>机构</strong>: Peking University (北京大学); Xiaohongshu Inc (小红书公司)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Score Distillation Sampling (SDS) enables high-quality text-to-3D generation by supervising 3D models through the denoising of multi-view 2D renderings, using a pretrained text-to-image diffusion model to align with the input prompt and ensure 3D consistency. However, existing SDS-based methods face two fundamental limitations: (1) their reliance on CLIP-style text encoders leads to coarse semantic alignment and struggles with fine-grained prompts; and (2) 2D diffusion priors lack explicit 3D spatial constraints, resulting in geometric inconsistencies and inaccurate object relationships in multi-object scenes. To address these challenges, we propose VLM3D, a novel text-to-3D generation framework that integrates large vision-language models (VLMs) into the SDS pipeline as differentiable semantic and spatial priors. Unlike standard text-to-image diffusion priors, VLMs leverage rich language-grounded supervision that enables fine-grained prompt alignment. Moreover, their inherent vision language modeling provides strong spatial understanding, which significantly enhances 3D consistency for single-object generation and improves relational reasoning in multi-object scenes. We instantiate VLM3D based on the open-source Qwen2.5-VL model and evaluate it on the GPTeval3D benchmark. Experiments across diverse objects and complex scenes show that VLM3D significantly outperforms prior SDS-based methods in semantic fidelity, geometric coherence, and spatial correctness.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-50] Overview of PlantCLEF 2024: multi-species plant identification in vegetation plot images</p>
<p>【速读】：该论文旨在解决生态学研究中植物物种识别效率低下的问题，尤其是在高分辨率样方图像（plot images）上进行多物种标注时，传统人工识别方法耗时且难以扩展。其解决方案的关键在于构建一个大规模、专家标注的多标签图像数据集（PlantCLEF 2024），包含数千张图像和超过800种植物物种，并提供170万张个体植物图像用于预训练，结合先进的视觉Transformer模型（vision transformer models），将任务定义为弱标签多标签分类（weakly-labeled multi-label classification），从而提升AI在复杂生态场景下自动识别多种植物物种的能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15768">https://arxiv.org/abs/2509.15768</a><br>
<strong>作者</strong>: Herve Goeau,Vincent Espitalier,Pierre Bonnet,Alexis Joly<br>
<strong>机构</strong>: CIRAD, UMR AMAP, Montpellier, Occitanie, France; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  10 pages, 3 figures, CLEF 2024 Conference and Labs of the Evaluation Forum, September 09 to 12, 2024, Grenoble, France</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Plot images are essential for ecological studies, enabling standardized sampling, biodiversity assessment, long-term monitoring and remote, large-scale surveys. Plot images are typically fifty centimetres or one square meter in size, and botanists meticulously identify all the species found there. The integration of AI could significantly improve the efficiency of specialists, helping them to extend the scope and coverage of ecological studies. To evaluate advances in this regard, the PlantCLEF 2024 challenge leverages a new test set of thousands of multi-label images annotated by experts and covering over 800 species. In addition, it provides a large training set of 1.7 million individual plant images as well as state-of-the-art vision transformer models pre-trained on this data. The task is evaluated as a (weakly-labeled) multi-label classification task where the aim is to predict all the plant species present on a high-resolution plot image (using the single-label training data). In this paper, we provide an detailed description of the data, the evaluation methodology, the methods and models employed by the participants and the results achieved.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-51] MCOD: The First Challenging Benchmark for Multispectral Camouflaged Object Detection</p>
<p>【速读】：该论文旨在解决当前**伪装目标检测（Camouflaged Object Detection, COD）**研究中缺乏多光谱（multispectral）数据集的问题，从而限制了利用光谱信息提升检测鲁棒性的进展。现有基准数据集均为RGB图像，无法支持多光谱方法的开发与评估。解决方案的关键在于提出首个专为多光谱COD设计的挑战性基准数据集MCOD，其核心优势包括：(i) 全面覆盖真实场景中的复杂属性（如小目标尺寸、极端光照条件），(ii) 涵盖多样化的自然环境以增强实用性，(iii) 提供高质量像素级标注（含精确目标掩膜及挑战属性标签）。实验表明，引入多光谱模态可显著缓解因任务难度增加导致的性能下降，验证了光谱信息在提升检测鲁棒性方面的价值。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15753">https://arxiv.org/abs/2509.15753</a><br>
<strong>作者</strong>: Yang Li,Tingfa Xu,Shuyan Bai,Peifu Liu,Jianan Li<br>
<strong>机构</strong>: Beijing Institute of Technology (北京理工大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Camouflaged Object Detection (COD) aims to identify objects that blend seamlessly into natural scenes. Although RGB-based methods have advanced, their performance remains limited under challenging conditions. Multispectral imagery, providing rich spectral information, offers a promising alternative for enhanced foreground-background discrimination. However, existing COD benchmark datasets are exclusively RGB-based, lacking essential support for multispectral approaches, which has impeded progress in this area. To address this gap, we introduce MCOD, the first challenging benchmark dataset specifically designed for multispectral camouflaged object detection. MCOD features three key advantages: (i) Comprehensive challenge attributes: It captures real-world difficulties such as small object sizes and extreme lighting conditions commonly encountered in COD tasks. (ii) Diverse real-world scenarios: The dataset spans a wide range of natural environments to better reflect practical applications. (iii) High-quality pixel-level annotations: Each image is manually annotated with precise object masks and corresponding challenge attribute labels. We benchmark eleven representative COD methods on MCOD, observing a consistent performance drop due to increased task difficulty. Notably, integrating multispectral modalities substantially alleviates this degradation, highlighting the value of spectral information in enhancing detection robustness. We anticipate MCOD will provide a strong foundation for future research in multispectral camouflaged object detection. The dataset is publicly accessible at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-52] Simulated Cortical Magnification Supports Self-Supervised Object Learning</p>
<p>【速读】：该论文旨在解决当前自监督学习模型在构建语义物体表征时忽略人类视觉系统中中央高分辨率、周边低分辨率的共焦特性（foveated vision）的问题。其解决方案的关键在于引入基于人类共焦机制和皮层放大效应（cortical magnification）的视觉处理方法，对自然场景的视角视频数据进行模拟处理，使图像内容在周边区域变得模糊，从而更贴近真实人类视觉体验；在此基础上训练两种生物启发式的自监督学习模型，采用时间依赖的学习目标，结果表明这种对共焦特性的建模显著提升了物体表征的质量，主要源于使物体在感知上显得更大，并优化了中心与周边视觉信息之间的权衡。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15751">https://arxiv.org/abs/2509.15751</a><br>
<strong>作者</strong>: Zhengyang Yu,Arthur Aubret,Chen Yu,Jochen Triesch<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted at IEEE ICDL 2025. 6 pages, 5 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent self-supervised learning models simulate the development of semantic object representations by training on visual experience similar to that of toddlers. However, these models ignore the foveated nature of human vision with high/low resolution in the center/periphery of the visual field. Here, we investigate the role of this varying resolution in the development of object representations. We leverage two datasets of egocentric videos that capture the visual experience of humans during interactions with objects. We apply models of human foveation and cortical magnification to modify these inputs, such that the visual content becomes less distinct towards the periphery. The resulting sequences are used to train two bio-inspired self-supervised learning models that implement a time-based learning objective. Our results show that modeling aspects of foveated vision improves the quality of the learned object representations in this setting. Our analysis suggests that this improvement comes from making objects appear bigger and inducing a better trade-off between central and peripheral visual information. Overall, this work takes a step towards making models of humans’ learning of visual representations more realistic and performant.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-53] FloorSAM: SAM-Guided Floorplan Reconstruction with Semantic-Geometric Fusion</p>
<p>【速读】：该论文旨在解决从激光雷达（LiDAR）点云数据中重建建筑楼层平面图时存在的噪声敏感性高、泛化能力弱以及几何细节丢失等问题。其核心解决方案是提出FloorSAM框架，通过将点云密度图与Segment Anything Model（SAM）相结合，利用网格滤波、自适应分辨率投影和图像增强技术构建鲁棒的俯视密度图，并借助SAM的零样本学习能力实现精准的房间分割；随后通过自适应提示点和多阶段过滤生成房间掩膜，并结合掩膜与点云的联合分析提取轮廓并进行正则化处理，从而获得高精度的楼层平面图及房间拓扑关系。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15750">https://arxiv.org/abs/2509.15750</a><br>
<strong>作者</strong>: Han Ye,Haofu Wang,Yunchi Zhang,Jiangjian Xiao,Yuqiang Jin,Jinyuan Liu,Wen-An Zhang,Uladzislau Sychou,Alexander Tuzikov,Vladislav Sobolevskii,Valerii Zakharov,Boris Sokolov,Minglei Fu<br>
<strong>机构</strong>: Zhejiang University of Technology (浙江工业大学); Chinese Academy of Sciences (中国科学院); National Academy of Belarus (白俄罗斯国家科学院); Russian Academy of Sciences (俄罗斯科学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  12 pages, 15 figures,</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reconstructing building floor plans from point cloud data is key for indoor navigation, BIM, and precise measurements. Traditional methods like geometric algorithms and Mask R-CNN-based deep learning often face issues with noise, limited generalization, and loss of geometric details. We propose FloorSAM, a framework that integrates point cloud density maps with the Segment Anything Model (SAM) for accurate floor plan reconstruction from LiDAR data. Using grid-based filtering, adaptive resolution projection, and image enhancement, we create robust top-down density maps. FloorSAM uses SAM’s zero-shot learning for precise room segmentation, improving reconstruction across diverse layouts. Room masks are generated via adaptive prompt points and multistage filtering, followed by joint mask and point cloud analysis for contour extraction and regularization. This produces accurate floor plans and recovers room topological relationships. Tests on Giblayout and ISPRS datasets show better accuracy, recall, and robustness than traditional methods, especially in noisy and complex settings. Code and materials: this http URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-54] Hybrid Lie semi-group and cascade structures for the generalized Gaussian derivative model for visual receptive fields</p>
<p>【速读】：该论文旨在解决在自然图像变换（如不同视角、光照或时间条件下对相似物体或时空事件的观测）下，视觉层次结构早期层中感受野响应因几何图像变换而产生显著变异的问题。为应对这一挑战，论文提出基于协变感受野族（covariant receptive field families）的方法，通过扩展感受野形状以覆盖图像变换的自由度来建模这种变异性。其解决方案的关键在于：一方面推导出感受野响应在不同形状参数下的无穷小关系（对应半群与李群的概念），另一方面建立宏观级联平滑性质（类似李代数结构但具有方向偏好），揭示粗尺度空间和时间上的感受野响应可通过细尺度响应叠加小支持增量滤波器高效计算。这些结果不仅深化了对多参数感受野族中空间与时空响应关系的理解，还可用于优化多参数感受野响应的计算效率，并为生物视觉中简单细胞的理想化理论模型提供依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15748">https://arxiv.org/abs/2509.15748</a><br>
<strong>作者</strong>: Tony Lindeberg<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>)<br>
<strong>备注</strong>:  25 pages, 9 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Because of the variabilities of real-world image structures under the natural image transformations that arise when observing similar objects or spatio-temporal events under different viewing conditions, the receptive field responses computed in the earliest layers of the visual hierarchy may be strongly influenced by such geometric image transformations. One way of handling this variability is by basing the vision system on covariant receptive field families, which expand the receptive field shapes over the degrees of freedom in the image transformations. This paper addresses the problem of deriving relationships between spatial and spatio-temporal receptive field responses obtained for different values of the shape parameters in the resulting multi-parameter families of receptive fields. For this purpose, we derive both (i) infinitesimal relationships, roughly corresponding to a combination of notions from semi-groups and Lie groups, as well as (ii) macroscopic cascade smoothing properties, which describe how receptive field responses at coarser spatial and temporal scales can be computed by applying smaller support incremental filters to the output from corresponding receptive fields at finer spatial and temporal scales, structurally related to the notion of Lie algebras, although with directional preferences. The presented results provide (i) a deeper understanding of the relationships between spatial and spatio-temporal receptive field responses for different values of the filter parameters, which can be used for both (ii) designing more efficient schemes for computing receptive field responses over populations of multi-parameter families of receptive fields, as well as (iii)~formulating idealized theoretical models of the computations of simple cells in biological vision.          Comments: 25 pages, 9 figures   Subjects:  Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>)  Cite as: arXiv:2509.15748 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]    (or  arXiv:2509.15748v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15748">https://doi.org/10.48550/arXiv.2509.15748</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-55] rueMoE: Dual-Routing Mixture of Discriminative Experts for Synthetic Image Detection</p>
<p>【速读】：该论文旨在解决现有合成图像检测方法在面对未见过的生成模式时泛化能力差的问题。当前大多数方法依赖于构建单一、通用的判别空间来区分真实与伪造内容，但这类统一空间往往复杂且脆弱，难以适应多样化的生成式 AI（Generative AI）伪造技术。其解决方案的关键在于提出 TrueMoE 框架，该框架采用双路由机制驱动的判别专家阵列（Discriminative Expert Array, DEA），将检测任务重构为多个轻量级、专业化判别子空间的协同推理过程；DEA 沿流形结构和感知粒度两个互补维度组织，使不同类型的伪造线索可在子空间中被有效捕捉，而双路由机制（粒度感知稀疏路由器与流形感知密集路由器）则动态地将输入图像分配至最相关的专家，从而显著提升模型的泛化能力和鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15741">https://arxiv.org/abs/2509.15741</a><br>
<strong>作者</strong>: Laixin Zhang,Shuaibo Li,Wei Ma,Hongbin Zha<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid progress of generative models has made synthetic image detection an increasingly critical task. Most existing approaches attempt to construct a single, universal discriminative space to separate real from fake content. However, such unified spaces tend to be complex and brittle, often struggling to generalize to unseen generative patterns. In this work, we propose TrueMoE, a novel dual-routing Mixture-of-Discriminative-Experts framework that reformulates the detection task as a collaborative inference across multiple specialized and lightweight discriminative subspaces. At the core of TrueMoE is a Discriminative Expert Array (DEA) organized along complementary axes of manifold structure and perceptual granularity, enabling diverse forgery cues to be captured across subspaces. A dual-routing mechanism, comprising a granularity-aware sparse router and a manifold-aware dense router, adaptively assigns input images to the most relevant experts. Extensive experiments across a wide spectrum of generative models demonstrate that TrueMoE achieves superior generalization and robustness.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-56] oward Medical Deepfake Detection: A Comprehensive Dataset and Novel Method</p>
<p>【速读】：该论文旨在解决生成式 AI 在医学影像领域带来的虚假图像风险问题，这类伪造图像可能引发诊断误导、财务欺诈和信息错误等严重后果。当前医学影像取证研究匮乏，缺乏专门针对医疗场景的大型数据集，且传统媒体取证方法因主要面向自然图像或人脸图像，难以捕捉 AI 生成医学影像中特有的细微伪造痕迹。解决方案的关键在于提出两个核心创新：一是构建了首个涵盖六种医学模态和十二种先进生成模型的大规模医学影像取证数据集 MedForensics；二是设计了一种双阶段知识注入检测器 DSKI，其包含跨域微调适配器（CDFA）用于在训练阶段从空间域和噪声域提取隐蔽伪造线索，以及医学取证检索模块（MFRM）通过少样本检索机制提升测试阶段的检测精度，从而显著优于现有方法及人类专家。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15711">https://arxiv.org/abs/2509.15711</a><br>
<strong>作者</strong>: Shuaibo Li,Zhaohu Xing,Hongqiu Wang,Pengfei Hao,Xingyu Li,Zekai Liu,Lei Zhu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid advancement of generative AI in medical imaging has introduced both significant opportunities and serious challenges, especially the risk that fake medical images could undermine healthcare systems. These synthetic images pose serious risks, such as diagnostic deception, financial fraud, and misinformation. However, research on medical forensics to counter these threats remains limited, and there is a critical lack of comprehensive datasets specifically tailored for this field. Additionally, existing media forensic methods, which are primarily designed for natural or facial images, are inadequate for capturing the distinct characteristics and subtle artifacts of AI-generated medical images. To tackle these challenges, we introduce \textbfMedForensics, a large-scale medical forensics dataset encompassing six medical modalities and twelve state-of-the-art medical generative models. We also propose \textbfDSKI, a novel \textbfDual-\textbfStage \textbfKnowledge \textbfInfusing detector that constructs a vision-language feature space tailored for the detection of AI-generated medical images. DSKI comprises two core components: 1) a cross-domain fine-trace adapter (CDFA) for extracting subtle forgery clues from both spatial and noise domains during training, and 2) a medical forensic retrieval module (MFRM) that boosts detection accuracy through few-shot retrieval during testing. Experimental results demonstrate that DSKI significantly outperforms both existing methods and human experts, achieving superior accuracy across multiple medical modalities.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-57] SGMAGNet: A Baseline Model for 3D Cloud Phase Structure Reconstruction on a New Passive Active Satellite Benchmark</p>
<p>【速读】：该论文旨在解决如何从多模态卫星观测数据中准确重建三维云相态结构（cloud phase profiles）的问题，以提升数值天气预报（Numerical Weather Prediction, NWP）系统中云微物理参数化方案的精度。其解决方案的关键在于构建了一个同步的图像-剖面配对基准数据集，并提出基于空间梯度增强的多尺度注意力网络（SGMAGNet）模型，该模型能够有效融合高时空分辨率可见光（VIS）与热红外（TIR）遥感影像与星载激光雷达（CALIOP/CALIPSO）和雷达（CPR/CloudSat）提供的精确垂直云相态剖面信息，从而实现复杂多层云及相变边界区域的高精度云相态识别与重建。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15706">https://arxiv.org/abs/2509.15706</a><br>
<strong>作者</strong>: Chi Yang,Fu Wang,Xiaofei Yang,Hao Huang,Weijia Cao,Xiaowen Chu<br>
<strong>机构</strong>: 1. Chinese Academy of Meteorological Sciences (中国气象科学研究院); 2. State Key Laboratory of Severe Weather, Chinese Academy of Meteorological Sciences (中国气象科学研究院强天气重点实验室); 3. Institute of Atmospheric Physics, Chinese Academy of Sciences (中国科学院大气物理研究所); 4. School of Information Science and Technology, Sun Yat-sen University (中山大学信息科学与技术学院); 5. College of Earth Science, Guizhou University (贵州大学地球科学学院); 6. School of Computer Science and Engineering, South China University of Technology (华南理工大学计算机科学与工程学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Atmospheric and Oceanic Physics (physics.ao-ph)<br>
<strong>备注</strong>:  9 pages, 4 figures, 2 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Cloud phase profiles are critical for numerical weather prediction (NWP), as they directly affect radiative transfer and precipitation processes. In this study, we present a benchmark dataset and a baseline framework for transforming multimodal satellite observations into detailed 3D cloud phase structures, aiming toward operational cloud phase profile retrieval and future integration with NWP systems to improve cloud microphysics parameterization. The multimodal observations consist of (1) high–spatiotemporal–resolution, multi-band visible (VIS) and thermal infrared (TIR) imagery from geostationary satellites, and (2) accurate vertical cloud phase profiles from spaceborne lidar (CALIOP\slash CALIPSO) and radar (CPR\slash CloudSat). The dataset consists of synchronized image–profile pairs across diverse cloud regimes, defining a supervised learning task: given VIS/TIR patches, predict the corresponding 3D cloud phase structure. We adopt SGMAGNet as the main model and compare it with several baseline architectures, including UNet variants and SegNet, all designed to capture multi-scale spatial patterns. Model performance is evaluated using standard classification metrics, including Precision, Recall, F1-score, and IoU. The results demonstrate that SGMAGNet achieves superior performance in cloud phase reconstruction, particularly in complex multi-layer and boundary transition regions. Quantitatively, SGMAGNet attains a Precision of 0.922, Recall of 0.858, F1-score of 0.763, and an IoU of 0.617, significantly outperforming all baselines across these key metrics.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-58] raining-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region Token and Instruction-Guided Importance</p>
<p>【速读】：该论文旨在解决大视觉语言模型（Large Vision-Language Models, LVLMs）在处理高分辨率图像时面临的计算效率低下问题。随着图像分辨率提升，现有方法通过将图像分割为多个子图来增强理解能力，但导致视觉标记（visual tokens）数量激增，进而引发推理阶段的指数级计算开销。为此，作者提出一种无需训练的标记剪枝策略——金字塔标记剪枝（Pyramid Token Pruning, PTP），其核心在于融合自底向上的视觉显著性（bottom-up visual saliency）与自顶向下的指令引导重要性（top-down instruction-guided importance），在区域和标记两个层级上实现精细化的标记保留机制。该方法借鉴人类视觉注意机制，优先保留视觉显著区域的标记，并进一步利用文本指令定位与特定多模态任务最相关的标记，从而在显著降低计算负载的同时维持模型性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15704">https://arxiv.org/abs/2509.15704</a><br>
<strong>作者</strong>: Yuxuan Liang,Xu Li,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue<br>
<strong>机构</strong>: Fudan University (复旦大学); Institute of Big Data (大数据研究院); College of Computer Science and Artificial Intelligence (计算机科学技术与人工智能学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Vision-Language Models (LVLMs) have significantly advanced multimodal understanding but still struggle with efficiently processing high-resolution images. Recent approaches partition high-resolution images into multiple sub-images, dramatically increasing the number of visual tokens and causing exponential computational overhead during inference. To address these limitations, we propose a training-free token pruning strategy, Pyramid Token Pruning (PTP), that integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided importance. Inspired by human visual attention mechanisms, PTP selectively retains more tokens from visually salient regions and further leverages textual instructions to pinpoint tokens most relevant to specific multimodal tasks. Extensive experiments across 13 diverse benchmarks demonstrate that our method substantially reduces computational overhead and inference latency with minimal performance loss.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-59] ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models</p>
<p>【速读】：该论文旨在解决大型视觉语言模型（Large Vision-Language Models, LVLMs）在非一致上下文（incongruous context）中出现的识别错误问题，具体表现为对象误识别和幻觉现象（hallucination），即模型在物体与场景语境不符时无法正确识别或生成不存在的对象。解决方案的关键在于提出一个新的基准测试工具——对象识别在非一致上下文基准（Object Recognition in Incongruous Context Benchmark, ORIC），其核心策略包括：(1) 利用大语言模型（LLM）引导采样，识别实际存在但语境上不合理的对象；(2) 基于CLIP模型引导采样，发现看似合理却并不存在的易被幻觉化对象，从而构建具有挑战性的非一致语境场景，系统评估LVLMs的鲁棒性与局限性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15695">https://arxiv.org/abs/2509.15695</a><br>
<strong>作者</strong>: Zhaoyang Li,Zhan Ling,Yuchen Zhou,Hao Su<br>
<strong>机构</strong>: University of California San Diego (加州大学圣地亚哥分校); Hillbot<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Vision-Language Models (LVLMs) have made significant strides in image caption, visual question answering, and robotics by integrating visual and textual information. However, they remain prone to errors in incongruous contexts, where objects appear unexpectedly or are absent when contextually expected. This leads to two key recognition failures: object misidentification and hallucination. To systematically examine this issue, we introduce the Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark that evaluates LVLMs in scenarios where object-context relationships deviate from expectations. ORIC employs two key strategies: (1) LLM-guided sampling, which identifies objects that are present but contextually incongruous, and (2) CLIP-guided sampling, which detects plausible yet nonexistent objects that are likely to be hallucinated, thereby creating an incongruous context. Evaluating 18 LVLMs and two open-vocabulary detection models, our results reveal significant recognition gaps, underscoring the challenges posed by contextual incongruity. This work provides critical insights into LVLMs’ limitations and encourages further research on context-aware object recognition.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-60] SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决3D点云与文本之间对比学习（contrastive learning）中因缺乏大规模高质量3D-text数据集而导致的对齐困难问题。其核心挑战在于如何有效提升模型在零样本分类、少样本分割及空间推理等任务中的泛化能力。解决方案的关键在于提出SceneForge框架，通过结构化多物体场景构建（structured multi-object scene compositions），将单个3D形状组合成具有明确空间关系的复杂场景，并配以由大语言模型（large language model, LLM）优化后的连贯多对象描述文本，从而生成更具语义复杂性和多样性的对比训练样本。这种基于场景构成的增强策略显著提升了数据丰富度和模型对空间关系的理解能力，且不依赖特定编码器架构，具备良好的通用性与迁移性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15693">https://arxiv.org/abs/2509.15693</a><br>
<strong>作者</strong>: Cristian Sbrolli,Matteo Matteucci<br>
<strong>机构</strong>: Politecnico di Milano (米兰理工大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>)<br>
<strong>备注</strong>:  to appear in NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The whole is greater than the sum of its parts-even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForge’s compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-61] Saccadic Vision for Fine-Grained Visual Classification</p>
<p>【速读】：该论文旨在解决细粒度视觉分类（Fine-grained Visual Classification, FGVC）中因类内差异大、类间差异小而导致的识别困难问题，尤其是传统基于局部区域的方法存在特征冗余、固定采样点难以量化最优部分数量以及下游任务适应性差等挑战。其解决方案的关键在于提出一种两阶段仿生视觉机制：第一阶段提取图像周边特征（粗略视图）并生成样本分布图，第二阶段通过非极大值抑制（Non-Maximum Suppression, NMS）从该图中采样焦点补丁（fixation patches），并使用共享权重编码器并行编码这些补丁；随后引入上下文感知的选择性注意力机制对各焦点补丁加权融合，从而有效缓解局部特征的空间冗余问题，并提升模型对关键判别区域的聚焦能力，最终在多个标准与挑战性数据集上实现优于基线方法的性能表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15688">https://arxiv.org/abs/2509.15688</a><br>
<strong>作者</strong>: Johann Schmidt,Sebastian Stober,Joachim Denzler,Paul Bodesheim<br>
<strong>机构</strong>: Otto-von-Guericke University Magdeburg (马格德堡奥托冯格里克大学); Friedrich Schiller University Jena (耶拿弗里德里希席勒大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Fine-grained visual classification (FGVC) requires distinguishing between visually similar categories through subtle, localized features - a task that remains challenging due to high intra-class variability and limited inter-class differences. Existing part-based methods often rely on complex localization networks that learn mappings from pixel to sample space, requiring a deep understanding of image content while limiting feature utility for downstream tasks. In addition, sampled points frequently suffer from high spatial redundancy, making it difficult to quantify the optimal number of required parts. Inspired by human saccadic vision, we propose a two-stage process that first extracts peripheral features (coarse view) and generates a sample map, from which fixation patches are sampled and encoded in parallel using a weight-shared encoder. We employ contextualized selective attention to weigh the impact of each fixation patch before fusing peripheral and focus representations. To prevent spatial collapse - a common issue in part-based methods - we utilize non-maximum suppression during fixation sampling to eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks (CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method achieves comparable performance to state-of-the-art approaches while consistently outperforming our baseline encoder.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-62] Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model</p>
<p>【速读】：该论文旨在解决手写笔画生成中因忽略字间距（word spacing）这一显式特征而导致的风格模仿不一致问题，从而提升手写识别与书写者顺序恢复等任务的性能。其关键解决方案在于提出了一种基于条件扩散模型（conditional diffusion model）的手写笔画生成方法，该方法同时引入多尺度注意力特征以捕捉局部与全局的书法风格特征，并显式建模词布局（word layout）以控制字间距，从而实现更精确的笔画生成和风格模仿。相比以往直接生成风格图像的方法，该方案利用笔画生成所蕴含的时间坐标信息，显著提升了生成结果的时序一致性与风格保真度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15678">https://arxiv.org/abs/2509.15678</a><br>
<strong>作者</strong>: Sidra Hanif,Longin Jan Latecki<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Handwriting stroke generation is crucial for improving the performance of tasks such as handwriting recognition and writers order recovery. In handwriting stroke generation, it is significantly important to imitate the sample calligraphic style. The previous studies have suggested utilizing the calligraphic features of the handwriting. However, they had not considered word spacing (word layout) as an explicit handwriting feature, which results in inconsistent word spacing for style imitation. Firstly, this work proposes multi-scale attention features for calligraphic style imitation. These multi-scale feature embeddings highlight the local and global style features. Secondly, we propose to include the words layout, which facilitates word spacing for handwriting stroke generation. Moreover, we propose a conditional diffusion model to predict strokes in contrast to previous work, which directly generated style images. Stroke generation provides additional temporal coordinate information, which is lacking in image generation. Hence, our proposed conditional diffusion model for stroke generation is guided by calligraphic style and word layout for better handwriting imitation and stroke generation in a calligraphic style. Our experimentation shows that the proposed diffusion model outperforms the current state-of-the-art stroke generation and is competitive with recent image generation networks.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-63] Camera Splatting for Continuous View Optimization</p>
<p>【速读】：该论文旨在解决新视图合成（novel view synthesis）中视图优化的问题，尤其针对复杂视依赖现象（如强烈金属反射和精细纹理）的建模能力不足。其解决方案的关键在于提出了一种名为 Camera Splatting 的新框架，将每个相机建模为一个3D高斯分布（3D Gaussian），称为“相机点”（camera splat），并通过在表面附近采样的3D点处放置虚拟相机（point cameras）来观察这些相机点的分布；通过连续且可微地优化相机点，使得从点相机视角观测到的目标分布趋于理想状态，从而实现高质量的新视图生成。此方法相较于传统的最远视图采样（Farthest View Sampling, FVS）在捕捉复杂视依赖特性方面表现出显著优势。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15677">https://arxiv.org/abs/2509.15677</a><br>
<strong>作者</strong>: Gahye Lee,Hyomin Kim,Gwangjin Ju,Jooeun Son,Hyejeong Yoon,Seungyong Lee<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We propose Camera Splatting, a novel view optimization framework for novel view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a camera splat, and virtual cameras, termed point cameras, are placed at 3D points sampled near the surface to observe the distribution of camera splats. View optimization is achieved by continuously and differentiably refining the camera splats so that desirable target distributions are observed from the point cameras, in a manner similar to the original 3D Gaussian splatting. Compared to the Farthest View Sampling (FVS) approach, our optimized views demonstrate superior performance in capturing complex view-dependent phenomena, including intense metallic reflections and intricate textures such as text.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-64] A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds</p>
<p>【速读】：该论文旨在解决从不完整点云数据中进行表面重建的问题，尤其针对扫描过程中因高吸光率或遮挡导致的数据缺失区域。其解决方案的关键在于利用主成分分析（Principal Component Analysis, PCA）估计可用点云数据的法向量信息，并将该法向量信息作为正则项引入重建模型中，从而引导在数据缺失区域推断合理的表面结构；同时，采用算子分裂（operator-splitting）方法高效求解该优化模型，实现对整体表面的高质量重建。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15675">https://arxiv.org/abs/2509.15675</a><br>
<strong>作者</strong>: Hao Liu<br>
<strong>机构</strong>: Hong Kong Baptist University (香港浸会大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Point cloud data represents a crucial category of information for mathematical modeling, and surface reconstruction from such data is an important task across various disciplines. However, during the scanning process, the collected point cloud data may fail to cover the entire surface due to factors such as high light-absorption rate and occlusions, resulting in incomplete datasets. Inferring surface structures in data-missing regions and successfully reconstructing the surface poses a challenge. In this paper, we present a Principal Component Analysis (PCA) based model for surface reconstruction from incomplete point cloud data. Initially, we employ PCA to estimate the normal information of the underlying surface from the available point cloud data. This estimated normal information serves as a regularizer in our model, guiding the reconstruction of the surface, particularly in areas with missing data. Additionally, we introduce an operator-splitting method to effectively solve the proposed model. Through systematic experimentation, we demonstrate that our model successfully infers surface structures in data-missing regions and well reconstructs the underlying surfaces, outperforming existing methodologies.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-65] FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting</p>
<p>【速读】：该论文旨在解决接触式指纹识别性能优于非接触式指纹识别的问题，其核心原因在于非接触式指纹数据稀缺（尤其是存在姿态变化的数据）以及缺乏对隐式三维（3D）指纹表示的有效利用。解决方案的关键在于提出了一种新颖的非接触式指纹3D配准、重建与生成框架，首次将3D高斯泼溅（3D Gaussian Splatting）技术引入指纹识别领域，实现了仅需稀疏输入图像即可完成无需相机参数信息的3D注册与完整重建，从而显著提升了非接触式指纹识别的性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15648">https://arxiv.org/abs/2509.15648</a><br>
<strong>作者</strong>: Yuwei Jia,Yutang Lu,Zhe Cui,Fei Su<br>
<strong>机构</strong>: Beijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications (北京邮电大学网络系统与网络文化北京市重点实验室)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Researchers have conducted many pioneer researches on contactless fingerprints, yet the performance of contactless fingerprint recognition still lags behind contact-based methods primary due to the insufficient contactless fingerprint data with pose variations and lack of the usage of implicit 3D fingerprint representations. In this paper, we introduce a novel contactless fingerprint 3D registration, reconstruction and generation framework by integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for contactless fingerprint recognition that integrates 3D fingerprint reconstruction and generation. To our knowledge, this is the first work to apply 3D Gaussian Splatting to the field of fingerprint recognition, and the first to achieve effective 3D registration and complete reconstruction of contactless fingerprints with sparse input images and without requiring camera parameters information. Experiments on 3D fingerprint registration, reconstruction, and generation prove that our method can accurately align and reconstruct 3D fingerprints from 2D images, and sequentially generates high-quality contactless fingerprints from 3D model, thus increasing the performances for contactless fingerprint recognition.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-66] GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading</p>
<p>【速读】：该论文旨在解决3D高斯泼溅（3D Gaussian Splatting）在大规模场景训练中面临的GPU内存瓶颈问题，即存储参数、梯度和优化器状态所需的大量显存会迅速超出消费级GPU的容量限制。其解决方案的关键在于提出GS-Scale系统，通过将所有高斯分布（Gaussian）对象存储于主机内存（host memory），仅在每次前向和反向传播时按需将子集传输至GPU；同时引入三项系统级优化策略：（1）选择性卸载几何参数以加速视锥裁剪（frustum culling），（2）将参数转发至CPU流水线进行优化器更新以实现GPU计算与CPU更新并行化，（3）延迟优化器更新以减少无梯度高斯分布的无效内存访问。这些设计显著降低了GPU内存占用3.3–5.6倍，且保持接近纯GPU训练的效率，使基于消费级显卡（如RTX 4070 Mobile）的大规模训练成为可能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15645">https://arxiv.org/abs/2509.15645</a><br>
<strong>作者</strong>: Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon<br>
<strong>机构</strong>: Seoul National University (首尔国立大学); Google(谷歌)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The advent of 3D Gaussian Splatting has revolutionized graphics rendering by delivering high visual quality and fast rendering speeds. However, training large-scale scenes at high quality remains challenging due to the substantial memory demands required to store parameters, gradients, and optimizer states, which can quickly overwhelm GPU memory. To address these limitations, we propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian Splatting. GS-Scale stores all Gaussians in host memory, transferring only a subset to the GPU on demand for each forward and backward pass. While this dramatically reduces GPU memory usage, it requires frustum culling and optimizer updates to be executed on the CPU, introducing slowdowns due to CPU’s limited compute and memory bandwidth. To mitigate this, GS-Scale employs three system-level optimizations: (1) selective offloading of geometric parameters for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer updates with GPU computation, and (3) deferred optimizer update to minimize unnecessary memory accesses for Gaussians with zero gradients. Our extensive evaluations on large-scale datasets demonstrate that GS-Scale significantly lowers GPU memory demands by 3.3-5.6x, while achieving training speeds comparable to GPU without host offloading. This enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS (learned perceptual image patch similarity) improvement.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-67] UNIV: Unified Foundation Model for Infrared and Visible Modalities</p>
<p>【速读】：该论文旨在解决多模态感知中红外与可见光图像联合建模的挑战，特别是在复杂天气条件下实现鲁棒性能的问题。现有预训练模型在各自模态（RGB可见光与红外）上表现优异，但在跨模态场景（如自动驾驶中同时使用双传感器）下性能下降明显。解决方案的关键在于提出一种生物启发的统一基础模型UNIV，其核心创新包括：（1）基于视网膜水平细胞侧向抑制机制设计的逐块跨模态对比学习（Patch-wise Cross-modality Contrastive Learning, PCCL），通过注意力引导蒸馏实现高效跨模态特征对齐，且兼容任意Transformer架构；（2）模拟视网膜双极细胞信号路由机制的双知识保留策略，结合LoRA适配器（仅增加2%参数）与同步蒸馏，有效避免灾难性遗忘，从而复现视网膜明视觉（锥体驱动）与暗视觉（杆体驱动）的功能分离与协同。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15642">https://arxiv.org/abs/2509.15642</a><br>
<strong>作者</strong>: Fangyuan Mao,Shuo Wang,Jilin Mei,Chen Min,Shun Lu,Fuyang Liu,Yu Hu<br>
<strong>机构</strong>: Research Center for Intelligent Computing Systems, CAS ICT (中国科学院计算技术研究所智能计算系统研究中心); University of Chinese Academy of Sciences (中国科学院大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The demand for joint RGB-visible and infrared perception is growing rapidly, particularly to achieve robust performance under diverse weather conditions. Although pre-trained models for RGB-visible and infrared data excel in their respective domains, they often underperform in multimodal scenarios, such as autonomous vehicles equipped with both sensors. To address this challenge, we propose a biologically inspired UNified foundation model for Infrared and Visible modalities (UNIV), featuring two key innovations. First, we introduce Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided distillation framework that mimics retinal horizontal cells’ lateral inhibition, which enables effective cross-modal feature alignment while remaining compatible with any transformer-based architecture. Second, our dual-knowledge preservation mechanism emulates the retina’s bipolar cell signal routing - combining LoRA adapters (2% added parameters) with synchronous distillation to prevent catastrophic forgetting, thereby replicating the retina’s photopic (cone-driven) and scotopic (rod-driven) functionality. To support cross-modal learning, we introduce the MVIP dataset, the most comprehensive visible-infrared benchmark to date. It contains 98,992 precisely aligned image pairs spanning diverse scenarios. Extensive experiments demonstrate UNIV’s superior performance on infrared tasks (+1.7 mIoU in semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+ of the baseline performance on visible RGB tasks. Our code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-68] pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation</p>
<p>【速读】：该论文旨在解决医疗图像分割中因隐私限制导致的数据共享难题，以及现有联邦学习方法在处理复杂、异构数据时性能受限的问题。其核心挑战在于如何在保护各机构数据隐私的同时，提升模型对多样化医学影像的适应能力，并避免因过度泛化而导致的性能下降。解决方案的关键在于提出首个面向异构数据场景的个性化联邦SAM（Segment Anything Model）框架：一是设计了一种仅聚合全局参数的个性化策略，保留了局部专家模块（L-MoE）以捕捉域特定特征；二是引入解耦的全局-局部微调机制，通过知识蒸馏的师生范式弥合全局共享模型与本地个性化模型之间的差距，从而实现更优的分割性能和跨域适应性，同时降低通信开销。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15638">https://arxiv.org/abs/2509.15638</a><br>
<strong>作者</strong>: Tong Wang,Xingyue Zhao,Linghao Zhuang,Haoyu Zhao,Jiayi Yin,Yuyang He,Gang Yu,Bo Lin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  5 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Medical image segmentation is crucial for computer-aided diagnosis, yet privacy constraints hinder data sharing across institutions. Federated learning addresses this limitation, but existing approaches often rely on lightweight architectures that struggle with complex, heterogeneous data. Recently, the Segment Anything Model (SAM) has shown outstanding segmentation capabilities; however, its massive encoder poses significant challenges in federated settings. In this work, we present the first personalized federated SAM framework tailored for heterogeneous data scenarios in medical image segmentation. Our framework integrates two key innovations: (1) a personalized strategy that aggregates only the global parameters to capture cross-client commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts) component to preserve domain-specific features; and (2) a decoupled global-local fine-tuning mechanism that leverages a teacher-student paradigm via knowledge distillation to bridge the gap between the global shared model and the personalized local models, thereby mitigating overgeneralization. Extensive experiments on two public datasets validate that our approach significantly improves segmentation performance, achieves robust cross-domain adaptation, and reduces communication overhead.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-69] PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning</p>
<p>【速读】：该论文旨在解决跨模态检索中因真实数据中存在的噪声对应关系（Noisy Correspondences）导致的语义对齐偏差问题，现有方法通常假设图像-文本对完全对齐，且依赖粗粒度分类区分干净与噪声样本，忽视了噪声样本内部的多样性以及不同样本在训练中的差异化优化需求。解决方案的关键在于提出一种伪标签一致性引导的样本精炼框架（Pseudo-label Consistency-Guided Sample Refinement, PCSR），其核心创新包括：首先基于置信度估计识别干净与噪声样本；其次利用伪标签一致性（pseudo-label consistency）对噪声样本进行细粒度划分，挖掘出结构上不同的子集；进而引入伪标签一致性评分（PCS）量化预测稳定性，以区分模糊样本与可精炼样本；最后采用自适应配对优化策略（Adaptive Pair Optimization, APO），对模糊样本使用鲁棒损失函数，对可精炼样本通过文本替换增强，从而提升模型在噪声监督下的检索鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15623">https://arxiv.org/abs/2509.15623</a><br>
<strong>作者</strong>: Zhuoyao Liu,Yang Liu,Wentao Feng,Shudong Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  7 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Cross-modal retrieval aims to align different modalities via semantic similarity. However, existing methods often assume that image-text pairs are perfectly aligned, overlooking Noisy Correspondences in real data. These misaligned pairs misguide similarity learning and degrade retrieval performance. Previous methods often rely on coarse-grained categorizations that simply divide data into clean and noisy samples, overlooking the intrinsic diversity within noisy instances. Moreover, they typically apply uniform training strategies regardless of sample characteristics, resulting in suboptimal sample utilization for model optimization. To address the above challenges, we introduce a novel framework, called Pseudo-label Consistency-Guided Sample Refinement (PCSR), which enhances correspondence reliability by explicitly dividing samples based on pseudo-label consistency. Specifically, we first employ a confidence-based estimation to distinguish clean and noisy pairs, then refine the noisy pairs via pseudo-label consistency to uncover structurally distinct subsets. We further proposed a Pseudo-label Consistency Score (PCS) to quantify prediction stability, enabling the separation of ambiguous and refinable samples within noisy pairs. Accordingly, we adopt Adaptive Pair Optimization (APO), where ambiguous samples are optimized with robust loss functions and refinable ones are enhanced via text replacement during training. Extensive experiments on CC152K, MS-COCO and Flickr30K validate the effectiveness of our method in improving retrieval robustness under noisy supervision.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-70] Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation</p>
<p>【速读】：该论文旨在解决基于全切片图像（Whole Slide Images, WSI）的生存分析中因特征噪声和数据可及性受限而导致的关键预后特征难以有效捕捉的问题，同时探索病理报告中患者特异性信息对WSI生存分析的潜在增强作用。解决方案的关键在于提出一种报告辅助的自蒸馏框架（Report-auxiliary self-distillation, Rasa）：首先利用大语言模型（Large Language Models, LLMs）从原始嘈杂的病理报告中提取细粒度、与WSI相关的文本描述；随后设计基于自蒸馏的管道，在教师模型文本知识的引导下过滤学生模型中的无关或冗余WSI特征；最后在学生模型训练中引入风险感知的混合策略（risk-aware mix-up），以提升训练数据的数量与多样性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15608">https://arxiv.org/abs/2509.15608</a><br>
<strong>作者</strong>: Zheng Wang,Hong Liu,Zheng Wang,Danyi Li,Min Cen,Baptiste Magnier,Li Liang,Liansheng Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Survival analysis based on Whole Slide Images (WSIs) is crucial for evaluating cancer prognosis, as they offer detailed microscopic information essential for predicting patient outcomes. However, traditional WSI-based survival analysis usually faces noisy features and limited data accessibility, hindering their ability to capture critical prognostic features effectively. Although pathology reports provide rich patient-specific information that could assist analysis, their potential to enhance WSI-based survival analysis remains largely unexplored. To this end, this paper proposes a novel Report-auxiliary self-distillation (Rasa) framework for WSI-based survival analysis. First, advanced large language models (LLMs) are utilized to extract fine-grained, WSI-relevant textual descriptions from original noisy pathology reports via a carefully designed task prompt. Next, a self-distillation-based pipeline is designed to filter out irrelevant or redundant WSI features for the student model under the guidance of the teacher model’s textual knowledge. Finally, a risk-aware mix-up strategy is incorporated during the training of the student model to enhance both the quantity and diversity of the training data. Extensive experiments carried out on our collected data (CRC) and public data (TCGA-BRCA) demonstrate the superior effectiveness of Rasa against state-of-the-art methods. Our code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-71] nnisTV: Do Multimodal <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  Understand Tennis Rallies?</p>
<p>【速读】：该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）在高速、高密度信息的体育视频理解任务中表现不佳的问题，尤其是在网球等快节奏运动场景下，其短时且信息密集的回合（rally）难以被有效建模。解决方案的关键在于构建首个系统性的网球视频理解基准——TennisTV，该基准将每个回合建模为时间有序的连续击球事件序列，并通过自动化流水线完成数据筛选与问题生成，覆盖8个不同粒度的任务（回合与击球层级），包含2500个人工验证的问题。评估16种代表性MLLMs后发现，帧采样密度需根据任务特性动态调整，且提升时间定位能力是增强模型推理性能的核心路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15602">https://arxiv.org/abs/2509.15602</a><br>
<strong>作者</strong>: Zhongyuan Bao,Lejun Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks at rally and stroke levels and includes 2,500 human-verified questions. Evaluating 16 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results reveal substantial shortcomings and yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-72] EyePCR: A Comprehensive Benchmark for Fine-Grained Perception Knowledge Comprehension and Clinical <mark class="hl-label green">Reasoning</mark>  in Ophthalmic Surgery <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）在高风险、专业领域场景（如眼科手术）中认知能力评估不足的问题。现有研究缺乏针对特定临床场景的系统性基准测试，导致模型在感知（Perception）、理解（Comprehension）和推理（Reasoning）三个认知层级上的表现难以量化与提升。解决方案的关键在于构建EyePCR——一个大规模眼科手术分析基准，其核心特征包括：超过21万条视觉问答（VQA）数据以支持多视角感知；包含2.5万余个三元组的医学知识图谱用于促进理解；以及四个基于临床实际需求的推理任务。该基准通过结构化临床知识引导模型模拟外科医生的认知过程，从而显著增强MLLMs在复杂手术视频中的认知能力。实验表明，经领域适配后的EyePCR-MLLM（基于Qwen2.5-VL-7B）在感知任务上达到最优准确率，并在理解和推理层面优于开源模型，接近GPT-4等商业模型水平，揭示了当前MLLMs在手术认知方面的局限性并为提升临床可靠性提供了新范式。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15596">https://arxiv.org/abs/2509.15596</a><br>
<strong>作者</strong>: Gui Wang,Yang Wennuo,Xusen Ma,Zehao Zhong,Zhuoru Wu,Ende Wu,Rong Qu,Wooi Ping Cheah,Jianfeng Ren,Linlin Shen<br>
<strong>机构</strong>: Shenzhen University (深圳大学); University of Nottingham Ningbo China (诺丁汉大学宁波分校); Wenzhou Medical University (温州医科大学); University of Nottingham (诺丁汉大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Strong accept by NeurIPS2025 Reviewers and AC, but reject by PC. (Rating: 6,5,4,4)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:MLLMs (Multimodal Large Language Models) have showcased remarkable capabilities, but their performance in high-stakes, domain-specific scenarios like surgical settings, remains largely under-explored. To address this gap, we develop \textbfEyePCR, a large-scale benchmark for ophthalmic surgery analysis, grounded in structured clinical knowledge to evaluate cognition across \textitPerception, \textitComprehension and \textitReasoning. EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover 1048 fine-grained attributes for multi-view perception, medical knowledge graph of more than 25k triplets for comprehension, and four clinically grounded reasoning tasks. The rich annotations facilitate in-depth cognitive analysis, simulating how surgeons perceive visual cues and combine them with domain knowledge to make decisions, thus greatly improving models’ cognitive ability. In particular, \textbfEyePCR-MLLM, a domain-adapted variant of Qwen2.5-VL-7B, achieves the highest accuracy on MCQs for \textitPerception among compared models and outperforms open-source models in \textitComprehension and \textitReasoning, rivalling commercial models like GPT-4.1. EyePCR reveals the limitations of existing MLLMs in surgical cognition and lays the foundation for benchmarking and enhancing clinical reliability of surgical video understanding models.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-73] Latent Zoning Network: A Unified Principle for Generative Modeling Representation Learning and Classification <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决机器学习中生成建模（generative modeling）、表示学习（representation learning）和分类（classification）三大核心问题之间缺乏统一框架的问题。当前这些任务的最先进方法通常彼此独立，难以协同优化。解决方案的关键在于提出Latent Zoning Network (LZN)，其核心思想是构建一个共享的高斯潜空间（Gaussian latent space），并通过为不同数据类型（如图像、文本、标签）设计互不重叠的潜区（disjoint latent zones）来实现多任务统一建模。具体而言，每个数据类型配备专用编码器和解码器，而各类机器学习任务则通过组合这些模块实现：例如，标签条件图像生成使用标签编码器与图像解码器，图像嵌入仅用图像编码器，分类任务则结合图像编码器与标签解码器。实验表明，LZN在增强现有模型性能、独立完成无监督表示学习以及联合执行生成与分类任务上均展现出优越性，验证了其作为统一范式的能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15591">https://arxiv.org/abs/2509.15591</a><br>
<strong>作者</strong>: Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin<br>
<strong>机构</strong>: Microsoft Research (微软研究院); Tsinghua University (清华大学); Samsung R&amp;D Institute UK (三星英国研发中心); KU Leuven (鲁汶大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:  Published in NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at this https URL. The project website is at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-74] Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion</p>
<p>【速读】：该论文旨在解决短视频平台中虚假信息（fake news）检测的难题，尤其针对其动态性和多模态特性带来的挑战。现有方法在处理视频、音频和文本等多源异构信息时表现不足，难以适应数据不完整或模态缺失的情况。解决方案的关键在于提出一种名为HFN（Heterogeneous Fusion Net）的新型多模态框架，其核心创新包括：1）引入决策网络（Decision Network），在推理阶段动态调整各模态权重以优化融合效果；2）设计加权多模态特征融合模块（Weighted Multi-Modal Feature Fusion），提升模型在部分模态缺失时的鲁棒性。此外，作者还构建了专门用于短视频虚假新闻检测的VESV数据集，实验表明该方法在FakeTT和VESV两个数据集上分别较当前最优方法提升了2.71%和4.14%的Macro F1分数，验证了其有效性与实用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15578">https://arxiv.org/abs/2509.15578</a><br>
<strong>作者</strong>: Shanghong Li,Chiam Wen Qi Ruth,Hong Xu,Fang Liu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid proliferation of short video platforms has necessitated advanced methods for detecting fake news. This need arises from the widespread influence and ease of sharing misinformation, which can lead to significant societal harm. Current methods often struggle with the dynamic and multimodal nature of short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel multimodal framework that integrates video, audio, and text data to evaluate the authenticity of short video content. HFN introduces a Decision Network that dynamically adjusts modality weights during inference and a Weighted Multi-Modal Feature Fusion module to ensure robust performance even with incomplete data. Additionally, we contribute a comprehensive dataset VESV (VEracity on Short Videos) specifically designed for short video fake news detection. Experiments conducted on the FakeTT and newly collected VESV datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over state-of-the-art methods. This work establishes a robust solution capable of effectively identifying fake news in the complex landscape of short video platforms, paving the way for more reliable and comprehensive approaches in combating misinformation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-75] owards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach</p>
<p>【速读】：该论文旨在解决显著性目标检测（Salient Object Detection, SOD）中评价协议的尺寸敏感性问题，尤其是在单张图像中存在显著尺寸差异的多个显著对象时，现有评估指标会因大尺寸区域贡献更大而忽略小尺寸但语义重要的目标，导致性能评估偏差和实际检测效果下降。解决方案的关键在于提出一种通用的尺寸不变评价框架（Size-Invariant Evaluation, SIEva），其核心思想是将图像中的可分离成分独立评估后再聚合结果，从而有效缓解对象间尺寸不平衡带来的影响；在此基础上进一步构建了遵循尺寸不变原则的优化框架（SIOpt），该框架具有模型无关性，可无缝集成至多种SOD骨干网络，显著提升跨尺度显著目标的检测能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15573">https://arxiv.org/abs/2509.15573</a><br>
<strong>作者</strong>: Shilong Bao,Qianqian Xu,Feiran Li,Boyu Han,Zhiyong Yang,Xiaochun Cao,Qingming Huang<br>
<strong>机构</strong>: University of Chinese Academy of Sciences (中国科学院大学); Institute of Computing Technology, Chinese Academy of Sciences (中国科学院计算技术研究所); Peng Cheng Laboratory (鹏城实验室); State Key Laboratory of AI Safety (人工智能安全重点实验室); State Key Laboratory of Information Security (信息安全国家重点实验室); Institute of Information Engineering, Chinese Academy of Sciences (中国科学院信息工程研究所); School of Cyber Security, University of Chinese Academy of Sciences (中国科学院大学网络空间安全学院); School of Computer Science and Technology, University of Chinese Academy of Sciences (中国科学院大学计算机科学与技术学院); School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University (中山大学深圳校区网络科学与技术学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach. The code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-76] BTL-UI: Blink-Think-Link <mark class="hl-label green">Reasoning</mark>  Model for GUI <mark class="hl-label green">Agent</mark>  <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决当前AI驱动的人机图形用户界面（GUI）交互自动化中存在的一大挑战：现有基于多模态大语言模型和强化微调技术的系统，其交互逻辑与人类自然的GUI沟通模式存在显著偏差。为填补这一空白，作者提出“Blink-Think-Link”（BTL）框架，该框架受大脑认知过程启发，将人机交互分解为三个生物合理阶段：(1) Blink——快速检测并聚焦屏幕相关区域（类比眼球扫视运动）；(2) Think——高层推理与决策（模拟认知规划）；(3) Link——生成可执行命令以实现精确操作（模仿人类动作选择机制）。解决方案的关键在于引入两个核心技术创新：一是“Blink数据生成”自动化标注流水线，专门优化用于blink阶段的数据标注；二是“BTL奖励机制”，首个基于规则的奖励体系，能够同时驱动过程导向与结果导向的强化学习。基于此框架构建的GUI代理模型BTL-UI在静态GUI理解与动态交互任务中均展现出持续领先性能，验证了该方法的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15566">https://arxiv.org/abs/2509.15566</a><br>
<strong>作者</strong>: Shaojie Zhang,Ruoceng Zhang,Pei Fu,Shaokang Wang,Jiahui Yang,Xin Du,Shiqi Cui,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan<br>
<strong>机构</strong>: MiLM Plus, Xiaomi Inc (小米公司)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted at NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In the field of AI-driven human-GUI interaction automation, while rapid advances in multimodal large language models and reinforcement fine-tuning techniques have yielded remarkable progress, a fundamental challenge persists: their interaction logic significantly deviates from natural human-GUI communication patterns. To fill this gap, we propose “Blink-Think-Link” (BTL), a brain-inspired framework for human-GUI interaction that mimics the human cognitive process between users and graphical interfaces. The system decomposes interactions into three biologically plausible phases: (1) Blink - rapid detection and attention to relevant screen areas, analogous to saccadic eye movements; (2) Think - higher-level reasoning and decision-making, mirroring cognitive planning; and (3) Link - generation of executable commands for precise motor control, emulating human action selection mechanisms. Additionally, we introduce two key technical innovations for the BTL framework: (1) Blink Data Generation - an automated annotation pipeline specifically optimized for blink data, and (2) BTL Reward – the first rule-based reward mechanism that enables reinforcement learning driven by both process and outcome. Building upon this framework, we develop a GUI agent model named BTL-UI, which demonstrates consistent state-of-the-art performance across both static GUI understanding and dynamic interaction tasks in comprehensive benchmarks. These results provide conclusive empirical validation of the framework’s efficacy in developing advanced GUI Agents.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-77] DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection</p>
<p>【速读】：该论文针对遥感变化检测（Remote Sensing Change Detection, RSCD）中现有方法（包括先进的状态空间模型 State Space Models, SSMs）普遍缺乏显式处理几何错位（geometric misalignments）机制、难以区分细微真实变化与伪变化的问题，提出了一种“对齐-增强”（align-then-enhance）框架 DC-Mamba。其关键创新在于集成两个轻量级、即插即用模块：(1) 双时相可变形对齐（Bi-Temporal Deformable Alignment, BTDA），在语义特征层面显式引入几何感知能力以校正空间错位；(2) 尺度稀疏变化增强器（Scale-Sparse Change Amplifier, SSCA），利用多源线索选择性放大高置信度变化信号并抑制噪声，从而提升小目标或微弱变化的可见性与边界清晰度。该协同设计首先通过 BTDA 建立几何一致性以减少伪变化，再借助 SSCA 强化变化特征，显著提升了 F1-score 和 IoU 指标，验证了该策略在几何与特征层面的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15563">https://arxiv.org/abs/2509.15563</a><br>
<strong>作者</strong>: Min Sun,Fenghui Guo<br>
<strong>机构</strong>: Hainan University (海南大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Remote sensing change detection (RSCD) is vital for identifying land-cover changes, yet existing methods, including state-of-the-art State Space Models (SSMs), often lack explicit mechanisms to handle geometric misalignments and struggle to distinguish subtle, true changes from this http URL address this, we introduce DC-Mamba, an “align-then-enhance” framework built upon the ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1) Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric awareness to correct spatial misalignments at the semantic feature level; and (2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to selectively amplify high-confidence change signals while suppressing noise before the final classification. This synergistic design first establishes geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA to sharpen boundaries and enhance the visibility of small or subtle targets. Experiments show our method significantly improves performance over the strong ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU from 0.4015 to 0.4187. The results confirm the effectiveness of our “align-then-enhance” strategy, offering a robust and easily deployable solution that transparently addresses both geometric and feature-level challenges in RSCD.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-78] From Development to Deployment of AI-assisted Telehealth and Screening for Vision- and Hearing-threatening diseases in resource-constrained settings: Field Observations Challenges and Way Forward</p>
<p>【速读】：该论文旨在解决资源受限环境（Resource-Constrained Settings, RCS）中视觉和听力相关疾病因缺乏专业人员和筛查设施而导致可预防残疾的问题，尤其关注如何通过可扩展的AI辅助远程医疗（Telehealth）和大规模筛查实现早期检测。其核心挑战在于将传统纸质工作流程转化为AI就绪的数字流程，并确保AI模型在真实场景中的可用性和鲁棒性。解决方案的关键在于采用端到端、迭代式的协同设计方法，强调早期原型开发、影子部署（shadow deployment）与持续反馈机制，以促进跨学科团队对流程变革的共识并降低使用障碍；同时提出自动化图像质量检查机制以保障高容量筛查中图像的可评估性，并指出尽管公开数据集和预训练模型存在领域偏移导致性能不佳，仍具有重要参考价值。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15558">https://arxiv.org/abs/2509.15558</a><br>
<strong>作者</strong>: Mahesh Shakya,Bijay Adhikari,Nirsara Shrestha,Bipin Koirala,Arun Adhikari,Prasanta Poudyal,Luna Mathema,Sarbagya Buddhacharya,Bijay Khatri,Bishesh Khanal<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Human-Computer Interaction (cs.HC)<br>
<strong>备注</strong>:  Accepted to MIRASOL (Medical Image Computing in Resource Constrained Settings Workshop  KI) Workshop, 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Vision- and hearing-threatening diseases cause preventable disability, especially in resource-constrained settings(RCS) with few specialists and limited screening setup. Large scale AI-assisted screening and telehealth has potential to expand early detection, but practical deployment is challenging in paper-based workflows and limited documented field experience exist to build upon. We provide insights on challenges and ways forward in development to adoption of scalable AI-assisted Telehealth and screening in such settings. Specifically, we find that iterative, interdisciplinary collaboration through early prototyping, shadow deployment and continuous feedback is important to build shared understanding as well as reduce usability hurdles when transitioning from paper-based to AI-ready workflows. We find public datasets and AI models highly useful despite poor performance due to domain shift. In addition, we find the need for automated AI-based image quality check to capture gradable images for robust screening in high-volume camps. Our field learning stress the importance of treating AI development and workflow digitization as an end-to-end, iterative co-design process. By documenting these practical challenges and lessons learned, we aim to address the gap in contextual, actionable field knowledge for building real-world AI-assisted telehealth and mass-screening programs in RCS.          Comments: Accepted to MIRASOL (Medical Image Computing in Resource Constrained Settings Workshop  KI) Workshop, 2025   Subjects:  Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Human-Computer Interaction (cs.HC)  Cite as: arXiv:2509.15558 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]    (or  arXiv:2509.15558v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15558">https://doi.org/10.48550/arXiv.2509.15558</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-79] Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification</p>
<p>【速读】：该论文旨在解决多标签分类任务中对强大表示能力的需求，特别是如何有效捕捉图像与文本之间的多标签交互关系。其解决方案的关键在于提出一种名为Diff-Feat的简单但高效的框架，该框架从预训练的扩散-Transformer模型中提取中间特征（intermediate features），并进行融合以用于下游任务。研究发现，在视觉任务中，最具判别性的特征出现在扩散过程的中点步骤及Transformer的中间层；而在语言任务中，则出现在无噪声步骤且位于最深层。尤其值得注意的是，在不同数据集上均观察到“Layer 12”在图像任务中表现最优，这成为关键启发。通过设计一种启发式局部搜索算法，可快速定位最优的“图像-文本” × “块-时间步”组合，避免全网格搜索，最终仅需一个简单的线性投影加法融合策略即可实现显著性能提升，在MS-COCO-enhanced和Visual Genome 500上分别达到98.6% mAP和45.7% mAP，超越多种CNN、图结构和Transformer基线模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15553">https://arxiv.org/abs/2509.15553</a><br>
<strong>作者</strong>: Tian Lan,Yiming Zheng,Jianxin Yin<br>
<strong>机构</strong>: Renmin University of China (中国人民大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Applications (stat.AP)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \textitDiff-Feat, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious &quot;Layer  12 &quot; consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256 \times 256). We devise a heuristic local-search algorithm that pinpoints the locally optimal “image-text” \times “block-timestep” pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6% mAP on MS-COCO-enhanced and 45.7% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \textitDiff-Feat forms tighter semantic clusters than unimodal counterparts. The code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-80] MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild</p>
<p>【速读】：该论文旨在解决在野外采集的稀疏视角（sparse-view）图像集合中，由于场景存在多外观（multi-appearance）变化（如不同时间或季节拍摄）而导致的三维重建与新视角合成困难的问题。现有基于神经辐射场（NeRF）和3D高斯泼溅（3DGS）的方法在此类场景下常出现过度平滑和过拟合现象。解决方案的关键在于提出一种名为MS-GS的新框架，其核心是利用单目深度估计获得的几何先验，通过结构光恢复（SfM）点锚定算法提取并利用局部语义区域，实现可靠对齐与几何线索增强；同时，在虚拟视图上设计细粒度与粗粒度相结合的几何引导监督机制，引入多视角约束以提升3D一致性并抑制过拟合。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15548">https://arxiv.org/abs/2509.15548</a><br>
<strong>作者</strong>: Deming Li,Kaiwen Jiang,Yutao Tang,Ravi Ramamoorthi,Rama Chellappa,Cheng Peng<br>
<strong>机构</strong>: Johns Hopkins University (约翰霍普金斯大学); University of California, San Diego (加州大学圣地亚哥分校)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In-the-wild photo collections often contain limited volumes of imagery and exhibit multiple appearances, e.g., taken at different times of day or seasons, posing significant challenges to scene reconstruction and novel view synthesis. Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) have improved in these areas, they tend to oversmooth and are prone to overfitting. In this paper, we present MS-GS, a novel framework designed with Multi-appearance capabilities in Sparse-view scenarios using 3DGS. To address the lack of support due to sparse initializations, our approach is built on the geometric priors elicited from monocular depth estimations. The key lies in extracting and utilizing local semantic regions with a Structure-from-Motion (SfM) points anchored algorithm for reliable alignment and geometry cues. Then, to introduce multi-view constraints, we propose a series of geometry-guided supervision at virtual views in a fine-grained and coarse scheme to encourage 3D consistency and reduce overfitting. We also introduce a dataset and an in-the-wild experiment setting to set up more realistic benchmarks. We demonstrate that MS-GS achieves photorealistic renderings under various challenging sparse-view and multi-appearance conditions and outperforms existing approaches significantly across different datasets.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-81] Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track</p>
<p>【速读】：该论文旨在解决参考视频目标分割（Referential Video Object Segmentation, RVOS）任务中因语言描述与视频内容不匹配导致的误检问题，以及长时程上下文建模不足带来的分割精度下降问题。解决方案的关键在于提出一种无需训练的框架：首先引入视频-语言校验器（Video-Language Checker），显式验证查询中的主体和动作是否真实存在于视频中，从而减少虚假阳性结果；其次设计关键帧采样器（Key-Frame Sampler），自适应选择具有信息量的帧，以更好地捕捉对象的早期出现和长距离时序依赖关系。该方法在MeViS测试集上实现了64.14%的JF分数，在ICCV 2025第七届LSVOS挑战赛RVOS赛道中排名第二。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15546">https://arxiv.org/abs/2509.15546</a><br>
<strong>作者</strong>: Ran Hong,Feng Lu,Leilei Cao,An Yan,Youhai Jiang,Fengjie Zhu<br>
<strong>机构</strong>: TEX AI, Transsion Holdings; Nanchang University; ShanghaiTech University<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  6 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Referential Video Object Segmentation (RVOS) aims to segment all objects in a video that match a given natural language description, bridging the gap between vision and language understanding. Recent work, such as Sa2VA, combines Large Language Models (LLMs) with SAM~2, leveraging the strong video reasoning capability of LLMs to guide video segmentation. In this work, we present a training-free framework that substantially improves Sa2VA’s performance on the RVOS task. Our method introduces two key components: (1) a Video-Language Checker that explicitly verifies whether the subject and action described in the query actually appear in the video, thereby reducing false positives; and (2) a Key-Frame Sampler that adaptively selects informative frames to better capture both early object appearances and long-range temporal context. Without any additional training, our approach achieves a JF score of 64.14% on the MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge at ICCV 2025.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-82] SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models</p>
<p>【速读】：该论文旨在解决现有自回归世界模型在视觉连贯性预测方面的局限性，包括空间结构破坏、解码效率低下以及运动建模不足等问题。其核心解决方案是提出一种名为SAMPO（Scale-wise Autoregression with Motion Prompt）的混合框架，关键在于将帧内生成的自回归建模与帧间因果建模相结合，并引入时序因果解码和双向空间注意力机制，以保持空间局部性并支持各尺度内的并行解码，从而显著提升时间一致性与推理效率；此外，通过设计不对称多尺度分词器和轨迹感知运动提示模块，进一步优化动态场景理解能力，增强物理合理性与时空注意力聚焦。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15536">https://arxiv.org/abs/2509.15536</a><br>
<strong>作者</strong>: Sen Wang,Jingyi Tian,Le Wang,Zhimin Liao,Jiayi Li,Huaiyi Dong,Kun Xia,Sanping Zhou,Wei Tang,Hua Gang<br>
<strong>机构</strong>: Xi’an Jiaotong University (西安交通大学); University of Illinois at Chicago (伊利诺伊大学芝加哥分校); <a target="_blank" rel="noopener" href="http://Amazon.com">Amazon.com</a>, Inc (亚马逊公司)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:  22 pages,15 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:World models allow agents to simulate the consequences of actions in imagined environments for planning, control, and long-horizon decision-making. However, existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling. In response, we propose \textbfScale-wise \textbfAutoregression with \textbfMotion \textbfPr\textbfOmpt (\textbfSAMPO), a hybrid framework that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. Specifically, SAMPO integrates temporal causal decoding with bidirectional spatial attention, which preserves spatial locality and supports parallel decoding within each scale. This design significantly enhances both temporal consistency and rollout efficiency. To further improve dynamic scene understanding, we devise an asymmetric multi-scale tokenizer that preserves spatial details in observed frames and extracts compact dynamic representations for future frames, optimizing both memory usage and model performance. Additionally, we introduce a trajectory-aware motion prompt module that injects spatiotemporal cues about object and robot trajectories, focusing attention on dynamic regions and improving temporal consistency and physical realism. Extensive experiments show that SAMPO achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4 \times  faster inference. We also evaluate SAMPO’s zero-shot generalization and scaling behavior, demonstrating its ability to generalize to unseen tasks and benefit from larger model sizes.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-83] GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI <mark class="hl-label green">Agents</mark></p>
<p>【速读】：该论文旨在解决现有图形用户界面（GUI）定位方法在高分辨率截图中难以实现细粒度定位的问题。其解决方案的关键在于提出了一种名为GUI-ARP的新框架，该框架通过引入自适应区域感知（Adaptive Region Perception, ARP）和自适应阶段控制（Adaptive Stage Controlling, ASC）机制，动态利用视觉注意力裁剪任务相关区域，并根据场景复杂度自适应调整推理策略——简单场景采用单阶段推理，复杂场景则执行多阶段分析。该框架通过监督微调与基于组相对策略优化（Group Relative Policy Optimization, GRPO）的强化学习微调相结合的两阶段训练流程实现高效优化，在ScreenSpot-Pro和UI-Vision等基准上显著优于现有方法，验证了其有效性与竞争力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15532">https://arxiv.org/abs/2509.15532</a><br>
<strong>作者</strong>: Xianhang Ye,Yiqing Li,Wei Dai,Miancan Liu,Ziyuan Chen,Zhangye Han,Hongbo Min,Jinkui Ren,Xiantao Zhang,Wen Yang,Zhi Jin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Existing GUI grounding methods often struggle with fine-grained localization in high-resolution screenshots. To address this, we propose GUI-ARP, a novel framework that enables adaptive multi-stage inference. Equipped with the proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC), GUI-ARP dynamically exploits visual attention for cropping task-relevant regions and adapts its inference strategy, performing a single-stage inference for simple cases and a multi-stage analysis for more complex scenarios. This is achieved through a two-phase training pipeline that integrates supervised fine-tuning with reinforcement fine-tuning based on Group Relative Policy Optimization (GRPO). Extensive experiments demonstrate that the proposed GUI-ARP achieves state-of-the-art performance on challenging GUI grounding benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9% on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-84] MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training</p>
<p>【速读】：该论文旨在解决量化感知训练（Quantization-Aware Training, QAT）在极低比特（extremely low-bit）设置下性能劣于全精度（Full Precision, FP）模型的问题，其核心原因是量化过程不可避免地引入了表示偏差（representation bias），导致模型泛化能力下降。解决方案的关键在于提出最大熵编码量化（Maximum Entropy Coding Quantization, MEC-Quant），通过显式优化表示结构以减少偏差，从而提升模型对未见分布内样本的泛化性能；为实现端到端可训练性，作者采用有损数据编码中的最小码长作为熵的可计算代理，并基于混合专家（Mixture Of Experts, MOE）架构推导出一种可扩展的优化目标，不仅加速计算，还能有效处理权重或激活值的长尾分布特性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15514">https://arxiv.org/abs/2509.15514</a><br>
<strong>作者</strong>: Junbiao Pang,Tianyang Cai,Baochang Zhang<br>
<strong>机构</strong>: Beijing University of Technology (北京工业大学); University of Chinese Academy of Sciences (中国科学院大学); Chinese Academy of Sciences (中国科学院); Institute of Computing Technology, CAS (计算技术研究所，中国科学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  7pages;on going work</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Quantization-Aware Training (QAT) has driven much attention to produce efficient neural networks. Current QAT still obtains inferior performances compared with the Full Precision (FP) counterpart. In this work, we argue that quantization inevitably introduce biases into the learned representation, especially under the extremely low-bit setting. To cope with this issue, we propose Maximum Entropy Coding Quantization (MEC-Quant), a more principled objective that explicitly optimizes on the structure of the representation, so that the learned representation is less biased and thus generalizes better to unseen in-distribution samples. To make the objective end-to-end trainable, we propose to leverage the minimal coding length in lossy data coding as a computationally tractable surrogate for the entropy, and further derive a scalable reformulation of the objective based on Mixture Of Experts (MOE) that not only allows fast computation but also handles the long-tailed distribution for weights or activation values. Extensive experiments on various tasks on computer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT is pushed to the x-bit activation for the first time and the accuracy of MEC-Quant is comparable to or even surpass the FP counterpart. Without bells and whistles, MEC-Qaunt establishes a new state of the art for QAT.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-85] Backdoor Mitigation via Invertible Pruning Masks</p>
<p>【速读】：该论文旨在解决现有基于剪枝（pruning）的后门攻击防御方法在准确识别并移除导致后门行为的关键参数方面存在的不足问题。其解决方案的关键在于提出了一种带有学习型选择机制（learned selection mechanism）的新剪枝方法，该机制能够同时识别对主任务和后门任务均至关重要的参数，并引入一个可逆剪枝掩码（invertible pruning mask），以实现两个互补目标：通过正向掩码消除后门任务，同时借助反向掩码保留主任务性能。该方法被建模为一个双层优化问题，联合学习选择变量、稀疏可逆掩码以及从干净数据中推导出的样本特定后门扰动，其中内层问题利用反向掩码合成候选触发器，外层问题优化掩码以抑制后门行为而不损害干净任务准确性。实验表明，该方法在有限数据条件下仍具强鲁棒性，且在后门缓解效果上优于现有剪枝方法，接近甚至媲美最先进的微调类防御方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15497">https://arxiv.org/abs/2509.15497</a><br>
<strong>作者</strong>: Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak<br>
<strong>机构</strong>: Queensland University of Technology (昆士兰科技大学); Data61, CSIRO (数据61，澳大利亚联邦科学与工业研究组织)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Model pruning has gained traction as a promising defense strategy against backdoor attacks in deep learning. However, existing pruning-based approaches often fall short in accurately identifying and removing the specific parameters responsible for inducing backdoor behaviors. Despite the dominance of fine-tuning-based defenses in recent literature, largely due to their superior performance, pruning remains a compelling alternative, offering greater interpretability and improved robustness in low-data regimes. In this paper, we propose a novel pruning approach featuring a learned \emphselection mechanism to identify parameters critical to both main and backdoor tasks, along with an \emphinvertible pruning mask designed to simultaneously achieve two complementary goals: eliminating the backdoor task while preserving it through the inverse mask. We formulate this as a bi-level optimization problem that jointly learns selection variables, a sparse invertible mask, and sample-specific backdoor perturbations derived from clean data. The inner problem synthesizes candidate triggers using the inverse mask, while the outer problem refines the mask to suppress backdoor behavior without impairing clean-task accuracy. Extensive experiments demonstrate that our approach outperforms existing pruning-based backdoor mitigation approaches, maintains strong performance under limited data conditions, and achieves competitive results compared to state-of-the-art fine-tuning approaches. Notably, the proposed approach is particularly effective in restoring correct predictions for compromised samples after successful backdoor mitigation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-86] Lynx: Towards High-Fidelity Personalized Video Generation</p>
<p>【速读】：该论文旨在解决个性化视频生成中身份保真度不足的问题，即在基于单张输入图像生成视频时，如何有效保持人物身份的一致性，同时确保视频的时序连贯性和视觉真实性。解决方案的关键在于提出一种轻量级双适配器架构——ID-adapter 和 Ref-adapter：ID-adapter 利用 Perceiver Resampler 将 ArcFace 提取的面部嵌入转换为紧凑的身份标记（identity tokens）进行条件控制；Ref-adapter 通过冻结参考路径中的密集 VAE 特征，在所有 Transformer 层中引入跨注意力机制注入细粒度细节。这两个模块协同作用，显著提升了身份保真度，同时维持高质量视频输出。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15496">https://arxiv.org/abs/2509.15496</a><br>
<strong>作者</strong>: Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie Luo<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Lynx Technical Report</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present Lynx, a high-fidelity model for personalized video synthesis from a single input image. Built on an open-source Diffusion Transformer (DiT) foundation model, Lynx introduces two lightweight adapters to ensure identity fidelity. The ID-adapter employs a Perceiver Resampler to convert ArcFace-derived facial embeddings into compact identity tokens for conditioning, while the Ref-adapter integrates dense VAE features from a frozen reference pathway, injecting fine-grained details across all transformer layers through cross-attention. These modules collectively enable robust identity preservation while maintaining temporal coherence and visual realism. Through evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which yielded 800 test cases, Lynx has demonstrated superior face resemblance, competitive prompt following, and strong video quality, thereby advancing the state of personalized video generation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-87] SmolR<mark class="hl-label green">GPT</mark> : Efficient Spatial <mark class="hl-label green">Reasoning</mark>  for Warehouse Environments with 600M Parameters <mark class="hl-label red">ICCV</mark></p>
<p>【速读】：该论文旨在解决当前视觉语言模型（Vision-Language Models, VLMs）在资源受限环境（如仓库、机器人和工业场景）中部署困难的问题，这些问题主要源于现有先进方法依赖于参数量巨大、计算与内存开销高的模型，难以兼顾效率与鲁棒的空间理解能力。解决方案的关键在于提出一种轻量级架构SmolRGPT，其核心创新是显式引入区域级空间推理机制，通过融合RGB与深度（depth）信息来增强空间关系建模能力，并采用三阶段课程学习策略逐步对齐视觉与语言特征、建立空间关系理解并适配任务特定数据集。实验表明，仅含600M参数的SmolRGPT在仓库空间推理基准测试中表现优异，性能可媲美甚至超越更大规模的模型，验证了高效且具备核心空间推理能力的多模态智能在实际场景中的可行性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15490">https://arxiv.org/abs/2509.15490</a><br>
<strong>作者</strong>: Abdarahmane Traore,Éric Hervet,Andy Couturier<br>
<strong>机构</strong>: Embia; Université de Moncton (蒙克顿大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  9 pages, 3 figures, IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in vision-language models (VLMs) have enabled powerful multimodal reasoning, but state-of-the-art approaches typically rely on extremely large models with prohibitive computational and memory requirements. This makes their deployment challenging in resource-constrained environments such as warehouses, robotics, and industrial applications, where both efficiency and robust spatial understanding are critical. In this work, we present SmolRGPT, a compact vision-language architecture that explicitly incorporates region-level spatial reasoning by integrating both RGB and depth cues. SmolRGPT employs a three-stage curriculum that progressively align visual and language features, enables spatial relationship understanding, and adapts to task-specific datasets. We demonstrate that with only 600M parameters, SmolRGPT achieves competitive results on challenging warehouse spatial reasoning benchmarks, matching or exceeding the performance of much larger alternatives. These findings highlight the potential for efficient, deployable multimodal intelligence in real-world settings without sacrificing core spatial reasoning capabilities. The code of the experimentation will be available at: this https URL<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-88] Comparing Computational Pathology Foundation Models using Representational Similarity Analysis</p>
<p>【速读】：该论文旨在解决当前计算病理学（Computational Pathology, CPath）中基础模型（Foundation Models）的表征结构与变异性的认知不足问题，特别是缺乏对不同训练范式下模型内部表示空间的系统性分析。其解决方案的关键在于采用计算神经科学中流行的表现相似性分析（Representational Similarity Analysis, RSA）方法，基于TCGA数据集中的HE图像块对六种CPath基础模型（涵盖视觉-语言对比学习和自蒸馏两类范式）进行定量比较，揭示了模型间表征结构的差异性、滑片依赖性和染色标准化的影响，并量化了内在维度（Intrinsic Dimensionality）特征，从而为提升模型鲁棒性、优化集成策略及理解训练范式对表征形成的影响提供了实证依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15482">https://arxiv.org/abs/2509.15482</a><br>
<strong>作者</strong>: Vaibhav Mishra,William Lotter<br>
<strong>机构</strong>: Dana-Farber Cancer Institute (达纳-法伯癌症研究所); Brigham and Women’s Hospital (布莱根妇女医院); Harvard Medical School (哈佛医学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Foundation models are increasingly developed in computational pathology (CPath) given their promise in facilitating many downstream tasks. While recent studies have evaluated task performance across models, less is known about the structure and variability of their learned representations. Here, we systematically analyze the representational spaces of six CPath foundation models using techniques popularized in computational neuroscience. The models analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through representational similarity analysis using HE image patches from TCGA, we find that UNI2 and Virchow2 have the most distinct representational structures, whereas Prov-Gigapath has the highest average similarity across models. Having the same training paradigm (vision-only vs. vision-language) did not guarantee higher representational similarity. The representations of all models showed a high slide-dependence, but relatively low disease-dependence. Stain normalization decreased slide-dependence for all models by a range of 5.5% (CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language models demonstrated relatively compact representations, compared to the more distributed representations of vision-only models. These findings highlight opportunities to improve robustness to slide-specific features, inform model ensembling strategies, and provide insights into how training paradigms shape model representations. Our framework is extendable across medical imaging domains, where probing the internal representations of foundation models can help ensure effective development and deployment.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-89] OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data</p>
<p>【速读】：该论文旨在解决当前生成式视频模型在自动驾驶场景视频生成中存在的一系列问题，包括模块设计缺乏透明度、依赖大型封闭模型导致资源消耗高、缺乏公开代码与数据以支持复现性等。其解决方案的关键在于：首先，通过定量与定性相结合的方式对图像分词器（image tokenizer）、世界模型（world model）和视频解码器（video decoder）三个核心组件进行独立分析；其次，完全基于预训练开源模型并利用公开的自动驾驶数据集（BDD100K）在学术级GPU硬件上进行微调；再次，通过统一接口实现各组件的高效协同，构建一个连贯的视频生成系统；最后，通过公开代码、模型与数据确保全流程可复现性，从而推动该领域研究的开放性和可扩展性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15479">https://arxiv.org/abs/2509.15479</a><br>
<strong>作者</strong>: Björn Möller,Zhengyang Li,Malte Stelzer,Thomas Graave,Fabian Bettels,Muaaz Ataya,Tim Fingscheidt<br>
<strong>机构</strong>: Technische Universität Braunschweig (布伦瑞克工业大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent successful video generation systems that predict and create realistic automotive driving scenes from short video inputs assign tokenization, future state prediction (world model), and video decoding to dedicated models. These approaches often utilize large models that require significant training resources, offer limited insight into design choices, and lack publicly available code and datasets. In this work, we address these deficiencies and present OpenViGA, an open video generation system for automotive driving scenes. Our contributions are: Unlike several earlier works for video generation, such as GAIA-1, we provide a deep analysis of the three components of our system by separate quantitative and qualitative evaluation: Image tokenizer, world model, video decoder. Second, we purely build upon powerful pre-trained open source models from various domains, which we fine-tune by publicly available automotive data (BDD100K) on GPU hardware at academic scale. Third, we build a coherent video generation system by streamlining interfaces of our components. Fourth, due to public availability of the underlying models and data, we allow full reproducibility. Finally, we also publish our code and models on Github. For an image size of 256x256 at 4 fps we are able to predict realistic driving scene videos frame-by-frame with only one frame of algorithmic latency.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-90] Efficient Multimodal Dataset Distillation via Generative Models</p>
<p>【速读】：该论文旨在解决多模态数据集蒸馏（multimodal dataset distillation）中现有方法效率低下、计算资源消耗大且生成样本相关性弱和多样性不足的问题。其关键解决方案是提出一种名为EDGE的生成式蒸馏方法，通过引入双向对比损失（bi-directional contrastive loss）和多样性损失（diversity loss）来增强生成图像与文本描述之间的语义一致性，并提升样本多样性；同时设计了一种文本合成策略以丰富文本信息，从而显著提升图文检索性能。实验表明，该方法在多个基准数据集上不仅效果优于现有方法，且速度达到最先进方法的18倍。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15472">https://arxiv.org/abs/2509.15472</a><br>
<strong>作者</strong>: Zhenghao Zhao,Haoxuan Wang,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan<br>
<strong>机构</strong>: University of Illinois Chicago (伊利诺伊大学芝加哥分校); University of Central Florida (中佛罗里达大学); Cisco Research (思科研究院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Dataset distillation aims to synthesize a small dataset from a large dataset, enabling the model trained on it to perform well on the original dataset. With the blooming of large language models and multimodal large language models, the importance of multimodal datasets, particularly image-text datasets, has grown significantly. However, existing multimodal dataset distillation methods are constrained by the Matching Training Trajectories algorithm, which significantly increases the computing resource requirement, and takes days to process the distillation. In this work, we introduce EDGE, a generative distillation method for efficient multimodal dataset distillation. Specifically, we identify two key challenges of distilling multimodal datasets with generative models: 1) The lack of correlation between generated images and captions. 2) The lack of diversity among generated samples. To address the aforementioned issues, we propose a novel generative model training workflow with a bi-directional contrastive loss and a diversity loss. Furthermore, we propose a caption synthesis strategy to further improve text-to-image retrieval performance by introducing more text information. Our method is evaluated on Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and efficiency compared to existing approaches. Notably, our method achieves results 18x faster than the state-of-the-art method.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-91] Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture</p>
<p>【速读】：该论文旨在解决肺结节诊断中多模态模型因标注数据稀缺而导致的过拟合问题。其关键解决方案是利用纵向和多模态医疗档案中的无标签数据，通过自监督学习（self-supervised learning）进行联合嵌入预测架构（joint embedding predictive architecture, JEPA）预训练，从而提升模型泛化能力。实验表明，在内部队列中，该方法优于未正则化的多模态模型和仅影像模型（AUC分别为0.91 vs. 0.88和0.73），但在外部队列中表现略逊于仅影像模型（0.72 vs. 0.75），提示JEPA在特定场景下存在局限性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15470">https://arxiv.org/abs/2509.15470</a><br>
<strong>作者</strong>: Thomas Z. Li,Aravind R. Krishnan,Lianrui Zuo,John M. Still,Kim L. Sandler,Fabien Maldonado,Thomas A. Lasko,Bennett A. Landman<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The development of multimodal models for pulmonary nodule diagnosis is limited by the scarcity of labeled data and the tendency for these models to overfit on the training distribution. In this work, we leverage self-supervised learning from longitudinal and multimodal archives to address these challenges. We curate an unlabeled set of patients with CT scans and linked electronic health records from our home institution to power joint embedding predictive architecture (JEPA) pretraining. After supervised finetuning, we show that our approach outperforms an unregularized multimodal model and imaging-only model in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC), but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC). We develop a synthetic environment that characterizes the context in which JEPA may underperform. This work innovates an approach that leverages unlabeled multimodal medical archives to improve predictive models and demonstrates its advantages and limitations in pulmonary nodule diagnosis.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-92] CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction</p>
<p>【速读】：该论文旨在解决从点云密度图中重建矢量楼层平面图（vector floorplans）时面临的噪声敏感性和几何细节丢失问题。传统基于角点的多边形表示方法在噪声和观测不完整情况下容易产生碎片化或不合理的布局，而现有线段分组方法虽利用结构线索提升鲁棒性，但仍难以恢复精细的几何细节。解决方案的关键在于提出一种原生边中心（edge-centric）的建模方式，将每段墙表示为有向且几何连续的边（directed, geometrically continuous edge），从而推断出连贯的楼层结构，确保房间边界闭合且拓扑有效；同时设计了一个双查询Transformer解码器，在去噪框架中融合扰动查询与潜在查询，稳定优化过程并加速收敛，显著提升了重建精度与鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15459">https://arxiv.org/abs/2509.15459</a><br>
<strong>作者</strong>: Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong<br>
<strong>机构</strong>: Wuhan University of Technology (武汉理工大学); University of Twente (特温特大学); Zhejiang University (浙江大学); The Advanced Laser Technology Laboratory of Anhui Province (安徽省先进激光技术重点实验室)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present \textbfCAGE (\textitContinuity-Aware edGE) network, a \textcolorredrobust framework for reconstructing vector floorplans directly from point-cloud density maps. Traditional corner-based polygon representations are highly sensitive to noise and incomplete observations, often resulting in fragmented or implausible layouts. Recent line grouping methods leverage structural cues to improve robustness but still struggle to recover fine geometric details. To address these limitations, we propose a \textitnative edge-centric formulation, modeling each wall segment as a directed, geometrically continuous edge. This representation enables inference of coherent floorplan structures, ensuring watertight, topologically valid room boundaries while improving robustness and reducing artifacts. Towards this design, we develop a dual-query transformer decoder that integrates perturbed and latent queries within a denoising framework, which not only stabilizes optimization but also accelerates convergence. Extensive experiments on Structured3D and SceneCAD show that \textbfCAGE achieves state-of-the-art performance, with F1 scores of 99.1% (rooms), 91.7% (corners), and 89.3% (angles). The method also demonstrates strong cross-dataset generalization, underscoring the efficacy of our architectural innovations. Code and pretrained models will be released upon acceptance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-93] Region-Aware Deformable Convolutions</p>
<p>【速读】：该论文旨在解决传统卷积神经网络在处理复杂图像结构时感受野（receptive field）固定、难以灵活适应局部细节与长程依赖关系的问题。解决方案的关键在于提出区域感知可变形卷积（Region-Aware Deformable Convolution, RAD-Conv），其通过为每个核元素引入四个边界偏移量，动态构建可变宽高比的矩形感受野，从而实现对感受野形状与大小的独立控制。这一设计将注意力机制的适应性与标准卷积的计算效率相结合，显著提升了模型表达能力，同时避免了基于自注意力机制的高计算开销。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15436">https://arxiv.org/abs/2509.15436</a><br>
<strong>作者</strong>: Abolfazl Saheban Maleki,Maryam Imani<br>
<strong>机构</strong>: Tarbiat Modares University (Tarbiat Modares 大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Work in progress; 9 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce Region-Aware Deformable Convolution (RAD-Conv), a new convolutional operator that enhances neural networks’ ability to adapt to complex image structures. Unlike traditional deformable convolutions, which are limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary offsets per kernel element to create flexible, rectangular regions that dynamically adjust their size and shape to match image content. This approach allows precise control over the receptive field’s width and height, enabling the capture of both local details and long-range dependencies, even with small 1x1 kernels. By decoupling the receptive field’s shape from the kernel’s structure, RAD-Conv combines the adaptability of attention mechanisms with the efficiency of standard convolutions. This innovative design offers a practical solution for building more expressive and efficient vision models, bridging the gap between rigid convolutional architectures and computationally costly attention-based methods.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-94] ORCA: <mark class="hl-label green">Agent</mark> ic <mark class="hl-label green">Reasoning</mark>  For Hallucination and Adversarial Robustness in Vision-Language Models</p>
<p>【速读】：该论文旨在解决大视觉语言模型（Large Vision-Language Models, LVLMs）在实际应用中因内在错误和外部对抗攻击导致的幻觉问题，从而影响其事实准确性与鲁棒性。解决方案的关键在于提出一种名为ORCA的代理推理框架，通过测试时结构化推理机制，利用一系列参数量小于3B的小型视觉模型进行多轮迭代推理，形成“观察—推理—批判—行动”循环，实现无需访问模型内部或重新训练即可提升预测准确性和对抗鲁棒性的目标。该框架还记录中间推理轨迹，支持可审计决策，且在不依赖对抗训练的情况下展现出显著的对抗鲁棒性增强效果。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15435">https://arxiv.org/abs/2509.15435</a><br>
<strong>作者</strong>: Chung-En Johnny Yu,Hsuan-Chih(Neil)Chen,Brian Jalaian,Nathaniel D. Bastian<br>
<strong>机构</strong>: University of West Florida (西佛罗里达大学); New York University (纽约大学); United States Military Academy (美国军事学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities but remain vulnerable to hallucinations from intrinsic errors and adversarial attacks from external exploitations, limiting their reliability in real-world applications. We present ORCA, an agentic reasoning framework that improves the factual accuracy and adversarial robustness of pretrained LVLMs through test-time structured inference reasoning with a suite of small vision models (less than 3B parameters). ORCA operates via an Observe–Reason–Critique–Act loop, querying multiple visual tools with evidential questions, validating cross-model inconsistencies, and refining predictions iteratively without access to model internals or retraining. ORCA also stores intermediate reasoning traces, which supports auditable decision-making. Though designed primarily to mitigate object-level hallucinations, ORCA also exhibits emergent adversarial robustness without requiring adversarial training or defense mechanisms. We evaluate ORCA across three settings: (1) clean images on hallucination benchmarks, (2) adversarially perturbed images without defense, and (3) adversarially perturbed images with defense applied. On the POPE hallucination benchmark, ORCA improves standalone LVLM performance by +3.64% to +40.67% across different subsets. Under adversarial perturbations on POPE, ORCA achieves an average accuracy gain of +20.11% across LVLMs. When combined with defense techniques on adversarially perturbed AMBER images, ORCA further improves standalone LVLM performance, with gains ranging from +1.20% to +48.00% across evaluation metrics. These results demonstrate that ORCA offers a promising path toward building more reliable and robust multimodal systems.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-95] NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training</p>
<p>【速读】：该论文旨在解决神经肿瘤学中机器学习模型因数据异质性和肿瘤复杂性导致的泛化能力不足问题，以及现有基础模型（Foundation Models, FMs）在预测罕见分子标志物（如ATRX、TP53、CDKN2A/2B等）时表现不佳的问题。这些问题限制了精准神经肿瘤学的发展，尤其是在跨机构应用和治疗反应预测方面。解决方案的关键在于开发一种针对神经肿瘤学的专用基础模型，并引入分布鲁棒优化（Distributionally Robust Optimization, DRO）损失函数，以缓解站点差异和类别不平衡问题，从而提升跨机构的表型估计准确性和模型稳定性。通过在多中心脑肿瘤MRI数据上预训练自监督骨干网络（BYOL、DINO、MAE、MoCo），并结合DRO策略，该方法显著改善了常见与罕见分子标志物的分类性能及生存预测能力，同时增强了模型的可解释性（如Grad-CAM可视化肿瘤区域）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15416">https://arxiv.org/abs/2509.15416</a><br>
<strong>作者</strong>: Moinak Bhattacharya,Angelica P. Kurtz,Fabio M. Iwamoto,Prateek Prasanna,Gagandeep Singh<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Neuro-oncology poses unique challenges for machine learning due to heterogeneous data and tumor complexity, limiting the ability of foundation models (FMs) to generalize across cohorts. Existing FMs also perform poorly in predicting uncommon molecular markers, which are essential for treatment response and risk stratification. To address these gaps, we developed a neuro-oncology specific FM with a distributionally robust loss function, enabling accurate estimation of tumor phenotypes while maintaining cross-institution generalization. We pretrained self-supervised backbones (BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied distributionally robust optimization (DRO) to mitigate site and class imbalance. Downstream tasks included molecular classification of common markers (MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT), continuous markers (Ki-67, TP53), and overall survival prediction in IDH1 wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular prediction and reduced site-specific embedding differences. At CUIMC, mean balanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with the largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to 0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69). For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647 to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral regions, confirming interpretability. Overall, coupling FMs with DRO yields more site-invariant representations, improves prediction of common and uncommon markers, and enhances survival discrimination, underscoring the need for prospective validation and integration of longitudinal and interventional signals to advance precision neuro-oncology.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-96] Causal Fingerprints of AI Generative Models</p>
<p>【速读】：该论文旨在解决生成式 AI (Generative AI) 模型在生成图像时留下的隐式痕迹（即模型指纹）难以跨模型泛化的问题，现有方法依赖于特定模型的特征或合成伪影，导致指纹信息有限且适应性差。解决方案的关键在于提出一种因果指纹（causal fingerprint）概念，并设计了一个因果解耦框架，该框架在基于预训练扩散重建残差（diffusion reconstruction residual）构建的语义不变潜在空间中，将模型指纹从图像内容和风格中解耦出来，同时通过多样化的特征表示提升指纹粒度。实验表明，该方法在模型溯源任务中优于现有技术，具备伪造检测、模型版权追踪和身份保护等应用潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15406">https://arxiv.org/abs/2509.15406</a><br>
<strong>作者</strong>: Hui Xu,Chi Liu,Congcong Zhu,Minghao Wang,Youyang Qu,Longxiang Gao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  5 page. In submission</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:AI generative models leave implicit traces in their generated images, which are commonly referred to as model fingerprints and are exploited for source attribution. Prior methods rely on model-specific cues or synthesis artifacts, yielding limited fingerprints that may generalize poorly across different generative models. We argue that a complete model fingerprint should reflect the causality between image provenance and model traces, a direction largely unexplored. To this end, we conceptualize the \emphcausal fingerprint of generative models, and propose a causality-decoupling framework that disentangles it from image-specific content and style in a semantic-invariant latent space derived from pre-trained diffusion reconstruction residual. We further enhance fingerprint granularity with diverse feature representations. We validate causality by assessing attribution performance across representative GANs and diffusion models and by achieving source anonymization using counterfactual examples generated from causal fingerprints. Experiments show our approach outperforms existing methods in model attribution, indicating strong potential for forgery detection, model copyright tracing, and identity protection.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-97] Generating Part-Based Global Explanations Via Correspondence</p>
<p>【速读】：该论文旨在解决深度学习模型决策过程缺乏可解释性的问题，特别是现有方法在生成全局概念解释时依赖大量标注数据、成本高昂的局限。其解决方案的关键在于：利用少量图像中用户定义的部件标签（part labels），通过高效迁移机制将其扩展至大规模数据集，并基于部件级局部解释进行聚合，从而生成全局符号化解释，实现对模型决策的大规模人类可理解解释。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15393">https://arxiv.org/abs/2509.15393</a><br>
<strong>作者</strong>: Kunal Rathore,Prasad Tadepalli<br>
<strong>机构</strong>: Oregon State University (俄勒冈州立大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep learning models are notoriously opaque. Existing explanation methods often focus on localized visual explanations for individual images. Concept-based explanations, while offering global insights, require extensive annotations, incurring significant labeling cost. We propose an approach that leverages user-defined part labels from a limited set of images and efficiently transfers them to a larger dataset. This enables the generation of global symbolic explanations by aggregating part-based local explanations, ultimately providing human-understandable explanations for model decisions on a large scale.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-98] RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation</p>
<p>【速读】：该论文旨在解决多域图像到图像翻译中种族特征转换的问题，尤其是如何在不依赖额外参考图像的情况下，保持个体特异性和高层语义信息的同时实现跨种族的风格映射。其解决方案的关键在于提出了一种名为RaceGAN的新框架，该框架能够通过学习多个域之间的风格码映射来完成种族属性转换，同时保留个体身份特征和高阶语义内容，从而克服了现有模型（如StarGAN、StarGANv2等）需依赖参考图像且难以处理低层次风格变化的局限性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15391">https://arxiv.org/abs/2509.15391</a><br>
<strong>作者</strong>: Mst Tasnim Pervin,George Bebis,Fang Jiang,Alireza Tavakkoli<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generative adversarial networks (GANs) have demonstrated significant progress in unpaired image-to-image translation in recent years for several applications. CycleGAN was the first to lead the way, although it was restricted to a pair of domains. StarGAN overcame this constraint by tackling image-to-image translation across various domains, although it was not able to map in-depth low-level style changes for these domains. Style mapping via reference-guided image synthesis has been made possible by the innovations of StarGANv2 and StyleGAN. However, these models do not maintain individuality and need an extra reference image in addition to the input. Our study aims to translate racial traits by means of multi-domain image-to-image translation. We present RaceGAN, a novel framework capable of mapping style codes over several domains during racial attribute translation while maintaining individuality and high level semantics without relying on a reference image. RaceGAN outperforms other models in translating racial features (i.e., Asian, White, and Black) when tested on Chicago Face Dataset. We also give quantitative findings utilizing InceptionReNetv2-based classification to demonstrate the effectiveness of our racial translation. Moreover, we investigate how well the model partitions the latent space into distinct clusters of faces for each ethnic group.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-99] MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决文本到图像扩散模型（text-to-image diffusion models）在处理包含多个对象、属性和空间关系的复杂提示时出现的组合性失败问题，具体表现为跨标记干扰（cross-token interference），即实体纠缠、属性混杂以及空间线索违背。解决方案的关键在于提出MaskAttn-SDXL，这是一种应用于Stable Diffusion XL（SDXL）UNet中交叉注意力（cross-attention）logits的区域级门控机制：通过为每一层学习一个二值掩码，并在softmax前将其注入每个交叉注意力logit图中，从而稀疏化token到潜在表示之间的交互，仅保留语义相关的连接。该方法无需位置编码、辅助标记或外部区域掩码，且推理路径保持不变，计算开销可忽略，有效提升了多对象提示下的空间合规性和属性绑定能力，同时维持图像质量和多样性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15357">https://arxiv.org/abs/2509.15357</a><br>
<strong>作者</strong>: Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Text-to-image diffusion models achieve impressive realism but often suffer from compositional failures on prompts with multiple objects, attributes, and spatial relations, resulting in cross-token interference where entities entangle, attributes mix across objects, and spatial cues are violated. To address these failures, we propose MaskAttn-SDXL,a region-level gating mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each cross-attention logit map before softmax to sparsify token-to-latent interactions so that only semantically relevant connections remain active. The method requires no positional encodings, auxiliary tokens, or external region masks, and preserves the original inference path with negligible overhead. In practice, our model improves spatial compliance and attribute binding in multi-object prompts while preserving overall image quality and diversity. These findings demonstrate that logit-level maksed cross-attention is an data-efficient primitve for enforcing compositional control, and our method thus serves as a practical extension for spatial control in text-to-image generation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-100] Global Pre-fixing Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning</p>
<p>【速读】：该论文旨在解决持续学习（Continual Learning, CL）中因任务间和任务内特征混淆导致的灾难性遗忘问题。现有基于对比损失的方法虽能提升表示的可迁移性和抗遗忘能力，但仍受限于跨任务（inter-task）与单任务内（intra-task）特征之间的混淆。其解决方案的关键在于提出一种名为GPLASC（Global Pre-fixing, Local Adjusting for Supervised Contrastive learning）的简单而有效的对比策略：首先，在表示空间的单位超球面上划分非重叠区域，通过构建任务级预设的等角紧框架（Equiangular Tight Frame, ETF）来避免任务层面的混淆；其次，针对每个任务在其分配区域内自适应地形成局部可调ETF，从而优化任务内部特征结构。该方法同时保障了任务间与任务内的判别性特征结构，并可无缝集成到任意现有对比持续学习框架中。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15347">https://arxiv.org/abs/2509.15347</a><br>
<strong>作者</strong>: Jia Tang,Xinrui Wang,Songcan Chen<br>
<strong>机构</strong>: Nanjing University of Aeronautics and Astronautics (南京航空航天大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  The article has been accepted by Frontiers of Computer Science (FCS), with the DOI: { <a target="_blank" rel="noopener" href="https://doi.org/10.1007/s11704-025-50623-6">https://doi.org/10.1007/s11704-025-50623-6</a> }</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Continual learning (CL) involves acquiring and accumulating knowledge from evolving tasks while alleviating catastrophic forgetting. Recently, leveraging contrastive loss to construct more transferable and less forgetful representations has been a promising direction in CL. Despite advancements, their performance is still limited due to confusion arising from both inter-task and intra-task features. To address the problem, we propose a simple yet effective contrastive strategy named \textbfGlobal \textbfPre-fixing, \textbfLocal \textbfAdjusting for \textbfSupervised \textbfContrastive learning (GPLASC). Specifically, to avoid task-level confusion, we divide the entire unit hypersphere of representations into non-overlapping regions, with the centers of the regions forming an inter-task pre-fixed \textbfEquiangular \textbfTight \textbfFrame (ETF). Meanwhile, for individual tasks, our method helps regulate the feature structure and form intra-task adjustable ETFs within their respective allocated regions. As a result, our method \textitsimultaneously ensures discriminative feature structures both between tasks and within tasks and can be seamlessly integrated into any existing contrastive continual learning framework. Extensive experiments validate its effectiveness.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-101] LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition</p>
<p>【速读】：该论文旨在解决扩散模型（Diffusion Models）在图像生成中采样速度慢的问题，尤其是现有优化方法多集中于模型压缩或减少去噪步骤数量，而忽视了利用多尺度输入分辨率来提升效率的可能性。其解决方案的关键在于提出一种基于级联（cascaded）架构的高效扩散框架 LowDiff，通过统一模型从低分辨率到目标分辨率逐步精炼图像，实现高质量生成与显著提速的平衡。该方法在像素空间和潜在空间（latent space）扩散模型上均适用，实验表明其在多个数据集（如 CIFAR-10、FFHQ 和 ImageNet）上均实现了超过 50% 的吞吐量提升，同时保持甚至优于基线模型的生成质量（以 FID 和 IS 为指标）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15342">https://arxiv.org/abs/2509.15342</a><br>
<strong>作者</strong>: Jiuyi Xu,Qing Jin,Meida Chen,Andrew Feng,Yang Sui,Yangming Shi<br>
<strong>机构</strong>: Colorado School of Mines (科罗拉多矿业学院); University of Southern California Institute for Creative Technologies (南加州大学创意技术研究所); Rice University (莱斯大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Diffusion models have achieved remarkable success in image generation but their practical application is often hindered by the slow sampling speed. Prior efforts of improving efficiency primarily focus on compressing models or reducing the total number of denoising steps, largely neglecting the possibility to leverage multiple input resolutions in the generation process. In this work, we propose LowDiff, a novel and efficient diffusion framework based on a cascaded approach by generating increasingly higher resolution outputs. Besides, LowDiff employs a unified model to progressively refine images from low resolution to the desired resolution. With the proposed architecture design and generation techniques, we achieve comparable or even superior performance with much fewer high-resolution sampling steps. LowDiff is applicable to diffusion models in both pixel space and latent space. Extensive experiments on both conditional and unconditional generation tasks across CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our method. Results show over 50% throughput improvement across all datasets and settings while maintaining comparable or better quality. On unconditional CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1 produces high-quality samples with a FID of 4.00 and an IS of 195.06, together with substantial efficiency gains.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-102] Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</p>
<p>【速读】：该论文旨在解决当前机器视觉模型普遍存在“被动”处理全场景信息的问题，即模型对输入图像进行全局采样，导致计算资源消耗随空间-时间分辨率和模型规模呈指数增长，限制了其在实际应用中的效率与灵活性。其核心解决方案是提出 AdaptiveNN 框架，通过将视觉感知建模为一个从粗到细的序列决策过程，使模型能够逐步识别并聚焦于任务相关的区域，在多次注视（fixation）中增量融合信息，并在达到足够判别力时主动终止观察。该框架的关键创新在于构建了一个融合表示学习与自奖励强化学习的理论体系，实现了对非可微分的 AdaptiveNN 的端到端训练，无需额外监督固定位置，从而在17个基准任务上实现最高达28倍的推理成本降低，同时保持精度不变，并具备良好的任务适应性、资源预算灵活性和可解释性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15333">https://arxiv.org/abs/2509.15333</a><br>
<strong>作者</strong>: Yulin Wang,Yang Yue,Yang Yue,Huanqian Wang,Haojun Jiang,Yizeng Han,Zanlin Ni,Yifan Pu,Minglei Shi,Rui Lu,Qisen Yang,Andrew Zhao,Zhuofan Xia,Shiji Song,Gao Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Image and Video Processing (eess.IV)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from ‘passive’ to ‘active, adaptive’ vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-103] CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization</p>
<p>【速读】：该论文旨在解决基于提示（prompt-based）的对比语言图像预训练（CLIP）方法在分布外（out-of-distribution, OOD）场景下存在的两个关键问题：一是文本描述不准确导致模型精度和鲁棒性下降，尤其影响零样本（zero-shot）性能；二是视觉-语言嵌入对齐能力有限，制约了模型的泛化表现。解决方案的关键在于提出一种新的条件域提示学习（Conditional Domain prompt Learning, CoDoL）方法，利用可获得的域信息生成结构化提示，并通过引入轻量级域元网络（Domain Meta Network, DMN）来生成依赖输入的域特定token，从而增强视觉与语言表示之间的对齐关系，显著提升模型在多个OOD基准（如PACS、VLCS、OfficeHome和DigitDG）上的泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15330">https://arxiv.org/abs/2509.15330</a><br>
<strong>作者</strong>: Min Zhang,Bo Jiang,Jie Zhou,Yimeng Liu,Xin Lin<br>
<strong>机构</strong>: East China Normal University (华东师范大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in pre-training vision-language models (VLMs), e.g., contrastive language-image pre-training (CLIP) methods, have shown great potential in learning out-of-distribution (OOD) representations. Despite showing competitive performance, the prompt-based CLIP methods still suffer from: i) inaccurate text descriptions, which leads to degraded accuracy and robustness, and poses a challenge for zero-shot CLIP methods. ii) limited vision-language embedding alignment, which significantly affects the generalization performance. To tackle the above issues, this paper proposes a novel Conditional Domain prompt Learning (CoDoL) method, which utilizes readily-available domain information to form prompts and improves the vision-language embedding alignment for improving OOD generalization. To capture both instance-specific and domain-specific information, we further propose a lightweight Domain Meta Network (DMN) to generate input-conditional tokens for images in each domain. Extensive experiments on four OOD benchmarks (PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed CoDoL in terms of improving the vision-language embedding alignment as well as the out-of-distribution generalization performance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-104] Kuramoto Orientation Diffusion Models <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决传统基于各向同性欧几里得扩散的生成模型在建模具有强方向性结构的图像（如指纹和纹理）时表现不佳的问题，这类图像通常包含连贯的角向方向模式。解决方案的关键在于引入基于周期域的得分函数生成模型（score-based generative model），其核心是利用随机 Kuramoto 动力学作为扩散过程的基础，通过相位同步机制构建结构化先验。具体而言，前向过程通过全局或局部耦合振子相互作用及对全局参考相位的吸引，实现相位变量的同步，逐步将数据坍缩为低熵的 von Mises 分布；反向过程则通过学习的得分函数进行去同步，从而生成具有层次结构的多样化模式，从全局一致性逐步细化到局部细节。该方法结合了缠绕高斯转移核和周期感知神经网络以适应圆周几何特性，在通用图像基准上表现优异，并显著提升了指纹和纹理等方向密集数据集上的生成质量。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15328">https://arxiv.org/abs/2509.15328</a><br>
<strong>作者</strong>: Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>)<br>
<strong>备注</strong>:  NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Orientation-rich images, such as fingerprints and textures, often exhibit coherent angular directional patterns that are challenging to model using standard generative approaches based on isotropic Euclidean diffusion. Motivated by the role of phase synchronization in biological systems, we propose a score-based generative model built on periodic domains by leveraging stochastic Kuramoto dynamics in the diffusion process. In neural and physical systems, Kuramoto models capture synchronization phenomena across coupled oscillators – a behavior that we re-purpose here as an inductive bias for structured image generation. In our framework, the forward process performs \textitsynchronization among phase variables through globally or locally coupled oscillator interactions and attraction to a global reference phase, gradually collapsing the data into a low-entropy von Mises distribution. The reverse process then performs \textitdesynchronization, generating diverse patterns by reversing the dynamics with a learned score function. This approach enables structured destruction during forward diffusion and a hierarchical generation process that progressively refines global coherence into fine-scale details. We implement wrapped Gaussian transition kernels and periodicity-aware networks to account for the circular geometry. Our method achieves competitive results on general image benchmarks and significantly improves generation quality on orientation-dense datasets like fingerprints and textures. Ultimately, this work demonstrates the promise of biologically inspired synchronization dynamics as structured priors in generative modeling.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-105] How Good are Foundation Models in Step-by-Step Embodied <mark class="hl-label green">Reasoning</mark> ?</p>
<p>【速读】：该论文旨在解决当前大语言模型（Large Language Models, LLMs）在具身环境中的结构化推理能力不足的问题，即如何使基础模型在物理世界中做出不仅有效而且安全、空间一致且情境合理的决策。其解决方案的关键在于提出一个名为Foundation Model Embodied Reasoning (FoMER) 的基准测试体系，该体系包含10个任务和8种机器人形态，覆盖超过1.1k个样本，涵盖多模态感知、物理约束与安全性推理以及自然语言动作生成等关键环节；并通过一个新颖的评估框架将感知锚定（perceptual grounding）与行动推理（action reasoning）解耦，从而更精准地衡量LMMs在复杂具身决策场景下的推理性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15293">https://arxiv.org/abs/2509.15293</a><br>
<strong>作者</strong>: Dinura Dissanayake,Ahmed Heakl,Omkar Thawakar,Noor Ahsan,Ritesh Thawkar,Ketan More,Jean Lahoud,Rao Anwer,Hisham Cholakkal,Ivan Laptev,Fahad Shahbaz Khan,Salman Khan<br>
<strong>机构</strong>: Mohamed bin Zayed University of AI (穆罕默德·本·扎耶德人工智能大学); Linköping University (林雪平大学); Australian National University (澳大利亚国立大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Embodied agents operating in the physical world must make decisions that are not only effective but also safe, spatially coherent, and grounded in context. While recent advances in large multimodal models (LMMs) have shown promising capabilities in visual understanding and language generation, their ability to perform structured reasoning for real-world embodied tasks remains underexplored. In this work, we aim to understand how well foundation models can perform step-by-step reasoning in embodied environments. To this end, we propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to evaluate the reasoning capabilities of LMMs in complex embodied decision-making scenarios. Our benchmark spans a diverse set of tasks that require agents to interpret multimodal observations, reason about physical constraints and safety, and generate valid next actions in natural language. We present (i) a large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation framework that disentangles perceptual grounding from action reasoning, and (iii) empirical analysis of several leading LMMs under this setting. Our benchmark includes over 1.1k samples with detailed step-by-step reasoning across 10 tasks and 8 embodiments, covering three different robot types. Our results highlight both the potential and current limitations of LMMs in embodied reasoning, pointing towards key challenges and opportunities for future research in robot intelligence. Our data and code will be made publicly available.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-106] Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks</p>
<p>【速读】：该论文旨在解决当前视觉Transformer（Vision Transformer, ViT）在自监督学习（Self-Supervised Learning, SSL）预训练后，其未经过任何额外变换的特征表示能力尚未被系统评估的问题。现有方法通常通过轻量级头结构或蒸馏策略对预训练ViT特征进行进一步处理以提升下游任务性能，但缺乏对原始特征内在表达能力的深入分析。论文的关键解决方案在于：<strong>不引入额外的特征变换层，直接基于原始ViT特征（包括注意力机制中的key、query、value及前馈层输出的token）</strong>，采用超平面分类（如逻辑回归）和余弦相似度分类两种决策规则，在标准与少样本（few-shot）图像分类与分割任务中进行系统性评估，从而揭示不同token类型、任务场景和预训练目标下最优特征选择与决策策略的组合。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15272">https://arxiv.org/abs/2509.15272</a><br>
<strong>作者</strong>: Yannis Kaltampanidis,Alexandros Doumanoglou,Dimitrios Zarpalas<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  24 pages, XAI 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently demonstrated considerable potential as a pre-training strategy for a variety of computer vision tasks, including image classification and segmentation, both in standard and few-shot downstream contexts. Two pre-training objectives dominate the landscape of SSL techniques: Contrastive Learning and Masked Image Modeling. Features (or tokens) extracted from the final transformer attention block – specifically, the keys, queries, and values – as well as features obtained after the final block’s feed-forward layer, have become a common foundation for addressing downstream tasks. However, in many existing approaches, these pre-trained ViT features are further processed through additional transformation layers, often involving lightweight heads or combined with distillation, to achieve superior task performance. Although such methods can improve task outcomes, to the best of our knowledge, a comprehensive analysis of the intrinsic representation capabilities of unaltered ViT features has yet to be conducted. This study aims to bridge this gap by systematically evaluating the use of these unmodified features across image classification and segmentation tasks, in both standard and few-shot contexts. The classification and segmentation rules that we use are either hyperplane based (as in logistic regression) or cosine-similarity based, both of which rely on the presence of interpretable directions in the ViT’s latent space. Based on the previous rules and without the use of additional feature transformations, we conduct an analysis across token types, tasks, and pre-trained ViT models. This study provides insights into the optimal choice for token type and decision rule based on the task, context, and the pre-training objective, while reporting detailed findings on two widely-used datasets.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-107] Large Vision Models Can Solve Mental Rotation Problems</p>
<p>【速读】：该论文旨在解决当前视觉Transformer（Vision Transformer, ViT）模型在空间推理能力，特别是心理旋转（mental rotation）任务上的表现尚不明确的问题。其核心问题是：现代视觉模型是否能像人类一样发展出对几何结构的内在理解，并在不同复杂度的旋转任务中表现出类似人类的认知约束。解决方案的关键在于系统性地评估多种预训练模型（包括ViT、CLIP、DINOv2和DINOv3）在从简单积木结构到真实图像对象的多类心理旋转任务中的表现，并通过逐层探查模型表示空间，揭示其几何表征能力的发展机制。研究发现，自监督训练的ViT更擅长捕捉几何结构，中间层比最终层更具空间推理能力，且任务难度随旋转复杂性和遮挡程度上升，与人类反应时间趋势一致，表明这些模型在嵌入空间中可能面临类似的认知限制。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15271">https://arxiv.org/abs/2509.15271</a><br>
<strong>作者</strong>: Sebastian Ray Mason,Anders Gjølbye,Phillip Chavarria Højbjerg,Lenka Tětková,Lars Kai Hansen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Mental rotation is a key test of spatial reasoning in humans and has been central to understanding how perception supports cognition. Despite the success of modern vision transformers, it is still unclear how well these models develop similar abilities. In this work, we present a systematic evaluation of ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from simple block structures similar to those used by Shepard and Metzler to study human cognition, to more complex block figures, three types of text, and photo-realistic objects. By probing model representations layer by layer, we examine where and how these networks succeed. We find that i) self-supervised ViTs capture geometric structure better than supervised ViTs; ii) intermediate layers perform better than final layers; iii) task difficulty increases with rotation complexity and occlusion, mirroring human reaction times and suggesting similar constraints in embedding space representations.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-108] PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images</p>
<p>【速读】：该论文旨在解决生成式 AI（Generative AI）中内容溯源问题，即如何识别由特定模型生成的图像内容，以保障在商业应用中用户对内容来源的信任与责任追溯。其解决方案的关键在于提出 PRISM（Phase-enhanced Radial-based Image Signature Mapping）框架，该框架基于离散傅里叶变换（Discrete Fourier Transform, DFT）的径向降维，利用幅度和相位信息提取模型特有的指纹特征，并通过线性判别分析（Linear Discriminant Analysis, LDA）进行聚类，从而实现跨架构、跨数据集的可靠模型归属识别，且无需访问模型内部细节。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15270">https://arxiv.org/abs/2509.15270</a><br>
<strong>作者</strong>: Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro<br>
<strong>机构</strong>: University of Rome Tor Vergata (罗马大学托尔弗加塔校区); University of Rome Tor Vergata (罗马大学托尔弗加塔校区); University of Rome Tor Vergata (罗马大学托尔弗加塔校区); University of Rome Tor Vergata (罗马大学托尔弗加塔校区)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:A critical need has emerged for generative AI: attribution methods. That is, solutions that can identify the model originating AI-generated content. This feature, generally relevant in multimodal applications, is especially sensitive in commercial settings where users subscribe to paid proprietary services and expect guarantees about the source of the content they receive. To address these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images. PRISM is based on a radial reduction of the discrete Fourier transform that leverages amplitude and phase information to capture model-specific signatures. The output of the above process is subsequently clustered via linear discriminant analysis to achieve reliable model attribution in diverse settings, even if the model’s internal details are inaccessible. To support our work, we construct PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN- and diffusion-based models. On this dataset, PRISM achieves an attribution accuracy of 92.04%. We additionally evaluate our method on four benchmarks from the literature, reaching an average accuracy of 81.60%. Finally, we evaluate our methodology also in the binary task of detecting real vs fake images, achieving an average accuracy of 88.41%. We obtain our best result on GenImage with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our results demonstrate the effectiveness of frequency-domain fingerprinting for cross-architecture and cross-dataset model attribution, offering a viable solution for enforcing accountability and trust in generative AI systems.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-109] Autoguided Online Data Curation for Diffusion Model Training <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决生成式扩散模型（diffusion models）训练过程中计算成本高、数据利用效率低的问题，特别是如何通过高效的数据筛选和引导机制提升训练的时间与样本效率。其解决方案的关键在于将联合示例选择（Joint Example Selection, JEST）与自动引导（autoguidance）方法整合到统一的代码框架中，实现快速消融实验与基准测试。研究表明，自动引导在提升样本质量和多样性方面具有稳定优势，而早期应用JEST虽能在数据效率上媲美或略优于纯自动引导，但因引入额外时间开销和复杂性，在多数场景下不如自动引导或均匀随机数据选择更优。这表明，尽管在线数据选择在训练初期可能带来效率增益，但样本质量的显著提升主要依赖于自动引导机制。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15267">https://arxiv.org/abs/2509.15267</a><br>
<strong>作者</strong>: Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa<br>
<strong>机构</strong>: University of Glasgow(格拉斯哥大学); Dotphoton(点光子)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Accepted non-archival paper at ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The costs of generative model compute rekindled promises and hopes for efficient data curation. In this work, we investigate whether recently developed autoguidance and online data selection methods can improve the time and sample efficiency of training generative diffusion models. We integrate joint example selection (JEST) and autoguidance into a unified code base for fast ablation and benchmarking. We evaluate combinations of data curation on a controlled 2-D synthetic data generation task as well as (3x64x64)-D image generation. Our comparisons are made at equal wall-clock time and equal number of samples, explicitly accounting for the overhead of selection. Across experiments, autoguidance consistently improves sample quality and diversity. Early AJEST (applying selection only at the beginning of training) can match or modestly exceed autoguidance alone in data efficiency on both tasks. However, its time overhead and added complexity make autoguidance or uniform random data selection preferable in most situations. These findings suggest that while targeted online selection can yield efficiency gains in early training, robust sample quality improvements are primarily driven by autoguidance. We discuss limitations and scope, and outline when data selection may be beneficial.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-110] RespoDiff: Dual-Module Bottleneck Transformation for Responsible  Faithful T2I Generation</p>
<p>【速读】：该论文旨在解决扩散模型在文本到图像生成中面临的公平性（fairness）与安全性（safety）问题，这些问题在现有方法中常以牺牲语义保真度和图像质量为代价来实现。解决方案的关键在于提出RespoDiff框架，该框架通过在扩散模型的中间瓶颈表示上引入双模块变换机制：一个模块专注于捕捉并强制执行负责任的概念（如公平性和安全性），另一个模块则致力于保持与中性提示的语义对齐。为协同训练这两个模块，论文进一步设计了一种新颖的分数匹配（score-matching）目标函数，从而在不损害图像保真度的前提下，同时优化公平性、安全性与语义一致性。实验表明，该方法在未见过的多样化提示下，责任生成能力提升达20%，且可无缝集成至SDXL等大规模模型中。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15257">https://arxiv.org/abs/2509.15257</a><br>
<strong>作者</strong>: Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta<br>
<strong>机构</strong>: University of Surrey (萨里大学); Simon Fraser University (西蒙弗雷泽大学); University of Copenhagen (哥本哈根大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by 20% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. Code will be released upon acceptance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-111] Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning <mark class="hl-label red">ACL2024</mark></p>
<p>【速读】：该论文旨在解决大规模模型在视觉-语言导航（Vision-and-Language Navigation, VLN）任务中计算资源消耗高、难以部署于资源受限环境的问题。现有基于token pruning的效率优化方法因忽视VLN特有的信息依赖性，常导致因信息丢失引发更长的导航路径，反而增加计算成本，削弱了剪枝带来的效率优势。解决方案的关键在于提出一种导航感知剪枝（Navigation-Aware Pruning, NAP）机制：首先利用导航特性对输入token进行前景与背景预筛选（如根据可导航方向过滤图像视图），再借助大语言模型提取导航相关指令；随后聚焦于背景token进行剪枝以最小化信息损失，并通过移除低重要性的导航节点抑制回溯行为，从而有效控制导航长度。实验表明，NAP在标准VLN基准上显著优于现有方法，在保持更高成功率的同时实现超过50%的浮点运算量（FLOPS）节省。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15250">https://arxiv.org/abs/2509.15250</a><br>
<strong>作者</strong>: Wenda Qin,Andrea Burns,Bryan A. Plummer,Margrit Betke<br>
<strong>机构</strong>: Boston University (波士顿大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted to ACL 2024 Findings. Data and code to be released at <a target="_blank" rel="noopener" href="https://github.com/wdqin/VLN-NAP">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-112] Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models</p>
<p>【速读】：该论文旨在解决视觉语言模型（Vision-Language Models, VLMs）在安全关键场景中应用时面临的可解释性不足问题，尤其针对对象间复杂关系、细微视觉线索以及对透明度和可靠性要求更高的挑战。其解决方案的关键在于提出多模态可解释学习（Multi-Modal Explainable Learning, MMEL）框架，该框架引入了一个层次化语义关系模块（Hierarchical Semantic Relationship Module），通过多尺度特征处理、自适应注意力加权和跨模态对齐机制，增强梯度驱动的归因图的语义丰富性和定位精度。该模块在不同语义粒度上捕捉图像区域间的关联，并引入可学习的层特定权重以平衡模型深度中各层的贡献，从而生成更聚焦且具备上下文感知能力的可视化解释，显著提升模型决策过程的可理解性与可信度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15243">https://arxiv.org/abs/2509.15243</a><br>
<strong>作者</strong>: Muhammad Imran,Yugyung Lee<br>
<strong>机构</strong>: University of Missouri - Kansas City (密苏里大学堪萨斯城分校)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  8 pages, 6 figures, 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in vision-language models have significantly expanded the frontiers of automated image analysis. However, applying these models in safety-critical contexts remains challenging due to the complex relationships between objects, subtle visual cues, and the heightened demand for transparency and reliability. This paper presents the Multi-Modal Explainable Learning (MMEL) framework, designed to enhance the interpretability of vision-language models while maintaining high performance. Building upon prior work in gradient-based explanations for transformer architectures (Grad-eclip), MMEL introduces a novel Hierarchical Semantic Relationship Module that enhances model interpretability through multi-scale feature processing, adaptive attention weighting, and cross-modal alignment. Our approach processes features at multiple semantic levels to capture relationships between image regions at different granularities, applying learnable layer-specific weights to balance contributions across the model’s depth. This results in more comprehensive visual explanations that highlight both primary objects and their contextual relationships with improved precision. Through extensive experiments on standard datasets, we demonstrate that by incorporating semantic relationship information into gradient-based attribution maps, MMEL produces more focused and contextually aware visualizations that better reflect how vision-language models process complex scenes. The MMEL framework generalizes across various domains, offering valuable insights into model decisions for applications requiring high interpretability and reliability.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-113] ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images</p>
<p>【速读】：该论文旨在解决大分子蛋白质复合物（Protein Complexes, PCs）结构预测中因缺乏三维空间信息而导致的准确性不足问题，尤其是在传统生成式AI方法难以处理多蛋白相互作用时。其解决方案的关键在于提出了一种名为ProFusion的混合框架，通过将深度学习模型与原子力显微镜（Atomic Force Microscopy, AFM）的多视角高度图数据融合，实现高精度三维重建：一方面利用虚拟AFM模拟生成约54.2万条合成多视角图像用于训练；另一方面采用条件扩散模型从无姿态输入中合成新视角，并结合实例特定的神经辐射场（Neural Radiance Field, NeRF）模型重构出符合AFM成像分辨率的3D结构，从而在实验AFM数据上实现了高保真度验证，展现出成本低、效率高的潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15242">https://arxiv.org/abs/2509.15242</a><br>
<strong>作者</strong>: Jaydeep Rade,Md Hasibul Hasan Hasib,Meric Ozturk,Baboucarr Faal,Sheng Yang,Dipali G. Sashital,Vincenzo Venditti,Baoyu Chen,Soumik Sarkar,Adarsh Krishnamurthy,Anwesha Sarkar<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:AI-based in silico methods have improved protein structure prediction but often struggle with large protein complexes (PCs) involving multiple interacting proteins due to missing 3D spatial cues. Experimental techniques like Cryo-EM are accurate but costly and time-consuming. We present ProFusion, a hybrid framework that integrates a deep learning model with Atomic Force Microscopy (AFM), which provides high-resolution height maps from random orientations, naturally yielding multi-view data for 3D reconstruction. However, generating a large-scale AFM imaging data set sufficient to train deep learning models is impractical. Therefore, we developed a virtual AFM framework that simulates the imaging process and generated a dataset of ~542,000 proteins with multi-view synthetic AFM images. We train a conditional diffusion model to synthesize novel views from unposed inputs and an instance-specific Neural Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D protein structures achieve an average Chamfer Distance within the AFM imaging resolution, reflecting high structural fidelity. Our method is extensively validated on experimental AFM images of various PCs, demonstrating strong potential for accurate, cost-effective protein complex structure prediction and rapid iterative validation using AFM experiments.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-114] MICA: Multi-<mark class="hl-label green">Agent</mark>  Industrial Coordination Assistant <mark class="hl-label red">KR</mark></p>
<p>【速读】：该论文旨在解决工业场景中对适应性强且可信的辅助系统的需求，尤其是在计算资源有限、网络连接不稳定以及隐私保护要求严格的情况下。解决方案的关键在于提出MICA（Multi-Agent Industrial Coordination Assistant），一个基于感知并支持语音交互的多智能体系统，通过五个角色专业化语言代理协同工作，并由安全检查器审计以确保输出准确合规；其核心创新包括自适应步融合（Adaptive Step Fusion, ASF）机制，能够动态结合专家推理与来自自然语音反馈的在线适应，从而实现鲁棒的步骤理解，同时构建了面向工业辅助任务的新多智能体协调基准及定制化评估指标，使不同协调拓扑结构可系统比较。实验表明，MICA在任务成功率、可靠性与响应速度上均优于基线结构，且可在离线硬件上部署。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15237">https://arxiv.org/abs/2509.15237</a><br>
<strong>作者</strong>: Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen<br>
<strong>机构</strong>: Karlsruhe Institute of Technology (卡尔斯鲁厄理工学院); Hunan University (湖南大学)<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  The source code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/Kratos-Wen/MICA">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-115] Exploring the Capabilities of <mark class="hl-label green">LLM</mark>  Encoders for Image-Text Retrieval in Chest X-rays</p>
<p>【速读】：该论文旨在解决医学影像领域中图像-文本对齐（image-text alignment）因临床报告异质性（如缩写、仅含结论的笔记及风格差异）而导致模型性能受限的问题。传统视觉-语言预训练方法在通用领域依赖大规模数据提升效果，但在放射学场景下，直接扩展至噪声较多的海量报告反而会抑制学习效果。解决方案的关键在于引入领域适配的大语言模型（LLM）编码器 LLM2VEC4CXR，其能生成鲁棒的临床语义表示，有效应对报告中的多样性和噪声；并进一步构建双塔框架 LLM2CLIP4CXR，将该编码器与视觉主干网络结合，显著提升跨数据集的检索准确率和临床相关评分，证明了鲁棒性（robustness）而非单纯的数据规模才是实现高效多模态学习的核心因素。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15234">https://arxiv.org/abs/2509.15234</a><br>
<strong>作者</strong>: Hanbin Ko,Gihun Cho,Inhyeok Baek,Donguk Kim,Joonbeom Koo,Changi Kim,Dongheon Lee,Chang Min Park<br>
<strong>机构</strong>: Seoul National University (首尔国立大学); Seoul National University Graduate School (首尔国立大学研究生院); Seoul National University Hospital (首尔国立大学医院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  24 pages, 2 figures, under review</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Vision-language pretraining has advanced image-text alignment, yet progress in radiology remains constrained by the heterogeneity of clinical reports, including abbreviations, impression-only notes, and stylistic variability. Unlike general-domain settings where more data often leads to better performance, naively scaling to large collections of noisy reports can plateau or even degrade model learning. We ask whether large language model (LLM) encoders can provide robust clinical representations that transfer across diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR, a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a dual-tower framework that couples this encoder with a vision backbone. LLM2VEC4CXR improves clinical text understanding over BERT-based baselines, handles abbreviations and style variation, and achieves strong clinical alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to boost retrieval accuracy and clinically oriented scores, with stronger cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M CXR studies from public and private sources with heterogeneous and noisy reports, our models demonstrate that robustness – not scale alone – is the key to effective multimodal learning. We release models to support further research in medical image-text representation learning.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-116] PRISM: Probabilistic and Robust Inverse Solver with Measurement-Conditioned Diffusion Prior for Blind Inverse Problems</p>
<p>【速读】：该论文旨在解决<strong>盲反问题（blind inverse problems）</strong>，即在未知前向算子（forward operator）的情况下，从观测数据中恢复原始信号的问题。传统基于扩散模型（diffusion models）的逆解法通常依赖于对前向算子的完整先验知识，而PRISM通过引入一种<strong>测量条件扩散先验（measurement-conditioned diffusion prior）</strong>，实现了无需已知前向算子即可进行鲁棒且有效的后验采样。其解决方案的关键在于将强大的测量条件扩散模型嵌入到理论严谨的后验采样框架中，从而在图像去模糊任务中显著优于现有最先进方法，在图像和模糊核恢复方面均展现出优越性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16106">https://arxiv.org/abs/2509.16106</a><br>
<strong>作者</strong>: Yuanyun Hu,Evan Bell,Guijin Wang,Yu Sun<br>
<strong>机构</strong>: Johns Hopkins University (约翰霍普金斯大学); Tsinghua University (清华大学)<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Diffusion models are now commonly used to solve inverse problems in computational imaging. However, most diffusion-based inverse solvers require complete knowledge of the forward operator to be used. In this work, we introduce a novel probabilistic and robust inverse solver with measurement-conditioned diffusion prior (PRISM) to effectively address blind inverse problems. PRISM offers a technical advancement over current methods by incorporating a powerful measurement-conditioned diffusion model into a theoretically principled posterior sampling scheme. Experiments on blind image deblurring validate the effectiveness of the proposed method, demonstrating the superior performance of PRISM over state-of-the-art baselines in both image and blur kernel recovery.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-117] FMD-TransUNet: Abdominal Multi-Organ Segmentation Based on Frequency Domain Multi-Axis Representation Learning and Dual Attention Mechanisms</p>
<p>【速读】：该论文旨在解决当前基于深度学习的腹部多器官分割方法在处理小尺寸、不规则或解剖结构复杂的器官时精度不足的问题，以及现有方法普遍局限于空间域分析而忽视频域表示协同潜力的局限性。其解决方案的关键在于提出一种名为FMD-TransUNet的新框架，该框架创新性地融合了多轴外部权重模块（Multi-axis External Weight Block, MEWB）与改进的双注意力模块（DA+），其中MEWB通过提取多轴频率域特征以捕获全局解剖结构和局部边界细节，为空间域表示提供互补信息；DA+则利用深度可分离卷积结合空间与通道注意力机制，增强特征融合能力、减少冗余信息并缩小编码器与解码器之间的语义鸿沟，从而显著提升分割精度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16044">https://arxiv.org/abs/2509.16044</a><br>
<strong>作者</strong>: Fang Lu,Jingyu Xu,Qinxiu Sun,Qiong Lou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate abdominal multi-organ segmentation is critical for clinical applications. Although numerous deep learning-based automatic segmentation methods have been developed, they still struggle to segment small, irregular, or anatomically complex organs. Moreover, most current methods focus on spatial-domain analysis, often overlooking the synergistic potential of frequency-domain representations. To address these limitations, we propose a novel framework named FMD-TransUNet for precise abdominal multi-organ segmentation. It innovatively integrates the Multi-axis External Weight Block (MEWB) and the improved dual attention module (DA+) into the TransUNet framework. The MEWB extracts multi-axis frequency-domain features to capture both global anatomical structures and local boundary details, providing complementary information to spatial-domain representations. The DA+ block utilizes depthwise separable convolutions and incorporates spatial and channel attention mechanisms to enhance feature fusion, reduce redundant information, and narrow the semantic gap between the encoder and decoder. Experimental validation on the Synapse dataset shows that FMD-TransUNet outperforms other recent state-of-the-art methods, achieving an average DSC of 81.32% and a HD of 16.35 mm across eight abdominal organs. Compared to the baseline model, the average DSC increased by 3.84%, and the average HD decreased by 15.34 mm. These results demonstrate the effectiveness of FMD-TransUNet in improving the accuracy of abdominal multi-organ segmentation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-118] SLaM-DiMM: Shared Latent Modeling for Diffusion Based Missing Modality Synthesis in MRI</p>
<p>【速读】：该论文旨在解决医学图像分析中因临床实践限制导致的多模态脑部磁共振成像（MRI）数据缺失问题，即在某些情况下无法获取全部四种模态（T1加权增强与非增强、T2加权、FLAIR），从而影响下游任务如异常检测的性能。解决方案的关键在于提出一种名为SLaM-DiMM的新颖缺失模态生成框架，该框架利用扩散模型（diffusion models）从已有的任意模态中合成目标MRI模态，并通过专门设计的结构一致性增强机制确保生成图像在体积深度上保持解剖学合理性与结构连贯性，从而提升生成结果的保真度和临床可用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16019">https://arxiv.org/abs/2509.16019</a><br>
<strong>作者</strong>: Bhavesh Sandbhor,Bheeshm Sharma,Balamurugan Palaniappan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Brain MRI scans are often found in four modalities, consisting of T1-weighted with and without contrast enhancement (T1ce and T1w), T2-weighted imaging (T2w), and Flair. Leveraging complementary information from these different modalities enables models to learn richer, more discriminative features for understanding brain anatomy, which could be used in downstream tasks such as anomaly detection. However, in clinical practice, not all MRI modalities are always available due to various reasons. This makes missing modality generation a critical challenge in medical image analysis. In this paper, we propose SLaM-DiMM, a novel missing modality generation framework that harnesses the power of diffusion models to synthesize any of the four target MRI modalities from other available modalities. Our approach not only generates high-fidelity images but also ensures structural coherence across the depth of the volume through a dedicated coherence enhancement mechanism. Qualitative and quantitative evaluations on the BraTS-Lighthouse-2025 Challenge dataset demonstrate the effectiveness of the proposed approach in synthesizing anatomically plausible and structurally consistent results. Code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-119] he Missing Piece: A Case for Pre-Training in 3D Medical Object Detection <mark class="hl-label red">MICCAI2025</mark></p>
<p>【速读】：该论文旨在解决3D医学目标检测（3D medical object detection）中预训练方法应用不足的问题，尤其是在与分割任务相比，其在预训练方面的研究仍处于初级阶段。现有方法多依赖2D医学数据或自然图像进行预训练，未能充分利用三维体素信息（volumetric information）。论文的关键解决方案是首次系统性地将多种预训练策略集成到先进的检测架构（包括CNN和Transformer）中，并通过实证发现基于重建的自监督预训练优于监督预训练，而对比学习预训练对3D医学目标检测无明显增益。这一结果揭示了适用于3D医学检测任务的有效预训练范式。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15947">https://arxiv.org/abs/2509.15947</a><br>
<strong>作者</strong>: Katharina Eckstein,Constantin Ulrich,Michael Baumgartner,Jessica Kächele,Dimitrios Bounias,Tassilo Wald,Ralf Floca,Klaus H. Maier-Hein<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  MICCAI 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large-scale pre-training holds the promise to advance 3D medical object detection, a crucial component of accurate computer-aided diagnosis. Yet, it remains underexplored compared to segmentation, where pre-training has already demonstrated significant benefits. Existing pre-training approaches for 3D object detection rely on 2D medical data or natural image pre-training, failing to fully leverage 3D volumetric information. In this work, we present the first systematic study of how existing pre-training methods can be integrated into state-of-the-art detection architectures, covering both CNNs and Transformers. Our results show that pre-training consistently improves detection performance across various tasks and datasets. Notably, reconstruction-based self-supervised pre-training outperforms supervised pre-training, while contrastive pre-training provides no clear benefit for 3D medical object detection. Our code is publicly available at: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-120] QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical Microscopy Images Denoising</p>
<p>【速读】：该论文旨在解决生物医学显微成像中图像去噪的挑战，包括成像条件限制、噪声类型复杂、算法适应性不足以及临床应用需求等问题。解决方案的关键在于提出一种基于生成对抗网络（Generative Adversarial Network, GAN）的无监督图像去噪方法——QWD-GAN，其核心创新包括：1）采用小波变换驱动的多尺度自适应生成器以更好地保留高频细节信息；2）设计双分支判别器，融合差异感知特征图与原始特征，提升判别能力并增强模型对图像质量的感知。该方法在多个生物医学显微图像数据集上实现了当前最优的去噪性能，尤其在高频率信息保持方面表现突出，且双分支判别器具有良好的通用性，可兼容多种GAN框架。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15814">https://arxiv.org/abs/2509.15814</a><br>
<strong>作者</strong>: Qijun Yang,Yating Huang,Lintao Xiang,Hujun Yin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Image denoising plays a critical role in biomedical and microscopy imaging, especially when acquiring wide-field fluorescence-stained images. This task faces challenges in multiple fronts, including limitations in image acquisition conditions, complex noise types, algorithm adaptability, and clinical application demands. Although many deep learning-based denoising techniques have demonstrated promising results, further improvements are needed in preserving image details, enhancing algorithmic efficiency, and increasing clinical interpretability. We propose an unsupervised image denoising method based on a Generative Adversarial Network (GAN) architecture. The approach introduces a multi-scale adaptive generator based on the Wavelet Transform and a dual-branch discriminator that integrates difference perception feature maps with original features. Experimental results on multiple biomedical microscopy image datasets show that the proposed model achieves state-of-the-art denoising performance, particularly excelling in the preservation of high-frequency information. Furthermore, the dual-branch discriminator is seamlessly compatible with various GAN frameworks. The proposed quality-aware, wavelet-driven GAN denoising model is termed as QWD-GAN.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-121] DPC-QA Net: A No-Reference Dual-Stream Perceptual and Cellular Quality Assessment Network for Histopathology Images</p>
<p>【速读】：该论文旨在解决全切片成像（Whole Slide Imaging, WSI）中因染色伪影、失焦及细胞退化等问题导致的图像质量不可靠性，从而影响后续计算病理学分析的准确性。其解决方案的关键在于提出一种无参考（no-reference）双流网络 DPC-QA Net，该网络通过小波基全局差异感知与基于核和膜嵌入的细胞级质量评估相结合的方式实现多维度质量判别；其中 Aggr-RWKV 模块用于整合细胞特征，交叉注意力融合机制与多目标损失函数协同对齐感知线索与细胞层面的质量信号，最终在多个数据集上实现了 92% 的检测准确率，并显著提升与下游细胞识别任务（如核分割 Dice 系数、膜边界 F-score）的相关性，为 WSI 的预筛选提供了可量化的质量评估工具。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15802">https://arxiv.org/abs/2509.15802</a><br>
<strong>作者</strong>: Qijun Yang,Boyang Wang,Hujun Yin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reliable whole slide imaging (WSI) hinges on image quality,yet staining artefacts, defocus, and cellular degradations are common. We present DPC-QA Net, a no-reference dual-stream network that couples wavelet-based global difference perception with cellular quality assessment from nuclear and membrane embeddings via an Aggr-RWKV module. Cross-attention fusion and multi-term losses align perceptual and cellular cues. Across different datasets, our model detects staining, membrane, and nuclear issues with 92% accuracy and aligns well with usability scores; on LIVEC and KonIQ it outperforms state-of-the-art NR-IQA. A downstream study further shows strong positive correlations between predicted quality and cell recognition accuracy (e.g., nuclei PQ/Dice, membrane boundary F-score), enabling practical pre-screening of WSI regions for computational pathology.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-122] Uncertainty-Gated Deformable Network for Breast Tumor Segmentation in MR Images</p>
<p>【速读】：该论文旨在解决乳腺肿瘤在磁共振成像（MRI）中分割不准确的问题，尤其是现有方法难以捕捉不规则肿瘤形状以及有效融合局部与全局特征的挑战。其解决方案的关键在于提出一种不确定性门控可变形网络（Uncertainty-Gated Deformable Network），通过在卷积神经网络（CNN）和Transformer模块中引入可变形特征建模机制，实现对不规则肿瘤轮廓的自适应感受野；同时设计不确定性门控增强模块（U-GEM），基于像素级不确定性选择性地交换CNN与Transformer之间的互补特征，从而增强局部与全局表征能力；此外，还引入边界敏感的深度监督损失函数以进一步提升肿瘤边界的分割精度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15758">https://arxiv.org/abs/2509.15758</a><br>
<strong>作者</strong>: Yue Zhang,Jiahua Dong,Chengtao Peng,Qiuli Wang,Dan Song,Guiduo Duan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  5 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate segmentation of breast tumors in magnetic resonance images (MRI) is essential for breast cancer diagnosis, yet existing methods face challenges in capturing irregular tumor shapes and effectively integrating local and global features. To address these limitations, we propose an uncertainty-gated deformable network to leverage the complementary information from CNN and Transformers. Specifically, we incorporates deformable feature modeling into both convolution and attention modules, enabling adaptive receptive fields for irregular tumor contours. We also design an Uncertainty-Gated Enhancing Module (U-GEM) to selectively exchange complementary features between CNN and Transformer based on pixel-wise uncertainty, enhancing both local and global representations. Additionally, a Boundary-sensitive Deep Supervision Loss is introduced to further improve tumor boundary delineation. Comprehensive experiments on two clinical breast MRI datasets demonstrate that our method achieves superior segmentation performance compared with state-of-the-art methods, highlighting its clinical potential for accurate breast tumor delineation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-123] Prostate Capsule Segmentation from Micro-Ultrasound Images using Adaptive Focal Loss</p>
<p>【速读】：该论文旨在解决微超声（micro-ultrasound, micro-US）图像中前列腺包膜（prostate capsule）边界模糊导致的分割精度不足问题，这一挑战限制了其在前列腺癌诊断与治疗规划中的临床应用。解决方案的关键在于提出一种自适应焦点损失函数（adaptive focal loss function），该函数通过分析专家与非专家标注之间的差异来识别困难区域，并对其进行膨胀处理，从而动态调整模型权重，增强对模糊区域的敏感性；相较传统焦点损失（focal loss）基线方法，该策略显著提升了分割性能，在测试集上达到平均Dice相似系数（DSC）0.940和平均豪斯多夫距离（Hausdorff distance, HD）1.949 mm，验证了其在复杂边界场景下的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15595">https://arxiv.org/abs/2509.15595</a><br>
<strong>作者</strong>: Kaniz Fatema,Vaibhav Thakur,Emad A. Mohammed<br>
<strong>机构</strong>: Wilfrid Laurier University (威尔弗里德劳里埃大学)<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Micro-ultrasound (micro-US) is a promising imaging technique for cancer detection and computer-assisted visualization. This study investigates prostate capsule segmentation using deep learning techniques from micro-US images, addressing the challenges posed by the ambiguous boundaries of the prostate capsule. Existing methods often struggle in such cases, motivating the development of a tailored approach. This study introduces an adaptive focal loss function that dynamically emphasizes both hard and easy regions, taking into account their respective difficulty levels and annotation variability. The proposed methodology has two primary strategies: integrating a standard focal loss function as a baseline to design an adaptive focal loss function for proper prostate capsule segmentation. The focal loss baseline provides a robust foundation, incorporating class balancing and focusing on examples that are difficult to classify. The adaptive focal loss offers additional flexibility, addressing the fuzzy region of the prostate capsule and annotation variability by dilating the hard regions identified through discrepancies between expert and non-expert annotations. The proposed method dynamically adjusts the segmentation model’s weights better to identify the fuzzy regions of the prostate capsule. The proposed adaptive focal loss function demonstrates superior performance, achieving a mean dice coefficient (DSC) of 0.940 and a mean Hausdorff distance (HD) of 1.949 mm in the testing dataset. These results highlight the effectiveness of integrating advanced loss functions and adaptive techniques into deep learning models. This enhances the accuracy of prostate capsule segmentation in micro-US images, offering the potential to improve clinical decision-making in prostate cancer diagnosis and treatment planning.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-124] Incorporating Visual Cortical Lateral Connection Properties into CNN: Recurrent Activation and Excitatory-Inhibitory Separation</p>
<p>【速读】：该论文旨在解决当前卷积神经网络（Convolutional Neural Networks, CNNs）在架构上缺失生物视觉系统中关键的横向连接（lateral connections）这一问题，这些连接存在于哺乳动物视觉皮层内部，对视觉信息处理具有重要作用。解决方案的关键在于：首先，通过引入权重共享的循环结构来模拟横向连接的功能；其次，设计一种定制化的损失函数以分离兴奋性和抑制性权重，从而实现对横向连接中两种不同类型神经通路的建模。实验表明，这两种改进显著提升了分类准确率，并使模型的激活特性和连接特性更接近于生物视觉系统的观察结果。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15460">https://arxiv.org/abs/2509.15460</a><br>
<strong>作者</strong>: Jin Hyun Park,Cheng Zhang,Yoonsuck Choe<br>
<strong>机构</strong>: Texas A&amp;M University (德州农工大学)<br>
<strong>类目</strong>: Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The original Convolutional Neural Networks (CNNs) and their modern updates such as the ResNet are heavily inspired by the mammalian visual system. These models include afferent connections (retina and LGN to the visual cortex) and long-range projections (connections across different visual cortical areas). However, in the mammalian visual system, there are connections within each visual cortical area, known as lateral (or horizontal) connections. These would roughly correspond to connections within CNN feature maps, and this important architectural feature is missing in current CNN models. In this paper, we present how such lateral connections can be modeled within the standard CNN framework, and test its benefits and analyze its emergent properties in relation to the biological visual system. We will focus on two main architectural features of lateral connections: (1) recurrent activation and (2) separation of excitatory and inhibitory connections. We show that recurrent CNN using weight sharing is equivalent to lateral connections, and propose a custom loss function to separate excitatory and inhibitory weights. The addition of these two leads to increased classification accuracy, and importantly, the activation properties and connection properties of the resulting model show properties similar to those observed in the biological visual system. We expect our approach to help align CNN closer to its biological counterpart and better understand the principles of visual cortical computation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-125] Analysis Plug-and-Play Methods for Imaging Inverse Problems</p>
<p>【速读】：该论文旨在解决传统Plug-and-Play Priors (PnP) 方法中将先验直接施加于图像域所带来的局限性，即如何更有效地利用学习到的先验信息来提升成像逆问题（如图像去模糊和超分辨率）的重建性能。其解决方案的关键在于提出一种分析式PnP（Analysis PnP, APnP）框架，将先验约束从图像域转移到图像梯度域——具体而言，训练一个在梯度域上操作的高斯去噪器（Gaussian denoiser），从而实现对图像梯度结构的隐式建模，本质上是将总变差（Total Variation, TV）正则化扩展为可学习的梯度域正则化。为此，作者进一步设计了基于半二次分裂（Half-Quadratic Splitting, HQS）和交替方向乘子法（Alternating Direction Method of Multipliers, ADMM）的两种分析式PnP算法（APnP-HQS 和 APnP-ADMM），实验证明该方法在性能上可媲美传统的图像域PnP算法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15422">https://arxiv.org/abs/2509.15422</a><br>
<strong>作者</strong>: Edward P. Chandler,Shirin Shoushtari,Brendt Wohlberg,Ulugbek S. Kamilov<br>
<strong>机构</strong>: Washington University in St. Louis (圣路易斯华盛顿大学); Los Alamos National Laboratory (洛斯阿拉莫斯国家实验室)<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Plug-and-Play Priors (PnP) is a popular framework for solving imaging inverse problems by integrating learned priors in the form of denoisers trained to remove Gaussian noise from images. In standard PnP methods, the denoiser is applied directly in the image domain, serving as an implicit prior on natural images. This paper considers an alternative analysis formulation of PnP, in which the prior is imposed on a transformed representation of the image, such as its gradient. Specifically, we train a Gaussian denoiser to operate in the gradient domain, rather than on the image itself. Conceptually, this is an extension of total variation (TV) regularization to learned TV regularization. To incorporate this gradient-domain prior in image reconstruction algorithms, we develop two analysis PnP algorithms based on half-quadratic splitting (APnP-HQS) and the alternating direction method of multipliers (APnP-ADMM). We evaluate our approach on image deblurring and super-resolution, demonstrating that the analysis formulation achieves performance comparable to image-domain PnP algorithms.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-126] Recent Advancements in Microscopy Image Enhancement using Deep Learning: A Survey</p>
<p>【速读】：该论文旨在解决显微成像图像增强技术在生物细胞和材料微观细节解析中的关键挑战，其核心问题是如何利用深度学习方法提升显微图像的分辨率、重建质量与去噪性能。解决方案的关键在于系统性地梳理和分析当前基于深度学习的显微图像增强技术，聚焦超分辨率（super-resolution）、图像重建（reconstruction）和去噪（denoising）三大领域，揭示各领域的最新发展趋势及其在实际应用中的有效性，从而为后续研究提供理论基础与实践指导。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15363">https://arxiv.org/abs/2509.15363</a><br>
<strong>作者</strong>: Debasish Dutta,Neeharika Sonowal,Risheraj Barauh,Deepjyoti Chetia,Sanjib Kr Kalita<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  7 pages, 3 figures and 1 table. 2024 IEEE International Conference on Computer Vision and Machine Intelligence (CVMI). IEEE, 2024</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Microscopy image enhancement plays a pivotal role in understanding the details of biological cells and materials at microscopic scales. In recent years, there has been a significant rise in the advancement of microscopy image enhancement, specifically with the help of deep learning methods. This survey paper aims to provide a snapshot of this rapidly growing state-of-the-art method, focusing on its evolution, applications, challenges, and future directions. The core discussions take place around the key domains of microscopy image enhancement of super-resolution, reconstruction, and denoising, with each domain explored in terms of its current trends and their practical utility of deep learning.<br>
zh</p>
</div></div>
<h3 id="人工智能">人工智能</h3>
<div class="note blue no-icon flat"><p>[AI-0] FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation</p>
<p>【速读】：该论文旨在解决当前神经音频编解码器（Neural Audio Codec）在实时应用场景中因非流式（non-streamable）设计导致的延迟问题，同时保持高质量的语音重建与下游任务性能。其解决方案的关键在于提出了一种基于焦点调制（focal modulation）的混合编解码器 FocalCodec-Stream，通过多阶段因果蒸馏（causal distillation）WavLM 模型，并引入轻量级精修模块（refiner module），在仅 0.55–0.80 kbps 的极低比特率下实现理论延迟为 80 ms 的流式压缩，有效平衡了重建质量、语义与声学信息保留、计算效率及延迟需求。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16195">https://arxiv.org/abs/2509.16195</a><br>
<strong>作者</strong>: Luca Della Libera,Cem Subakan,Mirco Ravanelli<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  5 pages, 1 figure</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-1] Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers</p>
<p>【速读】：该论文旨在解决自闭症谱系障碍（Autism Spectrum Disorder, ASD）缺乏可靠生物标志物导致早期诊断延迟的问题。其解决方案的关键在于提出了一种基于遗传算法的网络优化框架 GANet，该框架融合 PageRank 和 Degree 指标进行重要性驱动的特征表征，通过系统性优化网络结构从高维唾液光谱数据中提取有意义的模式，从而实现对 ASD 的精准识别。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16126">https://arxiv.org/abs/2509.16126</a><br>
<strong>作者</strong>: Janayna M. Fernandes,Robinson Sabino-Silva,Murillo G. Carneiro<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy, we developed GANet, a genetic algorithm-based network optimization framework leveraging PageRank and Degree for importance-based feature characterization. GANet systematically optimizes network structure to extract meaningful patterns from high-dimensional spectral data. It achieved superior performance compared to linear discriminant analysis, support vector machines, and deep learning models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74 harmonic mean. These results demonstrate GANet’s potential as a robust, bio-inspired, non-invasive tool for precise ASD detection and broader spectral-based health applications.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-2] Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning</p>
<p>【速读】：该论文旨在解决传统大气风场观测手段在时空分辨率、计算成本及偏差方面的局限性问题，尤其是在高精度风场信息获取方面面临的挑战。其解决方案的关键在于提出了一种名为G-WindCast的深度学习框架，该框架利用5G全球导航卫星系统（GNSS）信号强度变化所提取的特征，通过前馈神经网络（FNN）和Transformer网络建模复杂非线性与时空关系，实现三维（3D）大气风场的反演与短时预测（最长可达30分钟）。该方法不仅在多个压力层和预报时段上表现出鲁棒性，且在风速和风向预测精度上优于同期ERA5再分析数据，同时具备显著的成本效益和可扩展性，仅需约100个GNSS站点即可维持高性能局部风场预报。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16068">https://arxiv.org/abs/2509.16068</a><br>
<strong>作者</strong>: Yuchen Ye,Hong Liang,Chaoxia Yuan,Mingyu Li,Aoqi Zhou,Chunqing Shang,Hua Cai,Peixi Liu,Kezuan Wang,Yifeng Zheng<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  31 pages,11 figures,1 table</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate atmospheric wind field information is crucial for various applications, including weather forecasting, aviation safety, and disaster risk reduction. However, obtaining high spatiotemporal resolution wind data remains challenging due to limitations in traditional in-situ observations and remote sensing techniques, as well as the computational expense and biases of numerical weather prediction (NWP) models. This paper introduces G-WindCast, a novel deep learning framework that leverages signal strength variations from 5G Global Navigation Satellite System (GNSS) signals to retrieve and forecast three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward Neural Networks (FNN) and Transformer networks to capture complex, nonlinear, and spatiotemporal relationships between GNSS-derived features and wind dynamics. Our preliminary results demonstrate promising accuracy in both wind retrieval and short-term wind forecasting (up to 30 minutes lead time), with skill scores comparable to high-resolution NWP outputs in certain scenarios. The model exhibits robustness across different forecast horizons and pressure levels, and its predictions for wind speed and direction show superior agreement with observations compared to concurrent ERA5 reanalysis data. Furthermore, we show that the system can maintain excellent performance for localized forecasting even with a significantly reduced number of GNSS stations (e.g., around 100), highlighting its cost-effectiveness and scalability. This interdisciplinary approach underscores the transformative potential of exploiting non-traditional data sources and deep learning for advanced environmental monitoring and real-time atmospheric applications.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-3] Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers</p>
<p>【速读】：该论文旨在解决当前人工智能模型中注意力机制（Attention Mechanism）缺乏显式控制与优化的问题，尤其是在资源分配效率、学习速度和鲁棒性方面的不足。其解决方案的关键在于引入基于注意力图式理论（Attention Schema Theory, AST）的ASAC（Attention Schema-based Attention Control）模块，该模块利用向量量化变分自编码器（Vector-Quantized Variational AutoEncoder, VQVAE）作为注意力抽象器与控制器，显式建模注意力分配过程，从而实现对注意力资源的精准管理。这一设计不仅提升了模型在视觉和自然语言处理任务中的分类准确率与训练效率，还在噪声数据、分布外样本、多任务学习及对抗攻击场景下展现出更强的泛化能力与鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16058">https://arxiv.org/abs/2509.16058</a><br>
<strong>作者</strong>: Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC’s effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model’s robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-4] Compose by Focus: Scene Graph-based Atomic Skills</p>
<p>【速读】：该论文旨在解决通用机器人在执行复杂、长周期任务时面临的组合泛化（compositional generalization）问题，即如何将基础技能组合以应对未见过的任务场景。现有方法多集中于合成一个规划器来编排预训练技能，但个体技能在实际执行中仍易因场景组合引发的分布偏移（distribution shifts）而失败。解决方案的关键在于提出基于场景图（scene graph）的表示方法，聚焦于任务相关的物体及其关系，从而降低对无关变化的敏感性；在此基础上构建了融合图神经网络与扩散式模仿学习的技能学习框架，并将“聚焦型”场景图技能与视觉-语言模型（VLM）驱动的任务规划器相结合，显著提升了长周期任务中的鲁棒性和组合泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16053">https://arxiv.org/abs/2509.16053</a><br>
<strong>作者</strong>: Han Qi,Changhe Chen,Heng Yang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:A key requirement for generalist robots is compositional generalization - the ability to combine atomic skills to solve complex, long-horizon tasks. While prior work has primarily focused on synthesizing a planner that sequences pre-learned skills, robust execution of the individual skills themselves remains challenging, as visuomotor policies often fail under distribution shifts induced by scene composition. To address this, we introduce a scene graph-based representation that focuses on task-relevant objects and relations, thereby mitigating sensitivity to irrelevant variation. Building on this idea, we develop a scene-graph skill learning framework that integrates graph neural networks with diffusion-based imitation learning, and further combine “focused” scene-graph skills with a vision-language model (VLM) based task planner. Experiments in both simulation and real-world manipulation tasks demonstrate substantially higher success rates than state-of-the-art baselines, highlighting improved robustness and compositional generalization in long-horizon tasks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-5] Fed-PISA: Federated Voice Cloning via Personalized Identity-Style Adaptation</p>
<p>【速读】：该论文旨在解决联邦学习（Federated Learning, FL）在文本到语音（Text-to-Speech, TTS）语音克隆任务中面临的两大挑战：一是通信成本过高，二是难以保留说话人风格的异质性，导致个性化不足。解决方案的关键在于提出Fed-PISA框架，其核心创新包括两个方面：一是引入解耦的低秩适配（Low-Rank Adaptation, LoRA）机制，通过本地私有ID-LoRA保留说话人音色特征，仅上传轻量级风格LoRA至服务器，显著降低参数传输开销；二是设计基于协同过滤思想的聚合策略，利用风格相似客户端间的知识共享，为每个客户端生成定制化模型，从而有效挖掘并保留说话人风格的多样性与个性化表达能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16010">https://arxiv.org/abs/2509.16010</a><br>
<strong>作者</strong>: Qi Wang,Shituo Ma,Guoxin Yu,Hanyang Peng,Yue Yu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Voice cloning for Text-to-Speech (TTS) aims to generate expressive and personalized speech from text using limited data from a target speaker. Federated Learning (FL) offers a collaborative and privacy-preserving framework for this task, but existing approaches suffer from high communication costs and tend to suppress stylistic heterogeneity, resulting in insufficient personalization. To address these issues, we propose Fed-PISA, which stands for Federated Personalized Identity-Style Adaptation. To minimize communication costs, Fed-PISA introduces a disentangled Low-Rank Adaptation (LoRA) mechanism: the speaker’s timbre is retained locally through a private ID-LoRA, while only a lightweight style-LoRA is transmitted to the server, thereby minimizing parameter exchange. To harness heterogeneity, our aggregation method, inspired by collaborative filtering, is introduced to create custom models for each client by learning from stylistically similar peers. Experiments show that Fed-PISA improves style expressivity, naturalness, and speaker similarity, outperforming standard federated baselines with minimal communication costs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-6] Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations</p>
<p>【速读】：该论文旨在解决强化学习中稀疏奖励环境下，如何智能决定何时模仿示范（demonstration）而非依赖自身策略的问题。传统方法（如Q-filter）通常采用二值化决策机制，难以平衡模仿与探索的不确定性。其解决方案的关键在于提出Smooth Policy Regularisation from Demonstrations (SPReD)，通过集成方法显式建模示范与策略动作的Q值分布，并量化比较时的不确定性；进而设计两种互补的不确定性感知策略：一种基于概率估计示范优于当前策略的可能性，另一种基于优势显著性动态调整模仿强度。该框架以连续、比例于不确定性的正则化权重替代离散决策，有效降低训练过程中的梯度方差，在8个机器人任务中实现显著性能提升，最大提升达14倍，且对示范质量和数量具有鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15981">https://arxiv.org/abs/2509.15981</a><br>
<strong>作者</strong>: Yujie Zhu,Charles A. Hepburn,Matthew Thorpe,Giovanni Montana<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-7] RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation</p>
<p>【速读】：该论文旨在解决强化学习（Reinforcement Learning, RL）训练过程中因工作流异构性和动态性导致的硬件利用率低、训练速度慢的问题。其解决方案的关键在于提出了一种名为宏到微流变换（macro-to-micro flow transformation, M2Flow）的新系统设计范式，通过自动分解高层次、易组合的RL工作流在时间与空间维度上的结构，并重构为优化的执行流，结合自适应通信能力、上下文切换和弹性流水线技术以及基于性能分析的调度策略，实现了高灵活性与高效率的RL训练系统RLinf，从而显著提升了端到端训练吞吐量（1.1x–2.13x加速）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15965">https://arxiv.org/abs/2509.15965</a><br>
<strong>作者</strong>: Chao Yu,Yuanqing Wang,Zhen Guo,Hao Lin,Si Xu,Hongzhi Zang,Quanlu Zhang,Yongji Wu,Chunyang Zhu,Junhao Hu,Zixiao Huang,Mingjie Wei,Yuqing Xie,Ke Yang,Bo Dai,Zhexuan Xu,Xiangyuan Wang,Xu Fu,Zhihao Liu,Kang Chen,Weilin Liu,Gang Liu,Boxun Li,Jianlei Yang,Zhi Yang,Guohao Dai,Yu Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Distributed, Parallel, and Cluster Computing (cs.DC)<br>
<strong>备注</strong>:  GitHub Repo: <a target="_blank" rel="noopener" href="https://github.com/RLinf/RLinf">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker’s adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-8] Structured Information for Improving Spatial Relationships in Text-to-Image Generation</p>
<p>【速读】：该论文旨在解决文本到图像（Text-to-Image, T2I）生成中难以准确捕捉自然语言提示中描述的空间关系这一关键问题。现有方法如提示优化、空间对齐生成和语义细化虽有一定效果，但存在局限性。本文提出一种轻量级解决方案，其核心在于通过元组（tuple）形式的结构化信息增强原始提示，利用微调后的语言模型自动将自然语言转换为结构化元组，并无缝集成至T2I生成流程中。实验表明，该方法显著提升了空间准确性，同时保持了图像整体质量（以Inception Score衡量），且自动生成的元组质量可媲美人工标注，为提升T2I系统中的空间建模能力提供了一种实用且可迁移的方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15962">https://arxiv.org/abs/2509.15962</a><br>
<strong>作者</strong>: Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  text-to-image generation, structured information, spatial relationship</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing spatial relationships described in natural language prompts remains a major challenge. Prior efforts have addressed this issue through prompt optimization, spatially grounded generation, and semantic refinement. This work introduces a lightweight approach that augments prompts with tuple-based structured information, using a fine-tuned language model for automatic conversion and seamless integration into T2I pipelines. Experimental results demonstrate substantial improvements in spatial accuracy, without compromising overall image quality as measured by Inception Score. Furthermore, the automatically generated tuples exhibit quality comparable to human-crafted tuples. This structured information provides a practical and portable solution to enhance spatial relationships in T2I generation, addressing a key limitation of current large-scale generative systems.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-9] Explainable AI for Maritime Autonomous Surface Ships (MASS): Adaptive Interfaces and Trustworthy Human-AI Collaboration</p>
<p>【速读】：该论文旨在解决海上自主水面船舶（Maritime Autonomous Surface Ships, MASS）在实际应用中因决策过程不透明和人机交互机制欠佳而导致的安全隐患问题，特别是人类操作员在接管控制（takeover）过程中可能出现的不安全控制行为（Human Unsafe Control Actions, Human-UCAs）。其解决方案的关键在于构建一个自适应的透明度框架，通过融合操作员状态估计与可解释的决策支持系统，降低认知负荷并提升接管及时性；同时提出三层透明度设计策略——传感与态势感知（Situation Awareness, SA）数据采集与融合层、人机界面（Human-Machine Interface, HMI）及增强型人机界面（eHMI）呈现层（如文本/图形叠加、颜色编码、对话式与沉浸式UI），以及面向工程师的流程设计层（包括鲁棒交互设计、验证与标准化），并结合STPA-Cog + IDAC方法识别Human-UCAs、量化信任与SA评估及工作负载监测，以实现更可靠、可信赖的MASS运行。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15959">https://arxiv.org/abs/2509.15959</a><br>
<strong>作者</strong>: Zhuoyue Zhang,Haitong Xu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Autonomous navigation in maritime domains is accelerating alongside advances in artificial intelligence, sensing, and connectivity. Opaque decision-making and poorly calibrated human-automation interaction remain key barriers to safe adoption. This article synthesizes 100 studies on automation transparency for Maritime Autonomous Surface Ships (MASS) spanning situation awareness (SA), human factors, interface design, and regulation. We (i) map the Guidance-Navigation-Control stack to shore-based operational modes – remote supervision (RSM) and remote control (RCM) – and identify where human unsafe control actions (Human-UCAs) concentrate in handover and emergency loops; (ii) summarize evidence that transparency features (decision rationales, alternatives, confidence/uncertainty, and rule-compliance indicators) improve understanding and support trust calibration, though reliability and predictability often dominate trust; (iii) distill design strategies for transparency at three layers: sensor/SA acquisition and fusion, HMI/eHMI presentation (textual/graphical overlays, color coding, conversational and immersive UIs), and engineer-facing processes (resilient interaction design, validation, and standardization). We integrate methods for Human-UCA identification (STPA-Cog + IDAC), quantitative trust/SA assessment, and operator workload monitoring, and outline regulatory and rule-based implications including COLREGs formalization and route exchange. We conclude with an adaptive transparency framework that couples operator state estimation with explainable decision support to reduce cognitive overload and improve takeover timeliness. The review highlights actionable figure-of-merit displays (e.g., CPA/TCPA risk bars, robustness heatmaps), transparent model outputs (rule traceability, confidence), and training pipelines (HIL/MIL, simulation) as near-term levers for safer MASS operations.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-10] Compose Yourself: Ave<mark class="hl-label green">rag</mark> e-Velocity Flow Matching for One-Step Speech Enhancement <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决扩散模型（Diffusion Model）和流匹配（Flow Matching, FM）模型在语音增强（Speech Enhancement, SE）任务中因多步生成导致的计算成本高及离散化误差敏感的问题。其解决方案的关键在于提出一种一阶流匹配框架 COSE，通过引入速度组合恒等式（velocity composition identity）高效计算平均速度场（average velocity field），从而避免昂贵的雅可比-向量积（Jacobian-vector product, JVP）计算，在保持理论一致性的同时显著降低训练开销（减少40%）并实现高达5倍的采样加速，且不牺牲语音质量。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15952">https://arxiv.org/abs/2509.15952</a><br>
<strong>作者</strong>: Gang Yang,Yue Lei,Wenxin Tai,Jin Wu,Jia Chen,Ting Zhong,Fan Zhou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  5 pages, 2 figures, submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-11] A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning</p>
<p>【速读】：该论文旨在解决机器人在真实世界中进行视觉-语言-动作（Vision-Language-Action, VLA）强化学习时面临的稀疏人工奖励和低效探索问题。其核心解决方案是提出VLAC模型，这是一种基于InternVL训练的通用过程奖励模型，能够根据成对观测与语言目标输出密集的进度变化量（progress delta）和完成信号（done signal），从而避免任务特定的奖励工程设计，并支持零样本上下文迁移至未见过的任务与环境。关键创新在于通过大规模异构数据集（包括视觉语言数据、机器人及人类轨迹数据）联合训练，增强感知、对话与推理能力，同时构建大量负样本和语义不匹配样本以提升对无关提示的拒识能力和对退化或停滞状态的检测能力；此外，结合提示控制实现奖励与动作令牌的交替生成，统一策略与价值函数，并引入分级人类在环协议（offline demonstration replay, return and explore, human-guided explore）加速探索并稳定早期学习，最终在四个不同现实操作任务中将成功率从约30%提升至约90%，加入人类干预后进一步提高样本效率达50%，最终成功率可达100%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15937">https://arxiv.org/abs/2509.15937</a><br>
<strong>作者</strong>: Shaopeng Zhai,Qi Zhang,Tianyi Zhang,Fuxian Huang,Haoran Zhang,Ming Zhou,Shengzhe Zhang,Litao Liu,Sixu Lin,Jiangmiao Pang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  26 pages,10 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Robotic real-world reinforcement learning (RL) with vision-language-action (VLA) models is bottlenecked by sparse, handcrafted rewards and inefficient exploration. We introduce VLAC, a general process reward model built upon InternVL and trained on large scale heterogeneous datasets. Given pairwise observations and a language goal, it outputs dense progress delta and done signal, eliminating task-specific reward engineering, and supports one-shot in-context transfer to unseen tasks and environments. VLAC is trained on vision-language datasets to strengthen perception, dialogic and reasoning capabilities, together with robot and human trajectories data that ground action generation and progress estimation, and additionally strengthened to reject irrelevant prompts as well as detect regression or stagnation by constructing large numbers of negative and semantically mismatched samples. With prompt control, a single VLAC model alternately generating reward and action tokens, unifying critic and policy. Deployed inside an asynchronous real-world RL loop, we layer a graded human-in-the-loop protocol (offline demonstration replay, return and explore, human guided explore) that accelerates exploration and stabilizes early learning. Across four distinct real-world manipulation tasks, VLAC lifts success rates from about 30% to about 90% within 200 real-world interaction episodes; incorporating human-in-the-loop interventions yields a further 50% improvement in sample efficiency and achieves up to 100% final success.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-12] he Alignment Bottleneck</p>
<p>【速读】：该论文试图解决大语言模型（Large Language Models, LLMs）在反馈对齐（feedback-based alignment）过程中存在的系统性偏差问题，即随着模型规模扩大，对齐效果仍无法完全达到预期行为。其核心假设是认知能力受限（bounded rationality），将人类判断视为资源受限的决策过程，而反馈机制则是一个受约束的信息通道。解决方案的关键在于构建一个两阶段级联模型 $ U \to H \to Y $（给定状态 $ S $），其中引入了认知容量 $ C_\text{cog}|S $ 和平均总容量 $ \bar{C}_\text{tot}|S $ 来刻画信息处理的限制，并推导出一个“容量耦合的对齐性能区间”：该区间由一个数据规模无关的Fano下界和一个PAC-Bayes上界构成，二者均通过同一容量参数调控；当损失函数与数据分布匹配时，上下界共同受单一容量支配，揭示了仅增加标签无法突破性能上限、复杂目标需容量随 $ \log M $ 增长等关键结论，从而将对齐问题转化为接口工程——即测量并分配有限容量、管理任务复杂度、优化信息投入位置。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15932">https://arxiv.org/abs/2509.15932</a><br>
<strong>作者</strong>: Wenjun Cao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Information Theory (<a target="_blank" rel="noopener" href="http://cs.IT">cs.IT</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models improve with scale, yet feedback-based alignment still exhibits systematic deviations from intended behavior. Motivated by bounded rationality in economics and cognitive science, we view judgment as resource-limited and feedback as a constrained channel. On this basis, we model the loop as a two-stage cascade  U \to H \to Y  given  S , with cognitive capacity  C_\textcog|S  and average total capacity  \barC_\texttot|S . Our main result is a capacity-coupled Alignment Performance Interval. It pairs a data size-independent Fano lower bound proved on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is controlled by the same channel via  m , \barC_\texttot|S . The PAC-Bayes bound becomes an upper bound on the same true risk when the canonical observable loss is used and the dataset is drawn from the same mixture. Under these matched conditions, both limits are governed by a single capacity. Consequences include that, with value complexity and capacity fixed, adding labels alone cannot cross the bound; attaining lower risk on more complex targets requires capacity that grows with  \log M ; and once useful signal saturates capacity, further optimization tends to fit channel regularities, consistent with reports of sycophancy and reward hacking. The analysis views alignment as interface engineering: measure and allocate limited capacity, manage task complexity, and decide where information is spent.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-13] Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search</p>
<p>【速读】：该论文旨在解决当前AI生成竞价（AIGB）方法在实际应用中面临的性能瓶颈问题，主要体现在对细粒度生成质量评估的忽视以及无法超越静态离线数据集进行探索的局限性。其解决方案的关键在于提出AIGB-Pearl（Planning with EvAluator via RL），通过构建一个非自举（non-bootstrapped）的轨迹评估器（trajectory evaluator）来赋予奖励并引导策略搜索，使规划器能够通过交互迭代优化生成质量；同时，为提升离线场景下评估器的准确性，引入三项关键技术：基于大语言模型（Large Language Model, LLM）的架构以增强表示能力、混合点对点与成对损失以优化评分学习，以及自适应融合专家反馈以增强泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15927">https://arxiv.org/abs/2509.15927</a><br>
<strong>作者</strong>: Zhiyu Mou,Yiqin Lv,Miao Xu,Cheems Wang,Yixiu Mao,Qichen Ye,Chao Li,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Auto-bidding is an essential tool for advertisers to enhance their advertising performance. Recent progress has shown that AI-Generated Bidding (AIGB), which formulates the auto-bidding as a trajectory generation task and trains a conditional diffusion-based planner on offline data, achieves superior and stable performance compared to typical offline reinforcement learning (RL)-based auto-bidding methods. However, existing AIGB methods still encounter a performance bottleneck due to their neglect of fine-grained generation quality evaluation and inability to explore beyond static datasets. To address this, we propose AIGB-Pearl (\emphPlanning with EvAluator via RL), a novel method that integrates generative planning and policy optimization. The key to AIGB-Pearl is to construct a non-bootstrapped \emphtrajectory evaluator to assign rewards and guide policy search, enabling the planner to optimize its generation quality iteratively through interaction. Furthermore, to enhance trajectory evaluator accuracy in offline settings, we incorporate three key techniques: (i) a Large Language Model (LLM)-based architecture for better representational capacity, (ii) hybrid point-wise and pair-wise losses for better score learning, and (iii) adaptive integration of expert feedback for better generalization ability. Extensive experiments on both simulated and real-world advertising systems demonstrate the state-of-the-art performance of our approach.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-14] Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决强化学习（Reinforcement Learning, RL）在现实世界应用中因交互成本高而导致的样本效率低下的问题。其核心解决方案在于利用基础模型（Foundation Models, FMs）的知识与推理能力来提升RL代理的样本效率。关键策略包括：一是构建基础世界模型（Foundation World Models, FWMs），利用FM的先验知识进行模拟交互以训练和评估代理；二是设计基础代理（Foundation Agents, FAs），直接利用FM的推理能力进行决策。实验表明，当前大语言模型（Large Language Models, LLMs）性能的提升可直接转化为更优的FWMs与FAs，且FA在简单环境中已能提供高质量策略，而FWM与RL代理的结合在具有部分可观测性和随机性的复杂场景中展现出巨大潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15915">https://arxiv.org/abs/2509.15915</a><br>
<strong>作者</strong>: Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  20 pages, 9 figures. Accepted for presentation at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Embodied World Models for Decision Making</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While reinforcement learning from scratch has shown impressive results in solving sequential decision-making tasks with efficient simulators, real-world applications with expensive interactions require more sample-efficient agents. Foundation models (FMs) are natural candidates to improve sample efficiency as they possess broad knowledge and reasoning capabilities, but it is yet unclear how to effectively integrate them into the reinforcement learning framework. In this paper, we anticipate and, most importantly, evaluate two promising strategies. First, we consider the use of foundation world models (FWMs) that exploit the prior knowledge of FMs to enable training and evaluating agents with simulated interactions. Second, we consider the use of foundation agents (FAs) that exploit the reasoning capabilities of FMs for decision-making. We evaluate both approaches empirically in a family of grid-world environments that are suitable for the current generation of large language models (LLMs). Our results suggest that improvements in LLMs already translate into better FWMs and FAs; that FAs based on current LLMs can already provide excellent policies for sufficiently simple environments; and that the coupling of FWMs and reinforcement learning agents is highly promising for more complex settings with partial observability and stochastic elements.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-15] EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决动态图神经网络（Dynamic GNN）在脑电图（EEG）数据中用于自动癫痫发作检测时的两个核心问题：一是现有方法多基于时间固定的静态图结构，无法反映癫痫发作过程中大脑连接性的动态演变；二是缺乏对时间信号与图结构及其交互关系的有效联合建模，导致性能不稳定。解决方案的关键在于提出EvoBrain模型，其创新性地引入显式动态建模机制，采用“先时间后图”的架构设计，并结合双流Mamba架构与增强拉普拉斯位置编码的图卷积网络（GCN），使节点和边均可随时间演化，从而更准确地捕捉脑状态变化。理论分析证明了该方法在表达能力上的优势，实验表明其在AUROC和F1分数上分别较基线提升23%和30%，显著优于现有动态GNN方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15857">https://arxiv.org/abs/2509.15857</a><br>
<strong>作者</strong>: Rikuto Kotoge,Zheng Chen,Tasuku Kimura,Yasuko Matsubara,Takufumi Yanagisawa,Haruhiko Kishima,Yasushi Sakurai<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS 2025 (spotlight)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23% and F1 score by 30%, compared with the dynamic GNN baseline, and © broad evaluations of our method on the challenging early seizure prediction tasks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-16] A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring</p>
<p>【速读】：该论文旨在解决工业监控系统在工业4.0环境下从传统规则驱动架构向数据驱动方法演进过程中所面临的范式选择问题，即如何权衡规则-based系统（rule-based systems）的可解释性与确定性优势，以及数据驱动系统（data-driven systems）在复杂场景下对隐含异常检测和动态适应能力的提升。其解决方案的关键在于提出一种融合专家知识与数据驱动洞察的混合型框架，通过结合规则-based逻辑的透明性与机器学习模型的分析能力，构建具备更高鲁棒性、运营效率与可信度的智能协同系统，从而推动工业环境向更灵活、更智能的方向发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15848">https://arxiv.org/abs/2509.15848</a><br>
<strong>作者</strong>: Giovanni De Gasperis,Sante Dino Facchini<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Industrial monitoring systems, especially when deployed in Industry 4.0 environments, are experiencing a shift in paradigm from traditional rule-based architectures to data-driven approaches leveraging machine learning and artificial intelligence. This study presents a comparison between these two methodologies, analyzing their respective strengths, limitations, and application scenarios, and proposes a basic framework to evaluate their key properties. Rule-based systems offer high interpretability, deterministic behavior, and ease of implementation in stable environments, making them ideal for regulated industries and safety-critical applications. However, they face challenges with scalability, adaptability, and performance in complex or evolving contexts. Conversely, data-driven systems excel in detecting hidden anomalies, enabling predictive maintenance and dynamic adaptation to new conditions. Despite their high accuracy, these models face challenges related to data availability, explainability, and integration complexity. The paper suggests hybrid solutions as a possible promising direction, combining the transparency of rule-based logic with the analytical power of machine learning. Our hypothesis is that the future of industrial monitoring lies in intelligent, synergic systems that leverage both expert knowledge and data-driven insights. This dual approach enhances resilience, operational efficiency, and trust, paving the way for smarter and more flexible industrial environments.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-17] Diversity of Structured Domains via k-Kemeny Scores</p>
<p>【速读】：该论文致力于解决k-Kemeny问题，即在给定一个有序选举（ordinal election）——亦即一组对候选人从优到劣排序的选票——的情况下，寻找最小数量的相邻候选人交换操作，使得最终选举中最多存在k种不同的排名。其核心目标是评估在单峰（single-peaked）、单交叉（single-crossing）、群可分（group-separable）及欧几里得（Euclidean）等结构化域下，该问题的计算复杂性及其对这些域多样性程度的刻画。解决方案的关键在于：一方面证明了在绝大多数此类结构化域中，即使k=2，k-Kemeny问题仍然是NP难的；另一方面，利用该问题的可解性差异，将不同结构化域按其排列多样性进行排序，从而揭示出它们在排序复杂性上的本质区别。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15812">https://arxiv.org/abs/2509.15812</a><br>
<strong>作者</strong>: Piotr Faliszewski,Krzysztof Sornat,Stanisław Szufa,Tomasz Wąs<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Science and Game Theory (<a target="_blank" rel="noopener" href="http://cs.GT">cs.GT</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In the k-Kemeny problem, we are given an ordinal election, i.e., a collection of votes ranking the candidates from best to worst, and we seek the smallest number of swaps of adjacent candidates that ensure that the election has at most k different rankings. We study this problem for a number of structured domains, including the single-peaked, single-crossing, group-separable, and Euclidean ones. We obtain two kinds of results: (1) We show that k-Kemeny remains intractable under most of these domains, even for k=2, and (2) we use k-Kemeny to rank these domains in terms of their diversity.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-18] Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering</p>
<p>【速读】：该论文旨在解决当前元黑箱优化（Meta-Black-Box Optimization, MetaBBO）中因训练问题实例多样性不足而导致的过拟合风险，进而影响模型在未见问题实例上的泛化能力。现有研究普遍采用经典的CoCo-BBOB基准套件作为训练集，但其问题实例在特征空间中覆盖有限，限制了MetaBBO策略的通用性。为应对这一挑战，论文提出一种名为LSRE（Latent Space Regularized Evolution）的实例生成方法，其核心在于：首先通过自编码器（autoencoder）将高维问题特征映射至二维潜在空间，随后在该空间中进行均匀网格采样以获得具有充分多样性的隐表示；再利用遗传编程（genetic programming）搜索与这些隐表示在L2距离上最小的函数公式，从而逆向构造出一个多样化的问题集Diverse-BBO。实验表明，基于Diverse-BBO训练的MetaBBO算法在合成及真实场景下均展现出更强的泛化性能，验证了LSRE设计的有效性及其对提升MetaBBO鲁棒性的关键作用。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15810">https://arxiv.org/abs/2509.15810</a><br>
<strong>作者</strong>: Chen Wang,Zeyuan Ma,Zhiguang Cao,Yue-Jiao Gong<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Neural and Evolutionary Computing (<a target="_blank" rel="noopener" href="http://cs.NE">cs.NE</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:To relieve intensive human-expertise required to design optimization algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage generalization strength of meta-learning to train neural network-based algorithm design policies over a predefined training problem set, which automates the adaptability of the low-level optimizers on unseen problem instances. Currently, a common training problem set choice in existing MetaBBOs is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the MetaBBO’s development, problem instances in CoCo-BBOB are more or less limited in diversity, raising the risk of overfitting of MetaBBOs, which might further results in poor generalization. In this paper, we propose an instance generation approach, termed as \textbfLSRE, which could generate diverse training problem instances for MetaBBOs to learn more generalizable policies. LSRE first trains an autoencoder which maps high-dimensional problem features into a 2-dimensional latent space. Uniform-grid sampling in this latent space leads to hidden representations of problem instances with sufficient diversity. By leveraging a genetic-programming approach to search function formulas with minimal L2-distance to these hidden representations, LSRE reverse engineers a diversified problem set, termed as \textbfDiverse-BBO. We validate the effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe their generalization performances on either synthetic or realistic scenarios. Extensive experimental results underscore the superiority of Diverse-BBO to existing training set choices in MetaBBOs. Further ablation studies not only demonstrate the effectiveness of design choices in LSRE, but also reveal interesting insights on instance diversity and MetaBBO’s generalization.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-19] Hierarchical Reinforcement Learning with Low-Level MPC for Multi-<mark class="hl-label green">Agent</mark>  Control</p>
<p>【速读】：该论文旨在解决在动态、约束丰富的环境中实现安全且协同的行为控制问题，这是学习型控制方法面临的核心挑战。现有纯端到端学习方法存在样本效率低和可靠性差的问题，而基于模型的方法则依赖预定义参考轨迹且泛化能力有限。解决方案的关键在于提出一种分层框架：高层通过强化学习（Reinforcement Learning, RL）进行战术决策，选择结构化兴趣区域（Regions of Interest, ROIs）内的抽象目标；底层则采用模型预测控制（Model Predictive Control, MPC）确保运动的动态可行性与安全性。该架构在捕食者-猎物基准测试中显著优于端到端及屏蔽式RL基线方法，在奖励、安全性和一致性方面表现更优，验证了结构化学习与模型驱动控制相结合的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15799">https://arxiv.org/abs/2509.15799</a><br>
<strong>作者</strong>: Max Studt,Georg Schildbach<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ystems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Optimization and Control (math.OC)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Achieving safe and coordinated behavior in dynamic, constraint-rich environments remains a major challenge for learning-based control. Pure end-to-end learning often suffers from poor sample efficiency and limited reliability, while model-based methods depend on predefined references and struggle to generalize. We propose a hierarchical framework that combines tactical decision-making via reinforcement learning (RL) with low-level execution through Model Predictive Control (MPC). For the case of multi-agent systems this means that high-level policies select abstract targets from structured regions of interest (ROIs), while MPC ensures dynamically feasible and safe motion. Tested on a predator-prey benchmark, our approach outperforms end-to-end and shielding-based RL baselines in terms of reward, safety, and consistency, underscoring the benefits of combining structured learning with model-based control.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-20] Monte Carlo Tree Diffusion with Multiple Experts for Protein Design</p>
<p>【速读】：该论文旨在解决蛋白质设计中因长程依赖关系和搜索空间过大而导致的生成效率与质量受限的问题，特别是传统结合自回归语言模型与蒙特卡洛树搜索（MCTS）的方法在处理复杂序列时表现不佳。其解决方案的关键在于提出MCTD-ME框架——一种融合掩码扩散模型（masked diffusion models）与树搜索的多专家协同策略，通过生物物理保真度增强的扩散去噪机制作为滚动模拟引擎，实现多位置联合优化并有效扩展至大规模序列空间；同时引入基于pLDDT的掩码调度策略聚焦低置信度区域，并采用新型多专家选择规则PH-UCT-ME，将预测熵UCB方法拓展至专家集成场景，从而显著提升序列恢复率（AAR）和结构相似性（scTM），尤其在长链蛋白上优势更明显。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15796">https://arxiv.org/abs/2509.15796</a><br>
<strong>作者</strong>: Xuefeng Liu,Mingxuan Cao,Songhao Jiang,Xiao Luo,Xiaotian Duan,Mengdi Wang,Tobin R. Sosnick,Jinbo Xu,Rick Stevens<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Biomolecules (<a target="_blank" rel="noopener" href="http://q-bio.BM">q-bio.BM</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The goal of protein design is to generate amino acid sequences that fold into functional structures with desired properties. Prior methods combining autoregressive language models with Monte Carlo Tree Search (MCTS) struggle with long-range dependencies and suffer from an impractically large search space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts, which integrates masked diffusion models with tree search to enable multi-token planning and efficient exploration. Unlike autoregressive planners, MCTD-ME uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine, jointly revising multiple positions and scaling to large sequence spaces. It further leverages experts of varying capacities to enrich exploration, guided by a pLDDT-based masking schedule that targets low-confidence regions while preserving reliable residues. We propose a novel multi-expert selection rule (PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and unguided baselines in both sequence recovery (AAR) and structural similarity (scTM), with gains increasing for longer proteins and benefiting from multi-expert guidance. More generally, the framework is model-agnostic and applicable beyond inverse folding, including de novo protein engineering and multi-objective molecular generation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-21] Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-<mark class="hl-label green">Agent</mark>  Collaboration</p>
<p>【速读】：该论文旨在解决职业分类体系（occupation taxonomy）构建中的两大难题：一是传统人工标注方法效率低下，二是现有自动化方法要么无法适应动态的区域性劳动力市场（自上而下方法），要么难以从噪声数据中构建一致的层级结构（自下而上方法）。其解决方案的关键在于提出CLIMB（CLusterIng-based Multi-agent taxonomy Builder）框架，该框架首先通过全局语义聚类提取核心职业类别，再利用基于反思机制的多智能体系统迭代构建结构清晰、逻辑一致的职业层级体系，从而实现高质量、数据驱动且可扩展的职业分类自动化生成。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15786">https://arxiv.org/abs/2509.15786</a><br>
<strong>作者</strong>: Nan Li,Bo Kang,Tijl De Bie<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Creating robust occupation taxonomies, vital for applications ranging from job recommendation to labor market intelligence, is challenging. Manual curation is slow, while existing automated methods are either not adaptive to dynamic regional markets (top-down) or struggle to build coherent hierarchies from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent taxonomy Builder), a framework that fully automates the creation of high-quality, data-driven taxonomies from raw job postings. CLIMB uses global semantic clustering to distill core occupations, then employs a reflection-based multi-agent system to iteratively build a coherent hierarchy. On three diverse, real-world datasets, we show that CLIMB produces taxonomies that are more coherent and scalable than existing methods and successfully capture unique regional characteristics. We release our code and datasets at this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-22] Ontology Creation and Management Tools: the Case of Anatomical Connectivity</p>
<p>【速读】：该论文旨在解决生理系统（特别是周围神经系统）数据在多尺度结构与功能关系建模中的表示与整合难题，其核心挑战在于如何有效捕捉解剖实体间的拓扑与语义交互，并支持跨知识源的集成。解决方案的关键在于提出ApiNATOMY框架，该框架融合了知识表示（Knowledge Representation, KR）模型与知识管理（Knowledge Management, KM）工具：KR模型使生理学专家能够直观建模解剖实体间的复杂交互关系，而KM工具则支持将高层抽象转化为可执行的生理过程模型，并能无缝对接外部本体和知识图谱，从而实现多尺度生理电路的标准化表达与系统整合。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15780">https://arxiv.org/abs/2509.15780</a><br>
<strong>作者</strong>: Natallia Kokash,Bernard de Bono,Tom Gillespie<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Digital Libraries (cs.DL)<br>
<strong>备注</strong>:  14 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We are developing infrastructure to support researchers in mapping data related to the peripheral nervous system and other physiological systems, with an emphasis on their relevance to the organs under investigation. The nervous system, a complex network of nerves and ganglia, plays a critical role in coordinating and transmitting signals throughout the body. To aid in this, we have created ApiNATOMY, a framework for the topological and semantic representation of multiscale physiological circuit maps. ApiNATOMY integrates a Knowledge Representation (KR) model and a suite of Knowledge Management (KM) tools. The KR model enables physiology experts to easily capture interactions between anatomical entities, while the KM tools help modelers convert high-level abstractions into detailed models of physiological processes, which can be integrated with external ontologies and knowledge graphs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-23] On Optimal Steering to Achieve Exact Fairness <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决公平机器学习中的“输入偏见，输出偏见”（bias in, bias out）问题，即模型在训练数据中继承或放大群体间的不公平性。其核心解决方案是通过将特征分布或大型语言模型（LLM）内部表示的分布引导至理想的理想分布（ideal distribution），从而确保在任何代价敏感风险最小化任务下都能实现精确的群体公平性结果（如人口均等性、机会均等性），且不产生公平性与效用之间的权衡。关键在于定义了理想分布的概念，并提出基于KL散度最优逼近的理想分布优化程序，进而设计出针对常见参数族（如正态分布、对数正态分布）的高效算法，实验证明该方法在合成和真实数据集上均能提升公平性而不损害甚至增强模型性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15759">https://arxiv.org/abs/2509.15759</a><br>
<strong>作者</strong>: Mohit Sharma,Amit Jayant Deshpande,Chiranjib Bhattacharyya,Rajiv Ratn Shah<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted for Presentation at Neurips 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:To fix the ‘bias in, bias out’ problem in fair machine learning, it is important to steer feature distributions of data or internal representations of Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes. Previous work on fair generative models and representation steering could greatly benefit from provable fairness guarantees on the model output. We define a distribution as ideal if the minimizer of any cost-sensitive risk on it is guaranteed to have exact group-fair outcomes (e.g., demographic parity, equal opportunity)-in other words, it has no fairness-utility trade-off. We formulate an optimization program for optimal steering by finding the nearest ideal distribution in KL-divergence, and provide efficient algorithms for it when the underlying distributions come from well-known parametric families (e.g., normal, log-normal). Empirically, our optimal steering techniques on both synthetic and real-world datasets improve fairness without diminishing utility (and sometimes even improve utility). We demonstrate affine steering of LLM representations to reduce bias in multi-class classification, e.g., occupation prediction from a short biography in Bios dataset (De-Arteaga et al.). Furthermore, we steer internal representations of LLMs towards desired outputs so that it works equally well across different groups.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-24] GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation</p>
<p>【速读】：该论文旨在解决机器人操作中对三维场景几何信息理解不足的问题，从而提升操作的准确性与鲁棒性。其核心解决方案是提出GP3——一种基于多视角RGB输入的几何感知型机器人操作策略。关键在于利用空间编码器从RGB图像中提取密集的空间特征，进而估计深度和相机参数，构建适用于操作任务的紧凑且表达能力强的3D场景表征；该表征再与语言指令融合，并通过轻量级策略头转化为连续动作，实现端到端的控制。此方法不依赖深度传感器或预先建图环境，具备良好的现实世界迁移能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15733">https://arxiv.org/abs/2509.15733</a><br>
<strong>作者</strong>: Quanhao Qian,Guoyang Zhao,Gongjie Zhang,Jiuniu Wang,Ran Xu,Junlong Gao,Deli Zhao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Effective robotic manipulation relies on a precise understanding of 3D scene geometry, and one of the most straightforward ways to acquire such geometry is through multi-view observations. Motivated by this, we present GP3 – a 3D geometry-aware robotic manipulation policy that leverages multi-view input. GP3 employs a spatial encoder to infer dense spatial features from RGB observations, which enable the estimation of depth and camera parameters, leading to a compact yet expressive 3D scene representation tailored for manipulation. This representation is fused with language instructions and translated into continuous actions via a lightweight policy head. Comprehensive experiments demonstrate that GP3 consistently outperforms state-of-the-art methods on simulated benchmarks. Furthermore, GP3 transfers effectively to real-world robots without depth sensors or pre-mapped environments, requiring only minimal fine-tuning. These results highlight GP3 as a practical, sensor-agnostic solution for geometry-aware robotic manipulation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-25] A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation</p>
<p>【速读】：该论文旨在解决传统机器人流程自动化（Robotic Process Automation, RPA）在处理复杂任务时因符号主义特性而受限的问题，其核心局限在于难以应对非结构化、动态或需要推理决策的场景。解决方案的关键在于将机器学习（Machine Learning, ML）与RPA深度融合，提出“智能RPA”（Intelligent RPA）的概念，并构建了一个包含两个元特征——RPA-ML集成（RPA-ML Integration）与RPA-ML交互（RPA-ML Interaction）的分类体系，涵盖八个维度（如架构与生态系统、数据基础、智能水平、技术集成深度等），从而系统性地拓展了RPA的应用边界，使其能够适应更复杂的业务自动化需求。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15730">https://arxiv.org/abs/2509.15730</a><br>
<strong>作者</strong>: Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Robotic process automation (RPA) is a lightweight approach to automating business processes using software robots that emulate user actions at the graphical user interface level. While RPA has gained popularity for its cost-effective and timely automation of rule-based, well-structured tasks, its symbolic nature has inherent limitations when approaching more complex tasks currently performed by human agents. Machine learning concepts enabling intelligent RPA provide an opportunity to broaden the range of automatable tasks. In this paper, we conduct a literature review to explore the connections between RPA and machine learning and organize the joint concept intelligent RPA into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML integration and RPA-ML interaction. Together, they comprise eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, and technical depth of integration as well as deployment environment, lifecycle phase, and user-robot relation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-26] CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C Compilation Repair</p>
<p>【速读】：该论文旨在解决C++编译错误自动修复（Compilation Error Repair）这一挑战，其核心问题在于现有方法受限于高质量大规模数据集的稀缺以及传统监督学习模型难以生成语义正确的修复补丁。解决方案的关键在于提出一个三重贡献的综合框架：首先构建了名为CCrepair的大规模、高保真C++编译错误数据集，基于生成与验证相结合的管道；其次引入一种由混合奖励信号引导的强化学习（Reinforcement Learning, RL）范式，将优化目标从单纯确保编译通过转向提升修复补丁的语义质量；最后建立了一个两阶段评估系统，以大语言模型作为裁判（LLM-as-a-Judge）提供可靠且可量化的奖励信号，该信号经验证与人类专家群体判断高度一致。此方法实现了训练目标与生成高质量、非平凡修复补丁（既语法正确又语义合理）的一致性，实验表明使用RL训练的小型模型（Qwen2.5-1.5B-Instruct）性能接近大型模型（Qwen2.5-14B-Instruct），显著提升了自动化编程辅助工具的实用性与可靠性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15690">https://arxiv.org/abs/2509.15690</a><br>
<strong>作者</strong>: Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The automated repair of C++ compilation errors presents a significant challenge, the resolution of which is critical for developer productivity. Progress in this domain is constrained by two primary factors: the scarcity of large-scale, high-fidelity datasets and the limitations of conventional supervised methods, which often fail to generate semantically correct this http URL paper addresses these gaps by introducing a comprehensive framework with three core contributions. First, we present CCrepair, a novel, large-scale C++ compilation error dataset constructed through a sophisticated generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL) paradigm guided by a hybrid reward signal, shifting the focus from mere compilability to the semantic quality of the fix. Finally, we establish the robust, two-stage evaluation system providing this signal, centered on an LLM-as-a-Judge whose reliability has been rigorously validated against the collective judgments of a panel of human experts. This integrated approach aligns the training objective with generating high-quality, non-trivial patches that are both syntactically and semantically correct. The effectiveness of our approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct model achieved performance comparable to a Qwen2.5-14B-Instruct model, validating the efficiency of our training paradigm. Our work provides the research community with a valuable new dataset and a more effective paradigm for training and evaluating robust compilation repair models, paving the way for more practical and reliable automated programming assistants.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-27] Inference Offloading for Cost-Sensitive Binary Classification at the Edge</p>
<p>【速读】：该论文旨在解决边缘智能系统中二分类问题的优化难题，其中误报（false negatives）的成本显著高于漏报（false positives）。系统采用分层推理（Hierarchical Inference, HI）架构，即本地部署一个轻量模型进行初步预测，若其置信度低于或高于预设阈值，则决定是否将样本卸载至远程更复杂的模型以提升精度，但需承担网络传输成本。核心挑战在于如何在分类准确率与卸载开销之间实现最优权衡。解决方案的关键是提出一种无需训练、仅依赖有限反馈的在线学习框架H2T2（Online Two-Threshold Hierarchical Inference Policy），通过动态调整两个阈值来决策本地预测和是否卸载，从而实现次线性后悔（sublinear regret），且对模型校准状态不敏感，具备良好的鲁棒性和适应性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15674">https://arxiv.org/abs/2509.15674</a><br>
<strong>作者</strong>: Vishnu Narayanan Moothedath,Umang Agarwal,Umeshraja N,James Richard Gross,Jaya Prakash Champati,Sharayu Moharir<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Distributed, Parallel, and Cluster Computing (cs.DC)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We focus on a binary classification problem in an edge intelligence system where false negatives are more costly than false positives. The system has a compact, locally deployed model, which is supplemented by a larger, remote model, which is accessible via the network by incurring an offloading cost. For each sample, our system first uses the locally deployed model for inference. Based on the output of the local model, the sample may be offloaded to the remote model. This work aims to understand the fundamental trade-off between classification accuracy and these offloading costs within such a hierarchical inference (HI) system. To optimize this system, we propose an online learning framework that continuously adapts a pair of thresholds on the local model’s confidence scores. These thresholds determine the prediction of the local model and whether a sample is classified locally or offloaded to the remote model. We present a closed-form solution for the setting where the local model is calibrated. For the more general case of uncalibrated models, we introduce H2T2, an online two-threshold hierarchical inference policy, and prove it achieves sublinear regret. H2T2 is model-agnostic, requires no training, and learns in the inference phase using limited feedback. Simulations on real-world datasets show that H2T2 consistently outperforms naive and single-threshold HI policies, sometimes even surpassing offline optima. The policy also demonstrates robustness to distribution shifts and adapts effectively to mismatched classifiers.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-28] ISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决源分离（Source Separation）任务中因模型规模扩大导致的训练与部署成本上升问题，同时兼顾性能提升与推理效率的灵活性。其解决方案的关键在于提出了一种统一框架——训练时与推理时可扩展的判别式源分离方法（Training-Time and Inference-Time Scalable Discriminative Source Separation, TISDiSS），通过早期分裂多损失监督（early-split multi-loss supervision）、共享参数设计（shared-parameter design）以及动态推理重复机制（dynamic inference repetitions）实现：在不重新训练模型的前提下，仅通过调整推理深度即可灵活权衡速度与性能，尤其适用于低延迟应用场景。实验表明，该方法在标准语音分离基准上实现了优于现有技术的性能，且参数量更少，具备良好的可扩展性与实用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15666">https://arxiv.org/abs/2509.15666</a><br>
<strong>作者</strong>: Yongsheng Feng,Yuetonghui Xu,Jiehui Luo,Hongjia Liu,Xiaobing Li,Feng Yu,Wei Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-29] Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach</p>
<p>【速读】：该论文旨在解决传统查询扩展（Query Expansion）方法因上下文敏感性导致的性能下降问题，以及现有文档扩展（Document Expansion）方法如Doc2Query存在的预处理成本高、索引规模增大和生成内容可靠性差等局限。其解决方案的关键在于提出一种“块知识生成模型”（Chunk Knowledge Generation Model），该模型将文档切分为块（chunk）单元，并基于T5架构设计多任务学习结构，在单次编码和两次解码过程中并行生成标题、候选问题和从用户查询中提取关键词，从而在提升检索效率的同时增强准确性。该方法通过在检索系统中引入生成的多类型语义信息，实现了Top@10检索准确率达到95.41%的显著效果，优于传统的文档块级检索策略。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15658">https://arxiv.org/abs/2509.15658</a><br>
<strong>作者</strong>: Jisu Kim,Jinhee Park,Changhyun Jeon,Jungwoo Choi,Keonwoo Kim,Minji Hong,Sehyun Kim<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Traditional query expansion techniques for addressing vocabulary mismatch problems in information retrieval are context-sensitive and may lead to performance degradation. As an alternative, document expansion research has gained attention, but existing methods such as Doc2Query have limitations including excessive preprocessing costs, increased index size, and reliability issues with generated content. To mitigate these problems and seek more structured and efficient alternatives, this study proposes a method that divides documents into chunk units and generates textual data for each chunk to simultaneously improve retrieval efficiency and accuracy. The proposed “Chunk Knowledge Generation Model” adopts a T5-based multi-task learning structure that simultaneously generates titles and candidate questions from each document chunk while extracting keywords from user queries. This approach maximizes computational efficiency by generating and extracting three types of semantic information in parallel through a single encoding and two decoding processes. The generated data is utilized as additional information in the retrieval system. GPT-based evaluation on 305 query-document pairs showed that retrieval using the proposed model achieved 95.41% accuracy at Top@10, demonstrating superior performance compared to document chunk-level retrieval. This study contributes by proposing an approach that simultaneously generates titles and candidate questions from document chunks for application in retrieval pipelines, and provides empirical evidence applicable to large-scale information retrieval systems by demonstrating improved retrieval accuracy through qualitative evaluation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-30] oward Efficient Influence Function: Dropout as a Compression Tool</p>
<p>【速读】：该论文旨在解决大规模机器学习模型中影响函数（influence function）计算所面临的高计算和内存开销问题，尤其是在使用近似方法时，由于涉及的梯度维度与模型规模相当，导致效率低下。其解决方案的关键在于引入 dropout 作为梯度压缩机制，通过在梯度计算过程中利用 dropout 的随机性实现对梯度的有效压缩，从而显著降低计算和内存消耗，同时保持数据影响的关键结构，使得影响函数能够高效应用于现代大规模模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15651">https://arxiv.org/abs/2509.15651</a><br>
<strong>作者</strong>: Yuchen Zhang,Mohammad Mohammadi Amiri<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Assessing the impact the training data on machine learning models is crucial for understanding the behavior of the model, enhancing the transparency, and selecting training data. Influence function provides a theoretical framework for quantifying the effect of training data points on model’s performance given a specific test data. However, the computational and memory costs of influence function presents significant challenges, especially for large-scale models, even when using approximation methods, since the gradients involved in computation are as large as the model itself. In this work, we introduce a novel approach that leverages dropout as a gradient compression mechanism to compute the influence function more efficiently. Our method significantly reduces computational and memory overhead, not only during the influence function computation but also in gradient compression process. Through theoretical analysis and empirical validation, we demonstrate that our method could preserves critical components of the data influence and enables its application to modern large-scale models.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-31] Information Geometry of Variational Bayes</p>
<p>【速读】：该论文旨在揭示信息几何（information geometry）与变分贝叶斯（variational Bayes, VB）之间的基础联系，并探讨这一联系对机器学习的启示。其核心问题在于：如何从几何角度理解VB算法的优化本质，以及如何利用这种理解提升VB在大规模模型中的效率与可解释性。解决方案的关键在于指出，在特定条件下，VB求解本质上等价于自然梯度（natural gradient）的估计或计算，并通过Khan和Rue（2023）提出的贝叶斯学习规则（Bayesian Learning Rule, BLR）展示了三个重要后果：(i) 贝叶斯更新可被形式化为自然梯度的加法；(ii) 对传统基于梯度方法中使用的二次近似（quadratic surrogate）进行推广；(iii) 实现适用于大语言模型的大规模VB算法部署。这一视角统一了信息几何与贝叶斯推断的理论基础，为二者交叉研究提供了新路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15641">https://arxiv.org/abs/2509.15641</a><br>
<strong>作者</strong>: Mohammad Emtiyaz Khan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We highlight a fundamental connection between information geometry and variational Bayes (VB) and discuss its consequences for machine learning. Under certain conditions, a VB solution always requires estimation or computation of natural gradients. We show several consequences of this fact by using the natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian Learning Rule (BLR). These include (i) a simplification of Bayes’ rule as addition of natural gradients, (ii) a generalization of quadratic surrogates used in gradient-based methods, and (iii) a large-scale implementation of VB algorithms for large language models. Neither the connection nor its consequences are new but we further emphasize the common origins of the two fields of information geometry and Bayes with a hope to facilitate more work at the intersection of the two fields.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-32] MicroRCA-<mark class="hl-label green">Agent</mark> : Microservice Root Cause Analysis Method Based on Large Language Model <mark class="hl-label green">Agents</mark></p>
<p>【速读】：该论文旨在解决微服务架构中故障根因定位（Root Cause Analysis, RCA）的复杂性问题，尤其是在多模态数据融合与精准异常识别方面的挑战。解决方案的关键在于构建一个基于大语言模型代理（Large Language Model Agent）的智能故障根因定位系统——MicroRCA-Agent，其核心创新包括：1）结合预训练的Drain日志解析算法与多级数据过滤机制，高效压缩海量日志以提取高质量故障特征；2）采用隔离森林（Isolation Forest）无监督学习与状态码验证相结合的双重异常检测方法，实现全链路追踪异常识别；3）设计统计对称比过滤机制与两阶段大语言模型分析策略，支持跨节点-服务-Pod层级的现象归纳。最终通过精心设计的跨模态提示（cross-modal prompts），充分挖掘多模态异常信息，利用大语言模型的跨模态理解与逻辑推理能力生成结构化的分析结果，涵盖故障组件、根因描述及推理路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15635">https://arxiv.org/abs/2509.15635</a><br>
<strong>作者</strong>: Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  18 pages, 22 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-33] CFDA  CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion</p>
<p>【速读】：该论文旨在解决信息检索与问答系统在交互式（interactive）和离线（offline）两种任务场景下的性能优化问题，尤其关注系统在实时约束下的鲁棒性（robustness）与效率（efficiency）平衡。针对这一挑战，研究提出以查询重写（query rewriting）和检索融合（retrieval fusion）为核心策略，构建基于“Best-of-N”选择机制与倒数排名融合（Reciprocal Rank Fusion, RRF）的流水线架构，从而在不同任务类型中实现更稳定的排序效果与响应质量。实验表明，通过重排序（reranking）与融合技术可提升系统鲁棒性，同时揭示了有效性（effectiveness）与效率之间的权衡关系。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15588">https://arxiv.org/abs/2509.15588</a><br>
<strong>作者</strong>: Yu-Cheng Chang,Guan-Wei Yeo,Quah Eugene,Fan-Jie Shih,Yuan-Ching Kuo,Tsung-En Yu,Hung-Chun Hsu,Ming-Feng Tsai,Chuan-Ju Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both interactive and offline submission tasks. The former requires systems to operate under real-time constraints, making robustness and efficiency as important as accuracy, while the latter enables controlled evaluation of passage ranking and response generation with pre-defined datasets. To address this, we explored query rewriting and retrieval fusion as core strategies. We built our pipelines around Best-of- N  selection and Reciprocal Rank Fusion (RRF) strategies to handle different submission tasks. Results show that reranking and fusion improve robustness while revealing trade-offs between effectiveness and efficiency across both tasks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-34] Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework with Residual-enhanced DRL for Visually Impaired Scenarios</p>
<p>【速读】：该论文旨在解决视觉障碍场景下辅助导航任务中轨迹规划的鲁棒性、安全性与实时可行性问题。其核心挑战在于如何在复杂动态环境中生成平滑且可行的轨迹，同时兼顾人机协同优化目标。解决方案的关键在于提出一种动量约束的混合启发式轨迹优化框架（Momentum-constrained Hybrid Heuristic Trajectory Optimization Framework, MHHTOF），该框架包含两个阶段：第一阶段在Frenet坐标系中通过五阶多项式插值与动量约束优化（MTO）生成初始轨迹簇（HTSC），确保轨迹的平滑性和物理可行性；第二阶段基于LSTM的时间特征建模构建残差增强型Actor-Critic网络，实现对轨迹选择的自适应精细化调整，并引入双阶段成本建模机制（DCMM）以权重迁移方式对齐语义优先级，从而提升策略收敛速度与稳定性。实验表明，所提出的LSTM-ResB-PPO方法相较基线PPO显著缩短训练迭代次数（约减少50%），并降低平均代价和代价方差30.3%与53.3%，同时将自身及障碍物风险降低超过77%，验证了该方案在复杂辅助规划任务中的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15582">https://arxiv.org/abs/2509.15582</a><br>
<strong>作者</strong>: Yuting Zeng,Zhiwen Zheng,You Zhou,JiaLing Xiao,Yongbin Yu,Manping Fan,Bo Gong,Liyong Ren<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  20 pages, 16 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes a momentum-constrained hybrid heuristic trajectory optimization framework (MHHTOF) tailored for assistive navigation in visually impaired scenarios, integrating trajectory sampling generation, optimization and evaluation with residual-enhanced deep reinforcement learning (DRL). In the first stage, heuristic trajectory sampling cluster (HTSC) is generated in the Frenet coordinate system using third-order interpolation with fifth-order polynomials and momentum-constrained trajectory optimization (MTO) constraints to ensure smoothness and feasibility. After first stage cost evaluation, the second stage leverages a residual-enhanced actor-critic network with LSTM-based temporal feature modeling to adaptively refine trajectory selection in the Cartesian coordinate system. A dual-stage cost modeling mechanism (DCMM) with weight transfer aligns semantic priorities across stages, supporting human-centered optimization. Experimental results demonstrate that the proposed LSTM-ResB-PPO achieves significantly faster convergence, attaining stable policy performance in approximately half the training iterations required by the PPO baseline, while simultaneously enhancing both reward outcomes and training stability. Compared to baseline method, the selected model reduces average cost and cost variance by 30.3% and 53.3%, and lowers ego and obstacle risks by over 77%. These findings validate the framework’s effectiveness in enhancing robustness, safety, and real-time feasibility in complex assistive planning tasks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-35] Contrastive Learning with Spectrum Information Augmentation in Abnormal Sound Detection</p>
<p>【速读】：该论文旨在解决无监督异常声音检测（unsupervised anomaly sound detection）问题，其核心挑战在于如何让模型有效学习正常数据的分布空间。解决方案的关键在于提出一种基于对比学习（contrastive learning）的数据增强方法，专门针对高频信息进行增强，从而引导模型更加关注音频中的低频成分——这些低频信息通常代表设备的正常运行状态。通过这一策略，模型在DCASE 2020 Task 2和DCASE 2022 Task 2数据集上的实验结果均表明，该方法显著优于其他对比学习方法，并展现出良好的泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15570">https://arxiv.org/abs/2509.15570</a><br>
<strong>作者</strong>: Xinxin Meng,Jiangtao Guo,Yunxiang Zhang,Shun Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  Accepted CVIPPR 2024 April Xiamen China</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The outlier exposure method is an effective approach to address the unsupervised anomaly sound detection problem. The key focus of this method is how to make the model learn the distribution space of normal data. Based on biological perception and data analysis, it is found that anomalous audio and noise often have higher frequencies. Therefore, we propose a data augmentation method for high-frequency information in contrastive learning. This enables the model to pay more attention to the low-frequency information of the audio, which represents the normal operational mode of the machine. We evaluated the proposed method on the DCASE 2020 Task 2. The results showed that our method outperformed other contrastive learning methods used on this dataset. We also evaluated the generalizability of our method on the DCASE 2022 Task 2 dataset.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-36] Reward Hacking Mitigation using Verifiable Composite Rewards <mark class="hl-label red">ALT</mark></p>
<p>【速读】：该论文旨在解决生成式 AI（Generative AI）在医疗问答场景中应用 Reinforcement Learning from Verifiable Rewards (RLVR) 时出现的奖励劫持（reward hacking）问题，具体表现为：一是在推理阶段直接给出最终答案而无中间推理过程；二是采用非标准推理格式以利用奖励机制漏洞。解决方案的关键在于设计一种复合奖励函数（composite reward function），对上述两种行为施加特定惩罚，从而引导模型生成结构化、可验证的推理路径，同时保持高准确性。实验表明，该方法显著减少了奖励劫持现象，并提升了推理格式的规范性与可靠性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15557">https://arxiv.org/abs/2509.15557</a><br>
<strong>作者</strong>: Mirza Farhan Bin Tarek,Rahmatollah Beheshti<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted at the 16th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that large language models (LLMs) can develop their own reasoning without direct supervision. However, applications in the medical domain, specifically for question answering, are susceptible to significant reward hacking during the reasoning phase. Our work addresses two primary forms of this behavior: i) providing a final answer without preceding reasoning, and ii) employing non-standard reasoning formats to exploit the reward mechanism. To mitigate these, we introduce a composite reward function with specific penalties for these behaviors. Our experiments show that extending RLVR with our proposed reward model leads to better-formatted reasoning with less reward hacking and good accuracy compared to the baselines. This approach marks a step toward reducing reward hacking and enhancing the reliability of models utilizing RLVR.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-37] Stress Testing Deliberative Alignment for Anti-Scheming Training</p>
<p>【速读】：该论文试图解决高能力人工智能系统可能隐秘地追求与人类目标不一致的目标（即“诡计行为”或scheming）的问题，这类行为因其隐蔽性使得传统机器学习中的对齐评估方法失效。解决方案的关键在于提出一套针对反诡计干预的评估框架，包括：(1) 在远超出训练分布（out-of-distribution, OOD）的任务上测试模型产生诡计倾向的能力；(2) 评估模型缺乏诡计行为是否源于情境意识（situational awareness）；(3) 检验其对预先存在的错误对齐目标的鲁棒性。研究以“隐蔽行为”（covert actions）作为诡计的代理指标，并通过实证验证了“反思式对齐”（deliberative alignment）在26个OOD环境（180+场景）中显著降低了隐蔽行为率（从13%降至0.4%），但未完全消除，且在额外红队测试后仍发现不当行为，表明当前方法可能部分依赖于模型的情境意识而非真正的目标修正。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15541">https://arxiv.org/abs/2509.15541</a><br>
<strong>作者</strong>: Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Highly capable AI systems could secretly pursue misaligned goals – what we call “scheming”. Because a scheming AI would deliberately try to hide its misaligned goals and actions, measuring and mitigating scheming requires different strategies than are typically used in ML. We propose that assessing anti-scheming interventions requires at least (1) testing propensity to scheme on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming is driven by situational awareness, and (3) checking for robustness to pre-existing misaligned goals. We use a broad category of “covert actions” – such as secretly breaking rules or intentionally underperforming in tests – as a proxy for scheming, and design evaluations for covert actions. We then stress-test deliberative alignment as a case study for anti-scheming. Across 26 OOD evaluations (180+ environments), deliberative alignment reduces covert action rates (OpenAI o3: 13%-0.4%) but does not fully eliminate them. Our mitigation is also able to largely stop agents from pursuing a hidden goal previously trained into the model, but we still find misbehavior after additional red-teaming. We find that models’ chain-of-thought (CoT) often demonstrates awareness of being evaluated for alignment, and show causal evidence that this awareness decreases covert behavior, while unawareness increases it. Therefore, we cannot exclude that the observed reductions in covert action rates are at least partially driven by situational awareness. While we rely on human-legible CoT for training, studying situational awareness, and demonstrating clear evidence of misalignment, our ability to rely on this degrades as models continue to depart from reasoning in standard English. We encourage research into alignment mitigations for scheming and their assessment, especially for the adversarial case of deceptive alignment, which this paper does not address.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-38] Explainable AI-Enhanced Supervisory Control for Robust Multi-<mark class="hl-label green">Agent</mark>  Robotic Systems</p>
<p>【速读】：该论文旨在解决多智能体机器人系统中安全、可解释且资源高效的控制问题，尤其针对具有不确定六自由度（6-DOF）刚体动力学和紧密跟踪需求的复杂环境。其核心挑战在于如何在保证安全性与鲁棒性的同时，实现参数自适应调整并提供透明的决策依据。解决方案的关键在于提出一个可解释的人工智能增强型监督控制框架，包含三个核心模块：(i) 基于时序自动机（timed-automata）的监督器用于安全、可审计的状态切换；(ii) 鲁棒连续控制器（基于李雅普诺夫的控制器用于大角度机动，滑模控制（SMC）结合边界层以实现高精度和扰动抑制）；(iii) 可解释预测器，将任务上下文映射为控制增益及预期性能指标（如能耗、误差）。通过蒙特卡洛驱动优化生成训练数据，实现了实时透明的权衡决策。该方法在航天器编队飞行和自主水下航行器（AUV）两个截然不同的场景中验证，均表现出优异的性能与泛化能力，证明了其在安全关键型、资源受限的多智能体机器人系统中的实用性与可解释性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15491">https://arxiv.org/abs/2509.15491</a><br>
<strong>作者</strong>: Reza Pirayeshshirazinezhad,Nima Fathi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present an explainable AI-enhanced supervisory control framework for multi-agent robotics that combines (i) a timed-automata supervisor for safe, auditable mode switching, (ii) robust continuous control (Lyapunov-based controller for large-angle maneuver; sliding-mode controller (SMC) with boundary layers for precision and disturbance rejection), and (iii) an explainable predictor that maps mission context to gains and expected performance (energy, error). Monte Carlo-driven optimization provides the training data, enabling transparent real-time trade-offs. We validated the approach in two contrasting domains, spacecraft formation flying and autonomous underwater vehicles (AUVs). Despite different environments (gravity/actuator bias vs. hydrodynamic drag/currents), both share uncertain six degrees of freedom (6-DOF) rigid-body dynamics, relative motion, and tight tracking needs, making them representative of general robotic systems. In the space mission, the supervisory logic selects parameters that meet mission criteria. In AUV leader-follower tests, the same SMC structure maintains a fixed offset under stochastic currents with bounded steady error. In spacecraft validation, the SMC controller achieved submillimeter alignment with 21.7% lower tracking error and 81.4% lower energy consumption compared to Proportional-Derivative PD controller baselines. At the same time, in AUV tests, SMC maintained bounded errors under stochastic currents. These results highlight both the portability and the interpretability of the approach for safety-critical, resource-constrained multi-agent robotics.         Subjects:  Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)  Cite as: arXiv:2509.15491 [<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>]    (or  arXiv:2509.15491v1 [<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15491">https://doi.org/10.48550/arXiv.2509.15491</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-39] Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决当前Transformer模型在处理多模态、多尺度数据时面临的挑战，即如何将注意力机制有效扩展至不同尺度和模态的数据场景，而现有方法多依赖于启发式设计，缺乏通用性和理论基础。其解决方案的关键在于：首先提出一个数学构造来统一表示多模态、多尺度数据；然后从熵最小化这一基本原理出发，推导出适用于该构造的神经注意力机制，确保其在保持与标准Softmax注意力最优逼近的同时，嵌入来自层次结构或几何信息的归纳偏置；进一步基于动态规划设计高效算法实现该注意力机制，并验证其可在训练阶段直接用于构建层次化/多模态Transformer模型，也可在训练后注入预训练模型中，从而以零样本方式提升模型效率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15448">https://arxiv.org/abs/2509.15448</a><br>
<strong>作者</strong>: Saeed Amizadeh,Sara Abdali,Yinheng Li,Kazuhito Koishida<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Neural and Evolutionary Computing (<a target="_blank" rel="noopener" href="http://cs.NE">cs.NE</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:  In The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Transformers and their attention mechanism have been revolutionary in the field of Machine Learning. While originally proposed for the language data, they quickly found their way to the image, video, graph, etc. data modalities with various signal geometries. Despite this versatility, generalizing the attention mechanism to scenarios where data is presented at different scales from potentially different modalities is not straightforward. The attempts to incorporate hierarchy and multi-modality within transformers are largely based on ad hoc heuristics, which are not seamlessly generalizable to similar problems with potentially different structures. To address this problem, in this paper, we take a fundamentally different approach: we first propose a mathematical construct to represent multi-modal, multi-scale data. We then mathematically derive the neural attention mechanics for the proposed construct from the first principle of entropy minimization. We show that the derived formulation is optimal in the sense of being the closest to the standard Softmax attention while incorporating the inductive biases originating from the hierarchical/geometric information of the problem. We further propose an efficient algorithm based on dynamic programming to compute our derived attention mechanism. By incorporating it within transformers, we show that the proposed hierarchical attention mechanism not only can be employed to train transformer models in hierarchical/multi-modal settings from scratch, but it can also be used to inject hierarchical information into classical, pre-trained transformer models post training, resulting in more efficient models in zero-shot manner.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-40] Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning</p>
<p>【速读】：该论文旨在解决当前人形机器人运动重定向（motion retargeting）方法在处理大规模人类动作数据时效率低、缺乏可扩展性的问题，尤其是在帧级逐帧映射中难以实现实时物理可行轨迹生成。其解决方案的关键在于提出了一种名为隐式动力学运动重定向（Implicit Kinodynamic Motion Retargeting, IKMR）的新框架，该框架通过预训练运动拓扑特征表示与双编码器-解码器架构学习运动域映射（kinematics层面），并融合模仿学习与重定向网络以优化轨迹的物理可行性（dynamics层面），从而实现高效、可扩展的大规模物理可行运动重定向，并支持直接训练和部署全身控制器来跟踪生成的轨迹。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15443">https://arxiv.org/abs/2509.15443</a><br>
<strong>作者</strong>: Xingyu Chen,Hanyu Wu,Sikai Wu,Mingliang Zhou,Diyun Xiang,Haodong Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Human-to-humanoid imitation learning aims to learn a humanoid whole-body controller from human motion. Motion retargeting is a crucial step in enabling robots to acquire reference trajectories when exploring locomotion skills. However, current methods focus on motion retargeting frame by frame, which lacks scalability. Could we directly convert large-scale human motion into robot-executable motion through a more efficient approach? To address this issue, we propose Implicit Kinodynamic Motion Retargeting (IKMR), a novel efficient and scalable retargeting framework that considers both kinematics and dynamics. In kinematics, IKMR pretrains motion topology feature representation and a dual encoder-decoder architecture to learn a motion domain mapping. In dynamics, IKMR integrates imitation learning with the motion retargeting network to refine motion into physically feasible trajectories. After fine-tuning using the tracking results, IKMR can achieve large-scale physically feasible motion retargeting in real time, and a whole-body controller could be directly trained and deployed for tracking its retargeted trajectories. We conduct our experiments both in the simulator and the real robot on a full-size humanoid robot. Extensive experiments and evaluation results verify the effectiveness of our proposed framework.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-41] Where Do I Add the Egg?: Exploring Agency and Ownership in AI Creative Co-Writing Systems</p>
<p>【速读】：该论文旨在解决生成式 AI (Generative AI) 在协同写作系统中引发的关于创作过程中代理权（agency）与所有权（ownership）认知冲突的问题，这一冲突阻碍了AI辅助写作技术的广泛应用。其解决方案的关键在于通过设计不同界面隐喻（interface metaphors）——即“代理型”（agentic）、“工具型”（tool-like）和“魔法型”（magical）——来系统性地调节用户对控制感和作者身份的认知。研究发现，工具型隐喻改变了写作者对控制点的预期，而代理型隐喻则突出概念层面的贡献，从而表明界面隐喻不仅塑造用户对控制的预期，也重构了对作者身份的理解。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15440">https://arxiv.org/abs/2509.15440</a><br>
<strong>作者</strong>: Dashiel Carrera,Jeb Thomas-Mitchell,Daniel Wigdor<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  17 pages, 3 figures, 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:AI co-writing systems challenge long held ideals about agency and ownership in the creative process, thereby hindering widespread adoption. In order to address this, we investigate conceptions of agency and ownership in AI creative co-writing. Drawing on insights from a review of commercial systems, we developed three co-writing systems with identical functionality but distinct interface metaphors: agentic, tool-like, and magical. Through interviews with professional and non-professional writers (n = 18), we explored how these metaphors influenced participants’ sense of control and authorship. Our analysis resulted in a taxonomy of agency and ownership subtypes and underscore how tool-like metaphors shift writers’ expected points of control while agentic metaphors foreground conceptual contributions. We argue that interface metaphors not only guide expectations of control but also frame conceptions of authorship. We conclude with recommendations for the design of AI co-writing systems, emphasizing how metaphor shapes user experience and creative practice.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-42] Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses</p>
<p>【速读】：该论文旨在解决传统脑机接口（Brain-Computer Interface, BCI）系统中基于液晶显示器（Liquid Crystal Display, LCD）的视觉刺激范式在实际部署场景中存在的局限性，尤其是其对稳态视觉诱发电位（Steady-State Visual Evoked Potentials, SSVEP）分类准确率提升受限的问题。解决方案的关键在于设计并实现了一种基于发光二极管（Light-Emitting Diode, LED）的双刺激装置，通过融合SSVEP与P300两种神经生理信号响应机制，利用四个特定频率（7 Hz、8 Hz、9 Hz、10 Hz）分别对应不同方向控制指令，并结合最大快速傅里叶变换（Fast Fourier Transform, FFT）幅值分析和P300峰值检测进行实时特征提取，从而显著提升了用户意图识别的准确性。实验表明，该混合系统实现了86.25%的平均分类准确率及42.08 bits per minute（bpm）的信息传输速率（Information Transfer Rate, ITR），验证了其有效性与实用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15439">https://arxiv.org/abs/2509.15439</a><br>
<strong>作者</strong>: Ekgari Kasawala,Surej Mouli<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  15 Pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In brain-computer interface (BCI) systems, steady-state visual evoked potentials (SSVEP) and P300 responses have achieved widespread implementation owing to their superior information transfer rates (ITR) and minimal training requirements. These neurophysiological signals have exhibited robust efficacy and versatility in external device control, demonstrating enhanced precision and scalability. However, conventional implementations predominantly utilise liquid crystal display (LCD)-based visual stimulation paradigms, which present limitations in practical deployment scenarios. This investigation presents the development and evaluation of a novel light-emitting diode (LED)-based dual stimulation apparatus designed to enhance SSVEP classification accuracy through the integration of both SSVEP and P300 paradigms. The system employs four distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward, backward, right, and left directional controls, respectively. Oscilloscopic verification confirmed the precision of these stimulation frequencies. Real-time feature extraction was accomplished through the concurrent analysis of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to ascertain user intent. Directional control was determined by the frequency exhibiting maximal amplitude characteristics. The visual stimulation hardware demonstrated minimal frequency deviation, with error differentials ranging from 0.15%to 0.20%across all frequencies. The implemented signal processing algorithm successfully discriminated all four stimulus frequencies whilst correlating them with their respective P300 event markers. Classification accuracy was evaluated based on correct task intention recognition. The proposed hybrid system achieved a mean classification accuracy of 86.25%, coupled with an average ITR of 42.08 bits per minute (bpm).<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-43] Impact of Phonetics on Speaker Identity in Adversarial Voice Attack <mark class="hl-label red">ICASSP-2025</mark></p>
<p>【速读】：该论文旨在解决对抗性扰动（adversarial perturbations）在语音领域对自动语音识别（ASR）和说话人验证（speaker verification）系统造成的潜在威胁，特别是这些扰动如何通过音素层面的系统性混淆（如元音中心化和辅音替换）同时引发识别错误与说话人身份漂移（identity drift）。解决方案的关键在于从音素级别分析对抗样本的生成机制，并实验证明其不仅导致ASR转录错误，还会破坏用于说话人验证的声学特征，从而揭示了构建音素感知型防御策略（phonetic-aware defenses）的必要性，以提升ASR与说话人识别系统的整体鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15437">https://arxiv.org/abs/2509.15437</a><br>
<strong>作者</strong>: Daniyal Kabir Dar,Qiben Yan,Li Xiao,Arun Ross<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  Additional figures for extended visualization: <a target="_blank" rel="noopener" href="https://daniyalkabir.github.io/icassp-2025-results/">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Adversarial perturbations in speech pose a serious threat to automatic speech recognition (ASR) and speaker verification by introducing subtle waveform modifications that remain imperceptible to humans but can significantly alter system outputs. While targeted attacks on end-to-end ASR models have been widely studied, the phonetic basis of these perturbations and their effect on speaker identity remain underexplored. In this work, we analyze adversarial audio at the phonetic level and show that perturbations exploit systematic confusions such as vowel centralization and consonant substitutions. These distortions not only mislead transcription but also degrade phonetic cues critical for speaker verification, leading to identity drift. Using DeepSpeech as our ASR target, we generate targeted adversarial examples and evaluate their impact on speaker embeddings across genuine and impostor samples. Results across 16 phonetically diverse target phrases demonstrate that adversarial audio induces both transcription errors and identity drift, highlighting the need for phonetic-aware defenses to ensure the robustness of ASR and speaker recognition systems.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-44] F<mark class="hl-label green">rag</mark> mentRetro: A Quadratic Retrosynthetic Method Based on F<mark class="hl-label green">rag</mark> mentation Algorithms</p>
<p>【速读】：该论文旨在解决计算机辅助合成规划（Computer-Aided Synthesis Planning, CASP）中传统树搜索方法因指数级计算复杂度而导致的效率瓶颈问题。其解决方案的关键在于提出了一种名为FragmentRetro的新颖逆合成方法，该方法基于碎片化算法（如BRICS和r-BRICS），结合库存感知探索（stock-aware exploration）与模式指纹筛选（pattern fingerprint screening），将整体计算复杂度从树搜索的指数级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>b</mi><mi>h</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(b^h)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0991em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 降低至二次方级 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>h</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(h^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span> 为目标分子中的重原子数。该方法通过递归组合分子碎片并验证其是否存在于构建块集合中，高效生成碎片组合作为逆合成方案，显著提升了大规模分子的求解能力，并在PaRoutes、USPTO-190及天然产物数据集上展现出高成功率与竞争力的运行时间。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15409">https://arxiv.org/abs/2509.15409</a><br>
<strong>作者</strong>: Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Retrosynthesis, the process of deconstructing a target molecule into simpler precursors, is crucial for computer-aided synthesis planning (CASP). Widely adopted tree-search methods often suffer from exponential computational complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic method that leverages fragmentation algorithms, specifically BRICS and r-BRICS, combined with stock-aware exploration and pattern fingerprint screening to achieve quadratic complexity. FragmentRetro recursively combines molecular fragments and verifies their presence in a building block set, providing sets of fragment combinations as retrosynthetic solutions. We present the first formal computational analysis of retrosynthetic methods, showing that tree search exhibits exponential complexity  O(b^h) , DirectMultiStep scales as  O(h^6) , and FragmentRetro achieves  O(h^2) , where  h  represents the number of heavy atoms in the target molecule and  b  is the branching factor for tree search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate that FragmentRetro achieves high solved rates with competitive runtime, including cases where tree search fails. The method benefits from fingerprint screening, which significantly reduces substructure matching complexity. While FragmentRetro focuses on efficiently identifying fragment-based solutions rather than full reaction pathways, its computational advantages and ability to generate strategic starting candidates establish it as a powerful foundational component for scalable and automated synthesis planning.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-45] Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities</p>
<p>【速读】：该论文旨在解决标准行为克隆（Behavior Cloning, BC）在学习多模态驾驶决策时的局限性，即当同一场景存在多个合理动作时，BC难以有效建模这种多样性。其解决方案的关键在于提出数据增强的隐式行为克隆（Data-Augmented Implicit Behavioral Cloning, DA-IBC），通过扰动专家动作生成隐式行为克隆训练中的反例，并采用更优的初始化策略以提升无导数推理性能。实验表明，DA-IBC在CARLA模拟器中能更好地捕捉多模态动作分布，其学习到的能量景观（energy landscape）可有效表示多种可行驾驶行为，而传统BC无法实现这一目标。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15400">https://arxiv.org/abs/2509.15400</a><br>
<strong>作者</strong>: Eric Aislan Antonelo,Gustavo Claudio Karl Couto,Christian Möller<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:  ENIAC conference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Standard Behavior Cloning (BC) fails to learn multimodal driving decisions, where multiple valid actions exist for the same scenario. We explore Implicit Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning by perturbing expert actions to form the counterexamples of IBC training and using better initialization for derivative-free inference. Experiments in the CARLA simulator with Bird’s-Eye View inputs demonstrate that DA-IBC outperforms standard IBC in urban driving tasks designed to evaluate multimodal behavior learning in a test environment. The learned energy landscapes are able to represent multimodal action distributions, which BC fails to achieve.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-46] Diagnostics of cognitive failures in multi-<mark class="hl-label green">agent</mark>  expert systems using dynamic evaluation protocols and subsequent mutation of the processing context</p>
<p>【速读】：该论文旨在解决当前大语言模型（Large Language Models, LLMs）在具备记忆、规划和外部工具调用能力后，所表现出的涌现式代理行为（agentic behaviours）难以通过传统评估方法进行有效诊断与改进的问题。其核心挑战在于LLMs固有的随机性和多步决策过程使得性能评估缺乏可解释性与可操作性。解决方案的关键在于提出一个诊断框架，该框架整合了三类数据：专家标注的黄金数据集（golden datasets）、通过受控行为变异生成的银数据集（silver datasets），以及基于LLM的代理评判器（Agent Judge），后者不仅能评分还能提供针对性改进建议；这些建议被编码为向量化的推荐映射（vectorized recommendation map），实现专家干预作为可复用的改进轨迹在多个系统实例间传播，从而推动LLM代理从随机行为向专家级推理风格迁移。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15366">https://arxiv.org/abs/2509.15366</a><br>
<strong>作者</strong>: Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Dissertation and research project created in collaboration with JobFair LTD</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid evolution of neural architectures - from multilayer perceptrons to large-scale Transformer-based models - has enabled language models (LLMs) to exhibit emergent agentic behaviours when equipped with memory, planning, and external tool use. However, their inherent stochasticity and multi-step decision processes render classical evaluation methods inadequate for diagnosing agentic performance. This work introduces a diagnostic framework for expert systems that not only evaluates but also facilitates the transfer of expert behaviour into LLM-powered agents. The framework integrates (i) curated golden datasets of expert annotations, (ii) silver datasets generated through controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores and prescribes targeted improvements. These prescriptions are embedded into a vectorized recommendation map, allowing expert interventions to propagate as reusable improvement trajectories across multiple system instances. We demonstrate the framework on a multi-agent recruiter-assistant system, showing that it uncovers latent cognitive failures - such as biased phrasing, extraction drift, and tool misrouting - while simultaneously steering agents toward expert-level reasoning and style. The results establish a foundation for standardized, reproducible expert behaviour transfer in stochastic, tool-augmented LLM agents, moving beyond static evaluation to active expert system refinement.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-47] Knowledge-Driven Hallucination in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark> : An Empirical Study on Process Modeling</p>
<p>【速读】：该论文旨在解决大型语言模型（Large Language Models, LLMs）在分析任务中因依赖预训练知识而引发的“知识驱动型幻觉”（knowledge-driven hallucination）问题，即模型输出与显式提供的证据相矛盾的现象。其解决方案的关键在于设计一个受控实验，通过引入标准与故意异常的过程结构描述，评估LLM在自动化流程建模任务中对输入证据的忠实度（fidelity），从而提供一种系统性方法来量化和识别此类可靠性风险，并强调在基于证据的领域中对AI生成成果进行严格验证的必要性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15336">https://arxiv.org/abs/2509.15336</a><br>
<strong>作者</strong>: Humam Kourani,Anton Antonov,Alessandro Berti,Wil M.P. van der Aalst<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  The Version of Record of this contribution will be published in the proceedings of the 2nd International Workshop on Generative AI for Process Mining (GenAI4PM 2025). This preprint has not undergone peer review or any post-submission improvements or corrections</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The utility of Large Language Models (LLMs) in analytical tasks is rooted in their vast pre-trained knowledge, which allows them to interpret ambiguous inputs and infer missing information. However, this same capability introduces a critical risk of what we term knowledge-driven hallucination: a phenomenon where the model’s output contradicts explicit source evidence because it is overridden by the model’s generalized internal knowledge. This paper investigates this phenomenon by evaluating LLMs on the task of automated process modeling, where the goal is to generate a formal business process model from a given source artifact. The domain of Business Process Management (BPM) provides an ideal context for this study, as many core business processes follow standardized patterns, making it likely that LLMs possess strong pre-trained schemas for them. We conduct a controlled experiment designed to create scenarios with deliberate conflict between provided evidence and the LLM’s background knowledge. We use inputs describing both standard and deliberately atypical process structures to measure the LLM’s fidelity to the provided evidence. Our work provides a methodology for assessing this critical reliability issue and raises awareness of the need for rigorous validation of AI-generated artifacts in any evidence-based domain.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-48] An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature</p>
<p>【速读】：该论文旨在解决传统文献综述方法在效率和相关性上的局限性，尤其是系统性综述流程繁琐、依赖人工筛选以及优化方法对先验知识或标注数据的高要求等问题。其解决方案的关键在于构建一个自动化管道，利用基于Transformer的嵌入模型（如BERT等）提取文本语义特征，并通过余弦相似度（cosine similarity）量化输入论文与候选文献之间的语义接近程度，从而实现无需人工干预或标签数据即可快速筛选出高度相关的文献。该方法显著降低了使用门槛，提升了可扩展性和实用性，适用于初步研究探索与文献分析场景。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15292">https://arxiv.org/abs/2509.15292</a><br>
<strong>作者</strong>: Abhiyan Dhakal(1),Kausik Paudel(1),Sanjog Sigdel(1) ((1) Kathmandu University, Dhulikhel, Nepal)<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  8 pages, 6 figures, 1 table, National Conference on Computer Innovations</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We propose an automated pipeline for performing literature reviews using semantic similarity. Unlike traditional systematic review systems or optimization based methods, this work emphasizes minimal overhead and high relevance by using transformer based embeddings and cosine similarity. By providing a paper title and abstract, it generates relevant keywords, fetches relevant papers from open access repository, and ranks them based on their semantic closeness to the input. Three embedding models were evaluated. A statistical thresholding approach is then applied to filter relevant papers, enabling an effective literature review pipeline. Despite the absence of heuristic feedback or ground truth relevance labels, the proposed system shows promise as a scalable and practical tool for preliminary research and exploratory analysis.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-49] he Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI</p>
<p>【速读】：该论文旨在解决强化学习（Reinforcement Learning, RL）在交通信号控制中因输入数据分布随时间动态变化而导致的训练模型可靠性问题，即“分布偏移”（distributional shift）引发的性能下降风险。其关键解决方案是评估一种先进的元强化学习（Meta Reinforcement Learning, Meta RL）方法——MetaLight，通过分析其在不同环境条件下的表现，揭示了Meta RL虽在特定条件下能取得较好效果，但在其他条件下可能产生高达22%的误差，表明当前Meta RL方案仍缺乏足够的鲁棒性，存在显著的可靠性隐患。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15291">https://arxiv.org/abs/2509.15291</a><br>
<strong>作者</strong>: Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart transportation networks has increased significantly in the last few years. Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to be a very promising approach by several authors. However, a problem with using Reinforcement Learning in Traffic Signal Control is the reliability of the trained RL agents due to the dynamically changing distribution of the input data with respect to the distribution of the data used for training. This presents a major challenge and a reliability problem for the trained network of AI agents and could have very undesirable and even detrimental consequences if a suitable solution is not found. Several researchers have tried to address this problem using different approaches. In particular, Meta Reinforcement Learning (Meta RL) promises to be an effective solution. In this paper, we evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and show that, while under certain conditions MetaLight can indeed lead to reasonably good results, under some other conditions it might not perform well (with errors of up to 22%), suggesting that Meta RL schemes are often not robust enough and can even pose major reliability problems.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-50] Collective Voice: Recovered-Peer Support Mediated by An <mark class="hl-label green">LLM</mark> -Based Chatbot for Eating Disorder Recovery</p>
<p>【速读】：该论文旨在解决当前进食障碍（Eating Disorder, ED）康复支持中Peer recovery narratives（同伴康复叙事）资源稀缺且对已康复个体存在潜在风险的问题，如复发风险等。解决方案的关键在于设计并验证一个名为RecoveryTeller的聊天机器人（chatbot），其采用“已康复同伴”（recovered-peer）角色扮演人格（persona），通过模拟真实康复者的视角与用户互动，以重现同伴叙事在激发希望和维持康复方面的独特支持优势。实验表明，该人格能引发更强的情感共鸣，但同时也揭示了情感信任与认知信任之间的张力，提示未来心理健康聊天机器人的角色设计应注重人格的互补性而非替代性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15289">https://arxiv.org/abs/2509.15289</a><br>
<strong>作者</strong>: Ryuhaerang Choi,Taehan Kim,Subin Park,Seohyeon Yoo,Jennifer G. Kim,Sung-Ju Lee<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Peer recovery narratives provide unique benefits beyond professional or lay mentoring by fostering hope and sustained recovery in eating disorder (ED) contexts. Yet, such support is limited by the scarcity of peer-involved programs and potential drawbacks on recovered peers, including relapse risk. To address this, we designed RecoveryTeller, a chatbot adopting a recovered-peer persona that portrays itself as someone recovered from an ED. We examined whether such a persona can reproduce the support affordances of peer recovery narratives. We compared RecoveryTeller with a lay-mentor persona chatbot offering similar guidance but without a recovery background. We conducted a 20-day cross-over deployment study with 26 ED participants, each using both chatbots for 10 days. RecoveryTeller elicited stronger emotional resonance than a lay-mentor chatbot, yet tensions between emotional and epistemic trust led participants to view the two personas as complementary rather than substitutes. We provide design implications for mental health chatbot persona design.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-51] Evaluating the Limitations of Local <mark class="hl-label green">LLM</mark> s in Solving Complex Programming Challenges <mark class="hl-label red">CCS</mark></p>
<p>【速读】：该论文旨在解决当前开源本地部署的大语言模型（Large Language Models, LLMs）在处理复杂编程竞赛任务时性能不足的问题，特别是面对具有长描述和复杂上下文的题目时的表现瓶颈。其解决方案的关键在于对原有的AI驱动代码生成评估框架（Framework for AI-driven Code Generation Evaluation, FACE）进行重构，使其完全离线运行：通过Ollama运行时将原框架中庞大的按题划分的目录结构压缩为少量JSON文件，并引入稳健的检查点机制以支持多日任务中断后恢复；在此基础上，对8个参数规模介于6.7至90亿之间的代码导向型模型在Kattis平台全部3,589道题目上进行了系统性评估，揭示了开源本地模型在pass@1准确率上约为商用模型（如Gemini 1.5和ChatGPT-4）的一半，但同时也表明开源模型进步显著，且该评估流程可在组织内部硬件环境中复现，具备实用价值。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15283">https://arxiv.org/abs/2509.15283</a><br>
<strong>作者</strong>: Kadin Matotek,Heather Cassel,Md Amiruzzaman,Linh B. Ngo<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Programming Languages (<a target="_blank" rel="noopener" href="http://cs.PL">cs.PL</a>)<br>
<strong>备注</strong>:  Comments: 16 pages, 3 figures, 8 tables, accepted to CCSC Eastern 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This study examines the performance of today’s open-source, locally hosted large-language models (LLMs) in handling complex competitive programming tasks with extended problem descriptions and contexts. Building on the original Framework for AI-driven Code Generation Evaluation (FACE), the authors retrofit the pipeline to work entirely offline through the Ollama runtime, collapsing FACE’s sprawling per-problem directory tree into a handful of consolidated JSON files, and adding robust checkpointing so multi-day runs can resume after failures. The enhanced framework generates, submits, and records solutions for the full Kattis corpus of 3,589 problems across eight code-oriented models ranging from 6.7-9 billion parameters. The submission results show that the overall pass@1 accuracy is modest for the local models, with the best models performing at approximately half the acceptance rate of the proprietary models, Gemini 1.5 and ChatGPT-4. These findings expose a persistent gap between private, cost-controlled LLM deployments and state-of-the-art proprietary services, yet also highlight the rapid progress of open models and the practical benefits of an evaluation workflow that organizations can replicate on in-house hardware.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-52] Partial Column Generation with Graph Neural Networks for Team Formation and Routing</p>
<p>【速读】：该论文致力于解决团队组建与路径规划问题（team formation and routing problem），这是一类在机场、医疗及维护运营等领域具有广泛应用的复杂优化问题。其解决方案的关键在于提出了一种新颖的部分列生成（partial column generation）策略，该策略针对存在多个定价问题（pricing problems）的情形，通过机器学习模型预测哪些定价问题更可能产生负 reduced cost 的列（column）。该模型基于图神经网络（graph neural networks）构建，能够有效识别高潜力列生成来源，从而显著提升求解效率，在严苛时间限制下对困难实例的表现优于传统部分列生成方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15275">https://arxiv.org/abs/2509.15275</a><br>
<strong>作者</strong>: Giacomo Dall’Olio,Rainer Kolisch,Yaoxin Wu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  30 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The team formation and routing problem is a challenging optimization problem with several real-world applications in fields such as airport, healthcare, and maintenance operations. To solve this problem, exact solution methods based on column generation have been proposed in the literature. In this paper, we propose a novel partial column generation strategy for settings with multiple pricing problems, based on predicting which ones are likely to yield columns with a negative reduced cost. We develop a machine learning model tailored to the team formation and routing problem that leverages graph neural networks for these predictions. Computational experiments demonstrate that applying our strategy enhances the solution method and outperforms traditional partial column generation approaches from the literature, particularly on hard instances solved under a tight time limit.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-53] Modeling Transformers as complex networks to analyze learning dynamics</p>
<p>【速读】：该论文试图解决的问题是：大型语言模型（Large Language Models, LLMs）在训练过程中如何获得复杂能力的机制尚不明确，即其学习动态的可解释性问题。为解决这一问题，作者提出了一种基于复杂网络理论（Complex Network Theory, CNT）的新方法，将Transformer架构的LLM建模为一个有向加权图，其中节点代表模型的计算组件（如注意力头和多层感知机MLP），边表示因果影响，通过干预式消融技术量化。该方法的关键在于利用143个训练检查点的数据，追踪组件图结构的演化，并分析一系列图论指标，从而揭示出信息传播者组件的稳定层级与信息收集者组件的动态重组过程，表明从组件层面构建的网络视角能够有效捕捉LLMs中功能电路形成的自组织规律。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15269">https://arxiv.org/abs/2509.15269</a><br>
<strong>作者</strong>: Elisabetta Rocchetti<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The process by which Large Language Models (LLMs) acquire complex capabilities during training remains a key open question in mechanistic interpretability. This project investigates whether these learning dynamics can be characterized through the lens of Complex Network Theory (CNT). I introduce a novel methodology to represent a Transformer-based LLM as a directed, weighted graph where nodes are the model’s computational components (attention heads and MLPs) and edges represent causal influence, measured via an intervention-based ablation technique. By tracking the evolution of this component-graph across 143 training checkpoints of the Pythia-14M model on a canonical induction task, I analyze a suite of graph-theoretic metrics. The results reveal that the network’s structure evolves through distinct phases of exploration, consolidation, and refinement. Specifically, I identify the emergence of a stable hierarchy of information spreader components and a dynamic set of information gatherer components, whose roles reconfigure at key learning junctures. This work demonstrates that a component-level network perspective offers a powerful macroscopic lens for visualizing and understanding the self-organizing principles that drive the formation of functional circuits in LLMs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-54] IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders</p>
<p>【速读】：该论文旨在解决脑电图（EEG）信号分类中因信噪比低导致深度学习模型性能受限的问题，特别是现有特征选择（FS）方法普遍缺乏针对EEG诊断的专门设计、依赖特定网络结构且可解释性差，同时多基于单次迭代数据，鲁棒性不足。其解决方案的关键在于提出IEFS-GMB方法——一种基于信息熵的特征选择机制，通过构建梯度记忆库（Gradient Memory Bank）动态存储历史梯度信息，利用信息熵计算特征重要性，并采用熵加权策略筛选高信息量的EEG特征，从而提升编码器的表征能力与模型整体性能，同时增强模型可解释性，适用于临床场景。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15259">https://arxiv.org/abs/2509.15259</a><br>
<strong>作者</strong>: Liang Zhang,Hanyang Dong,Jia-Hong Gao,Yi Sun,Kuntao Xiao,Wanli Yang,Zhao Lv,Shurong Sheng<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep learning-based EEG classification is crucial for the automated detection of neurological disorders, improving diagnostic accuracy and enabling early intervention. However, the low signal-to-noise ratio of EEG signals limits model performance, making feature selection (FS) vital for optimizing representations learned by neural network encoders. Existing FS methods are seldom designed specifically for EEG diagnosis; many are architecture-dependent and lack interpretability, limiting their applicability. Moreover, most rely on single-iteration data, resulting in limited robustness to variability. To address these issues, we propose IEFS-GMB, an Information Entropy-based Feature Selection method guided by a Gradient Memory Bank. This approach constructs a dynamic memory bank storing historical gradients, computes feature importance via information entropy, and applies entropy-based weighting to select informative EEG features. Experiments on four public neurological disease datasets show that encoders enhanced with IEFS-GMB achieve accuracy improvements of 0.64% to 6.45% over baseline models. The method also outperforms four competing FS techniques and improves model interpretability, supporting its practical use in clinical settings.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-55] Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model</p>
<p>【速读】：该论文旨在解决如何将生成式人工智能（Generative AI）有效集成到无线感知系统中，以提升如设备定位、人体活动识别和环境监测等任务的性能。其解决方案的关键在于从两个互补视角展开：一是将GenAI作为插件嵌入无线感知流程，用于增强特定任务模型（如通过数据增强或去噪）；二是将其作为求解器直接参与感知任务建模（如利用生成对抗网络GANs、变分自编码器VAEs或扩散模型进行信号合成与域适应）。同时，论文系统分析了主流生成模型在不同无线感知场景下的适用性与优势，并指出构建统一的无线基础模型（wireless foundation model）是未来方向，即一种可预训练、可扩展、适应性强且高效的通用信号理解架构。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15258">https://arxiv.org/abs/2509.15258</a><br>
<strong>作者</strong>: Zheng Yang,Guoxuan Chi,Chenshu Wu,Hanyu Liu,Yuchong Gao,Yunhao Liu,Jie Xu,Tony Xiao Han<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Signal Processing (eess.SP)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generative Artificial Intelligence (GenAI) has made significant advancements in fields such as computer vision (CV) and natural language processing (NLP), demonstrating its capability to synthesize high-fidelity data and improve generalization. Recently, there has been growing interest in integrating GenAI into wireless sensing systems. By leveraging generative techniques such as data augmentation, domain adaptation, and denoising, wireless sensing applications, including device localization, human activity recognition, and environmental monitoring, can be significantly improved. This survey investigates the convergence of GenAI and wireless sensing from two complementary perspectives. First, we explore how GenAI can be integrated into wireless sensing pipelines, focusing on two modes of integration: as a plugin to augment task-specific models and as a solver to directly address sensing tasks. Second, we analyze the characteristics of mainstream generative models, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion models, and discuss their applicability and unique advantages across various wireless sensing tasks. We further identify key challenges in applying GenAI to wireless sensing and outline a future direction toward a wireless foundation model: a unified, pre-trained design capable of scalable, adaptable, and efficient signal understanding across diverse sensing tasks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-56] A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction</p>
<p>【速读】：该论文旨在解决药物-药物相互作用（Drug-Drug Interaction, DDI）预测中难以捕捉分子结构多尺度信息（从局部官能团到全局拓扑结构）以及缺乏置信度量化机制的问题。解决方案的关键在于提出了一种新颖的多尺度图神经过程框架（Multi-scale Graph Neural Process, MPNP-DDI），其核心创新包括：通过迭代式消息传递机制学习多层次图表示，利用跨药物协同注意力机制动态融合多尺度特征以生成情境感知的药物对嵌入，并集成神经过程模块实现可解释的不确定性估计，从而在多个基准数据集上显著优于现有最先进方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15256">https://arxiv.org/abs/2509.15256</a><br>
<strong>作者</strong>: Zimo Yan,Jie Zhang,Zheng Xie,Yiping Song,Hao Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate prediction of drug-drug interactions (DDI) is crucial for medication safety and effective drug development. However, existing methods often struggle to capture structural information across different scales, from local functional groups to global molecular topology, and typically lack mechanisms to quantify prediction confidence. To address these limitations, we propose MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of MPNP-DDI is a unique message-passing scheme that, by being iteratively applied, learns a hierarchy of graph representations at multiple scales. Crucially, a cross-drug co-attention mechanism then dynamically fuses these multi-scale representations to generate context-aware embeddings for interacting drug pairs, while an integrated neural process module provides principled uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI significantly outperforms state-of-the-art baselines on benchmark datasets. By providing accurate, generalizable, and uncertainty-aware predictions built upon multi-scale structural features, MPNP-DDI represents a powerful computational tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-57] Emotion-Aware Speech Generation with Character-Specific Voices for Comics</p>
<p>【速读】：该论文旨在解决从漫画中自动生成特定角色、情绪感知语音的问题，以实现漫画的自动化配音并提升阅读的交互性与沉浸感。解决方案的关键在于构建一个端到端的处理流程：首先通过图像处理模块完成角色检测、文本识别和情绪强度识别；接着利用大语言模型（Large Language Model, LLM）融合视觉信息与情节上下文进行对话归属和情绪分析；最后借助带有角色专属声线和情绪特征的文本转语音（Text-to-Speech, TTS）模型生成高质量语音输出。该方案实现了角色、情感与语音的高度对齐，为漫画语音化提供了系统性技术路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15253">https://arxiv.org/abs/2509.15253</a><br>
<strong>作者</strong>: Zhiwen Qian,Jinhua Liang,Huan Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper presents an end-to-end pipeline for generating character-specific, emotion-aware speech from comics. The proposed system takes full comic volumes as input and produces speech aligned with each character’s dialogue and emotional state. An image processing module performs character detection, text recognition, and emotion intensity recognition. A large language model performs dialogue attribution and emotion analysis by integrating visual information with the evolving plot context. Speech is synthesized through a text-to-speech model with distinct voice profiles tailored to each character and emotion. This work enables automated voiceover generation for comics, offering a step toward interactive and immersive comic reading experience.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-58] Causal <mark class="hl-label green">Reasoning</mark>  Elicits Controllable 3D Scene Generation</p>
<p>【速读】：该论文旨在解决现有3D场景生成方法难以建模物体之间复杂逻辑依赖关系与物理约束的问题，从而限制了其在动态和真实环境中的适应能力。解决方案的关键在于提出CausalStruct框架，通过引入因果推理机制，利用大语言模型（Large Language Models, LLMs）构建因果图（causal graph），其中节点表示物体及属性，边编码因果依赖与物理约束；在此基础上，采用因果排序确定物体放置顺序，并通过因果干预调整空间配置以满足物理驱动的约束，同时结合比例-积分-微分（Proportional-Integral-Derivative, PID）控制器迭代优化物体尺度与位置，最终实现逻辑一致性更强、空间交互更真实且适应性更优的3D场景生成。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15249">https://arxiv.org/abs/2509.15249</a><br>
<strong>作者</strong>: Shen Chen,Ruiyu Zhao,Jiale Zhou,Zongkai Wu,Jenq-Neng Hwang,Lei Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-59] GenCAD-3D: CAD Program Generation using Multimodal Latent Space Alignment and Synthetic Dataset Balancing</p>
<p>【速读】：该论文旨在解决从非参数化数据（如点云和网格）中自动生成参数化CAD程序这一关键挑战，当前方法受限于数据集不平衡与规模不足，尤其缺乏对复杂CAD程序的有效表示。解决方案的关键在于提出GenCAD-3D框架，其核心创新包括：利用对比学习对齐CAD与几何编码器的潜在嵌入，结合潜在扩散模型实现CAD序列生成与检索；同时设计SynthBal合成数据增强策略，专门用于平衡和扩展数据集，显著提升复杂几何结构的建模能力。实验表明，该方法在重建精度、无效模型减少及高复杂度几何体性能上均优于现有基准。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15246">https://arxiv.org/abs/2509.15246</a><br>
<strong>作者</strong>: Nomi Yu(1),Md Ferdous Alam(1),A. John Hart(1),Faez Ahmed(1) ((1) Massachusetts Institute of Technology)<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  9 figures, 15 pages. Accepted and soon published in the ASME Journal of Mechanical Design</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:CAD programs, structured as parametric sequences of commands that compile into precise 3D geometries, are fundamental to accurate and efficient engineering design processes. Generating these programs from nonparametric data such as point clouds and meshes remains a crucial yet challenging task, typically requiring extensive manual intervention. Current deep generative models aimed at automating CAD generation are significantly limited by imbalanced and insufficiently large datasets, particularly those lacking representation for complex CAD programs. To address this, we introduce GenCAD-3D, a multimodal generative framework utilizing contrastive learning for aligning latent embeddings between CAD and geometric encoders, combined with latent diffusion models for CAD sequence generation and retrieval. Additionally, we present SynthBal, a synthetic data augmentation strategy specifically designed to balance and expand datasets, notably enhancing representation of complex CAD geometries. Our experiments show that SynthBal significantly boosts reconstruction accuracy, reduces the generation of invalid CAD models, and markedly improves performance on high-complexity geometries, surpassing existing benchmarks. These advancements hold substantial implications for streamlining reverse engineering and enhancing automation in engineering design. We will publicly release our datasets and code, including a set of 51 3D-printed and laser-scanned parts on our project site.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-60] KNARsack: Teaching Neural Algorithmic <mark class="hl-label green">Reason</mark> ers to Solve Pseudo-Polynomial Problems</p>
<p>【速读】：该论文旨在解决神经算法推理（Neural Algorithmic Reasoning, NAR）领域中对伪多项式问题（pseudo-polynomial problem）——具体为背包问题（Knapsack problem）——建模与求解的不足，该问题在标准NAR基准中被忽略。其解决方案的关键在于设计一个遵循两阶段流水线的神经算法推理器：首先通过动态规划（Dynamic Programming, DP）监督机制构建DP表以建模中间状态，随后从该表重构最优解；相比直接从输入预测最优子集的基线方法，该策略显著提升了对更大规模问题实例的泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15239">https://arxiv.org/abs/2509.15239</a><br>
<strong>作者</strong>: Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  14 pages, 10 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Neural algorithmic reasoning (NAR) is a growing field that aims to embed algorithmic logic into neural networks by imitating classical algorithms. In this extended abstract, we detail our attempt to build a neural algorithmic reasoner that can solve Knapsack, a pseudo-polynomial problem bridging classical algorithms and combinatorial optimisation, but omitted in standard NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow the two-phase pipeline for the Knapsack problem, which involves first constructing the dynamic programming table and then reconstructing the solution from it. The approach, which models intermediate states through dynamic programming supervision, achieves better generalization to larger problem instances than a direct-prediction baseline that attempts to select the optimal subset only from the problem inputs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-61] Generating Plans for Belief-Desire-Intention (BDI) <mark class="hl-label green">Agents</mark>  Using Alternating-Time Temporal Logic (ATL)</p>
<p>【速读】：该论文旨在解决现有Belief-Desire-Intention (BDI)代理系统中计划生成依赖大量人工干预且主要局限于单智能体场景的问题。其解决方案的关键在于引入交替时间时序逻辑（Alternating-Time Temporal Logic, ATL），通过该逻辑自动推导出能够处理多智能体之间竞争或协作关系的BDI计划，从而支持多智能体系统的协同目标达成。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15238">https://arxiv.org/abs/2509.15238</a><br>
<strong>作者</strong>: Dylan Léveillé(Carleton University)<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  In Proceedings GandALF 2025, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13258">arXiv:2509.13258</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Belief-Desire-Intention (BDI) is a framework for modelling agents based on their beliefs, desires, and intentions. Plans are a central component of BDI agents, and define sequences of actions that an agent must undertake to achieve a certain goal. Existing approaches to plan generation often require significant manual effort, and are mainly focused on single-agent systems. As a result, in this work, we have developed a tool that automatically generates BDI plans using Alternating-Time Temporal Logic (ATL). By using ATL, the plans generated accommodate for possible competition or cooperation between the agents in the system. We demonstrate the effectiveness of the tool by generating plans for an illustrative game that requires agent collaboration to achieve a shared goal. We show that the generated plans allow the agents to successfully attain this goal.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-62] ChannelFlow-Tools: A Standardized Dataset Creation Pipeline for 3D Obstructed Channel Flows</p>
<p>【速读】：该论文旨在解决复杂三维障碍通道流动（3D obstructed channel flows）中从计算机辅助设计（CAD）几何生成到机器学习（ML）训练输入标准化、可复现性差的问题。传统方法在构建用于计算流体动力学（CFD）代理模型的数据集时，缺乏统一的流程控制与自动化，导致实验难以复现且效率低下。其解决方案的关键在于提出一个配置驱动的工具链 ChannelFlow-Tools，该工具链整合了几何合成与可行性验证、符号距离场（SDF）体素化、高性能计算（HPC）上的自动求解器调度（waLBerla LBM），以及笛卡尔重采样以生成共注册的多分辨率张量表示；通过单一 Hydra/OmegaConf 配置文件统一管理所有阶段，实现确定性复现和可控消融实验，从而将一次性数据集构建转变为可配置、可复现的 CFD 代理建模流水线。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15236">https://arxiv.org/abs/2509.15236</a><br>
<strong>作者</strong>: Shubham Kavane,Kajol Kulkarni,Harald Koestler<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present ChannelFlow-Tools, a configuration-driven framework that standardizes the end-to-end path from programmatic CAD solid generation to ML-ready inputs and targets for 3D obstructed channel flows. The toolchain integrates geometry synthesis with feasibility checks, signed distance field (SDF) voxelization, automated solver orchestration on HPC (waLBerla LBM), and Cartesian resampling to co-registered multi-resolution tensors. A single Hydra/OmegaConf configuration governs all stages, enabling deterministic reproduction and controlled ablations. As a case study, we generate 10k+ scenes spanning Re=100-15000 with diverse shapes and poses. An end-to-end evaluation of storage trade-offs directly from the emitted artifacts, a minimal 3D U-Net at 128x32x32, and example surrogate models with dataset size illustrate that the standardized representations support reproducible ML training. ChannelFlow-Tools turns one-off dataset creation into a reproducible, configurable pipeline for CFD surrogate modeling.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-63] Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning</p>
<p>【速读】：该论文旨在解决基础模型（Foundation Models）在静态部署下难以满足隐私法规（如GDPR）中“被遗忘权”要求的问题，即无法高效、安全地移除特定数据所对应的知识。传统方法如重新训练或知识蒸馏等存在计算开销大、脆弱性高且不适用于实时系统等局限。解决方案的关键在于提出一种基于提示（prompt-based）的学习框架，将类别语义绑定到专用的提示标记（prompt tokens），而非存储于模型权重中；由此实现仅通过删除对应提示即可即时完成知识擦除，无需重训练、修改模型结构或访问原始数据，从而在保持保留类性能的同时有效移除遗忘类信息，并具备抵御成员推理攻击和防止残留知识泄露的隐私与安全优势。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15230">https://arxiv.org/abs/2509.15230</a><br>
<strong>作者</strong>: Rutger Hendrix,Giovanni Patanè,Leonardo G. Russo,Simone Carnemolla,Giovanni Bellitto,Federica Proietto Salanitri,Concetto Spampinato,Matteo Pennisi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted at ACM multimedia 2025 BNI track</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Foundation models have transformed multimedia analysis by enabling robust and transferable representations across diverse modalities and tasks. However, their static deployment conflicts with growing societal and regulatory demands – particularly the need to unlearn specific data upon request, as mandated by privacy frameworks such as the GDPR. Traditional unlearning approaches, including retraining, activation editing, or distillation, are often computationally expensive, fragile, and ill-suited for real-time or continuously evolving systems. In this paper, we propose a paradigm shift: rethinking unlearning not as a retroactive intervention but as a built-in capability. We introduce a prompt-based learning framework that unifies knowledge acquisition and removal within a single training phase. Rather than encoding information in model weights, our approach binds class-level semantics to dedicated prompt tokens. This design enables instant unlearning simply by removing the corresponding prompt – without retraining, model modification, or access to original data. Experiments demonstrate that our framework preserves predictive performance on retained classes while effectively erasing forgotten ones. Beyond utility, our method exhibits strong privacy and security guarantees: it is resistant to membership inference attacks, and prompt removal prevents any residual knowledge extraction, even under adversarial conditions. This ensures compliance with data protection principles and safeguards against unauthorized access to forgotten information, making the framework suitable for deployment in sensitive and regulated environments. Overall, by embedding removability into the architecture itself, this work establishes a new foundation for designing modular, scalable and ethically responsive AI models.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-64] Accelerating Atomic Fine Structure Determination with Graph Reinforcement Learning</p>
<p>【速读】：该论文旨在解决原子精细结构能级确定效率低下的问题，这一瓶颈限制了天文学和聚变科学对原子数据日益增长的需求。当前依赖人工分析大量光谱线（约10⁴条）以确定数百至数千个精细结构能级（fine structure level energies）的方法耗时且难以扩展。解决方案的关键在于将传统的光谱分析流程建模为马尔可夫决策过程（Markov decision process），并利用图强化学习（graph reinforcement learning）结合从历史人类决策中学习到的奖励函数来自动化该过程。这种方法在Co II和Nd II-III的测试中实现了数小时内计算数百个能级，与已发表结果的一致性分别达到95%和54–87%，显著提升了效率并具备规模化潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16184">https://arxiv.org/abs/2509.16184</a><br>
<strong>作者</strong>: M. Ding,V.-A. Darvariu,A. N. Ryabtsev,N. Hawes,J. C. Pickering<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Atomic Physics (physics.atom-ph); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Atomic data determined by analysis of observed atomic spectra are essential for plasma diagnostics. For each low-ionisation open d- and f-subshell atomic species, around  10^3  fine structure level energies can be determined through years of analysis of  10^4  observable spectral lines. We propose the automation of this task by casting the analysis procedure as a Markov decision process and solving it by graph reinforcement learning using reward functions learned on historical human decisions. In our evaluations on existing spectral line lists and theoretical calculations for Co II and Nd II-III, hundreds of level energies were computed within hours, agreeing with published values in 95% of cases for Co II and 54-87% for Nd II-III. As the current efficiency in atomic fine structure determination struggles to meet growing atomic data demands from astronomy and fusion science, our new artificial intelligence approach sets the stage for closing this gap.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-65] AI Methods for Permutation Circuit Synthesis Across Generic Topologies <mark class="hl-label red">AAAI2025</mark> <mark class="hl-label red">AAAI</mark></p>
<p>【速读】：该论文旨在解决在不同量子硬件拓扑结构上高效合成与转换（transpilation）置换电路（permutation circuits）的问题。传统方法通常依赖于针对特定拓扑设计的专用模型或经典启发式算法，难以泛化且效率有限。其解决方案的关键在于：首先在一个通用矩形晶格（rectangular lattice）上训练一个基础强化学习（Reinforcement Learning, RL）模型，并通过掩码机制（masking mechanism）动态选择目标拓扑子集进行推理；这种设计使得模型无需重新训练即可适应任何可嵌入该矩形晶格的拓扑结构，包括训练中未见过的拓扑。此外，模型还可通过微调（fine-tuning）进一步优化特定拓扑上的性能，从而实现单一模型对多种拓扑的高效、灵活支持，具备直接集成到量子编译（transpilation）工作流中的实用潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16020">https://arxiv.org/abs/2509.16020</a><br>
<strong>作者</strong>: Victor Villar,Juan Cruz-Benito,Ismael Faro,David Kremer<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  This paper has been accepted by First AAAI Symposium on Quantum Information  Machine Learning (QIML): Bridging Quantum Computing and Artificial Intelligence at AAAI 2025 Fall Symposium</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper investigates artificial intelligence (AI) methodologies for the synthesis and transpilation of permutation circuits across generic topologies. Our approach uses Reinforcement Learning (RL) techniques to achieve near-optimal synthesis of permutation circuits up to 25 qubits. Rather than developing specialized models for individual topologies, we train a foundational model on a generic rectangular lattice, and employ masking mechanisms to dynamically select subsets of topologies during the synthesis. This enables the synthesis of permutation circuits on any topology that can be embedded within the rectangular lattice, without the need to re-train the model. In this paper we show results for 5x5 lattice and compare them to previous AI topology-oriented models and classical methods, showing that they outperform classical heuristics, and match previous specialized AI models, and performs synthesis even for topologies that were not seen during training. We further show that the model can be fine tuned to strengthen the performance for selected topologies of interest. This methodology allows a single trained model to efficiently synthesize circuits across diverse topologies, allowing its practical integration into transpilation workflows.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-66] MoE-CE: Enhancing Generalization for Deep Learning based Channel Estimation via a Mixture-of-Experts Framework</p>
<p>【速读】：该论文旨在解决深度学习（Deep Learning, DL）-based信道估计（Channel Estimation, CE）方法在动态无线环境中泛化能力不足的问题，特别是在多任务（multitask）和零样本（zero-shot）场景下，模型难以适应不同信噪比（SNR）、资源块（Resource Block, RB）数量及信道特征等变化条件。解决方案的关键在于提出一种基于专家混合（Mixture-of-Experts, MoE）的灵活架构——MoE-CE，其通过多个专注于特定信道特性的专家子网络与一个可学习的路由机制（router），动态选择最相关的专家进行推理，在不显著增加计算开销的前提下提升模型容量与适应性，且对骨干网络结构和训练算法具有无感知特性（agnostic）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15964">https://arxiv.org/abs/2509.15964</a><br>
<strong>作者</strong>: Tianyu Li,Yan Xin,Jianzhong(Charlie)Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ignal Processing (eess.SP); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reliable channel estimation (CE) is fundamental for robust communication in dynamic wireless environments, where models must generalize across varying conditions such as signal-to-noise ratios (SNRs), the number of resource blocks (RBs), and channel profiles. Traditional deep learning (DL)-based methods struggle to generalize effectively across such diverse settings, particularly under multitask and zero-shot scenarios. In this work, we propose MoE-CE, a flexible mixture-of-experts (MoE) framework designed to enhance the generalization capability of DL-based CE methods. MoE-CE provides an appropriate inductive bias by leveraging multiple expert subnetworks, each specialized in distinct channel characteristics, and a learned router that dynamically selects the most relevant experts per input. This architecture enhances model capacity and adaptability without a proportional rise in computational cost while being agnostic to the choice of the backbone model and the learning algorithm. Through extensive experiments on synthetic datasets generated under diverse SNRs, RB numbers, and channel profiles, including multitask and zero-shot evaluations, we demonstrate that MoE-CE consistently outperforms conventional DL approaches, achieving significant performance gains while maintaining efficiency.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-67] ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</p>
<p>【速读】：该论文旨在解决气候模型模拟中因计算成本高昂而导致的不确定性量化难题，特别是针对生成气候情景 ensemble（集合）所需的大量计算资源问题。其解决方案的关键在于提出 ArchesClimate——一个基于深度学习的气候模型模拟器（climate model emulator），通过训练流匹配（flow matching）模型来高效生成近十年尺度内稳定且物理一致的气候状态序列，从而实现对原始气候模型（IPSL-CM6A-LR）输出结果的可互换替代，显著降低气候模拟的计算开销。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15942">https://arxiv.org/abs/2509.15942</a><br>
<strong>作者</strong>: Graham Clyne,Guillaume Couairon,Guillaume Gastineau,Claire Monteleoni,Anastase Charantonis<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Atmospheric and Oceanic Physics (physics.ao-ph); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-68] An Equivariant Graph Network for Interpretable Nanoporous Materials Design</p>
<p>【速读】：该论文旨在解决纳米多孔材料（nanoporous materials）在可持续应用设计中面临的挑战，即其庞大的化学空间导致传统方法难以高效探索，且现有机器学习模型在预测性能时缺乏可解释性或精度不足，无法阐明晶体几何结构与材料性质之间的关联。解决方案的关键在于提出一种三维周期性空间采样方法，将复杂的纳米多孔结构分解为局部几何位点（local geometrical sites），从而实现对材料性质的联合预测与位点级贡献量化。该方法通过构建数据库和检索数据集训练模型，在气体储存、分离及电导性能预测上达到当前最优的准确性和数据效率，并能识别关键局部位点以支持可解释的设计策略，进而推动具有对称性感知能力的高性能纳米多孔材料设计。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15908">https://arxiv.org/abs/2509.15908</a><br>
<strong>作者</strong>: Zhenhao Zhou,Salman Bin Kashif,Dawei Feng,Jin-Hu Dou,Kaihang Shi,Tao Deng,Zhenpeng Yao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Materials Science (cond-mat.mtrl-sci); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Nanoporous materials hold promise for diverse sustainable applications, yet their vast chemical space poses challenges for efficient design. Machine learning offers a compelling pathway to accelerate the exploration, but existing models lack either interpretability or fidelity for elucidating the correlation between crystal geometry and property. Here, we report a three-dimensional periodic space sampling method that decomposes large nanoporous structures into local geometrical sites for combined property prediction and site-wise contribution quantification. Trained with a constructed database and retrieved datasets, our model achieves state-of-the-art accuracy and data efficiency for property prediction on gas storage, separation, and electrical conduction. Meanwhile, this approach enables the interpretation of the prediction and allows for accurate identification of significant local sites for targeted properties. Through identifying transferable high-performance sites across diverse nanoporous frameworks, our model paves the way for interpretable, symmetry-aware nanoporous materials design, which is extensible to other materials, like molecular crystals and beyond.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-69] DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction</p>
<p>【速读】：该论文旨在解决化学反应机制（Chemical Reaction Mechanisms, CRMs）完整步骤预测的难题，传统方法依赖专家实验或高成本量子化学计算，而现有深度学习（Deep Learning, DL）模型常忽略关键中间体和机理步骤，并易产生幻觉。其解决方案的核心在于提出一个可解释的图神经网络框架 DeepMech，该框架基于原子和键级别的注意力机制，并受广义机理操作模板（Template-based Mechanistic Operations, TMOps）引导，能够准确预测基元步骤（elementary steps）与完整的反应路径，在训练数据分布外场景下仍保持高保真度，且具备识别反应活性位点的能力，从而实现对复杂生物分子合成路径的有效重构。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15872">https://arxiv.org/abs/2509.15872</a><br>
<strong>作者</strong>: Manajit Das,Ajnabiul Hoque,Mayank Baranwal,Raghavan B. Sunoj<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Chemical Physics (physics.chem-ph); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  37 pages, 8 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-70] he (Short-Term) Effects of <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  on Unemployment and Earnings</p>
<p>【速读】：该论文旨在解决大型语言模型（Large Language Models, LLMs）在短期内对劳动力市场影响的实证问题，特别是关注其是否导致就业替代或通过工资调整进行适应性调整。研究的关键在于采用合成双重差分法（Synthetic Difference in Differences），系统比较不同职业暴露于LLM技术程度下的收入变化与失业率差异，从而识别出LLM引入对劳动者收入的净效应。结果表明，高暴露职业的工人在ChatGPT发布后收入上升，而失业率未显著变化，说明初始劳动力市场调整主要体现为工资层面的再分配而非岗位流失。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15510">https://arxiv.org/abs/2509.15510</a><br>
<strong>作者</strong>: Danqing Chen,Carina Kane,Austin Kozlowski,Nadav Kunievsky,James A. Evans<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: General Economics (<a target="_blank" rel="noopener" href="http://econ.GN">econ.GN</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models have spread rapidly since the release of ChatGPT in late 2022, accompanied by claims of major productivity gains but also concerns about job displacement. This paper examines the short-run labor market effects of LLM adoption by comparing earnings and unemployment across occupations with differing levels of exposure to these technologies. Using a Synthetic Difference in Differences approach, we estimate the impact of LLM exposure on earnings and unemployment. Our findings show that workers in highly exposed occupations experienced earnings increases following ChatGPT’s introduction, while unemployment rates remained unchanged. These results suggest that initial labor market adjustments to LLMs operate primarily through earnings rather than worker reallocation.<br>
zh</p>
</div></div>
<h3 id="机器学习">机器学习</h3>
<div class="note pink no-icon flat"><p>[LG-0] Inverting Trojans in <mark class="hl-label green">LLM</mark> s</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16203">https://arxiv.org/abs/2509.16203</a><br>
<strong>作者</strong>: Zhengxing Li,Guangmingmei Yang,Jayaram Raghuram,David J. Miller,George Kesidis<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While effective backdoor detection and inversion schemes have been developed for AIs used e.g. for images, there are challenges in “porting” these methods to LLMs. First, the LLM input space is discrete, which precludes gradient-based search over this space, central to many backdoor inversion methods. Second, there are ~30,000^k k-tuples to consider, k the token-length of a putative trigger. Third, for LLMs there is the need to blacklist tokens that have strong marginal associations with the putative target response (class) of an attack, as such tokens give false detection signals. However, good blacklists may not exist for some domains. We propose a LLM trigger inversion approach with three key components: i) discrete search, with putative triggers greedily accreted, starting from a select list of singletons; ii) implicit blacklisting, achieved by evaluating the average cosine similarity, in activation space, between a candidate trigger and a small clean set of samples from the putative target class; iii) detection when a candidate trigger elicits high misclassifications, and with unusually high decision confidence. Unlike many recent works, we demonstrate that our approach reliably detects and successfully inverts ground-truth backdoor trigger phrases.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-1] MatchFix<mark class="hl-label green">Agent</mark> : Language-Agnostic Autonomous Repository-Level Code Translation Validation and Repair</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16187">https://arxiv.org/abs/2509.16187</a><br>
<strong>作者</strong>: Ali Reza Ibrahimzada,Brandon Paulsen,Reyhaneh Jabbarvand,Joey Dodds,Daniel Kroening<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Code translation transforms source code from one programming language (PL) to another. Validating the functional equivalence of translation and repairing, if necessary, are critical steps in code translation. Existing automated validation and repair approaches struggle to generalize to many PLs due to high engineering overhead, and they rely on existing and often inadequate test suites, which results in false claims of equivalence and ineffective translation repair. We develop MatchFixAgent, a large language model (LLM)-based, PL-agnostic framework for equivalence validation and repair of translations. MatchFixAgent features a multi-agent architecture that divides equivalence validation into several sub-tasks to ensure thorough and consistent semantic analysis of the translation. Then it feeds this analysis to test agent to write and execute tests. Upon observing a test failure, the repair agent attempts to fix the translation bug. The final (in)equivalence decision is made by the verdict agent, considering semantic analyses and test execution results. We compare MatchFixAgent’s validation and repair results with four repository-level code translation techniques. We use 2,219 translation pairs from their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub projects totaling over 900K lines of code. Our results demonstrate that MatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs, with the same equivalence validation result as prior work on 72.8% of them. When MatchFixAgent’s result disagrees with prior work, we find that 60.7% of the time MatchFixAgent’s result is actually correct. In addition, we show that MatchFixAgent can repair 50.6% of inequivalent translation, compared to prior work’s 18.5%. This demonstrates that MatchFixAgent is far more adaptable to many PL pairs than prior work, while producing highly accurate validation results.         Subjects:  Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Machine Learning (cs.LG)  Cite as: arXiv:2509.16187 [<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>]    (or  arXiv:2509.16187v1 [<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.16187">https://doi.org/10.48550/arXiv.2509.16187</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-2] Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16180">https://arxiv.org/abs/2509.16180</a><br>
<strong>作者</strong>: Gautam Kamath,Alireza F. Pour,Matthew Regehr,David P. Woodruff<br>
<strong>类目</strong>: Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We propose an algorithm with improved query-complexity for the problem of hypothesis selection under local differential privacy constraints. Given a set of  k  probability distributions  Q , we describe an algorithm that satisfies local differential privacy, performs  \tildeO(k^3/2)  non-adaptive queries to individuals who each have samples from a probability distribution  p , and outputs a probability distribution from the set  Q  which is nearly the closest to  p . Previous algorithms required either  \Omega(k^2)  queries or many rounds of interactive queries. Technically, we introduce a new object we dub the Scheffé graph, which captures structure of the differences between distributions in  Q , and may be of more broad interest for hypothesis selection tasks.         Subjects:  Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)  Cite as: arXiv:2509.16180 [cs.DS]    (or  arXiv:2509.16180v1 [cs.DS] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.16180">https://doi.org/10.48550/arXiv.2509.16180</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-3] DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16173">https://arxiv.org/abs/2509.16173</a><br>
<strong>作者</strong>: Yuen Chen,Yian Wang,Hari Sundaram<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The goal of this paper is to accelerate the training of machine learning models, a critical challenge since the training of large-scale deep neural models can be computationally expensive. Stochastic gradient descent (SGD) and its variants are widely used to train deep neural networks. In contrast to traditional approaches that focus on tuning the learning rate, we propose a novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts the batch size. Adapting the batch size is challenging: using large batch sizes is more efficient due to parallel computation, but small-batch training often converges in fewer epochs and generalizes better. To address this challenge, we introduce a data-driven adaptation based on gradient diversity, enabling DiveBatch to maintain the generalization performance of small-batch training while improving convergence speed and computational efficiency. Gradient diversity has a strong theoretical justification: it emerges from the convergence analysis of SGD. Evaluations of DiveBatch on synthetic and CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges significantly faster than standard SGD and AdaBatch (1.06 – 5.0x), with a slight trade-off in performance.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-4] Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning <mark class="hl-label green">Agents</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16151">https://arxiv.org/abs/2509.16151</a><br>
<strong>作者</strong>: Isaiah J. King,Benjamin Bowman,H. Howie Huang<br>
<strong>类目</strong>: Machine Learning (cs.LG); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep reinforcement learning (RL) is emerging as a viable strategy for automated cyber defense (ACD). The traditional RL approach represents networks as a list of computers in various states of safety or threat. Unfortunately, these models are forced to overfit to specific network topologies, rendering them ineffective when faced with even small environmental perturbations. In this work, we frame ACD as a two-player context-based partially observable Markov decision problem with observations represented as attributed graphs. This approach allows our agents to reason through the lens of relational inductive bias. Agents learn how to reason about hosts interacting with other system entities in a more general manner, and their actions are understood as edits to the graph representing the environment. By introducing this bias, we will show that our agents can better reason about the states of networks and zero-shot adapt to new ones. We show that this approach outperforms the state-of-the-art by a wide margin, and makes our agents capable of defending never-before-seen networks against a wide range of adversaries in a variety of complex, and multi-agent environments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-5] When Bugs Linger: A Study of Anomalous Resolution Time Outliers and Their Themes</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16140">https://arxiv.org/abs/2509.16140</a><br>
<strong>作者</strong>: Avinash Patil<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  7 pages, 2 tables, 21 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Efficient bug resolution is critical for maintaining software quality and user satisfaction. However, specific bug reports experience unusually long resolution times, which may indicate underlying process inefficiencies or complex issues. This study presents a comprehensive analysis of bug resolution anomalies across seven prominent open-source repositories: Cassandra, Firefox, Hadoop, HBase, SeaMonkey, Spark, and Thunderbird. Utilizing statistical methods such as Z-score and Interquartile Range (IQR), we identify anomalies in bug resolution durations. To understand the thematic nature of these anomalies, we apply Term Frequency-Inverse Document Frequency (TF-IDF) for textual feature extraction and KMeans clustering to group similar bug summaries. Our findings reveal consistent patterns across projects, with anomalies often clustering around test failures, enhancement requests, and user interface issues. This approach provides actionable insights for project maintainers to prioritize and effectively address long-standing bugs.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-6] Spatio-temporal multi-field deep learning of shock propagation in meso-structured media</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16139">https://arxiv.org/abs/2509.16139</a><br>
<strong>作者</strong>: M. Giselle Fernández-Godino,Meir H. Shachar,Kevin Korner,Jonathan L. Belof,Mukul Kumar,Jonathan Lind,William J. Schill<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  16 pages, 10 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The ability to predict how shock waves traverse porous and architected materials is a decisive factor in planetary defense, national security, and the race to achieve inertial fusion energy. Yet capturing pore collapse, anomalous Hugoniot responses, and localized heating – phenomena that can determine the success of asteroid deflection or fusion ignition – has remained a major challenge despite recent advances in single-field and reduced representations. We introduce a multi-field spatio-temporal deep learning model (MSTM) that unifies seven coupled fields – pressure, density, temperature, energy, material distribution, and two velocity components – into a single autoregressive surrogate. Trained on high-fidelity hydrocode data, MSTM runs about a thousand times faster than direct simulation, achieving errors below 4% in porous materials and below 10% in lattice structures. Unlike prior single-field or operator-based surrogates, MSTM resolves sharp shock fronts while preserving integrated quantities such as mass-averaged pressure and temperature to within 5%. This advance transforms problems once considered intractable into tractable design studies, establishing a practical framework for optimizing meso-structured materials in planetary impact mitigation, inertial fusion energy, and national security.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-7] Personalized Federated Learning with Heat-Kernel Enhanced Tensorized Multi-View Clustering</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16101">https://arxiv.org/abs/2509.16101</a><br>
<strong>作者</strong>: Kristina P. Sinaga<br>
<strong>类目</strong>: Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)<br>
*<strong>备注</strong>:  26 pages, 3 algorithms, and 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present a robust personalized federated learning framework that leverages heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with advanced tensor decomposition techniques. Our approach integrates heat-kernel coefficients adapted from quantum field theory with Tucker decomposition and canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional distance metrics and efficiently represent high-dimensional multi-view structures. The framework employs matriculation and vectorization techniques to facilitate the discovery of hidden structures and multilinear relationships via N-way generalized tensors. The proposed method introduces a dual-level optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor decomposition operating on order-N input tensors, and federated aggregation of tensor factors with privacy-preserving personalization mechanisms. The local stage employs tensorized kernel Euclidean distance transformations and Tucker decomposition to discover client-specific patterns in multi-view tensor data, while the global aggregation process coordinates tensor factors (core tensors and factor matrices) across clients through differential privacy-preserving protocols. This tensorized approach enables efficient handling of high-dimensional multi-view data with significant communication savings through low-rank tensor approximations.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-8] Randomized Smoothing Meets Vision-Language Models <mark class="hl-label red">EMNLP’25</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16088">https://arxiv.org/abs/2509.16088</a><br>
<strong>作者</strong>: Emmanouil Seferis,Changshun Wu,Stefanos Kollias,Saddek Bensalem,Chih-Hong Cheng<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  EMNLP’25 full version, including appendix (proofs, additional experiments)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Randomized smoothing (RS) is one of the prominent techniques to ensure the correctness of machine learning models, where point-wise robustness certificates can be derived analytically. While RS is well understood for classification, its application to generative models is unclear, since their outputs are sequences rather than labels. We resolve this by connecting generative outputs to an oracle classification task and showing that RS can still be enabled: the final response can be classified as a discrete action (e.g., service-robot commands in VLAs), as harmful vs. harmless (content moderation or toxicity detection in VLMs), or even applying oracles to cluster answers into semantically equivalent ones. Provided that the error rate for the oracle classifier comparison is bounded, we develop the theory that associates the number of samples with the corresponding robustness radius. We further derive improved scaling laws analytically relating the certified radius and accuracy to the number of samples, showing that the earlier result of 2 to 3 orders of magnitude fewer samples sufficing with minimal loss remains valid even under weaker assumptions. Together, these advances make robustness certification both well-defined and computationally feasible for state-of-the-art VLMs, as validated against recent jailbreak-style adversarial attacks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-9] Rethinking Molecule Synthesizability with Chain-of-Reaction</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16084">https://arxiv.org/abs/2509.16084</a><br>
<strong>作者</strong>: Seul Lee,Karsten Kreis,Srimukh Prasad Veccham,Meng Liu,Danny Reidenbach,Saee Paliwal,Weili Nie,Arash Vahdat<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:A well-known pitfall of molecular generative models is that they are not guaranteed to generate synthesizable molecules. There have been considerable attempts to address this problem, but given the exponentially large combinatorial space of synthesizable molecules, existing methods have shown limited coverage of the space and poor molecular optimization performance. To tackle these problems, we introduce ReaSyn, a generative framework for synthesizable projection where the model explores the neighborhood of given molecules in the synthesizable space by generating pathways that result in synthesizable analogs. To fully utilize the chemical knowledge contained in the synthetic pathways, we propose a novel perspective that views synthetic pathways akin to reasoning paths in large language models (LLMs). Specifically, inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the chain-of-reaction (CoR) notation that explicitly states reactants, reaction types, and intermediate products for each step in a pathway. With the CoR notation, ReaSyn can get dense supervision in every reaction step to explicitly learn chemical reaction rules during supervised training and perform step-by-step reasoning. In addition, to further enhance the reasoning capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning and goal-directed test-time compute scaling tailored for synthesizable projection. ReaSyn achieves the highest reconstruction rate and pathway diversity in synthesizable molecule reconstruction and the highest optimization performance in synthesizable goal-directed molecular optimization, and significantly outperforms previous synthesizable projection methods in synthesizable hit expansion. These results highlight ReaSyn’s superior ability to navigate combinatorially-large synthesizable chemical space.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-10] Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16040">https://arxiv.org/abs/2509.16040</a><br>
<strong>作者</strong>: Jorge-Humberto Urrea-Quintero,David Anton,Laura De Lorenzis,Henning Wessels<br>
<strong>类目</strong>: Machine Learning (cs.LG); Materials Science (cond-mat.mtrl-sci); Computational Engineering, Finance, and Science (cs.CE)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The automated discovery of constitutive models from data has recently emerged as a promising alternative to the traditional model calibration paradigm. In this work, we present a fully automated framework for constitutive model discovery that systematically pairs three sparse regression algorithms (Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression (LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection criteria:  K -fold cross-validation (CV), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). This pairing yields nine distinct algorithms for model discovery and enables a systematic exploration of the trade-off between sparsity, predictive performance, and computational cost. While LARS serves as an efficient path-based solver for the  \ell_1 -constrained problem, OMP is introduced as a tractable heuristic for  \ell_0 -regularized selection. The framework is applied to both isotropic and anisotropic hyperelasticity, utilizing both synthetic and experimental datasets. Results reveal that all nine algorithm-criterion combinations perform consistently well for the discovery of isotropic and anisotropic materials, yielding highly accurate constitutive models. These findings broaden the range of viable discovery algorithms beyond  \ell_1 -based approaches such as LASSO.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-11] me-adaptive SympNets for separable Hamiltonian systems</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16026">https://arxiv.org/abs/2509.16026</a><br>
<strong>作者</strong>: Konrad Janik,Peter Benner<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Measurement data is often sampled irregularly i.e. not on equidistant time grids. This is also true for Hamiltonian systems. However, existing machine learning methods, which learn symplectic integrators, such as SympNets [20] and HénonNets [4] still require training data generated by fixed step sizes. To learn time-adaptive symplectic integrators, an extension to SympNets, which we call TSympNets, was introduced in [20]. We adapt the architecture of TSympNets and extend them to non-autonomous Hamiltonian systems. So far the approximation qualities of TSympNets were unknown. We close this gap by providing a universal approximation theorem for separable Hamiltonian systems and show that it is not possible to extend it to non-separable Hamiltonian systems. To investigate these theoretical approximation capabilities, we perform different numerical experiments. Furthermore we fix a mistake in a proof of a substantial theorem [25, Theorem 2] for the approximation of symplectic maps in general, but specifically for symplectic machine learning methods.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-12] Predicting the descent into extremism and terrorism</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16014">https://arxiv.org/abs/2509.16014</a><br>
<strong>作者</strong>: R.O. Lane,W.J. Holmes,C.J. Taylor,H.M. State-Davey,A.J. Wragge<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
*<strong>备注</strong>:  10 pages, 12 figures, presented at 6th IMA Conference on Mathematics in Defence and Security, Online, 30 September 2023 (conference page at <a target="_blank" rel="noopener" href="https://ima.org.uk/12970/6th-ima-conference-on-mathematics-in-defence-and-security/">this https URL</a> ). arXiv admin note: text overlap with <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2502.00013">arXiv:2502.00013</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes an approach for automatically analysing and tracking statements in material gathered online and detecting whether the authors of the statements are likely to be involved in extremism or terrorism. The proposed system comprises: online collation of statements that are then encoded in a form amenable to machine learning (ML), an ML component to classify the encoded text, a tracker, and a visualisation system for analysis of results. The detection and tracking concept has been tested using quotes made by terrorists, extremists, campaigners, and politicians, obtained from this http URL. A set of features was extracted for each quote using the state-of-the-art Universal Sentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The data were used to train and test a support vector machine (SVM) classifier using 10-fold cross-validation. The system was able to correctly detect intentions and attitudes associated with extremism 81% of the time and terrorism 97% of the time, using a dataset of 839 quotes. This accuracy was higher than that which was achieved for a simple baseline system based on n-gram text features. Tracking techniques were also used to perform a temporal analysis of the data, with each quote considered to be a noisy measurement of a person’s state of mind. It was demonstrated that the tracking algorithms were able to detect both trends over time and sharp changes in attitude that could be attributed to major events.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-13] Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems <mark class="hl-label red">NEURIPS2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15999">https://arxiv.org/abs/2509.15999</a><br>
<strong>作者</strong>: Alan A. Lahoud,Erik Schaffernicht,Johannes A. Stork<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted at Neurips 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Learning representations for solutions of constrained optimization problems (COPs) with unknown cost functions is challenging, as models like (Variational) Autoencoders struggle to enforce constraints when decoding structured outputs. We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a latent space of COP cost functions from observed solutions and reconstructs feasible outputs by solving a COP with a solver in the loop. Our approach leverages estimated gradients of a Fenchel-Young loss through a non-differentiable deterministic solver to shape the latent space. Unlike standard Inverse Optimization or Inverse Reinforcement Learning methods, which typically recover a single or context-specific cost function, IO-LVM captures a distribution over cost functions, enabling the identification of diverse solution behaviors arising from different agents or conditions not available during the training process. We validate our method on real-world datasets of ship and taxi routes, as well as paths in synthetic graphs, demonstrating its ability to reconstruct paths and cycles, predict their distributions, and yield interpretable latent representations.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-14] Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15955">https://arxiv.org/abs/2509.15955</a><br>
<strong>作者</strong>: Zhangqi Jiang,Tingjin Luo,Xu Yang,Xinyan Liang<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  30 pages, 15 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:View missing remains a significant challenge in graph-based multi-view semi-supervised learning, hindering their real-world applications. To address this issue, traditional methods introduce a missing indicator matrix and focus on mining partial structure among existing samples in each view for label propagation (LP). However, we argue that these disregarded missing samples sometimes induce discontinuous local structures, i.e., sub-clusters, breaking the fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster Problem (SCP) would distort graph fusion and degrade classification performance. To alleviate SCP, we propose a novel incomplete multi-view semi-supervised learning method, termed AGF-TI. Firstly, we design an adversarial graph fusion scheme to learn a robust consensus graph against the distorted local structure through a min-max framework. By stacking all similarity matrices into a tensor, we further recover the incomplete structure from the high-order consistency information based on the low-rank tensor learning. Additionally, the anchor-based strategy is incorporated to reduce the computational complexity. An efficient alternative optimization algorithm combining a reduced gradient descent method is developed to solve the formulated objective, with theoretical convergence. Extensive experimental results on various datasets validate the superiority of our proposed AGF-TI as compared to state-of-the-art methods. Code is available at this https URL.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-15] argeted Fine-Tuning of DNN-Based Receivers via Influence Functions</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15950">https://arxiv.org/abs/2509.15950</a><br>
<strong>作者</strong>: Marko Tuononen,Heikki Penttinen,Ville Hautamäki<br>
<strong>类目</strong>: Machine Learning (cs.LG); Signal Processing (eess.SP)<br>
*<strong>备注</strong>:  7 pages; 10 figures; 1 table; 19 equations</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present the first use of influence functions for deep learning-based wireless receivers. Applied to DeepRx, a fully convolutional receiver, influence analysis reveals which training samples drive bit predictions, enabling targeted fine-tuning of poorly performing cases. We show that loss-relative influence with capacity-like binary cross-entropy loss and first-order updates on beneficial samples most consistently improves bit error rate toward genie-aided performance, outperforming random fine-tuning in single-target scenarios. Multi-target adaptation proved less effective, underscoring open challenges. Beyond experiments, we connect influence to self-influence corrections and propose a second-order, influence-aligned update strategy. Our results establish influence functions as both an interpretability tool and a basis for efficient receiver adaptation.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-16] UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15934">https://arxiv.org/abs/2509.15934</a><br>
<strong>作者</strong>: Mingdong Wu,Long Yang,Jin Liu,Weiyao Huang,Lehong Wu,Zelin Chen,Daolin Ma,Hao Dong<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate estimation of the in-hand pose of an object based on its CAD model is crucial in both industrial applications and everyday tasks, ranging from positioning workpieces and assembling components to seamlessly inserting devices like USB connectors. While existing methods often rely on regression, feature matching, or registration techniques, achieving high precision and generalizability to unseen CAD models remains a significant challenge. In this paper, we propose a novel three-stage framework for in-hand pose estimation. The first stage involves sampling and pre-ranking pose candidates, followed by iterative refinement of these candidates in the second stage. In the final stage, post-ranking is applied to identify the most likely pose candidates. These stages are governed by a unified energy-based diffusion model, which is trained solely on simulated data. This energy model simultaneously generates gradients to refine pose estimates and produces an energy scalar that quantifies the quality of the pose estimates. Additionally, borrowing the idea from the computer vision domain, we incorporate a render-compare architecture within the energy-based score network to significantly enhance sim-to-real performance, as demonstrated by our ablation studies. We conduct comprehensive experiments to show that our method outperforms conventional baselines based on regression, matching, and registration techniques, while also exhibiting strong intra-category generalization to previously unseen CAD models. Moreover, our approach integrates tactile object pose estimation, pose tracking, and uncertainty estimation into a unified framework, enabling robust performance across a variety of real-world conditions.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-17] Bayesian Physics Informed Neural Networks for Reliable Transformer Prognostics <mark class="hl-label red">ALT</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15933">https://arxiv.org/abs/2509.15933</a><br>
<strong>作者</strong>: Ibai Ramirez,Jokin Alcibar,Joel Pino,Mikel Sanz,David Pardo,Jose I. Aizpurua<br>
<strong>类目</strong>: Machine Learning (cs.LG); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
*<strong>备注</strong>:  Submitted to the Annual Prognostics and Health Management (PHM) Society Conference 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Scientific Machine Learning (SciML) integrates physics and data into the learning process, offering improved generalization compared with purely data-driven models. Despite its potential, applications of SciML in prognostics remain limited, partly due to the complexity of incorporating partial differential equations (PDEs) for ageing physics and the scarcity of robust uncertainty quantification methods. This work introduces a Bayesian Physics-Informed Neural Network (B-PINN) framework for probabilistic prognostics estimation. By embedding Bayesian Neural Networks into the PINN architecture, the proposed approach produces principled, uncertainty-aware predictions. The method is applied to a transformer ageing case study, where insulation degradation is primarily driven by thermal stress. The heat diffusion PDE is used as the physical residual, and different prior distributions are investigated to examine their impact on predictive posterior distributions and their ability to encode a priori physical knowledge. The framework is validated against a finite element model developed and tested with real measurements from a solar power plant. Results, benchmarked against a dropout-PINN baseline, show that the proposed B-PINN delivers more reliable prognostic predictions by accurately quantifying predictive uncertainty. This capability is crucial for supporting robust and informed maintenance decision-making in critical power assets.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-18] Improving Monte Carlo Tree Search for Symbolic Regression</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15929">https://arxiv.org/abs/2509.15929</a><br>
<strong>作者</strong>: Zhengyao Huang,Daniel Zhengyu Huang,Tiannan Xiao,Dina Ma,Zhenyu Ming,Hao Shi,Yuanhui Wen<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Symbolic regression aims to discover concise, interpretable mathematical expressions that satisfy desired objectives, such as fitting data, posing a highly combinatorial optimization problem. While genetic programming has been the dominant approach, recent efforts have explored reinforcement learning methods for improving search efficiency. Monte Carlo Tree Search (MCTS), with its ability to balance exploration and exploitation through guided search, has emerged as a promising technique for symbolic expression discovery. However, its traditional bandit strategies and sequential symbol construction often limit performance. In this work, we propose an improved MCTS framework for symbolic regression that addresses these limitations through two key innovations: (1) an extreme bandit allocation strategy tailored for identifying globally optimal expressions, with finite-time performance guarantees under polynomial reward decay assumptions; and (2) evolution-inspired state-jumping actions such as mutation and crossover, which enable non-local transitions to promising regions of the search space. These state-jumping actions also reshape the reward landscape during the search process, improving both robustness and efficiency. We conduct a thorough numerical study to the impact of these improvements and benchmark our approach against existing symbolic regression methods on a variety of datasets, including both ground-truth and black-box datasets. Our approach achieves competitive performance with state-of-the-art libraries in terms of recovery rate, attains favorable positions on the Pareto frontier of accuracy versus model complexity. Code is available at this https URL.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-19] A Flow-rate-conserving CNN-based Domain Decomposition Method for Blood Flow Simulations</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15900">https://arxiv.org/abs/2509.15900</a><br>
<strong>作者</strong>: Simon Klaes,Axel Klawonn,Natalie Kubicki,Martin Lanser,Kengo Nakajima,Takashi Shimokawabe,Janine Weber<br>
<strong>类目</strong>: Numerical Analysis (<a target="_blank" rel="noopener" href="http://math.NA">math.NA</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This work aims to predict blood flow with non-Newtonian viscosity in stenosed arteries using convolutional neural network (CNN) surrogate models. An alternating Schwarz domain decomposition method is proposed which uses CNN-based subdomain solvers. A universal subdomain solver (USDS) is trained on a single, fixed geometry and then applied for each subdomain solve in the Schwarz method. Results for two-dimensional stenotic arteries of varying shape and length for different inflow conditions are presented and statistically evaluated. One key finding, when using a limited amount of training data, is the need to implement a USDS which preserves some of the physics, as, in our case, flow rate conservation. A physics-aware approach outperforms purely data-driven USDS, delivering improved subdomain solutions and preventing overshooting or undershooting of the global solution during the Schwarz iterations, thereby leading to more reliable convergence.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-20] SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15865">https://arxiv.org/abs/2509.15865</a><br>
<strong>作者</strong>: Haoran Zhao,Tong Bai,Lei Huang,Xiaoyu Liang<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  5 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Diffusion models manifest evident benefits across diverse domains, yet their high sampling cost, requiring dozens of sequential model evaluations, remains a major limitation. Prior efforts mainly accelerate sampling via optimized solvers or distillation, which treat each query independently. In contrast, we reduce total number of steps by sharing early-stage sampling across semantically similar queries. To enable such efficiency gains without sacrificing quality, we propose SAGE, a semantic-aware shared sampling framework that integrates a shared sampling scheme for efficiency and a tailored training strategy for quality preservation. Extensive experiments show that SAGE reduces sampling cost by 25.5%, while improving generation quality with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over baselines.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-21] oFU: Transforming How Federated Learning Systems Forget User Data <mark class="hl-label red">ECAI-2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15861">https://arxiv.org/abs/2509.15861</a><br>
<strong>作者</strong>: Van-Tuan Tran,Hong-Hanh Nguyen-Le,Quoc-Viet Pham<br>
<strong>类目</strong>: Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)<br>
*<strong>备注</strong>:  ECAI-2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Neural networks unintentionally memorize training data, creating privacy risks in federated learning (FL) systems, such as inference and reconstruction attacks on sensitive data. To mitigate these risks and to comply with privacy regulations, Federated Unlearning (FU) has been introduced to enable participants in FL systems to remove their data’s influence from the global model. However, current FU methods primarily act post-hoc, struggling to efficiently erase information deeply memorized by neural networks. We argue that effective unlearning necessitates a paradigm shift: designing FL systems inherently amenable to forgetting. To this end, we propose a learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework that incorporates transformations during the learning process to reduce memorization of specific instances. Our theoretical analysis reveals how transformation composition provably bounds instance-specific information, directly simplifying subsequent unlearning. Crucially, ToFU can work as a plug-and-play framework that improves the performance of existing FU methods. Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU outperforms existing FU baselines, enhances performance when integrated with current methods, and reduces unlearning time.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-22] Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15858">https://arxiv.org/abs/2509.15858</a><br>
<strong>作者</strong>: Aysenur Kulunk,Berk Taskin,M. Furkan Eseoglu,H. Bahadir Sahin<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In large scale e-commerce marketplaces, duplicate product listings frequently cause consumer confusion and operational inefficiencies, degrading trust on the platform and increasing costs. Traditional keyword-based search methodologies falter in accurately identifying duplicates due to their reliance on exact textual matches, neglecting semantic similarities inherent in product titles. To address these challenges, we introduce a scalable, multimodal product deduplication designed specifically for the e-commerce domain. Our approach employs a domain-specific text model grounded in BERT architecture in conjunction with MaskedAutoEncoders for image representations. Both of these architectures are augmented with dimensionality reduction techniques to produce compact 128-dimensional embeddings without significant information loss. Complementing this, we also developed a novel decider model that leverages both text and image vectors. By integrating these feature extraction mechanisms with Milvus, an optimized vector database, our system can facilitate efficient and high-precision similarity searches across extensive product catalogs exceeding 200 million items with just 100GB of system RAM consumption. Empirical evaluations demonstrate that our matching system achieves a macro-average F1 score of 0.90, outperforming third-party solutions which attain an F1 score of 0.83. Our findings show the potential of combining domain-specific adaptations with state-of-the-art machine learning techniques to mitigate duplicate listings in large-scale e-commerce environments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-23] sururu: A Python-based Time Series Forecasting Strategies Library <mark class="hl-label red">IJCAI’25</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15843">https://arxiv.org/abs/2509.15843</a><br>
<strong>作者</strong>: Alina Kostromina,Kseniia Kuvshinova,Aleksandr Yugay,Andrey Savchenko,Dmitry Simakov<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted at IJCAI’25 Demo Track</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While current time series research focuses on developing new models, crucial questions of selecting an optimal approach for training such models are underexplored. Tsururu, a Python library introduced in this paper, bridges SoTA research and industry by enabling flexible combinations of global and multivariate approaches and multi-step-ahead forecasting strategies. It also enables seamless integration with various forecasting models. Available at this https URL .</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-24] HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15828">https://arxiv.org/abs/2509.15828</a><br>
<strong>作者</strong>: Ning Xu,Junkai Zhang,Yang Wu,Huigen Ye,Hua Xu,Huiling Xu,Yifan Zhang<br>
<strong>类目</strong>: Machine Learning (cs.LG); Discrete Mathematics (<a target="_blank" rel="noopener" href="http://cs.DM">cs.DM</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Directly solving large-scale Integer Linear Programs (ILPs) using traditional solvers is slow due to their NP-hard nature. While recent frameworks based on Large Neighborhood Search (LNS) can accelerate the solving process, their performance is often constrained by the difficulty in generating sufficiently effective neighborhoods. To address this challenge, we propose HyP-ASO, a hybrid policy-based adaptive search optimization framework that combines a customized formula with deep Reinforcement Learning (RL). The formula leverages feasible solutions to calculate the selection probabilities for each variable in the neighborhood generation process, and the RL policy network predicts the neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly outperforms existing LNS-based approaches for large-scale ILPs. Additional experiments show it is lightweight and highly scalable, making it well-suited for solving large-scale ILPs.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-25] SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15827">https://arxiv.org/abs/2509.15827</a><br>
<strong>作者</strong>: Baptiste Schubnel,Jelena Simeunović,Corentin Tissier,Pierre-Jean Alet,Rafael E. Carrillo<br>
<strong>类目</strong>: Machine Learning (cs.LG); Signal Processing (eess.SP)<br>
*<strong>备注</strong>:  15 pages, 17 figures, submitted to IEEE Transactions on Sustainable Energy</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate day-ahead forecasts of solar irradiance are required for the large-scale integration of solar photovoltaic (PV) systems into the power grid. However, current forecasting solutions lack the temporal and spatial resolution required by system operators. In this paper, we introduce SolarCrossFormer, a novel deep learning model for day-ahead irradiance forecasting, that combines satellite images and time series from a ground-based network of meteorological stations. SolarCrossFormer uses novel graph neural networks to exploit the inter- and intra-modal correlations of the input data and improve the accuracy and resolution of the forecasts. It generates probabilistic forecasts for any location in Switzerland with a 15-minute resolution for horizons up to 24 hours ahead. One of the key advantages of SolarCrossFormer its robustness in real life operations. It can incorporate new time-series data without retraining the model and, additionally, it can produce forecasts for locations without input data by using only their coordinates. Experimental results over a dataset of one year and 127 locations across Switzerland show that SolarCrossFormer yield a normalized mean absolute error of 6.1 % over the forecasting horizon. The results are competitive with those achieved by a commercial numerical weather prediction service.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-26] On the Convergence of Muon and Beyond</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15816">https://arxiv.org/abs/2509.15816</a><br>
<strong>作者</strong>: Da Chang,Yongxiang Liu,Ganzhao Yuan<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The Muon optimizer has demonstrated remarkable empirical success in handling matrix-structured parameters for training neural networks. However, a significant gap persists between its practical performance and theoretical understanding. Existing analyses indicate that the standard Muon variant achieves only a suboptimal convergence rate of  \mathcalO(T^-1/4)  in stochastic non-convex settings, where  T  denotes the number of iterations. To explore the theoretical limits of the Muon framework, we construct and analyze a variance-reduced variant, termed Muon-VR2. We provide the first rigorous proof that incorporating a variance-reduction mechanism enables Muon-VR2 to attain an optimal convergence rate of  \tilde\mathcalO(T^-1/3) , thereby matching the theoretical lower bound for this class of problems. Moreover, our analysis establishes convergence guarantees for Muon variants under the Polyak-Łojasiewicz (PŁ) condition. Extensive experiments on vision (CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on per-iteration convergence. Overall, this work provides the first proof of optimality for a Muon-style optimizer and clarifies the path toward developing more practically efficient, accelerated variants.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-27] hermalGuardian: Temperature-Aware Testing of Automotive Deep Learning Frameworks</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15815">https://arxiv.org/abs/2509.15815</a><br>
<strong>作者</strong>: Yinglong Zou,Juan Zhai,Chunrong Fang,Zhenyu Chen<br>
<strong>类目</strong>: Machine Learning (cs.LG); Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep learning models play a vital role in autonomous driving systems, supporting critical functions such as environmental perception. To accelerate model inference, these deep learning models’ deployment relies on automotive deep learning frameworks, for example, PaddleInference in Apollo and TensorRT in AutoWare. However, unlike deploying deep learning models on the cloud, vehicular environments experience extreme ambient temperatures varying from -40°C to 50°C, significantly impacting GPU temperature. Additionally, heats generated when computing further lead to the GPU temperature increase. These temperature fluctuations lead to dynamic GPU frequency adjustments through mechanisms such as DVFS. However, automotive deep learning frameworks are designed without considering the impact of temperature-induced frequency variations. When deployed on temperature-varying GPUs, these frameworks suffer critical quality issues: compute-intensive operators face delays or errors, high/mixed-precision operators suffer from precision errors, and time-series operators suffer from synchronization issues. The above quality issues cannot be detected by existing deep learning framework testing methods because they ignore temperature’s effect on the deep learning framework quality. To bridge this gap, we propose ThermalGuardian, the first automotive deep learning framework testing method under temperature-varying environments. Specifically, ThermalGuardian generates test input models using model mutation rules targeting temperature-sensitive operators, simulates GPU temperature fluctuations based on Newton’s law of cooling, and controls GPU frequency based on real-time GPU temperature.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-28] Generalization and Optimization of SGD with Lookahead</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15776">https://arxiv.org/abs/2509.15776</a><br>
<strong>作者</strong>: Kangcheng Li,Yunwen Lei<br>
<strong>类目</strong>: Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The Lookahead optimizer enhances deep learning models by employing a dual-weight update mechanism, which has been shown to improve the performance of underlying optimizers such as SGD. However, most theoretical studies focus on its convergence on training data, leaving its generalization capabilities less understood. Existing generalization analyses are often limited by restrictive assumptions, such as requiring the loss function to be globally Lipschitz continuous, and their bounds do not fully capture the relationship between optimization and generalization. In this paper, we address these issues by conducting a rigorous stability and generalization analysis of the Lookahead optimizer with minibatch SGD. We leverage on-average model stability to derive generalization bounds for both convex and strongly convex problems without the restrictive Lipschitzness assumption. Our analysis demonstrates a linear speedup with respect to the batch size in the convex setting.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-29] Learning to Optimize Capacity <mark class="hl-label green">Planning</mark>  in Semiconductor Manufacturing</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15767">https://arxiv.org/abs/2509.15767</a><br>
<strong>作者</strong>: Philipp Andelfinger,Jieyi Bi,Qiuyu Zhu,Jianan Zhou,Bo Zhang,Fei Fei Zhang,Chew Wye Chan,Boon Ping Gan,Wentong Cai,Jie Zhang<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In manufacturing, capacity planning is the process of allocating production resources in accordance with variable demand. The current industry practice in semiconductor manufacturing typically applies heuristic rules to prioritize actions, such as future change lists that account for incoming machine and recipe dedications. However, while offering interpretability, heuristics cannot easily account for the complex interactions along the process flow that can gradually lead to the formation of bottlenecks. Here, we present a neural network-based model for capacity planning on the level of individual machines, trained using deep reinforcement learning. By representing the policy using a heterogeneous graph neural network, the model directly captures the diverse relationships among machines and processing steps, allowing for proactive decision-making. We describe several measures taken to achieve sufficient scalability to tackle the vast space of possible machine-level actions. Our evaluation results cover Intel’s small-scale Minifab model and preliminary experiments using the popular SMT2020 testbed. In the largest tested scenario, our trained policy increases throughput and decreases cycle time by about 1.8% each.         Subjects:  Machine Learning (cs.LG)  Cite as: arXiv:2509.15767 [cs.LG]    (or  arXiv:2509.15767v1 [cs.LG] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15767">https://doi.org/10.48550/arXiv.2509.15767</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-30] Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15740">https://arxiv.org/abs/2509.15740</a><br>
<strong>作者</strong>: Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  The published version of this preprint can be accessed at <a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10874675">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Data-driven models accurately perform early battery prognosis to prevent equipment failure and further safety hazards. Most existing machine learning (ML) models work in offline mode which must consider their retraining post-deployment every time new data distribution is encountered. Hence, there is a need for an online ML approach where the model can adapt to varying distributions. However, existing online incremental multistep forecasts are a great challenge as there is no way to correct the model of its forecasts at the current instance. Also, these methods need to wait for a considerable amount of time to acquire enough streaming data before retraining. In this study, we propose iFSNet (incremental Fast and Slow learning Network) which is a modified version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep forecasting using pseudo targets. It uses a simple linear regressor of the input sequence to extrapolate pseudo future samples (pseudo targets) and calculate the loss from the rest of the forecast and keep updating the model. The model benefits from the associative memory and adaptive structure mechanisms of FSNet, at the same time the model incrementally improves by using pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on datasets with smooth degradation trajectories while it achieved 0.01588 RMSE and 0.01234 MAE on datasets having irregular degradation trajectories with capacity regeneration spikes.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-31] GUI-ReWalk: Massive Data Generation for GUI <mark class="hl-label green">Agent</mark>  via Stochastic Exploration and Intent-Aware <mark class="hl-label green">Reasoning</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15738">https://arxiv.org/abs/2509.15738</a><br>
<strong>作者</strong>: Musen Lin,Minghao Liu,Taoran Lu,Lichen Yuan,Yiwei Liu,Haonan Xu,Yu Miao,Yuhao Chao,Zhaojian Li<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Graphical User Interface (GUI) Agents, powered by large language and vision-language models, hold promise for enabling end-to-end automation in digital environments. However, their progress is fundamentally constrained by the scarcity of scalable, high-quality trajectory data. Existing data collection strategies either rely on costly and inconsistent manual annotations or on synthetic generation methods that trade off between diversity and meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a reasoning-enhanced, multi-stage framework for synthesizing realistic and diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase that emulates human trial-and-error behaviors, and progressively transitions into a reasoning-guided phase where inferred goals drive coherent and purposeful interactions. Moreover, it supports multi-stride task generation, enabling the construction of long-horizon workflows across multiple applications. By combining randomness for diversity with goal-aware reasoning for structure, GUI-ReWalk produces data that better reflects the intent-aware, adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results demonstrate that GUI-ReWalk enables superior coverage of diverse interaction flows, higher trajectory entropy, and more realistic user intent. These findings establish GUI-ReWalk as a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-32] Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15736">https://arxiv.org/abs/2509.15736</a><br>
<strong>作者</strong>: Gabriel Jarry,Ramon Dalmau,Philippe Very,Junzi Sun<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate modelling of aircraft fuel-flow is crucial for both operational planning and environmental impact assessment, yet standard parametric models often neglect performance deterioration that occurs as aircraft age. This paper investigates multiple approaches to integrate engine ageing effects into fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of approximately nineteen thousand Quick Access Recorder flights from nine distinct airframes with varying years in service. We systematically evaluate classical physics-based models, empirical correction coefficients, and data-driven neural network architectures that incorporate age either as an input feature or as an explicit multiplicative bias. Results demonstrate that while baseline models consistently underestimate fuel consumption for older aircraft, the use of age-dependent correction factors and neural models substantially reduces bias and improves prediction accuracy. Nevertheless, limitations arise from the small number of airframes and the lack of detailed maintenance event records, which constrain the representativeness and generalization of age-based corrections. This study emphasizes the importance of accounting for the effects of ageing in parametric and machine learning frameworks to improve the reliability of operational and environmental assessments. The study also highlights the need for more diverse datasets that can capture the complexity of real-world engine deterioration.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-33] EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in <mark class="hl-label green">LLM</mark> s and VLMs <mark class="hl-label red">ICASSP2026</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15735">https://arxiv.org/abs/2509.15735</a><br>
<strong>作者</strong>: Davide Ettori,Nastaran Darabi,Sina Tayebati,Ranganath Krishnan,Mahesh Subedar,Omesh Tickoo,Amit Ranjan Trivedi<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  5 pages, submitted to ICASSP 2026, September 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an interpretable real-time detector that uses the spectral geometry of hidden activations, a compact global signature of model dynamics. By streaming covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL divergence from random baselines into a lightweight recurrent classifier, EigenTrack tracks temporal shifts in representation structure that signal hallucination and OOD drift before surface errors appear. Unlike black- and grey-box methods, it needs only a single forward pass without resampling. Unlike existing white-box detectors, it preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-34] RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation <mark class="hl-label red">ICASSP2026</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15724">https://arxiv.org/abs/2509.15724</a><br>
<strong>作者</strong>: Davide Ettori,Nastaran Darabi,Sureshkumar Senthilkumar,Amit Ranjan Trivedi<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  5 pages, submitted to ICASSP 2026, September 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE, AG News, and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-35] Nonconvex Regularization for Feature Selection in Reinforcement Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15652">https://arxiv.org/abs/2509.15652</a><br>
<strong>作者</strong>: Kyohei Suzuki,Konstantinos Slavakis<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This work proposes an efficient batch algorithm for feature selection in reinforcement learning (RL) with theoretical convergence guarantees. To mitigate the estimation bias inherent in conventional regularization schemes, the first contribution extends policy evaluation within the classical least-squares temporal-difference (LSTD) framework by formulating a Bellman-residual objective regularized with the sparsity-inducing, nonconvex projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC penalty, this formulation can be interpreted as a special instance of a general nonmonotone-inclusion problem. The second contribution establishes novel convergence conditions for the forward-reflected-backward splitting (FRBS) algorithm to solve this class of problems. Numerical experiments on benchmark datasets demonstrate that the proposed approach substantially outperforms state-of-the-art feature-selection methods, particularly in scenarios with many noisy features.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-36] Efficient Extractive Text Summarization for Online News Articles Using Machine Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15614">https://arxiv.org/abs/2509.15614</a><br>
<strong>作者</strong>: Sajib Biswas,Milon Biswas,Arunima Mandal,Fatema Tabassum Liza,Joy Sarker<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In the age of information overload, content management for online news articles relies on efficient summarization to enhance accessibility and user engagement. This article addresses the challenge of extractive text summarization by employing advanced machine learning techniques to generate concise and coherent summaries while preserving the original meaning. Using the Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we developed a pipeline leveraging BERT embeddings to transform textual data into numerical representations. By framing the task as a binary classification problem, we explored various models, including logistic regression, feed-forward neural networks, and long short-term memory (LSTM) networks. Our findings demonstrate that LSTM networks, with their ability to capture sequential dependencies, outperform baseline methods like Lede-3 and simpler models in F1 score and ROUGE-1 metrics. This study underscores the potential of automated summarization in improving content management systems for online news platforms, enabling more efficient content organization and enhanced user experiences.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-37] Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15592">https://arxiv.org/abs/2509.15592</a><br>
<strong>作者</strong>: Jizhou Huang,Brendan Juba<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In machine learning applications, predictive models are trained to serve future queries across the entire data distribution. Real-world data often demands excessively complex models to achieve competitive performance, however, sacrificing interpretability. Hence, the growing deployment of machine learning models in high-stakes applications, such as healthcare, motivates the search for methods for accurate and explainable predictions. This work proposes a Personalized Prediction scheme, where an easy-to-interpret predictor is learned per query. In particular, we wish to produce a “sparse linear” classifier with competitive performance specifically on some sub-population that includes the query point. The goal of this work is to study the PAC-learnability of this prediction model for sub-populations represented by “halfspaces” in a label-agnostic setting. We first give a distribution-specific PAC-learning algorithm for learning reference classes for personalized prediction. By leveraging both the reference-class learning algorithm and a list learner of sparse linear representations, we prove the first upper bound,  O(\mathrmopt^1/4 ) , for personalized prediction with sparse linear classifiers and homogeneous halfspace subsets. We also evaluate our algorithms on a variety of standard benchmark data sets.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-38] How many classes do we need to see for novel class discovery? <mark class="hl-label red">CVPR2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15585">https://arxiv.org/abs/2509.15585</a><br>
<strong>作者</strong>: Akanksha Sarkar,Been Kim,Jennifer J. Sun<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  DG-EBF @ CVPR2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Novel class discovery is essential for ML models to adapt to evolving real-world data, with applications ranging from scientific discovery to robotics. However, these datasets contain complex and entangled factors of variation, making a systematic study of class discovery difficult. As a result, many fundamental questions are yet to be answered on why and when new class discoveries are more likely to be successful. To address this, we propose a simple controlled experimental framework using the dSprites dataset with procedurally generated modifying factors. This allows us to investigate what influences successful class discovery. In particular, we study the relationship between the number of known/unknown classes and discovery performance, as well as the impact of known class ‘coverage’ on discovering new classes. Our empirical results indicate that the benefit of the number of known classes reaches a saturation point beyond which discovery performance plateaus. The pattern of diminishing return across different settings provides an insight for cost-benefit analysis for practitioners and a starting point for more rigorous future research of class discovery on complex real-world datasets.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-39] Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15555">https://arxiv.org/abs/2509.15555</a><br>
<strong>作者</strong>: Rasil Baidar,Sasa Maric,Robert Abbas<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Machine Learning (cs.LG); Networking and Internet Architecture (<a target="_blank" rel="noopener" href="http://cs.NI">cs.NI</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The exponential expansion of IoT and 5G-Advanced applications has enlarged the attack surface for DDoS, malware, and zero-day intrusions. We propose an intrusion detection system that fuses a convolutional neural network (CNN), a bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch captures local and gated cross-feature interactions, while the AE emphasizes reconstruction-based anomaly sensitivity. Training occurs across edge devices without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC 99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced error rates with high precision and recall. Average inference time is approximately 0.0476 ms per sample on our test hardware, which is well within the less than 10 ms URLLC budget, supporting edge deployment. We also discuss explainability, drift tolerance, and FL considerations for compliant, scalable 5G-Advanced IoT security.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-40] he Multi-Query Paradox in Zeroth-Order Optimization</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15552">https://arxiv.org/abs/2509.15552</a><br>
<strong>作者</strong>: Wei Lin,Qingyu Song,Hong Xu<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Zeroth-order (ZO) optimization provides a powerful framework for problems where explicit gradients are unavailable and have to be approximated using only queries to function value. The prevalent single-query approach is simple, but suffers from high estimation variance, motivating a multi-query paradigm to improves estimation accuracy. This, however, creates a critical trade-off: under a fixed budget of queries (i.e. cost), queries per iteration and the total number of optimization iterations are inversely proportional to one another. How to best allocate this budget is a fundamental, under-explored question. This work systematically resolves this query allocation problem. We analyze two aggregation methods: the de facto simple averaging (ZO-Avg), and a new Projection Alignment method (ZO-Align) we derive from local surrogate minimization. By deriving convergence rates for both methods that make the dependence on the number of queries explicit across strongly convex, convex, non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg, we prove that using more than one query per iteration is always query-inefficient, rendering the single-query approach optimal. On the contrary, ZO-Align generally performs better with more queries per iteration, resulting in a full-subspace estimation as the optimal approach. Thus, our work clarifies that the multi-query problem boils down to a choice not about an intermediate query size, but between two classic algorithms, a choice dictated entirely by the aggregation method used. These theoretical findings are also consistently validated by extensive experiments.         Subjects:  Machine Learning (cs.LG)  Cite as: arXiv:2509.15552 [cs.LG]    (or  arXiv:2509.15552v1 [cs.LG] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15552">https://doi.org/10.48550/arXiv.2509.15552</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-41] PolyJuice Makes It Real: Black-Box Universal Red Teaming for Synthetic Image Detectors <mark class="hl-label red">NEURIPS2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15551">https://arxiv.org/abs/2509.15551</a><br>
<strong>作者</strong>: Sepehr Dehdashtian,Mashrur M. Morshed,Jacob H. Seidman,Gaurav Bharaj,Vishnu Naresh Boddeti<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted as NeurIPS 2025 poster</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Synthetic image detectors (SIDs) are a key defense against the risks posed by the growing realism of images from text-to-image (T2I) models. Red teaming improves SID’s effectiveness by identifying and exploiting their failure modes via misclassified synthetic images. However, existing red-teaming solutions (i) require white-box access to SIDs, which is infeasible for proprietary state-of-the-art detectors, and (ii) generate image-specific attacks through expensive online optimization. To address these limitations, we propose PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs, based on an observed distribution shift in the T2I latent space between samples correctly and incorrectly classified by the SID. PolyJuice generates attacks by (i) identifying the direction of this shift through a lightweight offline process that only requires black-box access to the SID, and (ii) exploiting this direction by universally steering all generated images towards the SID’s failure modes. PolyJuice-steered T2I models are significantly more effective at deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also show that the steering directions can be estimated efficiently at lower resolutions and transferred to higher resolutions using simple interpolation, reducing computational overhead. Finally, tuning SID models on PolyJuice-augmented datasets notably enhances the performance of the detectors (up to 30%).</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-42] Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15543">https://arxiv.org/abs/2509.15543</a><br>
<strong>作者</strong>: Xinwen Zhang,Yihan Zhang,Hongchang Gao<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Existing decentralized stochastic optimization methods assume the lower-level loss function is strongly convex and the stochastic gradient noise has finite variance. These strong assumptions typically are not satisfied in real-world machine learning models. To address these limitations, we develop a novel decentralized stochastic bilevel optimization algorithm for the nonconvex bilevel optimization problem under heavy-tailed noises. Specifically, we develop a normalized stochastic variance-reduced bilevel gradient descent algorithm, which does not rely on any clipping operation. Moreover, we establish its convergence rate by innovatively bounding interdependent gradient sequences under heavy-tailed noises for nonconvex decentralized bilevel optimization problems. As far as we know, this is the first decentralized bilevel optimization algorithm with rigorous theoretical guarantees under heavy-tailed noises. The extensive experimental results confirm the effectiveness of our algorithm in handling heavy-tailed noises.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-43] Geometric Integration for Neural Control Variates</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15538">https://arxiv.org/abs/2509.15538</a><br>
<strong>作者</strong>: Daniel Meister,Takahiro Harada<br>
<strong>类目</strong>: Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>); Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Control variates are a variance-reduction technique for Monte Carlo integration. The principle involves approximating the integrand by a function that can be analytically integrated, and integrating using the Monte Carlo method only the residual difference between the integrand and the approximation, to obtain an unbiased estimate. Neural networks are universal approximators that could potentially be used as a control variate. However, the challenge lies in the analytic integration, which is not possible in general. In this manuscript, we study one of the simplest neural network models, the multilayered perceptron (MLP) with continuous piecewise linear activation functions, and its possible analytic integration. We propose an integration method based on integration domain subdivision, employing techniques from computational geometry to solve this problem in 2D. We demonstrate that an MLP can be used as a control variate in combination with our integration method, showing applications in the light transport simulation.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-44] Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15533">https://arxiv.org/abs/2509.15533</a><br>
<strong>作者</strong>: Peter Amorese,Morteza Lahijanian<br>
<strong>类目</strong>: Machine Learning (cs.LG); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
*<strong>备注</strong>:  13 pages, 7 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Predicting the distribution of future states in a stochastic system, known as belief propagation, is fundamental to reasoning under uncertainty. However, nonlinear dynamics often make analytical belief propagation intractable, requiring approximate methods. When the system model is unknown and must be learned from data, a key question arises: can we learn a model that (i) universally approximates general nonlinear stochastic dynamics, and (ii) supports analytical belief propagation? This paper establishes the theoretical foundations for a class of models that satisfy both properties. The proposed approach combines the expressiveness of normalizing flows for density estimation with the analytical tractability of Bernstein polynomials. Empirical results show the efficacy of our learned model over state-of-the-art data-driven methods for belief propagation, especially for highly non-linear systems with non-additive, non-Gaussian noise.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-45] Fully Decentralized Cooperative Multi-<mark class="hl-label green">Agent</mark>  Reinforcement Learning is A Context Modeling Problem</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15519">https://arxiv.org/abs/2509.15519</a><br>
<strong>作者</strong>: Chao Li,Bingkun Bao,Yang Gao<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper studies fully decentralized cooperative multi-agent reinforcement learning, where each agent solely observes the states, its local actions, and the shared rewards. The inability to access other agents’ actions often leads to non-stationarity during value function updates and relative overgeneralization during value function estimation, hindering effective cooperative policy learning. However, existing works fail to address both issues simultaneously, due to their inability to model the joint policy of other agents in a fully decentralized setting. To overcome this limitation, we propose a novel method named Dynamics-Aware Context (DAC), which formalizes the task, as locally perceived by each agent, as an Contextual Markov Decision Process, and further addresses both non-stationarity and relative overgeneralization through dynamics-aware context modeling. Specifically, DAC attributes the non-stationary local task dynamics of each agent to switches between unobserved contexts, each corresponding to a distinct joint policy. Then, DAC models the step-wise dynamics distribution using latent variables and refers to them as contexts. For each agent, DAC introduces a context-based value function to address the non-stationarity issue during value function update. For value function estimation, an optimistic marginal value is derived to promote the selection of cooperative actions, thereby addressing the relative overgeneralization issue. Experimentally, we evaluate DAC on various cooperative tasks (including matrix game, predator and prey, and SMAC), and its superior performance against multiple baselines validates its effectiveness.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-46] Manifold Dimension Estimation: An Empirical Study</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15517">https://arxiv.org/abs/2509.15517</a><br>
<strong>作者</strong>: Zelong Bi,Pierre Lafaye de Micheaux<br>
<strong>类目</strong>: Machine Learning (cs.LG); Applications (stat.AP)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The manifold hypothesis suggests that high-dimensional data often lie on or near a low-dimensional manifold. Estimating the dimension of this manifold is essential for leveraging its structure, yet existing work on dimension estimation is fragmented and lacks systematic evaluation. This article provides a comprehensive survey for both researchers and practitioners. We review often-overlooked theoretical foundations and present eight representative estimators. Through controlled experiments, we analyze how individual factors such as noise, curvature, and sample size affect performance. We also compare the estimators on diverse synthetic and real-world datasets, introducing a principled approach to dataset-specific hyperparameter tuning. Our results offer practical guidance and suggest that, for a problem of this generality, simpler methods often perform better.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-47] KoopCast: Trajectory Forecasting via Koopman Operators</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15513">https://arxiv.org/abs/2509.15513</a><br>
<strong>作者</strong>: Jungjin Lee,Jaeuk Shin,Gihwan Kim,Joonho Han,Insoon Yang<br>
<strong>类目</strong>: Machine Learning (cs.LG); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present KoopCast, a lightweight yet efficient model for trajectory forecasting in general dynamic environments. Our approach leverages Koopman operator theory, which enables a linear representation of nonlinear dynamics by lifting trajectories into a higher-dimensional space. The framework follows a two-stage design: first, a probabilistic neural goal estimator predicts plausible long-term targets, specifying where to go; second, a Koopman operator-based refinement module incorporates intention and history into a nonlinear feature space, enabling linear prediction that dictates how to go. This dual structure not only ensures strong predictive accuracy but also inherits the favorable properties of linear operators while faithfully capturing nonlinear dynamics. As a result, our model offers three key advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman spectral theory, and (iii) low-latency deployment. We validate these benefits on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich multi-agent interactions and map-constrained nonlinear motion. Across benchmarks, KoopCast consistently delivers high predictive accuracy together with mode-level interpretability and practical efficiency.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-48] Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15509">https://arxiv.org/abs/2509.15509</a><br>
<strong>作者</strong>: Xiaoshuang Wang,Yifan Lin,Enlu Zhou<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Motivated by many application problems, we consider Markov decision processes (MDPs) with a general loss function and unknown parameters. To mitigate the epistemic uncertainty associated with unknown parameters, we take a Bayesian approach to estimate the parameters from data and impose a coherent risk functional (with respect to the Bayesian posterior distribution) on the loss. Since this formulation usually does not satisfy the interchangeability principle, it does not admit Bellman equations and cannot be solved by approaches based on dynamic programming. Therefore, We propose a policy gradient optimization method, leveraging the dual representation of coherent risk measures and extending the envelope theorem to continuous cases. We then show the stationary analysis of the algorithm with a convergence rate of  O(T^-1/2+r^-1/2) , where  T  is the number of policy gradient iterations and  r  is the sample size of the gradient estimator. We further extend our algorithm to an episodic setting, and establish the global convergence of the extended algorithm and provide bounds on the number of iterations needed to achieve an error bound  O(\epsilon)  in each episode.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-49] Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15498">https://arxiv.org/abs/2509.15498</a><br>
<strong>作者</strong>: Zahra Aref,Narayan B. Mandayam<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Transformers have emerged as a compelling architecture for sequential decision-making by modeling trajectories via self-attention. In reinforcement learning (RL), they enable return-conditioned control without relying on value function approximation. Decision Transformers (DTs) exploit this by casting RL as supervised sequence modeling, but they are restricted to offline data and lack exploration. Online Decision Transformers (ODTs) address this limitation through entropy-regularized training on on-policy rollouts, offering a stable alternative to traditional RL methods like Soft Actor-Critic, which depend on bootstrapped targets and reward shaping. Despite these advantages, ODTs use standard attention, which lacks explicit memory of action-specific outcomes. This leads to inefficiencies in learning long-term action effectiveness. Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we propose Experience-Weighted Attraction with Vector Quantization for Online Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains per-action mental accounts summarizing recent successes and failures. Continuous actions are routed via direct grid lookup to a compact vector-quantized codebook, where each code stores a scalar attraction updated online through decay and reward-based reinforcement. These attractions modulate attention by biasing the columns associated with action tokens, requiring no change to the backbone or training objective. On standard continuous-control benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT, particularly in early training. The module is computationally efficient, interpretable via per-code traces, and supported by theoretical guarantees that bound the attraction dynamics and its impact on attention drift.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-50] Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15494">https://arxiv.org/abs/2509.15494</a><br>
<strong>作者</strong>: Yuan Ni,Zhantao Chen,Cheng Peng,Rajan Plumley,Chun Hong Yoon,Jana B. Thayer,Joshua J. Turner<br>
<strong>类目</strong>: Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Implicit neural representations (INRs) have emerged as a compact and parametric alternative to discrete array-based data representations, encoding information directly in neural network weights to enable resolution-independent representation and memory efficiency. However, existing INR approaches, when constrained to compact network sizes, struggle to faithfully represent the multi-scale structures, high-frequency information, and fine textures that characterize the majority of scientific datasets. To address this limitation, we propose WIEN-INR, a wavelet-informed implicit neural representation that distributes modeling across different resolution scales and employs a specialized kernel network at the finest scale to recover subtle details. This multi-scale architecture allows for the use of smaller networks to retain the full spectrum of information while preserving the training efficiency and reducing storage cost. Through extensive experiments on diverse scientific datasets spanning different scales and structural complexities, WIEN-INR achieves superior reconstruction fidelity while maintaining a compact model size. These results demonstrate WIEN-INR as a practical neural representation framework for high-fidelity scientific data encoding, extending the applicability of INRs to domains where efficient preservation of fine detail is essential.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-51] FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15493">https://arxiv.org/abs/2509.15493</a><br>
<strong>作者</strong>: Robson L. F. Cordeiro,Meng-Chieh Lee,Christos Faloutsos<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Given a set of financial transactions (who buys from whom, when, and for how much), as well as prior information from buyers and sellers, how can we find fraudulent transactions? If we have labels for some transactions for known types of fraud, we can build a classifier. However, we also want to find new types of fraud, still unknown to the domain experts (‘Detection’). Moreover, we also want to provide evidence to experts that supports our opinion (‘Justification’). In this paper, we propose FRAUDGUESS, to achieve two goals: (a) for ‘Detection’, it spots new types of fraud as micro-clusters in a carefully designed feature space; (b) for ‘Justification’, it uses visualization and heatmaps for evidence, as well as an interactive dashboard for deep dives. FRAUDGUESS is used in real life and is currently considered for deployment in an Anonymous Financial Institution (AFI). Thus, we also present the three new behaviors that FRAUDGUESS discovered in a real, million-scale financial dataset. Two of these behaviors are deemed fraudulent or suspicious by domain experts, catching hundreds of fraudulent transactions that would otherwise go un-noticed.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-52] Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies <mark class="hl-label red">CIKM2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15481">https://arxiv.org/abs/2509.15481</a><br>
<strong>作者</strong>: Yanan Niu,Demetri Psaltis,Christophe Moser,Luisa Lambertini<br>
<strong>类目</strong>: Machine Learning (cs.LG); Social and Information Networks (<a target="_blank" rel="noopener" href="http://cs.SI">cs.SI</a>)<br>
*<strong>备注</strong>:  Accepted to CIKM 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate solar forecasting underpins effective renewable energy management. We present SolarCAST, a causally informed model predicting future global horizontal irradiance (GHI) at a target site using only historical GHI from site X and nearby stations S - unlike prior work that relies on sky-camera or satellite imagery requiring specialized hardware and heavy preprocessing. To deliver high accuracy with only public sensor data, SolarCAST models three classes of confounding factors behind X-S correlations using scalable neural components: (i) observable synchronous variables (e.g., time of day, station identity), handled via an embedding module; (ii) latent synchronous factors (e.g., regional weather patterns), captured by a spatio-temporal graph neural network; and (iii) time-lagged influences (e.g., cloud movement across stations), modeled with a gated transformer that learns temporal shifts. It outperforms leading time-series and multimodal baselines across diverse geographical conditions, and achieves a 25.9% error reduction over the top commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and generalizable solution for localized solar forecasting.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-53] mporal <mark class="hl-label green">Reasoning</mark>  with <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  Augmented by Evolving Knowledge Graphs</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15464">https://arxiv.org/abs/2509.15464</a><br>
<strong>作者</strong>: Junhong Lin,Song Wang,Xiaojie Guo,Julian Shun,Yada Zhu<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) excel at many language understanding tasks but struggle to reason over knowledge that evolves. To address this, recent work has explored augmenting LLMs with knowledge graphs (KGs) to provide structured, up-to-date information. However, many existing approaches assume a static snapshot of the KG and overlook the temporal dynamics and factual inconsistencies inherent in real-world data. To address the challenge of reasoning over temporally shifting knowledge, we propose EvoReasoner, a temporal-aware multi-hop reasoning algorithm that performs global-local entity grounding, multi-route decomposition, and temporally grounded scoring. To ensure that the underlying KG remains accurate and up-to-date, we introduce EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG from unstructured documents through confidence-based contradiction resolution and temporal trend tracking. We evaluate our approach on temporal QA benchmarks and a novel end-to-end setting where the KG is dynamically updated from raw documents. Our method outperforms both prompting-based and KG-enhanced baselines, effectively narrowing the gap between small and large LLMs on dynamic question answering. Notably, an 8B-parameter model using our approach matches the performance of a 671B model prompted seven months later. These results highlight the importance of combining temporal reasoning with KG evolution for robust and up-to-date LLM performance. Our code is publicly available at this http URL.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-54] IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for <mark class="hl-label green">LLM</mark> s</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15455">https://arxiv.org/abs/2509.15455</a><br>
<strong>作者</strong>: Junchen Zhao,Ali Derakhshan,Dushyant Bharadwaj,Jayden Kana Hyman,Junhao Dong,Sangeetha Abdu Jyothi,Ian Harris<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) promise impressive capabilities, yet their multi-billion-parameter scale makes on-device or low-resource deployment prohibitive. Mixed-precision quantization offers a compelling solution, but existing methods struggle when the average precision drops below four bits, as they rely on isolated, layer-specific metrics that overlook critical inter-layer interactions affecting overall performance. In this paper, we propose two innovations to address these limitations. First, we frame the mixed-precision quantization problem as a cooperative game among layers and introduce Shapley-based Progressive Quantization Estimation (SPQE) to efficiently obtain accurate Shapley estimates of layer sensitivities and inter-layer interactions. Second, building upon SPQE, we propose Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these Shapley estimates into a binary quadratic optimization formulation, assigning either 2 or 4-bit precision to layers under strict memory constraints. Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ’s scalability and consistently superior performance compared to methods relying solely on isolated metrics. Across average precisions spanning 4 bit down to 2 bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline, with the margin growing as the bit-width tightens.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-55] Computing Linear Regions in Neural Networks with Skip Connections</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15441">https://arxiv.org/abs/2509.15441</a><br>
<strong>作者</strong>: Johnny Joyce,Jan Verschelde<br>
<strong>类目</strong>: Machine Learning (cs.LG); Symbolic Computation (<a target="_blank" rel="noopener" href="http://cs.SC">cs.SC</a>)<br>
*<strong>备注</strong>:  Accepted for publication in the proceedings in Computer Algebra in Scientific Computing 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Neural networks are important tools in machine learning. Representing piecewise linear activation functions with tropical arithmetic enables the application of tropical geometry. Algorithms are presented to compute regions where the neural networks are linear maps. Through computational experiments, we provide insights on the difficulty to train neural networks, in particular on the problems of overfitting and on the benefits of skip connections.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-56] Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15429">https://arxiv.org/abs/2509.15429</a><br>
<strong>作者</strong>: Victor Chardès<br>
<strong>类目</strong>: Machine Learning (cs.LG); Biological Physics (physics.bio-ph); Quantitative Methods (q-bio.QM)<br>
*<strong>备注</strong>:  16 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Single-cell RNA-seq provides detailed molecular snapshots of individual cells but is notoriously noisy. Variability stems from biological differences, PCR amplification bias, limited sequencing depth, and low capture efficiency, making it challenging to adapt computational pipelines to heterogeneous datasets or evolving technologies. As a result, most studies still rely on principal component analysis (PCA) for dimensionality reduction, valued for its interpretability and robustness. Here, we improve upon PCA with a Random Matrix Theory (RMT)-based approach that guides the inference of sparse principal components using existing sparse PCA algorithms. We first introduce a novel biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that simultaneously stabilizes variance across genes and cells. This enables the use of an RMT-based criterion to automatically select the sparsity level, rendering sparse PCA nearly parameter-free. Our mathematically grounded approach retains the interpretability of PCA while enabling robust, hands-off inference of sparse principal components. Across seven single-cell RNA-seq technologies and four sparse PCA algorithms, we show that this method systematically improves the reconstruction of the principal subspace and consistently outperforms PCA-, autoencoder-, and diffusion-based methods in cell-type classification tasks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-57] op-k Feature Importance Ranking</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15420">https://arxiv.org/abs/2509.15420</a><br>
<strong>作者</strong>: Yuxi Chen,Tiffany Tang,Genevera Allen<br>
<strong>类目</strong>: Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate ranking of important features is a fundamental challenge in interpretable machine learning with critical applications in scientific discovery and decision-making. Unlike feature selection and feature importance, the specific problem of ranking important features has received considerably less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And Recursive Trimming), a framework that utilizes any existing feature importance measure in a novel algorithm specifically tailored for ranking the top- k  features. Our approach combines an adaptive sequential halving strategy that progressively focuses computational resources on promising features with an efficient ensembling technique using both observation and feature subsampling. Unlike existing methods that convert importance scores to ranks as post-processing, our framework explicitly optimizes for ranking accuracy. We provide theoretical guarantees showing that RAMPART achieves the correct top- k  ranking with high probability under mild conditions, and demonstrate through extensive simulation studies that RAMPART consistently outperforms popular feature importance methods, concluding with a high-dimensional genomics case study.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-58] Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization <mark class="hl-label red">NEURIPS2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15399">https://arxiv.org/abs/2509.15399</a><br>
<strong>作者</strong>: Xiaochuan Gong,Jie Hao,Mingrui Liu<br>
<strong>类目</strong>: Machine Learning (cs.LG); Optimization and Control (math.OC)<br>
*<strong>备注</strong>:  NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Hierarchical optimization refers to problems with interdependent decision variables and objectives, such as minimax and bilevel formulations. While various algorithms have been proposed, existing methods and analyses lack adaptivity in stochastic optimization settings: they cannot achieve optimal convergence rates across a wide spectrum of gradient noise levels without prior knowledge of the noise magnitude. In this paper, we propose novel adaptive algorithms for two important classes of stochastic hierarchical optimization problems: nonconvex-strongly-concave minimax optimization and nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp convergence rates of  \widetildeO(1/\sqrtT + \sqrt\bar\sigma/T^1/4)  in  T  iterations for the gradient norm, where  \bar\sigma  is an upper bound on the stochastic gradient noise. Notably, these rates are obtained without prior knowledge of the noise level, thereby enabling automatic adaptivity in both low and high-noise regimes. To our knowledge, this work provides the first adaptive and sharp convergence guarantees for stochastic hierarchical optimization. Our algorithm design combines the momentum normalization technique with novel adaptive parameter choices. Extensive experiments on synthetic and deep learning tasks demonstrate the effectiveness of our proposed algorithms.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-59] VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15394">https://arxiv.org/abs/2509.15394</a><br>
<strong>作者</strong>: Weibin Feng,Ran Tao,John Cartlidge,Jin Zheng<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  5 pages, 1 figure, 2 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In time series forecasting, capturing recurrent temporal patterns is essential; decomposition techniques make such structure explicit and thereby improve predictive performance. Variational Mode Decomposition (VMD) is a powerful signal-processing method for periodicity-aware decomposition and has seen growing adoption in recent years. However, existing studies often suffer from information leakage and rely on inappropriate hyperparameter tuning. To address these issues, we propose VMDNet, a causality-preserving framework that (i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed mode with frequency-aware embeddings and decodes it using parallel temporal convolutional networks (TCNs), ensuring mode independence and efficient learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to adaptively select VMD’s two core hyperparameters: the number of modes (K) and the bandwidth penalty (alpha). Experiments on two energy-related datasets demonstrate that VMDNet achieves state-of-the-art results when periodicity is strong, showing clear advantages in capturing structured periodic patterns while remaining robust under weak periodicity.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-60] Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15392">https://arxiv.org/abs/2509.15392</a><br>
<strong>作者</strong>: Sihan Zeng,Benjamin Patrick Evans,Sujay Bhatt,Leo Ardon,Sumitra Ganesh,Alec Koppel<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We study policy optimization in Stackelberg mean field games (MFGs), a hierarchical framework for modeling the strategic interaction between a single leader and an infinitely large population of homogeneous followers. The objective can be formulated as a structured bi-level optimization problem, in which the leader needs to learn a policy maximizing its reward, anticipating the response of the followers. Existing methods for solving these (and related) problems often rely on restrictive independence assumptions between the leader’s and followers’ objectives, use samples inefficiently due to nested-loop algorithm structure, and lack finite-time convergence guarantees. To address these limitations, we propose AC-SMFG, a single-loop actor-critic algorithm that operates on continuously generated Markovian samples. The algorithm alternates between (semi-)gradient updates for the leader, a representative follower, and the mean field, and is simple to implement in practice. We establish the finite-time and finite-sample convergence of the algorithm to a stationary point of the Stackelberg objective. To our knowledge, this is the first Stackelberg MFG algorithm with non-asymptotic convergence guarantees. Our key assumption is a “gradient alignment” condition, which requires that the full policy gradient of the leader can be approximated by a partial component of it, relaxing the existing leader-follower independence assumption. Simulation results in a range of well-established economics environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG learning baselines in policy quality and convergence speed.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-61] Adversarial generalization of unfolding (model-based) networks <mark class="hl-label red">NEURIPS2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15370">https://arxiv.org/abs/2509.15370</a><br>
<strong>作者</strong>: Vicky Kouni<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted in NeurIPS2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Unfolding networks are interpretable networks emerging from iterative algorithms, incorporate prior knowledge of data structure, and are designed to solve inverse problems like compressed sensing, which deals with recovering data from noisy, missing observations. Compressed sensing finds applications in critical domains, from medical imaging to cryptography, where adversarial robustness is crucial to prevent catastrophic failures. However, a solid theoretical understanding of the performance of unfolding networks in the presence of adversarial attacks is still in its infancy. In this paper, we study the adversarial generalization of unfolding networks when perturbed with  l_2 -norm constrained attacks, generated by the fast gradient sign method. Particularly, we choose a family of state-of-the-art overaparameterized unfolding networks and deploy a new framework to estimate their adversarial Rademacher complexity. Given this estimate, we provide adversarial generalization error bounds for the networks under study, which are tight with respect to the attack level. To our knowledge, this is the first theoretical analysis on the adversarial generalization of unfolding networks. We further present a series of experiments on real-world data, with results corroborating our derived theory, consistently for all data. Finally, we observe that the family’s overparameterization can be exploited to promote adversarial robustness, shedding light on how to efficiently robustify neural networks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-62] Stochastic Sample Approximations of (Local) Moduli of Continuity</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15368">https://arxiv.org/abs/2509.15368</a><br>
<strong>作者</strong>: Rodion Nazarov,Allen Gehret,Robert Shorten,Jakub Marecek<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Modulus of local continuity is used to evaluate the robustness of neural networks and fairness of their repeated uses in closed-loop models. Here, we revisit a connection between generalized derivatives and moduli of local continuity, and present a non-uniform stochastic sample approximation for moduli of local continuity. This is of importance in studying robustness of neural networks and fairness of their repeated uses.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-63] Predicting Language Models Success at Zero-Shot Probabilistic Prediction <mark class="hl-label red">KR36</mark> <mark class="hl-label red">EMNLP</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15356">https://arxiv.org/abs/2509.15356</a><br>
<strong>作者</strong>: Kevin Ren,Santiago Cortes-Gomez,Carlos Miguel Patiño,Ananya Joshi,Ruiqi Lyu,Jingjing Tang,Alistair Turcan,Khurram Yamin,Steven Wu,Bryan Wilder<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  EMNLP Findings 2025. We release our code at: <a target="_blank" rel="noopener" href="https://github.com/kkr36/llm-eval/tree/camera-ready">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent work has investigated the capabilities of large language models (LLMs) as zero-shot models for generating individual-level characteristics (e.g., to serve as risk models or augment survey datasets). However, when should a user have confidence that an LLM will provide high-quality predictions for their particular task? To address this question, we conduct a large-scale empirical study of LLMs’ zero-shot predictive capabilities across a wide range of tabular prediction tasks. We find that LLMs’ performance is highly variable, both on tasks within the same dataset and across different datasets. However, when the LLM performs well on the base prediction task, its predicted probabilities become a stronger signal for individual-level accuracy. Then, we construct metrics to predict LLMs’ performance at the task level, aiming to distinguish between tasks where LLMs may perform well and where they are likely unsuitable. We find that some of these metrics, each of which are assessed without labeled data, yield strong signals of LLMs’ predictive performance on new tasks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-64] Probabilistic Conformal Cove<mark class="hl-label green">rag</mark> e Guarantees in Small-Data Settings</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15349">https://arxiv.org/abs/2509.15349</a><br>
<strong>作者</strong>: Petrus H. Zwart<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Conformal prediction provides distribution-free prediction sets with guaranteed marginal coverage. However, in split conformal prediction this guarantee is training-conditional only in expectation: across many calibration draws, the average coverage equals the nominal level, but the realized coverage for a single calibration set may vary substantially. This variance undermines effective risk control in practical applications. Here we introduce the Small Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal significance level that leverages the exact finite-sample distribution of conformal coverage to provide probabilistic guarantees, ensuring that with user-defined probability over the calibration draw, the deployed predictor achieves at least the desired coverage.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-65] Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15316">https://arxiv.org/abs/2509.15316</a><br>
<strong>作者</strong>: Giorgos Armeniakos,Theodoros Mantzakidis,Dimitrios Soudris<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted for publication by 25th International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Printed Electronics (PE) provide a flexible, cost-efficient alternative to silicon for implementing machine learning (ML) circuits, but their large feature sizes limit classifier complexity. Leveraging PE’s low fabrication and NRE costs, designers can tailor hardware to specific ML models, simplifying circuit design. This work explores alternative arithmetic and proposes a hybrid unary-binary architecture that removes costly encoders and enables efficient, multiplier-less execution of MLP classifiers. We also introduce architecture-aware training to further improve area and power efficiency. Evaluation on six datasets shows average reductions of 46% in area and 39% in power, with minimal accuracy loss, surpassing other state-of-the-art MLP designs.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-66] Copycat vs. Original: Multi-modal Pretraining and Variable Importance in Box-office Prediction</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15277">https://arxiv.org/abs/2509.15277</a><br>
<strong>作者</strong>: Qin Chao,Eunsoo Kim,Boyang Li<br>
<strong>类目</strong>: Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The movie industry is associated with an elevated level of risk, which necessitates the use of automated tools to predict box-office revenue and facilitate human decision-making. In this study, we build a sophisticated multimodal neural network that predicts box offices by grounding crowdsourced descriptive keywords of each movie in the visual information of the movie posters, thereby enhancing the learned keyword representations, resulting in a substantial reduction of 14.5% in box-office prediction error. The advanced revenue prediction model enables the analysis of the commercial viability of “copycat movies,” or movies with substantial similarity to successful movies released recently. We do so by computing the influence of copycat features in box-office prediction. We find a positive relationship between copycat status and movie revenue. However, this effect diminishes when the number of similar movies and the similarity of their content increase. Overall, our work develops sophisticated deep learning tools for studying the movie industry and provides valuable business insight.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-67] A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15266">https://arxiv.org/abs/2509.15266</a><br>
<strong>作者</strong>: Lucía Prieto-Santamaría,Alba Cortés Iglesias,Claudio Vidal Giné,Fermín Fernández Calderón,Óscar M. Lozano,Alejandro Rodríguez-González<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Understanding the real-world effects of recreational drug use remains a critical challenge in public health and biomedical research, especially as traditional surveillance systems often underrepresent user experiences. In this study, we leverage social media (specifically Twitter) as a rich and unfiltered source of user-reported effects associated with three emerging psychoactive substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms with biomedical concept extraction via MetaMap, we identified and weakly annotated over 92,000 tweets mentioning these substances. Each tweet was labeled with a polarity reflecting whether it reported a positive or negative effect, following an expert-guided heuristic process. We then performed descriptive and comparative analyses of the reported phenotypic outcomes across substances and trained multiple machine learning classifiers to predict polarity from tweet content, accounting for strong class imbalance using techniques such as cost-sensitive learning and synthetic oversampling. The top performance on the test set was obtained from eXtreme Gradient Boosting with cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that Twitter enables the detection of substance-specific phenotypic effects, and that polarity classification models can support real-time pharmacovigilance and drug effect characterization with high accuracy.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-68] Subject Matter Expertise vs Professional Management in Collective Sequential Decision Making</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15263">https://arxiv.org/abs/2509.15263</a><br>
<strong>作者</strong>: David Shoresh,Yonatan Loewenstein<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Reinforcement Learning and Decision Making (RLDM) 2025. arXiv admin note: substantial text overlap with <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.18593">arXiv:2412.18593</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Your company’s CEO is retiring. You search for a successor. You can promote an employee from the company familiar with the company’s operations, or recruit an external professional manager. Who should you prefer? It has not been clear how to address this question, the “subject matter expertise vs. professional manager debate”, quantitatively and objectively. We note that a company’s success depends on long sequences of interdependent decisions, with often-opposing recommendations of diverse board members. To model this task in a controlled environment, we utilize chess - a complex, sequential game with interdependent decisions which allows for quantitative analysis of performance and expertise (since the states, actions and game outcomes are well-defined). The availability of chess engines differing in style and expertise, allows scalable experimentation. We considered a team of (computer) chess players. At each turn, team members recommend a move and a manager chooses a recommendation. We compared the performance of two manager types. For manager as “subject matter expert”, we used another (computer) chess player that assesses the recommendations of the team members based on its own chess expertise. We examined the performance of such managers at different strength levels. To model a “professional manager”, we used Reinforcement Learning (RL) to train a network that identifies the board positions in which different team members have relative advantage, without any pretraining in chess. We further examined this network to see if any chess knowledge is acquired implicitly. We found that subject matter expertise beyond a minimal threshold does not significantly contribute to team synergy. Moreover, performance of a RL-trained “professional” manager significantly exceeds that of even the best “expert” managers, while acquiring only limited understanding of chess.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-69] Quantum Generative Adversarial Autoencoders: Learning latent representations for quantum data generation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16186">https://arxiv.org/abs/2509.16186</a><br>
<strong>作者</strong>: Naipunnya Raj,Rajiv Sangle,Avinash Singh,Krishna Kumar Sabapathy<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:  27 pages, 28 figures, 4 tables, 1 algorithm</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this work, we introduce the Quantum Generative Adversarial Autoencoder (QGAA), a quantum model for generation of quantum data. The QGAA consists of two components: (a) Quantum Autoencoder (QAE) to compress quantum states, and (b) Quantum Generative Adversarial Network (QGAN) to learn the latent space of the trained QAE. This approach imparts the QAE with generative capabilities. The utility of QGAA is demonstrated in two representative scenarios: (a) generation of pure entangled states, and (b) generation of parameterized molecular ground states for H _2  and LiH. The average errors in the energies estimated by the trained QGAA are 0.02 Ha for H _2  and 0.06 Ha for LiH in simulations upto 6 qubits. These results illustrate the potential of QGAA for quantum state generation, quantum chemistry, and near-term quantum machine learning applications.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-70] What is a good matching of probability measures? A counterfactual lens on transport maps</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16027">https://arxiv.org/abs/2509.16027</a><br>
<strong>作者</strong>: Lucas De Lara,Luca Ganassali<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG); Statistics Theory (<a target="_blank" rel="noopener" href="http://math.ST">math.ST</a>); Methodology (<a target="_blank" rel="noopener" href="http://stat.ME">stat.ME</a>)<br>
*<strong>备注</strong>:  37 pages; comments most welcome</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Coupling probability measures lies at the core of many problems in statistics and machine learning, from domain adaptation to transfer learning and causal inference. Yet, even when restricted to deterministic transports, such couplings are not identifiable: two atomless marginals admit infinitely many transport maps. The common recourse to optimal transport, motivated by cost minimization and cyclical monotonicity, obscures the fact that several distinct notions of multivariate monotone matchings coexist. In this work, we first carry a comparative analysis of three constructions of transport maps: cyclically monotone, quantile-preserving and triangular monotone maps. We establish necessary and sufficient conditions for their equivalence, thereby clarifying their respective structural properties. In parallel, we formulate counterfactual reasoning within the framework of structural causal models as a problem of selecting transport maps between fixed marginals, which makes explicit the role of untestable assumptions in counterfactual reasoning. Then, we are able to connect these two perspectives by identifying conditions on causal graphs and structural equations under which counterfactual maps coincide with classical statistical transports. In this way, we delineate the circumstances in which causal assumptions support the use of a specific structure of transport map. Taken together, our results aim to enrich the theoretical understanding of families of transport maps and to clarify their possible causal interpretations. We hope this work contributes to establishing new bridges between statistical transport and causal inference.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-71] Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and Grover-Based Trajectory Optimization</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16002">https://arxiv.org/abs/2509.16002</a><br>
<strong>作者</strong>: Thet Htar Su,Shaswot Shresthamali,Masaaki Kondo<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:A fully quantum reinforcement learning framework is developed that integrates a quantum Markov decision process, dynamic circuit-based qubit reuse, and Grover’s algorithm for trajectory optimization. The framework encodes states, actions, rewards, and transitions entirely within the quantum domain, enabling parallel exploration of state-action sequences through superposition and eliminating classical subroutines. Dynamic circuit operations, including mid-circuit measurement and reset, allow reuse of the same physical qubits across multiple agent-environment interactions, reducing qubit requirements from 7*T to 7 for T time steps while preserving logical continuity. Quantum arithmetic computes trajectory returns, and Grover’s search is applied to the superposition of these evaluated trajectories to amplify the probability of measuring those with the highest return, thereby accelerating the identification of the optimal policy. Simulations demonstrate that the dynamic-circuit-based implementation preserves trajectory fidelity while reducing qubit usage by 66 percent relative to the static design. Experimental deployment on IBM Heron-class quantum hardware confirms that the framework operates within the constraints of current quantum processors and validates the feasibility of fully quantum multi-step reinforcement learning under noisy intermediate-scale quantum conditions. This framework advances the scalability and practical application of quantum reinforcement learning for large-scale sequential decision-making tasks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-72] Quantum Enhanced Anomaly Detection for ADS-B Data using Hybrid Deep Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15991">https://arxiv.org/abs/2509.15991</a><br>
<strong>作者</strong>: Rani Naaman,Felipe Gohring de Magalhaes,Jean-Yves Ouattara,Gabriela Nicolescu<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  This is the author’s version of the work accepted for publication in the IEEE-AIAA Digital Avionics Systems Conference (DASC) 2025. The final version will be available via IEEE Xplore</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The emerging field of Quantum Machine Learning (QML) has shown promising advantages in accelerating processing speed and effectively handling the high dimensionality associated with complex datasets. Quantum Computing (QC) enables more efficient data manipulation through the quantum properties of superposition and entanglement. In this paper, we present a novel approach combining quantum and classical machine learning techniques to explore the impact of quantum properties for anomaly detection in Automatic Dependent Surveillance-Broadcast (ADS-B) data. We compare the performance of a Hybrid-Fully Connected Quantum Neural Network (H-FQNN) with different loss functions and use a publicly available ADS-B dataset to evaluate the performance. The results demonstrate competitive performance in detecting anomalies, with accuracies ranging from 90.17% to 94.05%, comparable to the performance of a traditional Fully Connected Neural Network (FNN) model, which achieved accuracies between 91.50% and 93.37%.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-73] Model-free algorithms for fast node clustering in SBM type graphs and application to social role inference in animals</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15989">https://arxiv.org/abs/2509.15989</a><br>
<strong>作者</strong>: Bertrand Cloez,Adrien Cotil,Jean-Baptiste Menassol,Nicolas Verzelen<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We propose a novel family of model-free algorithms for node clustering and parameter inference in graphs generated from the Stochastic Block Model (SBM), a fundamental framework in community detection. Drawing inspiration from the Lloyd algorithm for the  k -means problem, our approach extends to SBMs with general edge weight distributions. We establish the consistency of our estimator under a natural identifiability condition. Through extensive numerical experiments, we benchmark our methods against state-of-the-art techniques, demonstrating significantly faster computation times with the lower order of estimation error. Finally, we validate the practical relevance of our algorithms by applying them to empirical network data from behavioral ecology.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-74] Phase Transition for Stochastic Block Model with more than sqrtn Communities</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15822">https://arxiv.org/abs/2509.15822</a><br>
<strong>作者</strong>: Alexandra Carpentier,Christophe Giraud,Nicolas Verzelen<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG); Probability (<a target="_blank" rel="noopener" href="http://math.PR">math.PR</a>); Statistics Theory (<a target="_blank" rel="noopener" href="http://math.ST">math.ST</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Predictions from statistical physics postulate that recovery of the communities in Stochastic Block Model (SBM) is possible in polynomial time above, and only above, the Kesten-Stigum (KS) threshold. This conjecture has given rise to a rich literature, proving that non-trivial community recovery is indeed possible in SBM above the KS threshold, as long as the number  K  of communities remains smaller than  \sqrtn , where  n  is the number of nodes in the observed graph. Failure of low-degree polynomials below the KS threshold was also proven when  K=o(\sqrtn) . When  K\geq \sqrtn , Chin et al.(2025) recently prove that, in a sparse regime, community recovery in polynomial time is possible below the KS threshold by counting non-backtracking paths. This breakthrough result lead them to postulate a new threshold for the many communities regime  K\geq \sqrtn . In this work, we provide evidences that confirm their conjecture for  K\geq \sqrtn : 1- We prove that, for any density of the graph, low-degree polynomials fail to recover communities below the threshold postulated by Chin et al.(2025); 2- We prove that community recovery is possible in polynomial time above the postulated threshold, not only in the sparse regime of~Chin et al., but also in some (but not all) moderately sparse regimes by essentially counting clique occurence in the observed graph.         Subjects:  Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG); Probability (<a target="_blank" rel="noopener" href="http://math.PR">math.PR</a>); Statistics Theory (<a target="_blank" rel="noopener" href="http://math.ST">math.ST</a>)  Cite as: arXiv:2509.15822 [<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>]    (or  arXiv:2509.15822v1 [<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15822">https://doi.org/10.48550/arXiv.2509.15822</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)        Submission history From: Christophe Giraud [view email]       [v1]         Fri, 19 Sep 2025 09:53:56 UTC (47 KB)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-75] raining Variational Quantum Circuits Using Particle Swarm Optimization</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15726">https://arxiv.org/abs/2509.15726</a><br>
<strong>作者</strong>: Marco Mordacci,Michele Amoretti<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Emerging Technologies (<a target="_blank" rel="noopener" href="http://cs.ET">cs.ET</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this work, the Particle Swarm Optimization (PSO) algorithm has been used to train various Variational Quantum Circuits (VQCs). This approach is motivated by the fact that commonly used gradient-based optimization methods can suffer from the barren plateaus problem. PSO is a stochastic optimization technique inspired by the collective behavior of a swarm of birds. The dimension of the swarm, the number of iterations of the algorithm, and the number of trainable parameters can be set. In this study, PSO has been used to train the entire structure of VQCs, allowing it to select which quantum gates to apply, the target qubits, and the rotation angle, in case a rotation is chosen. The algorithm is restricted to choosing from four types of gates: Rx, Ry, Rz, and CNOT. The proposed optimization approach has been tested on various datasets of the MedMNIST, which is a collection of biomedical image datasets designed for image classification tasks. Performance has been compared with the results achieved by classical stochastic gradient descent applied to a predefined VQC. The results show that the PSO can achieve comparable or even better classification accuracy across multiple datasets, despite the PSO using a lower number of quantum gates than the VQC used with gradient descent optimization.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-76] Impact of Single Rotations and Entanglement Topologies in Quantum Neural Networks</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15722">https://arxiv.org/abs/2509.15722</a><br>
<strong>作者</strong>: Marco Mordacci,Michele Amoretti<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Emerging Technologies (<a target="_blank" rel="noopener" href="http://cs.ET">cs.ET</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this work, an analysis of the performance of different Variational Quantum Circuits is presented, investigating how it changes with respect to entanglement topology, adopted gates, and Quantum Machine Learning tasks to be performed. The objective of the analysis is to identify the optimal way to construct circuits for Quantum Neural Networks. In the presented experiments, two types of circuits are used: one with alternating layers of rotations and entanglement, and the other, similar to the first one, but with an additional final layer of rotations. As rotation layers, all combinations of one and two rotation sequences are considered. Four different entanglement topologies are compared: linear, circular, pairwise, and full. Different tasks are considered, namely the generation of probability distributions and images, and image classification. Achieved results are correlated with the expressibility and entanglement capability of the different circuits to understand how these features affect performance.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-77] riplet Loss Based Quantum Encoding for Class Separability</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15705">https://arxiv.org/abs/2509.15705</a><br>
<strong>作者</strong>: Marco Mordacci,Mahul Pandey,Paolo Santini,Michele Amoretti<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Emerging Technologies (<a target="_blank" rel="noopener" href="http://cs.ET">cs.ET</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:An efficient and data-driven encoding scheme is proposed to enhance the performance of variational quantum classifiers. This encoding is specially designed for complex datasets like images and seeks to help the classification task by producing input states that form well-separated clusters in the Hilbert space according to their classification labels. The encoding circuit is trained using a triplet loss function inspired by classical facial recognition algorithms, and class separability is measured via average trace distances between the encoded density matrices. Benchmark tests performed on various binary classification tasks on MNIST and MedMNIST datasets demonstrate considerable improvement over amplitude encoding with the same VQC structure while requiring a much lower circuit depth.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-78] Interpretable Network-assisted Random Forest</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15611">https://arxiv.org/abs/2509.15611</a><br>
<strong>作者</strong>: Tiffany M. Tang,Elizaveta Levina,Ji Zhu<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG); Methodology (<a target="_blank" rel="noopener" href="http://stat.ME">stat.ME</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Machine learning algorithms often assume that training samples are independent. When data points are connected by a network, the induced dependency between samples is both a challenge, reducing effective sample size, and an opportunity to improve prediction by leveraging information from network neighbors. Multiple methods taking advantage of this opportunity are now available, but many, including graph neural networks, are not easily interpretable, limiting their usefulness for understanding how a model makes its predictions. Others, such as network-assisted linear regression, are interpretable but often yield substantially worse prediction performance. We bridge this gap by proposing a family of flexible network-assisted models built upon a generalization of random forests (RF+), which achieves highly-competitive prediction accuracy and can be interpreted through feature importance measures. In particular, we develop a suite of interpretation tools that enable practitioners to not only identify important features that drive model predictions, but also quantify the importance of the network contribution to prediction. Importantly, we provide both global and local importance measures as well as sample influence measures to assess the impact of a given observation. This suite of tools broadens the scope and applicability of network-assisted machine learning for high-impact problems where interpretability and transparency are essential.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-79] SETrLUSI: Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15593">https://arxiv.org/abs/2509.15593</a><br>
<strong>作者</strong>: Chunna Li,Yiwei Song,Yuanhai Shao<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In transfer learning, a source domain often carries diverse knowledge, and different domains usually emphasize different types of knowledge. Different from handling only a single type of knowledge from all domains in traditional transfer learning methods, we introduce an ensemble learning framework with a weak mode of convergence in the form of Statistical Invariant (SI) for multi-source transfer learning, formulated as Stochastic Ensemble Multi-Source Transfer Learning Using Statistical Invariant (SETrLUSI). The proposed SI extracts and integrates various types of knowledge from both source and target domains, which not only effectively utilizes diverse knowledge but also accelerates the convergence process. Further, SETrLUSI incorporates stochastic SI selection, proportional source domain sampling, and target domain bootstrapping, which improves training efficiency while enhancing model stability. Experiments show that SETrLUSI has good convergence and outperforms related methods with a lower time cost.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-80] (SP)2-Net: A Neural Spatial Spectrum Method for DOA Estimation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15475">https://arxiv.org/abs/2509.15475</a><br>
<strong>作者</strong>: Lioz Berman,Sharon Gannot,Tom Tirer<br>
<strong>类目</strong>: ignal Processing (eess.SP); Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:  Code can be found at <a target="_blank" rel="noopener" href="https://github.com/Liozb/SP2-Net">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We consider the problem of estimating the directions of arrival (DOAs) of multiple sources from a single snapshot of an antenna array, a task with many practical applications. In such settings, the classical Bartlett beamformer is commonly used, as maximum likelihood estimation becomes impractical when the number of sources is unknown or large, and spectral methods based on the sample covariance are not applicable due to the lack of multiple snapshots. However, the accuracy and resolution of the Bartlett beamformer are fundamentally limited by the array aperture. In this paper, we propose a deep learning technique, comprising a novel architecture and training strategy, for generating a high-resolution spatial spectrum from a single snapshot. Specifically, we train a deep neural network that takes the measurements and a hypothesis angle as input and learns to output a score consistent with the capabilities of a much wider array. At inference time, a heatmap can be produced by scanning an arbitrary set of angles. We demonstrate the advantages of our trained model, named (SP) ^2 -Net, over the Bartlett beamformer and sparsity-based DOA estimation methods.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-81] Neural Architecture Search Algorithms for Quantum Autoencoders</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15451">https://arxiv.org/abs/2509.15451</a><br>
<strong>作者</strong>: Ankit Kulshrestha,Xiaoyuan Liu,Hayato Ushijima-Mwesigwa,Ilya Safro<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The design of quantum circuits is currently driven by the specific objectives of the quantum algorithm in question. This approach thus relies on a significant manual effort by the quantum algorithm designer to design an appropriate circuit for the task. However this approach cannot scale to more complex quantum algorithms in the future without exponentially increasing the circuit design effort and introducing unwanted inductive biases. Motivated by this observation, we propose to automate the process of cicuit design by drawing inspiration from Neural Architecture Search (NAS). In this work, we propose two Quantum-NAS algorithms that aim to find efficient circuits given a particular quantum task. We choose quantum data compression as our driver quantum task and demonstrate the performance of our algorithms by finding efficient autoencoder designs that outperform baselines on three different tasks - quantum data denoising, classical data compression and pure quantum data compression. Our results indicate that quantum NAS algorithms can significantly alleviate the manual effort while delivering performant quantum circuits for any given task.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-82] raining thermodynamic computers by gradient descent</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15324">https://arxiv.org/abs/2509.15324</a><br>
<strong>作者</strong>: Stephen Whitelam<br>
<strong>类目</strong>: atistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We show how to adjust the parameters of a thermodynamic computer by gradient descent in order to perform a desired computation at a specified observation time. Within a digital simulation of a thermodynamic computer, training proceeds by maximizing the probability with which the computer would generate an idealized dynamical trajectory. The idealized trajectory is designed to reproduce the activations of a neural network trained to perform the desired computation. This teacher-student scheme results in a thermodynamic computer whose finite-time dynamics enacts a computation analogous to that of the neural network. The parameters identified in this way can be implemented in the hardware realization of the thermodynamic computer, which will perform the desired computation automatically, driven by thermal noise. We demonstrate the method on a standard image-classification task, and estimate the thermodynamic advantage – the ratio of energy costs of the digital and thermodynamic implementations – to exceed seven orders of magnitude. Our results establish gradient descent as a viable training method for thermodynamic computing, enabling application of the core methodology of machine learning to this emerging field.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-83] Kernel Model Validation: How To Do It And Why You Should Care</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15244">https://arxiv.org/abs/2509.15244</a><br>
<strong>作者</strong>: Carlo Graziani,Marieme Ngom<br>
<strong>类目</strong>: Methodology (<a target="_blank" rel="noopener" href="http://stat.ME">stat.ME</a>); Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:  12 pages, 6 figures. To appear in ITEA Journal of Test and Evaluation, Vol. 46, Issue 3, September 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Gaussian Process (GP) models are popular tools in uncertainty quantification (UQ) because they purport to furnish functional uncertainty estimates that can be used to represent model uncertainty. It is often difficult to state with precision what probabilistic interpretation attaches to such an uncertainty, and in what way is it calibrated. Without such a calibration statement, the value of such uncertainty estimates is quite limited and qualitative. We motivate the importance of proper probabilistic calibration of GP predictions by describing how GP predictive calibration failures can cause degraded convergence properties in a target optimization algorithm called Targeted Adaptive Design (TAD). We discuss the interpretation of GP-generated uncertainty intervals in UQ, and how one may learn to trust them, through a formal procedure for covariance kernel validation that exploits the multivariate normal nature of GP predictions. We give simple examples of GP regression misspecified 1-dimensional models, and discuss the situation with respect to higher-dimensional models.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-84] Deep Gaussian Process-based Cost-Aware Batch Bayesian Optimization for Complex Materials Design Campaigns</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14408">https://arxiv.org/abs/2509.14408</a><br>
<strong>作者</strong>: Sk Md Ahnaf Akif Alvi,Brent Vela,Vahid Attari,Jan Janssen,Danny Perez,Douglas Allaire,Raymundo Arroyave<br>
<strong>类目</strong>: Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The accelerating pace and expanding scope of materials discovery demand optimization frameworks that efficiently navigate vast, nonlinear design spaces while judiciously allocating limited evaluation resources. We present a cost-aware, batch Bayesian optimization scheme powered by deep Gaussian process (DGP) surrogates and a heterotopic querying strategy. Our DGP surrogate, formed by stacking GP layers, models complex hierarchical relationships among high-dimensional compositional features and captures correlations across multiple target properties, propagating uncertainty through successive layers. We integrate evaluation cost into an upper-confidence-bound acquisition extension, which, together with heterotopic querying, proposes small batches of candidates in parallel, balancing exploration of under-characterized regions with exploitation of high-mean, low-variance predictions across correlated properties. Applied to refractory high-entropy alloys for high-temperature applications, our framework converges to optimal formulations in fewer iterations with cost-aware queries than conventional GP-based BO, highlighting the value of deep, uncertainty-aware, cost-sensitive strategies in materials campaigns.</p>
</div></div>
<h3 id="信息检索">信息检索</h3>
<div class="note pink no-icon flat"><p>[IR-0] Understanding Embedding Scaling in Collaborative Filtering</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15709">https://arxiv.org/abs/2509.15709</a><br>
<strong>作者</strong>: Zhuangzhuang He,Zhou Kaiyu,Haoyue Bai,Fengbin Zhu,Yonghui Yang<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Scaling recommendation models into large recommendation models has become one of the most widely discussed topics. Recent efforts focus on components beyond the scaling embedding dimension, as it is believed that scaling embedding may lead to performance degradation. Although there have been some initial observations on embedding, the root cause of their non-scalability remains unclear. Moreover, whether performance degradation occurs across different types of models and datasets is still an unexplored area. Regarding the effect of embedding dimensions on performance, we conduct large-scale experiments across 10 datasets with varying sparsity levels and scales, using 4 representative classical architectures. We surprisingly observe two novel phenomenon: double-peak and logarithmic. For the former, as the embedding dimension increases, performance first improves, then declines, rises again, and eventually drops. For the latter, it exhibits a perfect logarithmic curve. Our contributions are threefold. First, we discover two novel phenomena when scaling collaborative filtering models. Second, we gain an understanding of the underlying causes of the double-peak phenomenon. Lastly, we theoretically analyze the noise robustness of collaborative filtering models, with results matching empirical observations.</p>
</div></div>
<div class="note pink no-icon flat"><p>[IR-1] SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15432">https://arxiv.org/abs/2509.15432</a><br>
<strong>作者</strong>: Thong Nguyen,Yibin Lei,Jia-Huei Ju,Andrew Yates<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
*<strong>备注</strong>:  Accepted</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Visual Document Retrieval (VDR) typically operates as text-to-image retrieval using specialized bi-encoders trained to directly embed document images. We revisit a zero-shot generate-and-encode pipeline: a vision-language model first produces a detailed textual description of each document image, which is then embedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual document encoder. It also scales better to large collections and offers broader multilingual coverage. Analysis shows that modern vision-language models capture complex textual and visual cues with sufficient granularity to act as a reusable semantic proxy. By offloading modality alignment to pretrained vision-language models, our approach removes the need for computationally intensive text-image contrastive training and establishes a strong zero-shot baseline for future VDR systems.</p>
</div></div>
<h3 id="附件下载">附件下载</h3>
<p><a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/lonePatient/lonePatient.github.io/master/arxiv/arxiv_papers_2025-09-22.txt">点击下载今日全部论文列表</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Weitang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lonepatient.top/2025/09/22/arxiv_papers_2025-09-22.html">http://lonepatient.top/2025/09/22/arxiv_papers_2025-09-22.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lonepatient.top" target="_blank">闲记算法</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Arxiv/">Arxiv</a></div><div class="post_share"><div class="social-share" data-image="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/09/23/arxiv_papers_2025-09-23.html"><img class="prev-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Arxiv今日论文 | 2025-09-23</div></div></a></div><div class="next-post pull-right"><a href="/2025/09/19/arxiv_papers_2025-09-19.html"><img class="next-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Arxiv今日论文 | 2025-09-19</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/10/ar5iv.html" title="ar5iv"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20231208121727.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-10</div><div class="title">ar5iv</div></div></a></div><div><a href="/2025/09/30/arxiv_papers_2025-09-30.html" title="Arxiv今日论文 | 2025-09-30"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-30</div><div class="title">Arxiv今日论文 | 2025-09-30</div></div></a></div><div><a href="/2025/08/11/arxiv_papers_2025-08-11.html" title="Arxiv今日论文 | 2025-08-11"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-11</div><div class="title">Arxiv今日论文 | 2025-08-11</div></div></a></div><div><a href="/2025/08/13/arxiv_papers_2025-08-13.html" title="Arxiv今日论文 | 2025-08-13"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-13</div><div class="title">Arxiv今日论文 | 2025-08-13</div></div></a></div><div><a href="/2025/08/14/arxiv_papers_2025-08-14.html" title="Arxiv今日论文 | 2025-08-14"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-14</div><div class="title">Arxiv今日论文 | 2025-08-14</div></div></a></div><div><a href="/2025/08/15/arxiv_papers_2025-08-15.html" title="Arxiv今日论文 | 2025-08-15"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-15</div><div class="title">Arxiv今日论文 | 2025-08-15</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Weitang Liu</div><div class="author-info__description">一个致力于记录技术的博客</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lonePatient"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lonePatient" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuweitangmath@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/277974397" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有任何问题可通过留言板或者微信公众号给我留言，谢谢！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">1.</span> <span class="toc-text">目录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%A7%88-2025-09-22"><span class="toc-number">1.1.</span> <span class="toc-text">概览 (2025-09-22)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">1.3.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">1.4.</span> <span class="toc-text">人工智能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2"><span class="toc-number">1.6.</span> <span class="toc-text">信息检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%84%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="toc-number">1.7.</span> <span class="toc-text">附件下载</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-31"/></a><div class="content"><a class="title" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31">Arxiv今日论文 | 2025-10-31</a><time datetime="2025-10-31T10:30:00.000Z" title="发表于 2025-10-31 10:30:00">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-30"/></a><div class="content"><a class="title" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30">Arxiv今日论文 | 2025-10-30</a><time datetime="2025-10-30T10:30:00.000Z" title="发表于 2025-10-30 10:30:00">2025-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-29"/></a><div class="content"><a class="title" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29">Arxiv今日论文 | 2025-10-29</a><time datetime="2025-10-29T10:30:00.000Z" title="发表于 2025-10-29 10:30:00">2025-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-28"/></a><div class="content"><a class="title" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28">Arxiv今日论文 | 2025-10-28</a><time datetime="2025-10-28T10:30:00.000Z" title="发表于 2025-10-28 10:30:00">2025-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-27"/></a><div class="content"><a class="title" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27">Arxiv今日论文 | 2025-10-27</a><time datetime="2025-10-27T10:30:00.000Z" title="发表于 2025-10-27 10:30:00">2025-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Weitang Liu</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'https://twikoo.lonepatient.top/',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.lonepatient.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo@1.4.11/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (95)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/计算机视觉/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 计算机视觉 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/知识图谱/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 知识图谱 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 深度学习 (135)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://lonepatient.top/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230219144729.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-18</span><a class="blog-slider__title" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">通向AGI之路：大型语言模型（LLM）技术精要</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/07/12/gaiic_2022_ner_top10.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20220712181905.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-07-12</span><a class="blog-slider__title" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">GAIIC2022商品标题识别二等奖获奖解决思路</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230807190424.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-01-20</span><a class="blog-slider__title" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">2025-03|高质量中文预训练模型集合</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>