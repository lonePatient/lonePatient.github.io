<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Arxiv今日论文 | 2025-09-19 | 闲记算法</title><meta name="keywords" content="Arxiv"><meta name="author" content="Weitang Liu"><meta name="copyright" content="Weitang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本篇博文主要内容为 2025-09-19 从Arxiv.org论文网站获取的最新论文列表，自动更新，按照NLP、CV、ML、AI、IR五个大方向区分，若需要邮件定时接收，请在评论区留下你的邮箱号。 说明：每日论文数据从Arxiv.org获取，每天早上12:00左右定时自动更新。  友情提示: 如何您需要邮箱接收每日论文数据，请在评论处留下你的邮箱。  目录  概览 自然语言处理CL 人工智能AI">
<meta property="og:type" content="article">
<meta property="og:title" content="Arxiv今日论文 | 2025-09-19">
<meta property="og:url" content="http://lonepatient.top/2025/09/19/arxiv_papers_2025-09-19.html">
<meta property="og:site_name" content="闲记算法">
<meta property="og:description" content="本篇博文主要内容为 2025-09-19 从Arxiv.org论文网站获取的最新论文列表，自动更新，按照NLP、CV、ML、AI、IR五个大方向区分，若需要邮件定时接收，请在评论区留下你的邮箱号。 说明：每日论文数据从Arxiv.org获取，每天早上12:00左右定时自动更新。  友情提示: 如何您需要邮箱接收每日论文数据，请在评论处留下你的邮箱。  目录  概览 自然语言处理CL 人工智能AI">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png">
<meta property="article:published_time" content="2025-09-19T10:30:00.000Z">
<meta property="article:modified_time" content="2025-10-31T07:26:14.668Z">
<meta property="article:author" content="Weitang Liu">
<meta property="article:tag" content="Arxiv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://lonepatient.top/2025/09/19/arxiv_papers_2025-09-19"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//s4.cnzz.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" data-pjax="data-pjax" src="https://s4.cnzz.com/z_stat.php?id=1273275888&amp;web_id=1273275888"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-31 07:26:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/backgroud.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><script src="https://at.alicdn.com/t/c/font_3570527_dthoqrrv2tv.css"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JwRQtLKZggvJH4sJ",ck:"JwRQtLKZggvJH4sJ"})</script><link rel="stylesheet" href="/css/universe.css"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3536467946304280" crossorigin="anonymous"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">闲记算法</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Arxiv今日论文 | 2025-09-19<a class="post-edit-link" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts/arxiv_papers_2025-09-19.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-19T10:30:00.000Z" title="发表于 2025-09-19 10:30:00">2025-09-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T07:26:14.668Z" title="更新于 2025-10-31 07:26:14">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/">学术会议</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/Arxiv/">Arxiv</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">142.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>681分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2025/09/19/arxiv_papers_2025-09-19.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>本篇博文主要内容为 2025-09-19 从<a target="_blank" rel="noopener" href="https://arxiv.org">Arxiv.org</a>论文网站获取的最新论文列表，自动更新，按照NLP、CV、ML、AI、IR五个大方向区分，若需要邮件定时接收，请在评论区留下你的邮箱号。</p>
<div class="note info flat"><p>说明：每日论文数据从<a target="_blank" rel="noopener" href="https://arxiv.org">Arxiv.org</a>获取，每天早上12:00左右定时自动更新。</p>
</div>
<div class="note warning flat"><p>友情提示: 如何您需要<strong>邮箱</strong>接收每日论文数据，请在评论处留下你的邮箱。</p>
</div>
<h2 id="目录">目录</h2>
<ul>
<li><a href="#%E6%A6%82%E8%A7%88">概览</a></li>
<li><a href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86">自然语言处理CL</a></li>
<li><a href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD">人工智能AI</a></li>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习LG</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89">计算机视觉CV</a></li>
<li><a href="#%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2">信息检索IR</a></li>
</ul>
<h3 id="概览-2025-09-19">概览 (2025-09-19)</h3>
<p>今日共更新<strong>521</strong>篇论文,其中:</p>
<ul>
<li><strong>自然语言处理</strong>共<strong>99</strong>篇(Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>))</li>
<li><strong>人工智能</strong>共<strong>133</strong>篇(Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>))</li>
<li><strong>计算机视觉</strong>共<strong>82</strong>篇(Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>))</li>
<li><strong>机器学习</strong>共<strong>142</strong>篇(Machine Learning (cs.LG))</li>
</ul>
<h3 id="自然语言处理">自然语言处理</h3>
<div class="note orange no-icon flat"><p>[NLP-0] LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决大规模语言模型（Large Language Models, LLMs）训练过程中因数据污染（data contamination）导致的评估失真问题，即训练数据中无意包含评估基准数据集，使得模型在测试时表现出超出实际能力的性能。为应对这一挑战，作者提出了一种新颖的框架——LNE-Blocking，其核心在于两个关键组件：一是基于局部邻域嵌入（Local Neighborhood Embedding, LNE）的污染检测机制，用于量化模型对潜在泄露数据的依赖程度；二是自适应干扰操作（Blocking），根据污染检测结果动态调整干扰强度，以诱导模型生成非记忆化的响应。该框架首次实现了对贪婪解码性能的有效恢复，并在多个存在泄露风险的数据集上展现出稳定且一致的性能恢复效果。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15218">https://arxiv.org/abs/2509.15218</a><br>
<strong>作者</strong>: Ruijie Hou,Yueyang Jiao,Hanxu Hu,Yingming Li,Wai Lam,Huajian Zhang,Hongyuan Lu<br>
<strong>机构</strong>: Zhejiang University (浙江大学); FaceMind Corporation; University of Zurich (苏黎世大学); The Chinese University of Hong Kong (香港中文大学); Westlake University (西湖大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbfLNE-Blocking, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbfLNE, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbfBlocking, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the model’s greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at this https URL to facilitate research.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-1] Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文试图解决传统历史结构性压迫测量方法在跨国比较中缺乏有效性的问题，这一问题源于各国独特的排他性、殖民和身份地位历史，且现有指标多侧重物质资源而忽视基于身份的 lived（亲历的）排斥。解决方案的关键在于引入一种基于大型语言模型（Large Language Models, LLMs）的新框架，通过规则引导的提示策略（rule-guided prompting strategies），从多语种自报族裔表述中生成具有情境敏感性和理论依据的压迫程度评分，从而捕捉国家内部细微的身份型历史不公，并提供可扩展、跨文化的系统性排斥测量工具。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15216">https://arxiv.org/abs/2509.15216</a><br>
<strong>作者</strong>: Sreejato Chatterjee,Linh Tran,Quoc Duy Nguyen,Roni Kirson,Drue Hamlin,Harvest Aquino,Hanjia Lyu,Jiebo Luo,Timothy Dye<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (this https URL).<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-2] Whats the Best Way to Retrieve Slides? A Comparative Study of Multimodal Caption-Based and Hybrid Retrieval Techniques</p>
<p>【速读】： 该论文旨在解决多模态幻灯片（slide decks）在检索增强生成（Retrieval-Augmented Generation, RAG）系统中的高效与准确检索问题。由于幻灯片融合了文本、图像和图表等多种模态，传统分模态独立索引的方法不仅复杂度高，还容易丢失跨模态上下文信息，从而影响下游任务性能。解决方案的关键在于探索多种先进的检索策略：包括采用视觉后交互嵌入模型（如ColPali）实现多模态对齐，引入视觉重排序器（visual reranker）提升排序精度，以及结合密集检索（dense retrieval）与BM25的混合检索方法，并辅以文本重排序器和Reciprocal Rank Fusion（RRF）等融合技术优化结果；此外，提出一种基于视觉-语言模型（Vision-Language Models）的标题生成流水线，在显著降低嵌入存储需求的同时保持与视觉后交互方法相当的检索效果，为实际部署提供了兼顾效率与性能的实用方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15211">https://arxiv.org/abs/2509.15211</a><br>
<strong>作者</strong>: Petros Stylianos Giouroukis,Dimitris Dimitriadis,Dimitrios Papadopoulos,Zhenwen Shao,Grigorios Tsoumakas<br>
<strong>机构</strong>: Aristotle University of Thessaloniki (亚里士多德大学萨洛尼卡分校); Johnson &amp; Johnson (强生公司)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Slide decks, serving as digital reports that bridge the gap between presentation slides and written documents, are a prevalent medium for conveying information in both academic and corporate settings. Their multimodal nature, combining text, images, and charts, presents challenges for retrieval-augmented generation systems, where the quality of retrieval directly impacts downstream performance. Traditional approaches to slide retrieval often involve separate indexing of modalities, which can increase complexity and lose contextual information. This paper investigates various methodologies for effective slide retrieval, including visual late-interaction embedding models like ColPali, the use of visual rerankers, and hybrid retrieval techniques that combine dense retrieval with BM25, further enhanced by textual rerankers and fusion methods like Reciprocal Rank Fusion. A novel Vision-Language Models-based captioning pipeline is also evaluated, demonstrating significantly reduced embedding storage requirements compared to visual late-interaction techniques, alongside comparable retrieval performance. Our analysis extends to the practical aspects of these methods, evaluating their runtime performance and storage demands alongside retrieval efficacy, thus offering practical guidance for the selection and development of efficient and robust slide retrieval systems for real-world applications.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-3] FlowRL: Matching Reward Distributions for <mark class="hl-label green">LLM</mark>  <mark class="hl-label green">Reasoning</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Model, LLM）在强化学习（Reinforcement Learning, RL）过程中因过度优化主导奖励信号而导致推理路径多样性不足的问题。现有方法如PPO和GRPO倾向于聚焦于高频、高分的策略，忽视了低频但有效的推理路径，从而限制了模型的探索能力和泛化性能。其解决方案的关键在于提出FlowRL框架，通过引入可学习的分区函数（partition function）将标量奖励转化为归一化的目标分布，并最小化策略分布与目标分布之间的反向KL散度（reverse KL divergence），实现对奖励分布的匹配而非单纯最大化奖励。该方法通过流平衡（flow balancing）机制促进多样化的探索，提升LLM在数学和代码推理任务中的表现，实验表明其相比GRPO和PPO分别平均提升10.0%和5.1%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15207">https://arxiv.org/abs/2509.15207</a><br>
<strong>作者</strong>: Xuekai Zhu,Daixuan Cheng,Dinghuai Zhang,Hengli Li,Kaiyan Zhang,Che Jiang,Youbang Sun,Ermo Hua,Yuxin Zuo,Xingtai Lv,Qizheng Zhang,Lin Chen,Fanghao Shao,Bo Xue,Yunchong Song,Zhenjie Yang,Ganqu Cui,Ning Ding,Jianfeng Gao,Xiaodong Liu,Bowen Zhou,Hongyuan Mei,Zhouhan Lin<br>
<strong>机构</strong>: Shanghai Jiao Tong University (上海交通大学); Shanghai AI Laboratory (上海人工智能实验室); Microsoft Research (微软研究院); Tsinghua University (清华大学); Peking University (北京大学); Renmin University of China (中国人民大学); Stanford University (斯坦福大学); Toyota Technological Institute at Chicago (芝加哥丰田技术学院)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of  10.0%  over GRPO and  5.1%  over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-4] Fair-<mark class="hl-label green">GPT</mark> Q: Bias-Aware Quantization for <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决生成式语言模型在量化（quantization）过程中引入的公平性问题，即现有量化方法（如GPTQ）虽能有效降低内存和计算开销，但可能加剧模型对特定群体（如性别、种族、宗教）的刻板印象和偏见输出。其解决方案的关键在于提出Fair-GPTQ，一种在量化目标中显式加入群体公平性约束（group-fairness constraints）的方法，通过引导舍入操作的学习过程，使受保护群体的文本生成更加公平，同时保持4-bit量化带来的内存与速度优势，并在零样本基准测试中维持至少90%的基线准确率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15206">https://arxiv.org/abs/2509.15206</a><br>
<strong>作者</strong>: Irina Proskurina,Guillaume Metzler,Julien Velcin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:High memory demands of generative language models have drawn attention to quantization, which reduces computational cost, memory usage, and latency by mapping model weights to lower-precision integers. Approaches such as GPTQ effectively minimize input-weight product errors during quantization; however, recent empirical studies show that they can increase biased outputs and degrade performance on fairness benchmarks, and it remains unclear which specific weights cause this issue. In this work, we draw new links between quantization and model fairness by adding explicit group-fairness constraints to the quantization objective and introduce Fair-GPTQ, the first quantization method explicitly designed to reduce unfairness in large language models. The added constraints guide the learning of the rounding operation toward less-biased text generation for protected groups. Specifically, we focus on stereotype generation involving occupational bias and discriminatory language spanning gender, race, and religion. Fair-GPTQ has minimal impact on performance, preserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces unfairness relative to a half-precision model, and retains the memory and speed benefits of 4-bit quantization. We also compare the performance of Fair-GPTQ with existing debiasing methods and find that it achieves performance on par with the iterative null-space projection debiasing approach on racial-stereotype benchmarks. Overall, the results validate our theoretical solution to the quantization problem with a group-bias term, highlight its applicability for reducing group bias at quantization time in generative models, and demonstrate that our approach can further be used to analyze channel- and weight-level contributions to fairness during quantization.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-5] Evolving Language Models without Labels: Majority Drives Selection Novelty Promotes Variation</p>
<p>【速读】： 该论文旨在解决标签-free强化学习中因探索能力衰退导致的熵崩溃（entropy collapse）问题，即模型在无监督训练下生成内容趋于短促、单一且脆弱，从而损害其推理多样性与泛化能力。解决方案的关键在于提出EVOL-RL（EVolution-Oriented and Label-free Reinforcement Learning），其核心机制为“多数投票选择 + 新颖性驱动变化”：以多数投票结果作为稳定锚点（selection），同时引入基于语义空间的新颖性奖励（novelty-aware reward）鼓励模型产生不同于已有推理路径的响应（variation），并通过GRPO算法实现对强信号的保留和熵正则项维持搜索活跃度，从而在不牺牲探索能力的前提下实现持续进化。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15194">https://arxiv.org/abs/2509.15194</a><br>
<strong>作者</strong>: Yujun Zhou,Zhenwen Liang,Haolin Liu,Wenhao Yu,Kishan Panaganti,Linfeng Song,Dian Yu,Xiangliang Zhang,Haitao Mi,Dong Yu<br>
<strong>机构</strong>: Tencent AI Lab (腾讯AI实验室); University of Notre Dame (圣母大学); University of Virginia (弗吉尼亚大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) are increasingly trained with reinforcement learning from verifiable rewards (RLVR), yet real-world deployment demands models that can self-improve without labels or external judges. Existing label-free methods, confidence minimization, self-consistency, or majority-vote objectives, stabilize learning but steadily shrink exploration, causing an entropy collapse: generations become shorter, less diverse, and brittle. Unlike prior approaches such as Test-Time Reinforcement Learning (TTRL), which primarily adapt models to the immediate unlabeled dataset at hand, our goal is broader: to enable general improvements without sacrificing the model’s inherent exploration capacity and generalization ability, i.e., evolving. We formalize this issue and propose EVolution-Oriented and Label-free Reinforcement Learning (EVOL-RL), a simple rule that couples stability with variation under a label-free setting. EVOL-RL keeps the majority-voted answer as a stable anchor (selection) while adding a novelty-aware reward that favors responses whose reasoning differs from what has already been produced (variation), measured in semantic space. Implemented with GRPO, EVOL-RL also uses asymmetric clipping to preserve strong signals and an entropy regularizer to sustain search. This majority-for-selection + novelty-for-variation design prevents collapse, maintains longer and more informative chains of thought, and improves both pass@1 and pass@n. EVOL-RL consistently outperforms the majority-only TTRL baseline; e.g., training on label-free AIME24 lifts Qwen3-4B-Base AIME25 pass@1 from TTRL’s 4.6% to 16.4%, and pass@16 from 18.5% to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks stronger generalization across domains (e.g., GPQA). Furthermore, we demonstrate that EVOL-RL also boosts performance in the RLVR setting, highlighting its broad applicability.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-6] Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】： 该论文旨在解决扩散语言模型（Diffusion Language Models, DLMs）在生成长文本时面临的“长解码窗口问题”（long decoding-window problem），即远距离上下文生成的token容易变得无关或重复，从而影响生成质量。其解决方案的关键在于提出两种创新方法：一是<strong>卷积解码（Convolutional Decoding, Conv）</strong>，通过基于归一化的机制动态缩小解码窗口，避免硬性分块带来的速度与双向性的损失；二是<strong>基于规则的拒绝微调（Rejecting Rule-based Fine-Tuning, R2FT）</strong>，一种后训练策略，用于优化远离输入上下文位置的token对齐，提升生成一致性与流畅性。二者协同作用，在保持并行解码优势的同时显著提升了生成质量和效率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15188">https://arxiv.org/abs/2509.15188</a><br>
<strong>作者</strong>: Yeongbin Seo,Dongha Lee,Jaehyung Kim,Jinyoung Yeo<br>
<strong>机构</strong>: Yonsei University (延世大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  NeurIPS 2025 spotlight</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-7] SMARTER: A Data-efficient Framework to Improve Toxicity Detection with Explanation via Self-augmenting <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决社交媒体平台上毒性内容（toxic content）泛滥问题，特别是如何在低资源条件下实现可解释的内容审核（explainable content moderation）。其解决方案的关键在于提出一种数据高效的两阶段框架SMARTER：第一阶段利用大语言模型（Large Language Models, LLMs）自动生成带有正确与错误标签的合成解释，并通过偏好优化（preference optimization）实现最小人工监督下的对齐；第二阶段通过跨模型训练进一步提升解释的质量，使弱模型在风格和语义上与强模型保持一致。该方法显著提升了分类性能（最高达13.5%宏F1分数提升），同时仅需少量标注数据，展现出在低资源场景下利用LLMs自我改进能力进行分类与解释的可扩展性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15174">https://arxiv.org/abs/2509.15174</a><br>
<strong>作者</strong>: Huy Nghiem,Advik Sachdeva,Hal Daumé III<br>
<strong>机构</strong>: University of Maryland (马里兰大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  NLP, Hate speech detection, explanation, LLM</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:WARNING: This paper contains examples of offensive materials. Toxic content has become pervasive on social media platforms. We introduce SMARTER, a data-efficient two-stage framework for explainable content moderation using Large Language Models (LLMs). In Stage 1, we leverage LLMs’ own outputs to generate synthetic explanations for both correct and incorrect labels, enabling alignment via preference optimization with minimal human supervision. In Stage 2, we refine explanation quality through cross-model training, allowing weaker models to align stylistically and semantically with stronger ones. Experiments on three benchmark tasks – HateXplain, Latent Hate, and Implicit Hate – demonstrate that SMARTER enables LLMs to achieve up to a 13.5% macro-F1 improvement over standard few-shot baselines while using only a fraction of the full training data. Our framework offers a scalable strategy for low-resource settings by harnessing LLMs’ self-improving capabilities for both classification and explanation.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-8] An Evaluation-Centric Paradigm for Scientific Visualization <mark class="hl-label green">Agents</mark></p>
<p>【速读】： 该论文试图解决科学可视化（Scientific Visualization, SciVis）领域中缺乏全面、大规模基准测试的问题，从而阻碍了对自主可视化智能体（autonomous visualization agents）真实能力的评估与比较。解决方案的关键在于推动构建一个面向SciVis智能体的评估基准（evaluation benchmark），该基准不仅能够量化现有智能体的能力，还能通过提供标准化的评测机制促进智能体的自我改进和持续优化，进而驱动该领域的技术创新与发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15160">https://arxiv.org/abs/2509.15160</a><br>
<strong>作者</strong>: Kuangshi Ai,Haichao Miao,Zhimin Li,Chaoli Wang,Shusen Liu<br>
<strong>机构</strong>: University of Notre Dame (圣母大学); Lawrence Livermore National Laboratory (劳伦斯利弗莫尔国家实验室); Vanderbilt University (范德比尔特大学)<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in multi-modal large language models (MLLMs) have enabled increasingly sophisticated autonomous visualization agents capable of translating user intentions into data visualizations. However, measuring progress and comparing different agents remains challenging, particularly in scientific visualization (SciVis), due to the absence of comprehensive, large-scale benchmarks for evaluating real-world capabilities. This position paper examines the various types of evaluation required for SciVis agents, outlines the associated challenges, provides a simple proof-of-concept evaluation example, and discusses how evaluation benchmarks can facilitate agent self-improvement. We advocate for a broader collaboration to develop a SciVis agentic evaluation benchmark that would not only assess existing capabilities but also drive innovation and stimulate future development in the field.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-9] AIP: Subverting Retrieval-Augmented Generation via Adversarial Instructional Prompt <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决检索增强生成（Retrieval-Augmented Generation, RAG）系统中一个此前被忽视但极具现实威胁的漏洞问题：即攻击者如何通过操纵**指令提示（instructional prompts）**来隐蔽地改变RAG系统的输出，而无需修改用户查询本身。传统攻击多依赖于对用户输入的直接篡改，但在实际部署中往往受限于输入保护机制或固定接口。本文提出一种新型攻击方法——<strong>对抗性指令提示（Adversarial Instructional Prompt, AIP）</strong>，其核心在于利用广泛共享且未受审计的指令提示作为攻击载体，通过优化策略使攻击在保持自然性、可用性和鲁棒性三方面达到平衡，从而实现高成功率（最高达95.23%攻击成功率）的同时不被察觉。关键创新在于采用基于遗传算法的联合优化框架，在模拟真实语义变体的基础上自动演化出能泛化至多种查询形式的恶意提示，揭示了RAG架构中接口组件潜在的安全风险，并呼吁重新评估公共指令提示的安全性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15159">https://arxiv.org/abs/2509.15159</a><br>
<strong>作者</strong>: Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Xiaoyong Yuan<br>
<strong>机构</strong>: Clemson University (克莱姆森大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted at EMNLP 2025 Conference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly. We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.          Comments: Accepted at EMNLP 2025 Conference   Subjects:  Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)  Cite as: arXiv:2509.15159 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]    (or  arXiv:2509.15159v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15159">https://doi.org/10.48550/arXiv.2509.15159</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-10] Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning</p>
<p>【速读】： 该论文旨在解决大语言模型在监督微调（Supervised Fine-Tuning, SFT）过程中因行为策略（behavior policy）与目标策略（target policy）之间分布差异过大而导致的重要性采样方差高、训练不稳定的问题。其解决方案的关键在于提出一种简单而有效的数据重写（data rewriting）框架：通过保留正确解作为在线策略（on-policy）数据，并对错误解进行引导式重求解（guided re-solving）以重构为高质量样本，仅在必要时回退至专家示范，从而在优化前主动缩小策略差距，使训练分布更贴近目标策略，显著降低重要性采样方差并提升训练稳定性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15157">https://arxiv.org/abs/2509.15157</a><br>
<strong>作者</strong>: Shiwan Zhao,Xuyang Zhao,Jiaming Zhou,Aobo Kong,Qicheng Li,Yong Qin<br>
<strong>机构</strong>: Nankai University (南开大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Supervised fine-tuning (SFT) of large language models can be viewed as an off-policy learning problem, where expert demonstrations come from a fixed behavior policy while training aims to optimize a target policy. Importance sampling is the standard tool for correcting this distribution mismatch, but large policy gaps lead to high variance and training instability. Existing approaches mitigate this issue using KL penalties or clipping, which passively constrain updates rather than actively reducing the gap. We propose a simple yet effective data rewriting framework that proactively shrinks the policy gap by keeping correct solutions as on-policy data and rewriting incorrect ones with guided re-solving, falling back to expert demonstrations only when needed. This aligns the training distribution with the target policy before optimization, reducing importance sampling variance and stabilizing off-policy fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate consistent and significant gains over both vanilla SFT and the state-of-the-art Dynamic Fine-Tuning (DFT) approach. The data and code will be released at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-11] A1: Asynchronous Test-Time Scaling via Conformal Prediction</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在测试时扩展（test-time scaling）过程中面临的严重同步开销、内存瓶颈和延迟问题，尤其是在采用长推理链的推测解码（speculative decoding）场景下。解决方案的关键在于提出A1（Asynchronous Test-Time Scaling），其核心创新包括：通过优化算术强度（arithmetic intensity）识别同步成为主导瓶颈，并设计在线校准策略实现异步推理；同时构建三阶段拒绝采样（rejection sampling）流水线，支持顺序与并行扩展。该方法在保持准确拒绝率控制的前提下，显著提升推理吞吐量（4.14倍）并实现56.7倍的速度加速，且无精度损失。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15148">https://arxiv.org/abs/2509.15148</a><br>
<strong>作者</strong>: Jing Xiong,Qiujiang Chen,Fanghua Ye,Zhongwei Wan,Chuanyang Zheng,Chenyang Zhao,Hui Shen,Alexander Hanbo Li,Chaofan Tao,Haochen Tan,Haoli Bai,Lifeng Shang,Lingpeng Kong,Ngai Wong<br>
<strong>机构</strong>: The University of Hong Kong (香港大学); Independent Researcher (独立研究员); Huawei Noah’s Ark Lab (华为诺亚方舟实验室)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Tech Report</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) benefit from test-time scaling, but existing methods face significant challenges, including severe synchronization overhead, memory bottlenecks, and latency, especially during speculative decoding with long reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a statistically guaranteed adaptive inference framework that addresses these challenges. A1 refines arithmetic intensity to identify synchronization as the dominant bottleneck, proposes an online calibration strategy to enable asynchronous inference, and designs a three-stage rejection sampling pipeline that supports both sequential and parallel scaling. Through experiments on the MATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model families, we demonstrate that A1 achieves a remarkable 56.7x speedup in test-time scaling and a 4.14x improvement in throughput, all while maintaining accurate rejection-rate control, reducing latency and memory overhead, and no accuracy loss compared to using target model scaling alone. These results position A1 as an efficient and principled solution for scalable LLM inference. We have released the code at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-12] FCPE: A Fast Context-based Pitch Estimation Model</p>
<p>【速读】： 该论文旨在解决单音音频中基频（pitch）估计在噪声环境下性能显著下降的问题，这一问题直接影响MIDI转录和歌声转换（Singing Voice Conversion, SVC）的准确性。解决方案的关键在于提出一种名为FCPE的快速上下文感知基频估计模型，其核心是采用Lynx-Net架构并引入深度可分离卷积（depth-wise separable convolutions），在有效捕捉梅尔频谱特征的同时保持极低的计算开销和强健的抗噪能力。实验表明，该方法在MIR-1K数据集上达到96.79%的Raw Pitch Accuracy（RPA），且实时因子（Real-Time Factor, RTF）仅为0.0062（单张RTX 4090 GPU），显著优于现有算法的效率表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15140">https://arxiv.org/abs/2509.15140</a><br>
<strong>作者</strong>: Yuxin Luo,Ruoyi Zhang,Lu-Chuan Liu,Tianyu Li,Hangyu Liu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Under review</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Pitch estimation (PE) in monophonic audio is crucial for MIDI transcription and singing voice conversion (SVC), but existing methods suffer significant performance degradation under noise. In this paper, we propose FCPE, a fast context-based pitch estimation model that employs a Lynx-Net architecture with depth-wise separable convolutions to effectively capture mel spectrogram features while maintaining low computational cost and robust noise tolerance. Experiments show that our method achieves 96.79% Raw Pitch Accuracy (RPA) on the MIR-1K dataset, on par with the state-of-the-art methods. The Real-Time Factor (RTF) is 0.0062 on a single RTX 4090 GPU, which significantly outperforms existing algorithms in efficiency. Code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-13] Large Language Model probabilities cannot distinguish between possible and impossible language</p>
<p>【速读】： 该论文试图解决的问题是：大型语言模型（Large Language Models, LLMs）是否能够准确区分语法上可能与不可能的语言结构，即它们是否具备对“语法可能性”的内在敏感性。此前的研究存在争议，部分研究认为模型能识别语法错误，但其结论受到测试材料可靠性的质疑。本文的关键解决方案在于使用模型内部表示（model-internal representations），通过一个新型基准测试，计算最小配对（minimal-pair）的困惑度差异（surprisal differences），并系统比较模型对四类句子的概率响应：（i）低频语法正确句、（ii）语法错误句、（iii）语义异常句、（iv）语用异常句。结果表明，语法错误句并未在困惑度上表现出独特峰值，而语义和语用异常句反而显示出更高困惑度，说明基于输出概率的代理指标不能可靠反映模型内部的句法知识，从而揭示了现有方法在评估LLMs语法能力上的局限性，并呼吁采用更直接的方法验证其语法辨别能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15114">https://arxiv.org/abs/2509.15114</a><br>
<strong>作者</strong>: Evelina Leivada,Raquel Montero,Paolo Morosi,Natalia Moskvina,Tamara Serrano,Marcel Aguilar,Fritz Guenther<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:A controversial test for Large Language Models concerns the ability to discern possible from impossible language. While some evidence attests to the models’ sensitivity to what crosses the limits of grammatically impossible language, this evidence has been contested on the grounds of the soundness of the testing material. We use model-internal representations to tap directly into the way Large Language Models represent the ‘grammatical-ungrammatical’ distinction. In a novel benchmark, we elicit probabilities from 4 models and compute minimal-pair surprisal differences, juxtaposing probabilities assigned to grammatical sentences to probabilities assigned to (i) lower frequency grammatical sentences, (ii) ungrammatical sentences, (iii) semantically odd sentences, and (iv) pragmatically odd sentences. The prediction is that if string-probabilities can function as proxies for the limits of grammar, the ungrammatical condition will stand out among the conditions that involve linguistic violations, showing a spike in the surprisal rates. Our results do not reveal a unique surprisal signature for ungrammatical prompts, as the semantically and pragmatically odd conditions consistently show higher surprisal. We thus demonstrate that probabilities do not constitute reliable proxies for model-internal representations of syntactic knowledge. Consequently, claims about models being able to distinguish possible from impossible language need verification through a different methodology.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-14] DRM: Smooth Reward Models with Temporal Difference for <mark class="hl-label green">LLM</mark>  RL and Inference</p>
<p>【速读】： 该论文旨在解决现有奖励模型（Reward Models）在强化学习（Reinforcement Learning, RL）过程中缺乏时间一致性（temporal consistency）的问题，这会导致策略更新无效和训练不稳定。其解决方案的关键在于引入TDRM（Temporal Difference Regularization for Reward Modeling），通过在训练中最小化时间差分（Temporal Difference, TD）误差，使奖励模型输出更加平滑且与长期目标对齐。该方法可集成到基于Actor-Critic的在线RL循环中，并显著提升性能，尤其在Best-of-N和树搜索（tree-search）场景下效果明显；同时，TDRM还可与可验证奖励方法（如RLVR）结合使用，实现更高效的数据利用，例如仅用2.5k数据即可达到传统方法50.1k数据的效果。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15110">https://arxiv.org/abs/2509.15110</a><br>
<strong>作者</strong>: Dan Zhang,Min Cai,Jonathan Li,Ziniu Hu,Yisong Yue,Yuxiao Dong,Jie Tang<br>
<strong>机构</strong>: Tsinghua University (清华大学); University of Alberta (阿尔伯塔大学); California Institute of Technology (加州理工学院); University of Southampton (南安普顿大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  9 figures, 7 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reward models are central to both reinforcement learning (RL) with language models and inference-time verification. However, existing reward models often lack temporal consistency, leading to ineffective policy updates and unstable RL training. We introduce TDRM, a method for learning smoother and more reliable reward models by minimizing temporal differences during training. This temporal-difference (TD) regularization produces smooth rewards and improves alignment with long-term objectives. Incorporating TDRM into the actor-critic style online RL loop yields consistent empirical gains. It is worth noting that TDRM is a supplement to verifiable reward methods, and both can be used in series. Experiments show that TD-trained process reward models (PRMs) improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. When combined with Reinforcement Learning with Verifiable Rewards (RLVR), TD-trained PRMs lead to more data-efficient RL – achieving comparable performance with just 2.5k data to what baseline methods require 50.1k data to attain – and yield higher-quality language model policies on 8 model variants (5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414, Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release all code at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-15] xtMine: <mark class="hl-label green">LLM</mark> -Powered Knowledge Extraction for Humanitarian Mine Action</p>
<p>【速读】： 该论文旨在解决人道主义排雷（Humanitarian Mine Action, HMA）领域中大量最佳实践知识被锁在非结构化报告中的问题，从而阻碍了知识的高效利用与共享。其解决方案的关键在于提出TextMine——一个基于本体（ontology）引导的流水线框架，利用大语言模型（Large Language Models, LLMs）从HMA文本中自动抽取结构化的知识三元组（knowledge triples）。该方法通过文档分块、领域感知提示设计、三元组提取以及基于参考和LLM-as-a-Judge的双重评估机制，显著提升了抽取准确性（提升44.2%）、减少幻觉（降低22.5%）并增强格式一致性（提升20.9%），同时构建了首个HMA本体和真实排雷报告数据集，为非结构化文本向结构化知识的转化提供了可迁移的技术路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15098">https://arxiv.org/abs/2509.15098</a><br>
<strong>作者</strong>: Chenyue Zhou,Gürkan Solmaz,Flavio Cirillo,Kiril Gashteovski,Jonathan Fürst<br>
<strong>机构</strong>: NEC Laboratories Europe (NEC实验室欧洲); University of Stuttgart (斯图加特大学); Zurich University of Applied Sciences (苏黎世应用科学大学); CAIR, Ss. Cyril and Methodius University of Skopje (CAIR，斯科普里圣西里尔和美多德大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Humanitarian Mine Action has generated extensive best-practice knowledge, but much remains locked in unstructured reports. We introduce TextMine, an ontology-guided pipeline that uses Large Language Models to extract knowledge triples from HMA texts. TextMine integrates document chunking, domain-aware prompting, triple extraction, and both reference-based and LLM-as-a-Judge evaluation. We also create the first HMA ontology and a curated dataset of real-world demining reports. Experiments show ontology-aligned prompts boost extraction accuracy by 44.2%, cut hallucinations by 22.5%, and improve format conformance by 20.9% over baselines. While validated on Cambodian reports, TextMine can adapt to global demining efforts or other domains, transforming unstructured data into structured knowledge.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-16] <mark class="hl-label green">LLM</mark> -OREF: An Open Relation Extraction Framework Based on <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决开放关系抽取（Open Relation Extraction, OpenRE）中依赖人工标注新关系的问题，其核心挑战在于如何在不引入人类干预的情况下，自动为测试实例分配新的关系类别。现有方法通常将OpenRE建模为聚类任务，通过相似性聚类后由人工定义关系标签，限制了实际应用的可扩展性。本文提出了一种基于大语言模型（Large Language Models, LLMs）的框架，关键创新在于设计了一个包含关系发现器（Relation Discoverer, RD）和关系预测器（Relation Predictor, RP）的两阶段机制：首先利用训练集中已知关系的示例（demonstrations）引导RD初步预测测试实例的新关系；随后通过交叉验证筛选高置信度实例构建高质量演示集，并借助RP进行二次推理以提升预测准确性。整个流程结合自校正推理策略（包括关系发现、去噪与再预测），显著提升了对未见关系的识别能力，在三个公开数据集上验证了有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15089">https://arxiv.org/abs/2509.15089</a><br>
<strong>作者</strong>: Hongyao Tu,Liang Zhang,Yujie Lin,Xin Lin,Haibo Zhang,Long Zhang,Jinsong Su<br>
<strong>机构</strong>: School of Informatics, Xiamen University, China(厦门大学信息学院); LLM Team, Shopee Pte. Ltd.(Shopee 语言模型团队); National Institute for Data Science in Health and Medicine, Xiamen University(厦门大学健康与医学数据科学国家研究所)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The goal of open relation extraction (OpenRE) is to develop an RE model that can generalize to new relations not encountered during training. Existing studies primarily formulate OpenRE as a clustering task. They first cluster all test instances based on the similarity between the instances, and then manually assign a new relation to each cluster. However, their reliance on human annotation limits their practicality. In this paper, we propose an OpenRE framework based on large language models (LLMs), which directly predicts new relations for test instances by leveraging their strong language understanding and generation abilities, without human intervention. Specifically, our framework consists of two core components: (1) a relation discoverer (RD), designed to predict new relations for test instances based on \textitdemonstrations formed by training instances with known relations; and (2) a relation predictor (RP), used to select the most likely relation for a test instance from  n  candidate relations, guided by \textitdemonstrations composed of their instances. To enhance the ability of our framework to predict new relations, we design a self-correcting inference strategy composed of three stages: relation discovery, relation denoising, and relation prediction. In the first stage, we use RD to preliminarily predict new relations for all test instances. Next, we apply RP to select some high-reliability test instances for each new relation from the prediction results of RD through a cross-validation method. During the third stage, we employ RP to re-predict the relations of all test instances based on the demonstrations constructed from these reliable test instances. Extensive experiments on three OpenRE datasets demonstrate the effectiveness of our framework. We release our code at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-17] Can maiBERT Speak for Maithili?</p>
<p>【速读】： 该论文旨在解决低资源语言（如Maithili）在自然语言理解（Natural Language Understanding, NLU）任务中因高质量语料稀缺和缺乏专用语言模型而导致的计算资源不足问题。解决方案的关键在于构建并预训练了一个专为Maithili设计的BERT基础语言模型maiBERT，该模型基于掩码语言建模（Masked Language Modeling, MLM）技术，在新构建的Maithili语料库上进行训练，并通过新闻分类任务验证其性能。实验表明，maiBERT在准确率上优于现有区域模型（如NepBERTa和HindiBERT），整体提升达0.13%，且在多个类别上提升达5–7%，从而为Maithili等低资源语言的下游任务（如情感分析和命名实体识别）提供了可复用的高质量预训练模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15048">https://arxiv.org/abs/2509.15048</a><br>
<strong>作者</strong>: Sumit Yadav,Raju Kumar Yadav,Utsav Maskey,Gautam Siddharth Kashyap Md Azizul Hoque,Ganesh Gautam<br>
<strong>机构</strong>: IOE, Pulchowk Campus, Lalitpur, Nepal (工程学院，普尔乔克校区，拉利特普尔，尼泊尔); Macquarie University, Sydney, Australia (麦考瑞大学，悉尼，澳大利亚)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Preprint</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Natural Language Understanding (NLU) for low-resource languages remains a major challenge in NLP due to the scarcity of high-quality data and language-specific models. Maithili, despite being spoken by millions, lacks adequate computational resources, limiting its inclusion in digital and AI-driven applications. To address this gap, we introducemaiBERT, a BERT-based language model pre-trained specifically for Maithili using the Masked Language Modeling (MLM) technique. Our model is trained on a newly constructed Maithili corpus and evaluated through a news classification task. In our experiments, maiBERT achieved an accuracy of 87.02%, outperforming existing regional models like NepBERTa and HindiBERT, with a 0.13% overall accuracy gain and 5-7% improvement across various classes. We have open-sourced maiBERT on Hugging Face enabling further fine-tuning for downstream tasks such as sentiment analysis and Named Entity Recognition (NER).<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-18] Value-Guided KV Compression for <mark class="hl-label green">LLM</mark> s via Approximated CUR Decomposition</p>
<p>【速读】： 该论文旨在解决自回归语言模型推理过程中键值（Key-Value, KV）缓存压缩带来的精度损失问题。现有方法主要基于查询-键注意力分数来排序和淘汰缓存标记，假设注意力强度与语义重要性相关，但忽略了值向量（value vectors）对注意力输出的直接影响。解决方案的关键在于提出一种以值为中心的KV压缩方法CurDKV，其核心是利用CUR矩阵分解计算杠杆得分（leverage scores），从而选择关键的键和值向量，以近似注意力输出的主导子空间，即 softmax(QK^T)V 的主要成分，确保保留的缓存项最大程度地保持模型预测行为。理论分析表明，仅优化注意力分数近似无法保证输出保真度，而CurDKV通过最小化端到端注意力重建损失实现了更优的压缩效果，在LLaMA和Mistral模型上相比SnapKV和ChunkKV等先进方法在高压缩率下提升高达9.6%的准确性，并显著降低生成延迟达40%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15038">https://arxiv.org/abs/2509.15038</a><br>
<strong>作者</strong>: Ayan Sengupta,Siddhant Chaudhary,Tanmoy Chakraborty<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Key-value (KV) cache compression has emerged as a critical technique for reducing the memory and latency overhead of autoregressive language models during inference. Prior approaches predominantly rely on query-key attention scores to rank and evict cached tokens, assuming that attention intensity correlates with semantic importance. However, this heuristic overlooks the contribution of value vectors, which directly influence the attention output. In this paper, we propose CurDKV, a novel, value-centric KV compression method that selects keys and values based on leverage scores computed from CUR matrix decomposition. Our approach approximates the dominant subspace of the attention output  softmax(QK^T)V , ensuring that the retained tokens best preserve the model’s predictive behavior. Theoretically, we show that attention score approximation does not guarantee output preservation, and demonstrate that CUR-based selection minimizes end-to-end attention reconstruction loss. Empirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art methods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA and Mistral, while maintaining compatibility with FlashAttention and Grouped Query Attention. In addition to improved accuracy, CurDKV reduces generation latency by up to 40% at high compression, offering a practical speed-accuracy tradeoff.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-19] CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在文本改写任务，特别是论点改进（Argument Improvement, ArgImp）中的行为机制不明确的问题。现有研究多集中于通用文本生成任务，而对LLMs如何具体优化论证性文本的细节缺乏系统分析。其解决方案的关键在于提出CLEAR评估框架——一个包含57项指标、覆盖词汇（lexical）、句法（syntactic）、语义（semantic）和语用（pragmatic）四个语言层次的综合评价体系，并基于此对多种LLM在多个论证语料库上的改写效果进行量化分析。结果表明，LLMs在ArgImp中主要通过缩短文本长度、增加平均词长及合并句子来提升说服力（persuasion）与连贯性（coherence）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15027">https://arxiv.org/abs/2509.15027</a><br>
<strong>作者</strong>: Thomas Huber,Christina Niklaus<br>
<strong>机构</strong>: University of St. Gallen (圣加仑大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted at EMNLP 2025 Findings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-20] Mind the Gap: A Closer Look at Tokenization for Multiple-Choice Question Answering with <mark class="hl-label green">LLM</mark> s <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在多项选择题问答（Multiple-Choice Question Answering, MCQA）评估中因提示词（prompt）后空格与冒号的分词方式不一致而导致的性能差异问题。研究发现，这种看似微不足道的分词策略变化可引发高达11%的准确率波动，并导致模型排名重排，从而影响LLM比较结果的可靠性。解决方案的关键在于采用一种特定的分词策略——将冒号后的空格与答案字母合并为一个token，该策略在多个数据集上均表现出一致且统计显著的性能提升，并增强了模型输出的校准性（calibration），从而提高了评估结果的可信度和可比性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15020">https://arxiv.org/abs/2509.15020</a><br>
<strong>作者</strong>: Mario Sanz-Guerrero,Minh Duc Bui,Katharina von der Wense<br>
<strong>机构</strong>: Johannes Gutenberg University Mainz (美因茨约翰内斯古腾堡大学); University of Colorado Boulder (科罗拉多大学博尔德分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted to EMNLP 2025 Main</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:When evaluating large language models (LLMs) with multiple-choice question answering (MCQA), it is common to end the prompt with the string “Answer:” to facilitate automated answer extraction via next-token probabilities. However, there is no consensus on how to tokenize the space following the colon, often overlooked as a trivial choice. In this paper, we uncover accuracy differences of up to 11% due to this (seemingly irrelevant) tokenization variation as well as reshuffled model rankings, raising concerns about the reliability of LLM comparisons in prior work. Surprisingly, we are able to recommend one specific strategy – tokenizing the space together with the answer letter – as we observe consistent and statistically significant performance improvements. Additionally, it improves model calibration, enhancing the reliability of the model’s confidence estimates. Our findings underscore the importance of careful evaluation design and highlight the need for standardized, transparent evaluation protocols to ensure reliable and comparable results.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-21] Explicit vs. Implicit Biographies: Evaluating and Adapting <mark class="hl-label green">LLM</mark>  Information Extraction on Wikidata-Derived Texts</p>
<p>【速读】： 该论文旨在解决自然语言处理（Natural Language Processing, NLP）中文本隐含性（Text Implicitness）带来的挑战，即如何在缺乏显式表述的情况下准确提取实体及其关系。例如，在句子“Zuhdi attends church every Sunday”中，人类可推断出Zuhdi与基督教（Christianity）之间的关联，但传统方法难以自动识别此类隐含信息。为应对这一问题，研究提出使用低秩适配（LoRA, Low-Rank Adaptation）对预训练大语言模型（LLMs）如LLaMA 2.3、DeepSeekV1和Phi1.5进行微调，并通过构建包含10k条隐含与显式人物信息的合成数据集来评估其在信息抽取（Information Extraction, IE）任务中的表现。实验结果表明，基于LoRA的微调显著提升了模型在隐含语境下的推理能力，增强了模型在IE任务中的泛化性能、可解释性和可靠性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14943">https://arxiv.org/abs/2509.14943</a><br>
<strong>作者</strong>: Alessandra Stramiglio,Andrea Schimmenti,Valentina Pasqual,Marieke van Erp,Francesco Sovrano,Fabio Vitali<br>
<strong>机构</strong>: University of Bologna (博洛尼亚大学); Automobili Lamborghini SpA (兰博基尼公司); Digital Humanities Advanced Research Center (/DH.arc) (数字人文高级研究中心); KNAW Humanities Cluster, DHLab (荷兰皇家科学院人文学科集群，数字人文实验室); ETH Zurich (苏黎世联邦理工学院)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Text Implicitness has always been challenging in Natural Language Processing (NLP), with traditional methods relying on explicit statements to identify entities and their relationships. From the sentence “Zuhdi attends church every Sunday”, the relationship between Zuhdi and Christianity is evident for a human reader, but it presents a challenge when it must be inferred automatically. Large language models (LLMs) have proven effective in NLP downstream tasks such as text comprehension and information extraction (IE). This study examines how textual implicitness affects IE tasks in pre-trained LLMs: LLaMA 2.3, DeepSeekV1, and Phi1.5. We generate two synthetic datasets of 10k implicit and explicit verbalization of biographic information to measure the impact on LLM performance and analyze whether fine-tuning implicit data improves their ability to generalize in implicit reasoning tasks. This research presents an experiment on the internal reasoning processes of LLMs in IE, particularly in dealing with implicit and explicit contexts. The results demonstrate that fine-tuning LLM models with LoRA (low-rank adaptation) improves their performance in extracting information from implicit texts, contributing to better model interpretability and reliability.         Subjects:  Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)  Cite as: arXiv:2509.14943 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]    (or  arXiv:2509.14943v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14943">https://doi.org/10.48550/arXiv.2509.14943</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-22] Cross-Modal Knowledge Distillation for Speech <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决语音大语言模型（Speech Large Language Models, Speech LLMs）中存在的灾难性遗忘（catastrophic forgetting）和模态不均衡（modality inequivalence）问题，即在引入语音能力后，即使输入仍为文本，模型的知识保留与推理能力也会下降，且在处理语音查询时性能进一步恶化。解决方案的关键在于提出一种跨模态知识蒸馏框架（cross-modal knowledge distillation framework），通过文本到文本（text-to-text）和语音到文本（speech-to-text）两条通道，将基于文本的教师模型（teacher model）的知识迁移至语音大语言模型，从而有效保持文本知识、提升跨模态对齐能力，并增强基于语音交互的推理性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14930">https://arxiv.org/abs/2509.14930</a><br>
<strong>作者</strong>: Enzhi Wang,Qicheng Li,Zhiyuan Tang,Yuhang Jia<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this work, we present the first systematic evaluation of catastrophic forgetting and modality inequivalence in speech large language models, showing that introducing speech capabilities can degrade knowledge and reasoning even when inputs remain textual, and performance further decreases with spoken queries. To address these challenges, we propose a cross-modal knowledge distillation framework that leverages both text-to-text and speech-to-text channels to transfer knowledge from a text-based teacher model to a speech LLM. Extensive experiments on dialogue and audio understanding tasks validate the effectiveness of our approach in preserving textual knowledge, improving cross-modal alignment, and enhancing reasoning in speech-based interactions.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-23] Patent Language Model Pretraining with ModernBERT</p>
<p>【速读】： 该论文旨在解决通用Transformer语言模型（如BERT）在专利文本等专业领域中性能下降的问题，因为专利文本具有长篇幅、技术性强及法律结构复杂等特点。解决方案的关键在于：首先，基于ModernBERT架构预训练三个面向专利领域的掩码语言模型（masked language models），使用包含超过6000万条专利记录的定制语料库进行训练；其次，引入多项架构优化技术，包括FlashAttention、旋转位置编码（rotary embeddings）和门控线性单元（GLU）前馈层，以提升模型效率与表达能力；最后，通过下游专利分类任务验证，发现所提模型在多个指标上优于通用模型，并在推理速度上显著快于同类专利专用模型（如PatentBERT），体现出领域特定预训练与架构改进对专利自然语言处理任务的重要价值。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14926">https://arxiv.org/abs/2509.14926</a><br>
<strong>作者</strong>: Amirhossein Yousefiramandi,Ciaran Cooney<br>
<strong>机构</strong>: Clarivate(科睿唯安)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  7 pages, 2 figures, 4 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-24] A Comparative Evaluation of <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  for Persian Sentiment Analysis and Emotion Detection in Social Media Texts</p>
<p>【速读】： 该论文旨在解决当前大型语言模型（Large Language Models, LLMs）在跨语言场景下，尤其是对波斯语社交媒体文本的情感分析（sentiment analysis）与情绪检测（emotion detection）任务中性能评估不足的问题。现有研究多集中于英语任务，缺乏对非英语语言如波斯语的系统性比较，导致对多语言AI系统部署中的文化与语言挑战理解有限。解决方案的关键在于设计严谨的实验框架：使用平衡的波斯语数据集（情感分析900条、情绪检测1800条），统一提示（prompt）格式与处理参数，并通过精确率（precision）、召回率（recall）、F1分数及误分类模式等指标进行公平对比。结果表明，尽管所有模型均达到可接受性能水平，GPT-4o在准确率上略优，Gemini 2.0 Flash最具成本效益，同时揭示了情绪检测任务更具挑战性且存在特定于波斯语的误判模式，为波斯语自然语言处理（NLP）应用提供了基准和选型依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14922">https://arxiv.org/abs/2509.14922</a><br>
<strong>作者</strong>: Kian Tohidi,Kia Dashtipour,Simone Rebora,Sevda Pourfaramarz<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  19 pages, 8 Figures, 9 Tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This study presents a comprehensive comparative evaluation of four state-of-the-art Large Language Models (LLMs)–Claude 3.7 Sonnet, DeepSeek-V3, Gemini 2.0 Flash, and GPT-4o–for sentiment analysis and emotion detection in Persian social media texts. Comparative analysis among LLMs has witnessed a significant rise in recent years, however, most of these analyses have been conducted on English language tasks, creating gaps in understanding cross-linguistic performance patterns. This research addresses these gaps through rigorous experimental design using balanced Persian datasets containing 900 texts for sentiment analysis (positive, negative, neutral) and 1,800 texts for emotion detection (anger, fear, happiness, hate, sadness, surprise). The main focus was to allow for a direct and fair comparison among different models, by using consistent prompts, uniform processing parameters, and by analyzing the performance metrics such as precision, recall, F1-scores, along with misclassification patterns. The results show that all models reach an acceptable level of performance, and a statistical comparison of the best three models indicates no significant differences among them. However, GPT-4o demonstrated a marginally higher raw accuracy value for both tasks, while Gemini 2.0 Flash proved to be the most cost-efficient. The findings indicate that the emotion detection task is more challenging for all models compared to the sentiment analysis task, and the misclassification patterns can represent some challenges in Persian language texts. These findings establish performance benchmarks for Persian NLP applications and offer practical guidance for model selection based on accuracy, efficiency, and cost considerations, while revealing cultural and linguistic challenges that require consideration in multilingual AI system deployment.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-25] FURINA: Free from Unmergeable Router via LINear Aggregation of mixed experts</p>
<p>【速读】： 该论文旨在解决现有混合专家（Mixture of Experts, MoE）增强的低秩适配（Low-Rank Adaptation, LoRA）方法依赖离散路由机制（discrete router）的问题，该机制导致MoE组件无法被整合进主干模型，从而引入额外的推理时间开销和复杂性。解决方案的关键在于提出FURINA框架——一种无需不可合并路由（unmergeable router）的新型MoE-LoRA方法，其核心创新包括：(1) 解耦LoRA适配器的方向与幅值学习；(2) 引入共享可学习幅值向量以实现一致的激活缩放；(3) 设计专家选择损失函数促进专家激活的多样性与稀疏性。通过输入与各适配器方向分量之间的角度相似度实现自路由（Self-Routing），并由共享幅值向量动态调整专家输出幅度，使模型在不依赖显式路由器的情况下实现高效、灵活且可完全融合至主干模型的专家选择机制，从而在保持高性能的同时消除推理阶段的额外计算负担。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14900">https://arxiv.org/abs/2509.14900</a><br>
<strong>作者</strong>: Jiayi Han,Liang Du,Yinda Chen,Xiao Kang,Weiyang Ding,Donghong Han<br>
<strong>机构</strong>: Inspur Genersoft (浪潮通用软件); Inspur Group (浪潮集团); Interactive Entertainment Group (腾讯互动娱乐集团); Tencent Inc. (腾讯公司); Shandong University (山东大学); Fudan University (复旦大学); Northeastern University (东北大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  15 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The Mixture of Experts (MoE) paradigm has been successfully integrated into Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning (PEFT), delivering performance gains with minimal parameter overhead. However, a key limitation of existing MoE-LoRA methods is their reliance on a discrete router, which prevents the integration of the MoE components into the backbone model. To overcome this, we propose FURINA, a novel Free from Unmergeable Router framework based on the LINear Aggregation of experts. FURINA eliminates the router by introducing a Self-Routing mechanism. This is achieved through three core innovations: (1) decoupled learning of the direction and magnitude for LoRA adapters, (2) a shared learnable magnitude vector for consistent activation scaling, and (3) expert selection loss that encourages divergent expert activation. The proposed mechanism leverages the angular similarity between the input and each adapter’s directional component to activate experts, which are then scaled by the shared magnitude vector. This design allows the output norm to naturally reflect the importance of each expert, thereby enabling dynamic, router-free routing. The expert selection loss further sharpens this behavior by encouraging sparsity and aligning it with standard MoE activation patterns. We also introduce a shared expert within the MoE-LoRA block that provides stable, foundational knowledge. To the best of our knowledge, FURINA is the first router-free, MoE-enhanced LoRA method that can be fully merged into the backbone model, introducing zero additional inference-time cost or complexity. Extensive experiments demonstrate that FURINA not only significantly outperforms standard LoRA but also matches or surpasses the performance of existing MoE-LoRA methods, while eliminating the extra inference-time overhead of MoE.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-26] A Multi-To-One Interview Paradigm for Efficient M<mark class="hl-label green">LLM</mark>  Evaluation</p>
<p>【速读】： 该论文旨在解决多模态大语言模型（Multi-Modal Large Language Models, MLLMs）评估中传统全覆盖问答评测因冗余度高、效率低而导致的瓶颈问题。其解决方案的关键在于提出一种受人类面试流程启发的“多对一访谈”（multi-to-one interview）范式，包含三个核心机制：(i) 两阶段访谈策略（预访谈与正式访谈），(ii) 动态调整访谈者权重以保障公平性，以及 (iii) 自适应选择题目难度层级的机制。实验证明，该方法在保持高评估相关性的同时显著减少所需题量，在PLCC和SRCC指标上相较随机采样提升最高达17.6%和16.7%，为大规模MLLM基准测试提供了一种可靠且高效的替代方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14886">https://arxiv.org/abs/2509.14886</a><br>
<strong>作者</strong>: Ye Shen,Junying Wang,Farong Wen,Yijin Guo,Qi Jia,Zicheng Zhang,Guangtao Zhai<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  5 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid progress of Multi-Modal Large Language Models (MLLMs) has spurred the creation of numerous benchmarks. However, conventional full-coverage Question-Answering evaluations suffer from high redundancy and low efficiency. Inspired by human interview processes, we propose a multi-to-one interview paradigm for efficient MLLM evaluation. Our framework consists of (i) a two-stage interview strategy with pre-interview and formal interview phases, (ii) dynamic adjustment of interviewer weights to ensure fairness, and (iii) an adaptive mechanism for question difficulty-level chosen. Experiments on different benchmarks show that the proposed paradigm achieves significantly higher correlation with full-coverage results than random sampling, with improvements of up to 17.6% in PLCC and 16.7% in SRCC, while reducing the number of required questions. These findings demonstrate that the proposed paradigm provides a reliable and efficient alternative for large-scale MLLM benchmarking.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-27] Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens</p>
<p>【速读】： 该论文旨在解决多模态语音建模中语义与声学信息难以协同优化的问题，尤其关注如何在统一框架下实现高质量语音生成的同时保持说话人身份一致性和长期语义连贯性。其解决方案的关键在于提出Llama-Mimi，一个采用统一分词器（unified tokenizer）和单一Transformer解码器的语音语言模型（speech language model），能够联合建模交错排列的语义令牌（semantic tokens）与声学令牌（acoustic tokens），从而在端到端架构中实现声学一致性与语音内容质量的协同提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14882">https://arxiv.org/abs/2509.14882</a><br>
<strong>作者</strong>: Issa Sugiura,Shuhei Kurita,Yusuke Oda,Ryuichiro Higashinaka<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  5 pages, 1 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We propose Llama-Mimi, a speech language model that uses a unified tokenizer and a single Transformer decoder to jointly model sequences of interleaved semantic and acoustic tokens. Comprehensive evaluation shows that Llama-Mimi achieves state-of-the-art performance in acoustic consistency and possesses the ability to preserve speaker identity. Our analysis further demonstrates that increasing the number of quantizers improves acoustic fidelity but degrades linguistic performance, highlighting the inherent challenge of maintaining long-term coherence. We additionally introduce an LLM-as-a-Judge-based evaluation to assess the spoken content quality of generated outputs. Our models, code, and speech samples are publicly available.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-28] MARIC: Multi-<mark class="hl-label green">Agent</mark>  <mark class="hl-label green">Reasoning</mark>  for Image Classification</p>
<p>【速读】： 该论文旨在解决传统图像分类方法依赖参数密集型训练、需大规模标注数据及大量微调的问题，以及现有视觉语言模型（VLMs）因单次遍历表示而难以捕捉视觉内容互补信息的局限性。其解决方案的关键在于提出一种多智能体推理框架——MARIC（Multi Agent based Reasoning for Image Classification），将图像分类重构为协作式推理过程：首先由Outline Agent分析图像全局主题并生成目标提示，随后三个Aspect Agent从不同视觉维度提取细粒度描述，最终由Reasoning Agent通过集成反思步骤融合这些互补输出，形成统一表征用于分类。该设计通过显式分解任务视角并促进反思性合成，有效缓解了参数冗余与单一模型推理的不足。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14860">https://arxiv.org/abs/2509.14860</a><br>
<strong>作者</strong>: Wonduk Seo,Minhyeong Yu,Hyunjin An,Seunghyun Lee<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>)<br>
<strong>备注</strong>:  Preprint</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Image classification has traditionally relied on parameter-intensive model training, requiring large-scale annotated datasets and extensive fine tuning to achieve competitive performance. While recent vision language models (VLMs) alleviate some of these constraints, they remain limited by their reliance on single pass representations, often failing to capture complementary aspects of visual content. In this paper, we introduce Multi Agent based Reasoning for Image Classification (MARIC), a multi agent framework that reformulates image classification as a collaborative reasoning process. MARIC first utilizes an Outliner Agent to analyze the global theme of the image and generate targeted prompts. Based on these prompts, three Aspect Agents extract fine grained descriptions along distinct visual dimensions. Finally, a Reasoning Agent synthesizes these complementary outputs through integrated reflection step, producing a unified representation for classification. By explicitly decomposing the task into multiple perspectives and encouraging reflective synthesis, MARIC mitigates the shortcomings of both parameter-heavy training and monolithic VLM reasoning. Experiments on 4 diverse image classification benchmark datasets demonstrate that MARIC significantly outperforms baselines, highlighting the effectiveness of multi-agent visual reasoning for robust and interpretable image classification.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-29] Empathy-R1: A Chain-of-Empathy and Reinforcement Learning Framework for Long-Form Mental Health Support</p>
<p>【速读】： 该论文旨在解决当前大型语言模型（Large Language Models, LLMs）在处理长篇心理咨询文本（Long Counseling Texts, LCTs）时，虽能生成语义流畅的回复，但缺乏结构化推理能力，难以提供真正有效的心理支持，尤其是在中文语境下的适配性不足的问题。解决方案的关键在于提出Empathy-R1框架，其核心创新是引入链式共情推理（Chain-of-Empathy, CoE）机制，模拟认知行为疗法（Cognitive-Behavioral Therapy）的逻辑顺序，引导模型依次分析求助者的情绪、成因与意图，从而实现可解释且具治疗意义的响应生成；同时结合监督微调（Supervised Fine-Tuning）与强化学习（Reinforcement Learning, RL）两阶段训练策略，利用专门构建的中文共情问答数据集Empathy-QA和奖励模型优化最终输出的临床相关性和情境适切性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14851">https://arxiv.org/abs/2509.14851</a><br>
<strong>作者</strong>: Xianrong Yao,Dong She,Chenxu Zhang,Yimeng Zhang,Yueru Sun,Noman Ahmed,Yang Gao,Zhanpeng Jin<br>
<strong>机构</strong>: South China University of Technology (华南理工大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Empathy is critical for effective mental health support, especially when addressing Long Counseling Texts (LCTs). However, existing Large Language Models (LLMs) often generate replies that are semantically fluent but lack the structured reasoning necessary for genuine psychological support, particularly in a Chinese context. To bridge this gap, we introduce Empathy-R1, a novel framework that integrates a Chain-of-Empathy (CoE) reasoning process with Reinforcement Learning (RL) to enhance response quality for LCTs. Inspired by cognitive-behavioral therapy, our CoE paradigm guides the model to sequentially reason about a help-seeker’s emotions, causes, and intentions, making its thinking process both transparent and interpretable. Our framework is empowered by a new large-scale Chinese dataset, Empathy-QA, and a two-stage training process. First, Supervised Fine-Tuning instills the CoE’s reasoning structure. Subsequently, RL, guided by a dedicated reward model, refines the therapeutic relevance and contextual appropriateness of the final responses. Experiments show that Empathy-R1 achieves strong performance on key automatic metrics. More importantly, human evaluations confirm its superiority, showing a clear preference over strong baselines and achieving a Win@1 rate of 44.30% on our new benchmark. By enabling interpretable and contextually nuanced responses, Empathy-R1 represents a significant advancement in developing responsible and genuinely beneficial AI for mental health support.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-30] V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决视觉语言模型（Vision-Language Models, VLMs）在因果可解释性研究中，因视觉干预多依赖粗粒度像素级扰动而导致语义层面理解不足的问题。其解决方案的关键在于提出V-SEAM框架，该框架融合了视觉语义编辑（Visual Semantic Editing）与注意力调制（Attention Modulating），实现了概念级别的视觉操作，并能识别在对象、属性和关系三个语义层级上对预测具有正向或负向贡献的注意力头（attention heads）。通过该方法，研究者不仅揭示了不同语义层级下注意力头的共享与差异特性，还开发了一种自动调节关键注意力头嵌入的方法，在LLaVA和InstructBLIP模型上显著提升了跨三个多样化视觉问答（VQA）基准的任务性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14837">https://arxiv.org/abs/2509.14837</a><br>
<strong>作者</strong>: Qidong Wang,Junjie Hu,Ming Jiang<br>
<strong>机构</strong>: Tongji University (同济大学); University of Wisconsin-Madison (威斯康星大学麦迪逊分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  EMNLP 2025 Main</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in causal interpretability have extended from language models to vision-language models (VLMs), seeking to reveal their internal mechanisms through input interventions. While textual interventions often target semantics, visual interventions typically rely on coarse pixel-level perturbations, limiting semantic insights on multimodal integration. In this study, we introduce V-SEAM, a novel framework that combines Visual Semantic Editing and Attention Modulating for causal interpretation of VLMs. V-SEAM enables concept-level visual manipulations and identifies attention heads with positive or negative contributions to predictions across three semantic levels: objects, attributes, and relationships. We observe that positive heads are often shared within the same semantic level but vary across levels, while negative heads tend to generalize broadly. Finally, we introduce an automatic method to modulate key head embeddings, demonstrating enhanced performance for both LLaVA and InstructBLIP across three diverse VQA benchmarks. Our data and code are released at: this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-31] <mark class="hl-label green">LLM</mark>  <mark class="hl-label green">Agents</mark>  at the Roundtable: A Multi-Perspective and Dialectical <mark class="hl-label green">Reasoning</mark>  Framework for Essay Scoring</p>
<p>【速读】： 该论文旨在解决自动化作文评分（Automated Essay Scoring, AES）中难以实现人类水平的多维度理解与判断的问题，尤其是在零样本（zero-shot）场景下。其解决方案的关键在于提出了一种名为Roundtable Essay Scoring (RES) 的多智能体评估框架：该框架基于大语言模型（Large Language Models, LLMs）构建多个具有特定提示和主题背景的评估代理（evaluator agents），每个代理独立生成基于特质的评分量规并执行多视角评估；随后通过模拟圆桌讨论的形式，利用辩证推理过程整合个体评估结果，从而输出更贴近人类评判的综合分数。此机制通过促进不同评估视角代理间的协作与共识，显著提升了零样本条件下的评分准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14834">https://arxiv.org/abs/2509.14834</a><br>
<strong>作者</strong>: Jinhee Jang,Ayoung Moon,Minkyoung Jung,YoungBin Kim. Seung Jin Lee<br>
<strong>机构</strong>: NC AI; Chung-Ang University (中央大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The emergence of large language models (LLMs) has brought a new paradigm to automated essay scoring (AES), a long-standing and practical application of natural language processing in education. However, achieving human-level multi-perspective understanding and judgment remains a challenge. In this work, we propose Roundtable Essay Scoring (RES), a multi-agent evaluation framework designed to perform precise and human-aligned scoring under a zero-shot setting. RES constructs evaluator agents based on LLMs, each tailored to a specific prompt and topic context. Each agent independently generates a trait-based rubric and conducts a multi-perspective evaluation. Then, by simulating a roundtable-style discussion, RES consolidates individual evaluations through a dialectical reasoning process to produce a final holistic score that more closely aligns with human evaluation. By enabling collaboration and consensus among agents with diverse evaluation perspectives, RES outperforms prior zero-shot AES approaches. Experiments on the ASAP dataset using ChatGPT and Claude show that RES achieves up to a 34.86% improvement in average QWK over straightforward prompting (Vanilla) methods.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-32] ReCoVeR the Target Language: Language Steering without Sacrificing Task Performance</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在多语言环境下出现的语言混淆问题，即模型在接收到某种语言的提示（prompt）时，倾向于生成与提示语言不一致的答案，从而影响跨语言任务的准确性。解决方案的关键在于提出了一种轻量级方法 ReCoVeR（REducing language COnfusion in VEctor Representations），其核心是利用多语种平行语料库分离出特定语言的向量表示（language-specific vectors），并通过固定（无监督）或可训练的“语言引导函数”（steering functions）对这些向量进行有效调制，从而实现对模型输出语言的有效控制，同时在不牺牲任务性能的前提下显著降低语言混淆现象。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14814">https://arxiv.org/abs/2509.14814</a><br>
<strong>作者</strong>: Hannah Sterz,Fabian David Schmidt,Goran Glavaš,Ivan Vulić<br>
<strong>机构</strong>: University of Cambridge (剑桥大学); University of Würzburg (维尔茨堡大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As they become increasingly multilingual, Large Language Models (LLMs) exhibit more language confusion, i.e., they tend to generate answers in a language different from the language of the prompt or the answer language explicitly requested by the user. In this work, we propose ReCoVeR (REducing language COnfusion in VEctor Representations), a novel lightweight approach for reducing language confusion based on language-specific steering vectors. We first isolate language vectors with the help of multi-parallel corpus and then effectively leverage those vectors for effective LLM steering via fixed (i.e., unsupervised) as well as trainable steering functions. Our extensive evaluation, encompassing three benchmarks and 18 languages, shows that ReCoVeR effectively mitigates language confusion in both monolingual and cross-lingual setups while at the same time – and in contrast to prior language steering methods – retaining task performance. Our data code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-33] SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating Disorders with Natural Language Processing</p>
<p>【速读】： 该论文旨在解决两个临床风险识别问题：一是早期检测病理性赌博的迹象（Task 1），二是量化进食障碍症状的严重程度（Task 3）。针对Task 1，其解决方案的关键在于融合Transformer模型生成的句子嵌入（sentence embeddings）与多维特征，包括体积度量（volumetry）、词汇多样性（lexical diversity）、复杂度指标及情感相关得分；而对于Task 3，则依赖于基于Transformer的上下文词嵌入（contextualized word embeddings）进行文本相似性估计。两种方法均体现出对自然语言处理技术的有效应用，最终在各自任务中均取得第二名的优异成绩。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14806">https://arxiv.org/abs/2509.14806</a><br>
<strong>作者</strong>: Alba Maria Marmol-Romero,Salud Maria Jimenez-Zafra,Flor Miriam Plaza-del-Arco,M. Dolores Molina-Gonzalez,Maria-Teresa Martin-Valdivia,Arturo Montejo-Raez<br>
<strong>机构</strong>: Universidad de Jaén (哈恩大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  11 pages, 1 figure, 4 tables. CLEF (Working Notes). 2022</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, two of the proposed tasks have been addressed: i) Task 1 on the early detection of signs of pathological gambling, and ii) Task 3 on measuring the severity of the signs of eating disorders. The approach presented in Task 1 is based on the use of sentence embeddings from Transformers with features related to volumetry, lexical diversity, complexity metrics, and emotion-related scores, while the approach for Task 3 is based on text similarity estimation using contextualized word embeddings from Transformers. In Task 1, our team has been ranked in second position, with an F1 score of 0.808, out of 41 participant submissions. In Task 3, our team also placed second out of a total of 3 participating teams.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-34] SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing</p>
<p>【速读】： 该论文旨在解决病理赌博（pathological gambling）早期检测的问题，通过分析用户行为数据以识别潜在的赌博成瘾迹象。其解决方案的关键在于采用基于Transformer架构的预训练模型，并结合全面的数据预处理与数据平衡技术，同时引入长短期记忆（Long Short-Term Memory, LSTM）网络与Transformer自动建模（automodels from Transformers）相结合的混合结构，从而提升对早期风险信号的识别能力。该方法在eRisk@CLEF评测中取得第七名的成绩，F1得分为0.126，并在召回率及早期检测相关指标上表现最优。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14797">https://arxiv.org/abs/2509.14797</a><br>
<strong>作者</strong>: Alba Maria Marmol-Romero,Flor Miriam Plaza-del-Arco,Arturo Montejo-Raez<br>
<strong>机构</strong>: University of Jaén (胡安大学); Bocconi University (博科尼大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  9 pages, 2 figures, 4 tables. CLEF (Working Notes). 2023</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper describes the participation of the SINAI team in the eRisk@CLEF lab. Specifically, one of the proposed tasks has been addressed: Task 2 on the early detection of signs of pathological gambling. The approach presented in Task 2 is based on pre-trained models from Transformers architecture with comprehensive preprocessing data and data balancing techniques. Moreover, we integrate Long-short Term Memory (LSTM) architecture with automodels from Transformers. In this Task, our team has been ranked in seventh position, with an F1 score of 0.126, out of 49 participant submissions and achieves the highest values in recall metrics and metrics related to early detection.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-35] Frame Sampling Strategies Matter: A Benchmark for small vision language models</p>
<p>【速读】： 该论文旨在解决当前视频视觉语言模型（Video Vision Language Models, VLMs）评估中存在的帧采样偏差问题，即不同模型在视频问答任务中性能差异可能并非完全由其视觉表征能力决定，而是受所采用的帧采样策略影响。解决方案的关键在于提出首个针对小型视频VLMs的帧级精确基准测试（frame-accurate benchmark），并在统一且受控的帧采样策略下进行评估，从而准确揭示模型性能与采样方法之间的数据特异性与任务特异性关系。通过开源基准测试代码，该研究为社区提供了可复现、无偏的视频VLM评估协议，并强调未来应为每个基准数据集设计标准化的帧采样策略。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14769">https://arxiv.org/abs/2509.14769</a><br>
<strong>作者</strong>: Marija Brkic,Anas Filali Razzouki,Yannis Tevissen,Khalil Guetari,Mounim A. El Yacoubi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Comparing vision language models on videos is particularly complex, as the performances is jointly determined by the model’s visual representation capacity and the frame-sampling strategy used to construct the input. Current video benchmarks are suspected to suffer from substantial frame-sampling bias, as models are evaluated with different frame selection strategies. In this work, we propose the first frame-accurate benchmark of state-of-the-art small VLMs for video question-answering, evaluated under controlled frame-sampling strategies. Our results confirm the suspected bias and highlight both data-specific and task-specific behaviors of SVLMs under different frame-sampling techniques. By open-sourcing our benchmarking code, we provide the community with a reproducible and unbiased protocol for evaluating video VLMs and emphasize the need for standardized frame-sampling strategies tailored to each benchmarking dataset in future research.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-36] <mark class="hl-label green">Reasoning</mark>  over Boundaries: Enhancing Specification Alignment via Test-time Delibration</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在真实场景中因用户或组织定制的**行为规范（behavioral-spec）<strong>和</strong>安全规范（safety-spec）<strong>动态变化而导致的</strong>规范对齐（specification alignment）<strong>问题。现有方法难以同时满足不同场景下对模型行为与安全性的多样化、演化性要求。解决方案的关键在于提出一种轻量级方法 Align3，其核心是引入</strong>测试时反思（Test-Time Deliberation, TTD）**机制，通过分层反思与修正策略来动态推理规范边界，从而提升模型在不修改训练过程的前提下对动态规范的适应能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14760">https://arxiv.org/abs/2509.14760</a><br>
<strong>作者</strong>: Haoran Zhang,Yafu Li,Xuyang Hu,Dongrui Liu,Zhilin Wang,Bo Li,Yu Cheng<br>
<strong>机构</strong>: Shanghai Jiao Tong University (上海交通大学); The Chinese University of Hong Kong (香港中文大学); Shanghai AI Laboratory (上海人工智能实验室); University of Science and Technology of China (中国科学技术大学); University of Illinois at Urbana-Champaign (伊利诺伊大学厄巴纳-香槟分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  10 pages main text, 52 pages total (including appendix)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) are increasingly applied in diverse real-world scenarios, each governed by bespoke behavioral and safety specifications (spec) custom-tailored by users or organizations. These spec, categorized into safety-spec and behavioral-spec, vary across scenarios and evolve with changing preferences and requirements. We formalize this challenge as specification alignment, focusing on LLMs’ ability to follow dynamic, scenario-specific spec from both behavioral and safety perspectives. To address this challenge, we propose Align3, a lightweight method that employs Test-Time Deliberation (TTD) with hierarchical reflection and revision to reason over the specification boundaries. We further present SpecBench, a unified benchmark for measuring specification alignment, covering 5 scenarios, 103 spec, and 1,500 prompts. Experiments on 15 reasoning and 18 instruct models with several TTD methods, including Self-Refine, TPO, and MoreThink, yield three key findings: (i) test-time deliberation enhances specification alignment; (ii) Align3 advances the safety-helpfulness trade-off frontier with minimal overhead; (iii) SpecBench effectively reveals alignment gaps. These results highlight the potential of test-time deliberation as an effective strategy for reasoning over the real-world specification boundaries.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-37] KAIO: A Collection of More Challenging Korean Questions</p>
<p>【速读】： 该论文旨在解决韩国语（Korean）领域缺乏能够有效评估和排名前沿大语言模型（Large Language Models, LLMs）的基准测试（benchmark）的问题。当前韩国语基准普遍数量少、范围窄、更新慢，导致快速饱和与污染，难以追踪模型进展。为此，作者提出了KAIO——一个以数学为中心、强调长链推理能力的韩语基准测试。其关键创新在于：首先，通过聚焦于复杂推理任务保持基准的未饱和状态（如GPT-5得分62.8，远低于理论上限），从而为前沿模型提供持续可测量的性能提升空间；其次，采用私有化部署并由保留评估器（held-out evaluator）控制访问，直到公开最佳模型达到至少80%准确率后再开放数据集，以此显著降低 contamination 风险，确保基准长期有效性与公平性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14752">https://arxiv.org/abs/2509.14752</a><br>
<strong>作者</strong>: Nahyun Lee,Guijin Son,Hyunwoo Ko,Kyubeen Han<br>
<strong>机构</strong>: Chung-Ang University(中央大学); OneLineAI; MODULABS; Konkuk University(国民大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  4 pages paper</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:With the advancement of mid/post-training techniques, LLMs are pushing their boundaries at an accelerated pace. Legacy benchmarks saturate quickly (e.g., broad suites like MMLU over the years, newer ones like GPQA-D even faster), which makes frontier progress hard to track. The problem is especially acute in Korean: widely used benchmarks are fewer, often translated or narrow in scope, and updated more slowly, so saturation and contamination arrive sooner. Accordingly, at this moment, there is no Korean benchmark capable of evaluating and ranking frontier models. To bridge this gap, we introduce KAIO, a Korean, math-centric benchmark that stresses long-chain reasoning. Unlike recent Korean suites that are at or near saturation, KAIO remains far from saturated: the best-performing model, GPT-5, attains 62.8, followed by Gemini-2.5-Pro (52.3). Open models such as Qwen3-235B and DeepSeek-R1 cluster falls below 30, demonstrating substantial headroom, enabling robust tracking of frontier progress in Korean. To reduce contamination, KAIO will remain private and be served via a held-out evaluator until the best publicly known model reaches at least 80% accuracy, after which we will release the set and iterate to a harder version.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-38] Evaluating <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  for Cross-Lingual Retrieval <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决跨语言信息检索（Cross-lingual Information Retrieval, CLIR）中两阶段检索系统中检索器（retriever）与重排序器（reranker）协同作用不明确的问题，特别是针对大语言模型（Large Language Models, LLMs）作为重排序器时的性能表现及其与不同第一阶段检索器配合的效果。其解决方案的关键在于系统性地评估多语言双编码器（multilingual bi-encoders）作为第一阶段检索器的效能，并发现无需机器翻译（Machine Translation, MT）即可显著提升CLIR性能，同时指出随着重排序模型能力增强，翻译带来的收益逐渐减弱；此外，基于指令微调的LLM的成对重排序器（pairwise reranker）在性能上可媲美列表式重排序器（listwise reranker），为CLIR提供更高效、低成本的端到端方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14749">https://arxiv.org/abs/2509.14749</a><br>
<strong>作者</strong>: Longfei Zuo,Pingjun Hong,Oliver Kraus,Barbara Plank,Robert Litschko<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
<strong>备注</strong>:  Accepted at EMNLP 2025 (Findings)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multi-stage information retrieval (IR) has become a widely-adopted paradigm in search. While Large Language Models (LLMs) have been extensively evaluated as second-stage reranking models for monolingual IR, a systematic large-scale comparison is still lacking for cross-lingual IR (CLIR). Moreover, while prior work shows that LLM-based rerankers improve CLIR performance, their evaluation setup relies on lexical retrieval with machine translation (MT) for the first stage. This is not only prohibitively expensive but also prone to error propagation across stages. Our evaluation on passage-level and document-level CLIR reveals that further gains can be achieved with multilingual bi-encoders as first-stage retrievers and that the benefits of translation diminishes with stronger reranking models. We further show that pairwise rerankers based on instruction-tuned LLMs perform competitively with listwise rerankers. To the best of our knowledge, we are the first to study the interaction between retrievers and rerankers in two-stage CLIR with LLMs. Our findings reveal that, without MT, current state-of-the-art rerankers fall severely short when directly applied in CLIR.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-39] UnifiedVisual: A Framework for Constructing Unified Vision-Language Datasets <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决统一视觉大语言模型（Unified Vision Large Language Models, VLLMs）在发展过程中因缺乏能够充分激发其多模态理解与生成能力协同潜力的数据集而受到的限制问题。现有数据集通常将理解与生成任务割裂处理，难以推动模型在两者间的相互促进。解决方案的关键在于提出一种全新的数据集构建框架 UnifiedVisual，并基于此构建了高质量的 UnifiedVisual-240K 数据集，该数据集通过无缝融合多样化的视觉与文本输入输出，支持全面的跨模态推理和精确的文本到图像对齐，从而实现多模态理解与生成之间的显著互增强效应。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14738">https://arxiv.org/abs/2509.14738</a><br>
<strong>作者</strong>: Pengyu Wang,Shaojun Zhou,Chenkun Tan,Xinghao Wang,Wei Huang,Zhen Ye,Zhaowei Li,Botian Jiang,Dong Zhang,Xipeng Qiu<br>
<strong>机构</strong>: Fudan University (复旦大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted by Findings of EMNLP2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Unified vision large language models (VLLMs) have recently achieved impressive advancements in both multimodal understanding and generation, powering applications such as visual question answering and text-guided image synthesis. However, progress in unified VLLMs remains constrained by the lack of datasets that fully exploit the synergistic potential between these two core abilities. Existing datasets typically address understanding and generation in isolation, thereby limiting the performance of unified VLLMs. To bridge this critical gap, we introduce a novel dataset construction framework, UnifiedVisual, and present UnifiedVisual-240K, a high-quality dataset meticulously designed to facilitate mutual enhancement between multimodal understanding and generation. UnifiedVisual-240K seamlessly integrates diverse visual and textual inputs and outputs, enabling comprehensive cross-modal reasoning and precise text-to-image alignment. Our dataset encompasses a wide spectrum of tasks and data sources, ensuring rich diversity and addressing key shortcomings of prior resources. Extensive experiments demonstrate that models trained on UnifiedVisual-240K consistently achieve strong performance across a wide range of tasks. Notably, these models exhibit significant mutual reinforcement between multimodal understanding and generation, further validating the effectiveness of our framework and dataset. We believe UnifiedVisual represents a new growth point for advancing unified VLLMs and unlocking their full potential. Our code and datasets is available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-40] Decoupled Proxy Alignment: Mitigating Language Prior Conflict for Multimodal Alignment in M<mark class="hl-label green">LLM</mark>  <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）中存在的“语言先验冲突”（language prior conflict）问题，即大语言模型（Large Language Models, LLMs）固有的语言先验与训练数据中语言先验之间的不匹配，导致视觉-语言对齐效果不佳，模型倾向于适应训练样本的语言风格而非真正理解视觉内容。解决方案的关键在于提出一种名为解耦代理对齐（Decoupled Proxy Alignment, DPA）的新型训练方法：其一，引入一个代理语言模型（proxy LLM）在预训练阶段将视觉-语言对齐过程与语言先验干扰解耦；其二，基于视觉相关性动态调整损失函数，强化对视觉相关token的优化信号，从而显著缓解语言先验冲突，提升模型在多种数据集、模型架构和规模下的对齐性能与泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14735">https://arxiv.org/abs/2509.14735</a><br>
<strong>作者</strong>: Chenkun Tan,Pengyu Wang,Shaojun Zhou,Botian Jiang,Zhaowei Li,Dong Zhang,Xinghao Wang,Yaqian Zhou,Xipeng Qiu<br>
<strong>机构</strong>: Fudan University (复旦大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted by Findings of EMNLP2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal large language models (MLLMs) have gained significant attention due to their impressive ability to integrate vision and language modalities. Recent advancements in MLLMs have primarily focused on improving performance through high-quality datasets, novel architectures, and optimized training strategies. However, in this paper, we identify a previously overlooked issue, language prior conflict, a mismatch between the inherent language priors of large language models (LLMs) and the language priors in training datasets. This conflict leads to suboptimal vision-language alignment, as MLLMs are prone to adapting to the language style of training samples. To address this issue, we propose a novel training method called Decoupled Proxy Alignment (DPA). DPA introduces two key innovations: (1) the use of a proxy LLM during pretraining to decouple the vision-language alignment process from language prior interference, and (2) dynamic loss adjustment based on visual relevance to strengthen optimization signals for visually relevant tokens. Extensive experiments demonstrate that DPA significantly mitigates the language prior conflict, achieving superior alignment performance across diverse datasets, model families, and scales. Our method not only improves the effectiveness of MLLM training but also shows exceptional generalization capabilities, making it a robust approach for vision-language alignment. Our code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-41] oolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning</p>
<p>【速读】： 该论文旨在解决强化学习（Reinforcement Learning, RL）在大语言模型（Large Language Models, LLM）工具学习中因大量低价值简单样本导致训练效率低下这一问题。现有动态采样技术难以适配工具学习所固有的多任务结构和细粒度奖励机制。解决方案的关键在于提出一种面向课程学习的动态采样框架（Dynamic Sampling with Curriculum Learning, DSCL），其核心包括两个组件：基于奖励的动态采样（Reward-Based Dynamic Sampling），利用多维奖励统计量（均值与方差）筛选高价值数据；以及基于任务的动态课程学习（Task-Based Dynamic Curriculum Learning），自适应地将训练聚焦于尚未掌握的子任务。该方法有效利用了工具学习中的复杂奖励信号与子任务动态特性，显著提升了训练效率与模型性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14718">https://arxiv.org/abs/2509.14718</a><br>
<strong>作者</strong>: Zihao Feng,Xiaoxue Wang,Bowen Wu,Hailong Cao,Tiejun Zhao,Qun Yu,Baoxun Wang<br>
<strong>机构</strong>: Tencent(腾讯); University of Science and Technology of China (中国科学技术大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While reinforcement learning (RL) is increasingly used for LLM-based tool learning, its efficiency is often hampered by an overabundance of simple samples that provide diminishing learning value as training progresses. Existing dynamic sampling techniques are ill-suited for the multi-task structure and fine-grained reward mechanisms inherent to tool learning. This paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework specifically designed to address this challenge by targeting the unique characteristics of tool learning: its multiple interdependent sub-tasks and multi-valued reward functions. DSCL features two core components: Reward-Based Dynamic Sampling, which uses multi-dimensional reward statistics (mean and variance) to prioritize valuable data, and Task-Based Dynamic Curriculum Learning, which adaptively focuses training on less-mastered sub-tasks. Through extensive experiments, we demonstrate that DSCL significantly improves training efficiency and model performance over strong baselines, achieving a 3.29% improvement on the BFCLv3 benchmark. Our method provides a tailored solution that effectively leverages the complex reward signals and sub-task dynamics within tool learning to achieve superior results.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-42] From Ground Trust to Truth: Disparities in Offensive Language Judgments on Contemporary Korean Political Discourse <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前生成式 AI (Generative AI) 在检测政治话语中攻击性语言（offensive language）时存在的两大问题：一是现有研究多依赖过时的数据集，难以反映当代语言演变；二是缺乏对模型在未见文本上泛化能力的有效评估。解决方案的关键在于构建了一个大规模的当代政治话语数据集，并采用三种精细化的无真实标签（ground truth）判断策略，每种策略对应一种典型的攻击性语言检测方法，且针对最优条件进行设计。通过留一法（leave-one-out）分析标签一致性并建立伪标签（pseudo-labels）作为近似真实标签，作者发现经过精心设计的单次提示（single prompting）即可达到与资源密集型方法相当的性能，从而为实际应用场景提供了高效可行的替代方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14712">https://arxiv.org/abs/2509.14712</a><br>
<strong>作者</strong>: Seunguk Yu,Jungmin Yun,Jinhee Jang,Youngbin Kim<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  EMNLP 2025 findings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Although offensive language continually evolves over time, even recent studies using LLMs have predominantly relied on outdated datasets and rarely evaluated the generalization ability on unseen texts. In this study, we constructed a large-scale dataset of contemporary political discourse and employed three refined judgments in the absence of ground truth. Each judgment reflects a representative offensive language detection method and is carefully designed for optimal conditions. We identified distinct patterns for each judgment and demonstrated tendencies of label agreement using a leave-one-out strategy. By establishing pseudo-labels as ground trust for quantitative performance assessment, we observed that a strategically designed single prompting achieves comparable performance to more resource-intensive methods. This suggests a feasible approach applicable in real-world settings with inherent constraints.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-43] HARNESS: Lightweight Distilled Arabic Speech Foundation Models</p>
<p>【速读】： 该论文旨在解决大型预训练语音模型在资源受限环境中的部署难题，同时提升阿拉伯语语音特征的建模能力。其关键解决方案是提出首个以阿拉伯语为中心的自监督语音模型家族 HArnESS，通过迭代自蒸馏（iterative self-distillation）技术将大型双语教师模型（HL）的知识迁移至轻量级学生模型（HS、HST），并结合低秩近似进一步压缩教师模型的离散监督信号，从而在保持阿拉伯语特异性表征的同时显著降低模型复杂度。实验表明，HArnESS 在阿拉伯语自动语音识别（ASR）、说话人情绪识别（SER）和方言识别（DID）任务上优于或媲美 HuBERT 和 XLS-R，且仅需极少微调即可实现最优性能，为低资源场景下的高效语音模型部署提供了可行方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14689">https://arxiv.org/abs/2509.14689</a><br>
<strong>作者</strong>: Vrunda N. sukhadia,Shammur Absar Chowdhury<br>
<strong>机构</strong>: Qatar Computing Research Institute (卡塔尔计算研究研究所)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  5 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large pre-trained speech models excel in downstream tasks but their deployment is impractical for resource-limited environments. In this paper, we introduce HArnESS, the first Arabic-centric self-supervised speech model family, designed to capture Arabic speech nuances. Using iterative self-distillation, we train large bilingual HArnESS (HL) SSL models and then distill knowledge into compressed student models (HS, HST), preserving Arabic-specific representations. We use low-rank approximation to further compact the teacher’s discrete supervision into shallow, thin models. We evaluate HArnESS on Arabic ASR, Speaker Emotion Recognition (SER), and Dialect Identification (DID), demonstrating effectiveness against HuBERT and XLS-R. With minimal fine-tuning, HArnESS achieves SOTA or comparable performance, making it a lightweight yet powerful alternative for real-world use. We release our distilled models and findings to support responsible research and deployment in low-resource settings.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-44] ableDART: Dynamic Adaptive Multi-Modal Routing for Table Understanding</p>
<p>【速读】： 该论文旨在解决表格理解中语义与结构信息建模的挑战，现有方法如Table-as-Text会丢失表格结构线索，而Table-as-Image虽保留结构但难以捕捉细粒度语义；当前Table-as-Multimodality策略则因静态处理多模态输入导致冗余和冲突，并依赖昂贵的多模态大语言模型（Multimodal Large Language Models, MLLMs）微调。其解决方案的关键在于提出TableDART框架，通过一个轻量级2.59M参数的MLP门控网络动态选择最优路径（纯文本、纯图像或融合），实现对每个表-查询对的自适应模态决策，从而减少冗余与冲突；同时引入代理机制（agent）分析文本与图像模型输出，进行结果选择或推理合成，避免了全量MLLM微调的高昂成本。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14671">https://arxiv.org/abs/2509.14671</a><br>
<strong>作者</strong>: Xiaobo Xing,Wei Yuan,Tong Chen,Quoc Viet Hung Nguyen,Xiangliang Zhang,Hongzhi Yin<br>
<strong>机构</strong>: The University of Queensland (昆士兰大学); Griffith University (格里菲斯大学); University of Notre Dame (圣母大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Modeling semantic and structural information from tabular data remains a core challenge for effective table understanding. Existing Table-as-Text approaches flatten tables for large language models (LLMs), but lose crucial structural cues, while Table-as-Image methods preserve structure yet struggle with fine-grained semantics. Recent Table-as-Multimodality strategies attempt to combine textual and visual views, but they (1) statically process both modalities for every query-table pair within a large multimodal LLMs (MLLMs), inevitably introducing redundancy and even conflicts, and (2) depend on costly fine-tuning of MLLMs. In light of this, we propose TableDART, a training-efficient framework that integrates multimodal views by reusing pretrained single-modality models. TableDART introduces a lightweight 2.59M-parameter MLP gating network that dynamically selects the optimal path (either Text-only, Image-only, or Fusion) for each table-query pair, effectively reducing redundancy and conflicts from both modalities. In addition, we propose a novel agent to mediate cross-modal knowledge integration by analyzing outputs from text- and image-based models, either selecting the best result or synthesizing a new answer through reasoning. This design avoids the prohibitive costs of full MLLM fine-tuning. Extensive experiments on seven benchmarks show that TableDART establishes new state-of-the-art performance among open-source models, surpassing the strongest baseline by an average of 4.02%. The code is available at: this https URL<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-45] Spatial Audio Motion Understanding and <mark class="hl-label green">Reasoning</mark></p>
<p>【速读】： 该论文旨在解决动态音频场景中移动声源的空间音频理解与推理问题，特别是如何从复杂、重叠的声学事件中提取空间属性并支持对移动声源的语义化问答。其解决方案的关键在于构建一个端到端的框架：首先设计了一个空间音频编码器（spatial audio encoder），能够检测多源重叠事件并估计每帧的到达方向（Direction of Arrival, DoA）和源距离；其次引入音频定位模型（audio grounding model），通过交叉注意力机制将音频特征与语义类别文本嵌入对齐，以增强对未见事件的泛化能力；最后，将结构化的空间属性作为条件输入至大语言模型（Large Language Model, LLM），实现对动态音频场景中移动声源的复杂语义查询推理。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14666">https://arxiv.org/abs/2509.14666</a><br>
<strong>作者</strong>: Arvind Krishna Sridhar,Yinyi Guo,Erik Visser<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  5 pages, 2 figures, 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Spatial audio reasoning enables machines to interpret auditory scenes by understanding events and their spatial attributes. In this work, we focus on spatial audio understanding with an emphasis on reasoning about moving sources. First, we introduce a spatial audio encoder that processes spatial audio to detect multiple overlapping events and estimate their spatial attributes, Direction of Arrival (DoA) and source distance, at the frame level. To generalize to unseen events, we incorporate an audio grounding model that aligns audio features with semantic audio class text embeddings via a cross-attention mechanism. Second, to answer complex queries about dynamic audio scenes involving moving sources, we condition a large language model (LLM) on structured spatial attributes extracted by our model. Finally, we introduce a spatial audio motion understanding and reasoning benchmark dataset and demonstrate our framework’s performance against the baseline model.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-46] Understanding the Thinking Process of <mark class="hl-label green">Reasoning</mark>  Models: A Perspective from Schoenfelds Episode Theory <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大型推理模型（Large Reasoning Models, LRM）在生成链式思维（chain-of-thought reasoning）时缺乏系统性结构理解框架的问题。其解决方案的关键在于引入Schoenfeld的Episode理论——一种经典的人类数学问题求解认知框架，并将其应用于LRM推理轨迹的细粒度分析。研究通过七种认知标签（如Plan、Implement、Verify）对数千条模型生成的解题语句和段落进行标注，构建了首个公开可用的机器推理细粒度分析基准，包括大规模标注语料库与详细的标注指南。该方法为解释LRM的认知机制提供了理论基础，并推动未来可控性和透明度更高的推理系统发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14662">https://arxiv.org/abs/2509.14662</a><br>
<strong>作者</strong>: Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou<br>
<strong>机构</strong>: University of Maryland (马里兰大学)<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  EMNLP2025 main, Camera-ready</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While Large Reasoning Models (LRMs) generate extensive chain-of-thought reasoning, we lack a principled framework for understanding how these thoughts are structured. In this paper, we introduce a novel approach by applying Schoenfeld’s Episode Theory, a classic cognitive framework for human mathematical problem-solving, to analyze the reasoning traces of LRMs. We annotated thousands of sentences and paragraphs from model-generated solutions to math problems using seven cognitive labels (e.g., Plan, Implement, Verify). The result is the first publicly available benchmark for the fine-grained analysis of machine reasoning, including a large annotated corpus and detailed annotation guidebooks. Our preliminary analysis reveals distinct patterns in LRM reasoning, such as the transition dynamics between cognitive states. This framework provides a theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-47] UMA-Split: unimodal aggregation for both English and Mandarin non-autoregressive speech recognition <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决<strong>单模态聚合（Unimodal Aggregation, UMA）方法在非自回归语音识别中对英语等语言效果不佳的问题</strong>。其核心挑战在于：UMA依赖于每个文本词元（token）对应一段先升后降的单峰权重分布来聚合声学帧，但英语中一个音节常被细粒度切分为多个词元，或某些词元覆盖不足3个声学帧，导致无法形成有效的单峰权重，从而削弱模型性能。解决方案的关键是引入一个<strong>简单的分割模块（split module）</strong>，在计算CTC损失前，将每个UMA聚合后的声学帧映射到多个词元，从而增强跨语言的泛化能力，尤其改善了英语语音识别的效果。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14653">https://arxiv.org/abs/2509.14653</a><br>
<strong>作者</strong>: Ying Fang,Xiaofei Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Submit to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes a unimodal aggregation (UMA) based nonautoregressive model for both English and Mandarin speech recognition. The original UMA explicitly segments and aggregates acoustic frames (with unimodal weights that first monotonically increase and then decrease) of the same text token to learn better representations than regular connectionist temporal classification (CTC). However, it only works well in Mandarin. It struggles with other languages, such as English, for which a single syllable may be tokenized into multiple fine-grained tokens, or a token spans fewer than 3 acoustic frames and fails to form unimodal weights. To address this problem, we propose allowing each UMA-aggregated frame map to multiple tokens, via a simple split module that generates two tokens from each aggregated frame before computing the CTC loss.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-48] MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决多轮对话场景下大语言模型（Large Language Models, LLMs）面临的对抗性越狱攻击（multi-turn jailbreaks）问题，这类攻击利用对话上下文逐步规避安全机制，从而诱导模型生成有害内容。解决方案的关键在于提出一个双视角框架 MUSE：一方面，MUSE-A 通过帧语义（frame semantics）与启发式树搜索（heuristic tree search）探索多样化的语义轨迹，有效识别多轮攻击路径；另一方面，MUSE-D 采用细粒度的安全对齐策略，在对话早期阶段实施干预，降低模型在交互过程中暴露的脆弱性。实验证明，该框架能显著提升模型在复杂多轮交互中的安全性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14651">https://arxiv.org/abs/2509.14651</a><br>
<strong>作者</strong>: Siyu Yan,Long Zeng,Xuecheng Wu,Chengcheng Han,Kongcheng Zhang,Chong Peng,Xuezhi Cao,Xunliang Cai,Chenjuan Guo<br>
<strong>机构</strong>: East China Normal University (华东师范大学); Xi’an Jiaotong University (西安交通大学); Meituan (美团); Zhejiang University (浙江大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  EMNLP 2025 main conference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \hrefthis https URLthis https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-49] <mark class="hl-label green">Agent</mark> Compass: Towards Reliable Evaluation of <mark class="hl-label green">Agent</mark> ic Workflows in Production</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在自动化多智能体（multi-agent）工作流部署后，因错误、涌现行为和系统性故障导致的风险难以被现有评估方法捕捉的问题。解决方案的关键在于提出AgentCompass——首个专为智能体工作流后部署监控与调试设计的评估框架，其核心是通过结构化的多阶段分析流程（包括错误识别与分类、主题聚类、量化评分与策略总结）模拟专家调试者的推理过程，并引入双记忆机制（情景记忆与语义记忆）实现跨执行的持续学习能力，从而在真实场景中有效发现人类标注遗漏的关键问题，显著提升生产环境中智能体系统的可靠性与可维护性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14647">https://arxiv.org/abs/2509.14647</a><br>
<strong>作者</strong>: NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek<br>
<strong>机构</strong>: FutureAGI Inc.<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:With the growing adoption of Large Language Models (LLMs) in automating complex, multi-agent workflows, organizations face mounting risks from errors, emergent behaviors, and systemic failures that current evaluation methods fail to capture. We present AgentCompass, the first evaluation framework designed specifically for post-deployment monitoring and debugging of agentic workflows. AgentCompass models the reasoning process of expert debuggers through a structured, multi-stage analytical pipeline: error identification and categorization, thematic clustering, quantitative scoring, and strategic summarization. The framework is further enhanced with a dual memory system-episodic and semantic-that enables continual learning across executions. Through collaborations with design partners, we demonstrate the framework’s practical utility on real-world deployments, before establishing its efficacy against the publicly available TRAIL benchmark. AgentCompass achieves state-of-the-art results on key metrics, while uncovering critical issues missed in human annotations, underscoring its role as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-50] SWE-QA: Can Language Models Answer Repository-level Code Questions?</p>
<p>【速读】： 该论文旨在解决现有代码问答（Code Question Answering, Code QA）基准测试在真实软件仓库环境中能力不足的问题。当前主流基准如CoSQA和CodeQA主要聚焦于小型、自包含的代码片段，无法有效模拟实际开发中所需的跨文件理解、架构认知及长距离依赖分析等复杂任务。为应对这一挑战，作者提出了SWE-QA，一个面向整个软件仓库级别的代码问答基准，其关键在于构建了一个包含576个高质量问题-答案对的数据集，涵盖意图理解、跨文件推理和多跳依赖分析等多样化场景，并通过从11个热门GitHub项目中爬取77,100个issue提取自然开发者提问，设计了两级分类体系以系统化组织问题类型。此外，研究进一步提出SWE-QA-Agent框架，利用大语言模型（Large Language Models, LLMs）实现自动推理与行动机制，在不同上下文增强策略下对六种先进LLMs进行评估，验证了其在处理仓库级代码问答任务上的潜力，同时也揭示了当前技术仍面临的开放性挑战。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14635">https://arxiv.org/abs/2509.14635</a><br>
<strong>作者</strong>: Weihan Peng,Yuling Shi,Yuhang Wang,Xinyun Zhang,Beijun Shen,Xiaodong Gu<br>
<strong>机构</strong>: Shanghai Jiao Tong University (上海交通大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Programming Languages (<a target="_blank" rel="noopener" href="http://cs.PL">cs.PL</a>); Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>)<br>
<strong>备注</strong>:  Code and data available at <a target="_blank" rel="noopener" href="https://github.com/peng-weihan/SWE-QA-Bench">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-51] owards Human-like Multimodal Conversational <mark class="hl-label green">Agent</mark>  by Generating Engaging Speech <mark class="hl-label red">INTERSPEECH2025</mark></p>
<p>【速读】： 该论文旨在解决当前多模态大语言模型（Multimodal Large Language Models, MLLMs）在对话系统中对自然且富有情感的语音响应生成关注不足的问题。现有方法主要聚焦于从多种输入模态生成文本响应，而忽视了语音中蕴含的韵律、语调等副语言信息（paralinguistic information）对于提升交互真实感与沉浸感的重要性。解决方案的关键在于构建一个专注于语音的多感官对话数据集（MultiSensory Conversation dataset），并提出一种基于多模态大语言模型的生成框架：该框架不仅能输出文本响应，还能同时生成语音描述（voice descriptions），从而驱动语音合成系统生成包含情绪状态（conversation mood）和响应风格（responsive style）信息的自然语音。实验表明，融合视觉与音频模态有助于显著提升语音响应的吸引力与真实性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14627">https://arxiv.org/abs/2509.14627</a><br>
<strong>作者</strong>: Taesoo Kim,Yongsik Jo,Hyunmin Song,Taehwan Kim<br>
<strong>机构</strong>: UNIST (韩国科学技术院)<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Published in Interspeech 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in this https URL<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-52] Reveal and Release: Iterative <mark class="hl-label green">LLM</mark>  Unlearning with Self-generated Data <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Model, LLM）在执行遗忘学习（unlearning）时面临的两个关键问题：一是遗忘数据（forget data）通常具有隐私敏感性、稀有性或受法律监管，难以获取；二是即便获得遗忘数据，其分布也可能与模型内部表示不一致，导致遗忘效果不佳。为此，作者提出了一种“Reveal-and-Release”方法，通过优化提示（optimized instructions）引导模型自动生成与其记忆相关的数据，从而实现无需原始遗忘数据即可进行有效遗忘。该方案的核心在于结合迭代式遗忘框架，利用参数高效模块（parameter-efficient modules）对模型权重空间进行增量调整，以在保证模型性能（utility preservation）的同时提升遗忘质量（forget quality）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14624">https://arxiv.org/abs/2509.14624</a><br>
<strong>作者</strong>: Linxi Xie,Xin Teng,Shichang Ke,Hongyi Wen,Shengjie Wang<br>
<strong>机构</strong>: New York University Shanghai (上海纽约大学); Center for Data Science (数据科学中心)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Accepted to EMNLP 2025 Findings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language model (LLM) unlearning has demonstrated effectiveness in removing the influence of undesirable data (also known as forget data). Existing approaches typically assume full access to the forget dataset, overlooking two key challenges: (1) Forget data is often privacy-sensitive, rare, or legally regulated, making it expensive or impractical to obtain (2) The distribution of available forget data may not align with how that information is represented within the model. To address these limitations, we propose a ``Reveal-and-Release’’ method to unlearn with self-generated data, where we prompt the model to reveal what it knows using optimized instructions. To fully utilize the self-generated forget data, we propose an iterative unlearning framework, where we make incremental adjustments to the model’s weight space with parameter-efficient modules trained on the forget data. Experimental results demonstrate that our method balances the tradeoff between forget quality and utility preservation.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-53] Leve<mark class="hl-label green">rag</mark> ing IndoBERT and DistilBERT for Indonesian Emotion Classification in E-Commerce Reviews</p>
<p>【速读】： 该论文旨在解决印尼语情感分类（Emotion Classification）准确率较低的问题，以提升电子商务场景下的用户体验。其解决方案的关键在于利用先进的语言模型（如IndoBERT和DistilBERT）并结合数据增强技术，特别是回译（back-translation）和同义词替换（synonym replacement），从而有效提升模型性能；实验表明，经过超参数调优后的IndoBERT达到80%的准确率，且数据增强被证实是实现高精度的核心因素。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14611">https://arxiv.org/abs/2509.14611</a><br>
<strong>作者</strong>: William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono<br>
<strong>机构</strong>: Bina Nusantara University (宾纳努桑塔拉大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Understanding emotions in the Indonesian language is essential for improving customer experiences in e-commerce. This study focuses on enhancing the accuracy of emotion classification in Indonesian by leveraging advanced language models, IndoBERT and DistilBERT. A key component of our approach was data processing, specifically data augmentation, which included techniques such as back-translation and synonym replacement. These methods played a significant role in boosting the model’s performance. After hyperparameter tuning, IndoBERT achieved an accuracy of 80%, demonstrating the impact of careful data processing. While combining multiple IndoBERT models led to a slight improvement, it did not significantly enhance performance. Our findings indicate that IndoBERT was the most effective model for emotion classification in Indonesian, with data augmentation proving to be a vital factor in achieving high accuracy. Future research should focus on exploring alternative architectures and strategies to improve generalization for Indonesian NLP tasks.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-54] Position: Thematic Analysis of Unstructured Clinical Transcripts with <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  <mark class="hl-label red">NEURIPS2025</mark> <mark class="hl-label red">ALT</mark></p>
<p>【速读】： 该论文试图解决当前大型语言模型（Large Language Models, LLMs）在支持临床未结构化转录文本的主题分析（thematic analysis）中存在方法碎片化、评估标准不统一的问题，这阻碍了研究间的可比性和领域进展。其解决方案的关键在于提出一个以有效性（validity）、可靠性（reliability）和可解释性（interpretability）为核心的标准化评估框架，旨在推动该领域从分散探索走向系统化发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14597">https://arxiv.org/abs/2509.14597</a><br>
<strong>作者</strong>: Seungjun Yi,Joakim Nguyen,Terence Lim,Andrew Well,Joseph Skrovan,Mehak Beri,YongGeon Lee,Kavita Radhakrishnan,Liu Leqi,Mia Markey,Ying Ding<br>
<strong>机构</strong>: The University of Texas at Austin (德克萨斯大学奥斯汀分校); Vanderbilt University School of Medicine (范德比尔特大学医学院)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Submitted to GenAI4Health@NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This position paper examines how large language models (LLMs) can support thematic analysis of unstructured clinical transcripts, a widely used but resource-intensive method for uncovering patterns in patient and provider narratives. We conducted a systematic review of recent studies applying LLMs to thematic analysis, complemented by an interview with a practicing clinician. Our findings reveal that current approaches remain fragmented across multiple dimensions including types of thematic analysis, datasets, prompting strategies and models used, most notably in evaluation. Existing evaluation methods vary widely (from qualitative expert review to automatic similarity metrics), hindering progress and preventing meaningful benchmarking across studies. We argue that establishing standardized evaluation practices is critical for advancing the field. To this end, we propose an evaluation framework centered on three dimensions: validity, reliability, and interpretability.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-55] <mark class="hl-label green">LLM</mark>  Jailbreak Detection for (Almost) Free!</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在广泛部署中因对齐机制不足而易受越狱攻击（jailbreak attacks）的问题，此类攻击可诱导模型生成不当内容。现有检测方法虽有效，但通常依赖多模型推理或额外计算资源，导致显著的计算开销。解决方案的关键在于发现并利用越狱提示与良性提示在输出分布上的差异，提出一种无额外计算成本的自由越狱检测方法（Free Jailbreak Detection, FJD）：通过在输入前添加肯定性指令并调整温度参数缩放logits，以第一token的置信度差异实现判别；进一步结合虚拟指令学习（virtual instruction learning）提升检测性能。实验证明，FJD在对齐LLM上能高效准确地识别越狱提示，且推理阶段几乎不增加计算负担。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14558">https://arxiv.org/abs/2509.14558</a><br>
<strong>作者</strong>: Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu<br>
<strong>机构</strong>: Wuhan University (武汉大学); Nanyang Technological University (南洋理工大学); University of Oxford (牛津大学)<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) enhance security through alignment when widely used, but remain susceptible to jailbreak attacks capable of producing inappropriate content. Jailbreak detection methods show promise in mitigating jailbreak attacks through the assistance of other models or multiple model inferences. However, existing methods entail significant computational costs. In this paper, we first present a finding that the difference in output distributions between jailbreak and benign prompts can be employed for detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak Detection (FJD) which prepends an affirmative instruction to the input and scales the logits by temperature to further distinguish between jailbreak and benign prompts through the confidence of the first token. Furthermore, we enhance the detection performance of FJD through the integration of virtual instruction learning. Extensive experiments on aligned LLMs show that our FJD can effectively detect jailbreak prompts with almost no additional computational costs during LLM inference.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-56] Controlling Language Difficulty in Dialogues with Linguistic Features</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在第二语言习得场景中生成对话时，难以根据学习者的语言水平动态调整输出文本难度的问题。其解决方案的关键在于构建一个基于三类语言特征的可控框架：可读性特征（如Flesch-Kincaid Grade Level）、句法特征（如句法树深度）和词汇特征（如简单词比例），通过在语言学标注的对话数据上训练LLMs，实现对生成文本语言复杂度的精确调控，从而在保持对话质量的同时显著提升语言难度控制的灵活性与稳定性，优于传统的提示工程方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14545">https://arxiv.org/abs/2509.14545</a><br>
<strong>作者</strong>: Shuyao Xu,Wenguang Wang,Handong Gao,Wei Kang,Long Qin,Weizhi Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  15 pages,9 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) have emerged as powerful tools for supporting second language acquisition, particularly in simulating interactive dialogues for speaking practice. However, adapting the language difficulty of LLM-generated responses to match learners’ proficiency levels remains a challenge. This work addresses this issue by proposing a framework for controlling language proficiency in educational dialogue systems. Our approach leverages three categories of linguistic features, readability features (e.g., Flesch-Kincaid Grade Level), syntactic features (e.g., syntactic tree depth), and lexical features (e.g., simple word ratio), to quantify and regulate text complexity. We demonstrate that training LLMs on linguistically annotated dialogue data enables precise modulation of language proficiency, outperforming prompt-based methods in both flexibility and stability. To evaluate this, we introduce Dilaprix, a novel metric integrating the aforementioned features, which shows strong correlation with expert judgments of language difficulty. Empirical results reveal that our approach achieves superior controllability of language proficiency while maintaining high dialogue quality.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-57] Catch Me If You Can? Not Yet: <mark class="hl-label green">LLM</mark> s Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在仅依赖少量用户文本样本的情况下，能否忠实模仿个体写作风格的问题。由于个人风格通常具有隐含性和细微性，难以通过显式提示（prompt）精确控制，但对生成内容与用户意图的一致性至关重要。解决方案的关键在于构建一套多维度评估体系，包括作者归属（authorship attribution）、作者验证（authorship verification）、风格匹配（style matching）和AI检测（AI detection）等互补指标，从而系统性地衡量LLMs在不同文本领域（如新闻、邮件、论坛和博客）中对真实作者风格的模仿能力。实验覆盖超过40000次生成结果及400多位真实作者的数据，揭示了LLMs在结构化文本中表现良好但在非正式语境下存在显著局限，凸显了当前个性化适配技术的根本性不足，并为未来研究提供了可复现的数据与代码资源。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14543">https://arxiv.org/abs/2509.14543</a><br>
<strong>作者</strong>: Zhengxiang Wang,Nafis Irtiza Tripto,Solha Park,Zhenzhen Li,Jiawei Zhou<br>
<strong>机构</strong>: Stony Brook University (石溪大学); The Pennsylvania State University (宾夕法尼亚州立大学); Bosch Center for AI (博世人工智能中心)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  EMNLP 2025 (Findings)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As large language models (LLMs) become increasingly integrated into personal writing tools, a critical question arises: can LLMs faithfully imitate an individual’s writing style from just a few examples? Personal style is often subtle and implicit, making it difficult to specify through prompts yet essential for user-aligned generation. This work presents a comprehensive evaluation of state-of-the-art LLMs’ ability to mimic personal writing styles via in-context learning from a small number of user-authored samples. We introduce an ensemble of complementary metrics-including authorship attribution, authorship verification, style matching, and AI detection-to robustly assess style imitation. Our evaluation spans over 40000 generations per model across domains such as news, email, forums, and blogs, covering writing samples from more than 400 real-world authors. Results show that while LLMs can approximate user styles in structured formats like news and email, they struggle with nuanced, informal writing in blogs and forums. Further analysis on various prompting strategies such as number of demonstrations reveal key limitations in effective personalization. Our findings highlight a fundamental gap in personalized LLM adaptation and the need for improved techniques to support implicit, style-consistent generation. To aid future research and for reproducibility, we open-source our data and code.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-58] Delta Knowledge Distillation for <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决传统知识蒸馏（Knowledge Distillation, KD）在大语言模型中假设学生模型与教师模型共享相同最优表示空间的问题，而这一假设在实际场景中往往不成立。其解决方案的关键在于提出了一种名为Delta知识蒸馏（Delta Knowledge Distillation, Delta-KD）的新方法，通过显式保留教师模型在监督微调（Supervised Fine-Tuning, SFT）过程中引入的分布偏移量Δ（即分布变化），使学生模型能够逼近这一最优表示空间，从而更有效地迁移教师的知识。实验证明，该方法在ROUGE指标上显著提升了学生模型性能，并更好地保留了教师模型的知识。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14526">https://arxiv.org/abs/2509.14526</a><br>
<strong>作者</strong>: Yihan Cao,Yanbin Kang,Zhengming Xing,Ruijie Jiang<br>
<strong>机构</strong>: LinkedIn Corporation (领英公司)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  8 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Knowledge distillation (KD) is a widely adopted approach for compressing large neural networks by transferring knowledge from a large teacher model to a smaller student model. In the context of large language models, token level KD, typically minimizing the KL divergence between student output distribution and teacher output distribution, has shown strong empirical performance. However, prior work assumes student output distribution and teacher output distribution share the same optimal representation space, a premise that may not hold in many cases. To solve this problem, we propose Delta Knowledge Distillation (Delta-KD), a novel extension of token level KD that encourages the student to approximate an optimal representation space by explicitly preserving the distributional shift Delta introduced during the teacher’s supervised finetuning (SFT). Empirical results on ROUGE metrics demonstrate that Delta KD substantially improves student performance while preserving more of the teacher’s knowledge.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-59] From Turn-Taking to Synchronous Dialogue: A Survey of Full-Duplex Spoken Language Models</p>
<p>【速读】： 该论文旨在解决生成式 AI（Generative AI）在实现真正全双工（True Full-Duplex, TFD）语音交互中的关键瓶颈问题，即如何构建具备自然话语轮转、重叠说话和打断能力的全双工语音语言模型（Full-Duplex Spoken Language Models, FD-SLMs）。其解决方案的关键在于提出一个系统性的分类框架，将FD-SLM架构区分为“工程同步”（Engineered Synchronization，模块化设计）与“学习同步”（Learned Synchronization，端到端训练），并统一碎片化的评估体系，形成涵盖时间动态性（Temporal Dynamics）、行为仲裁（Behavioral Arbitration）、语义连贯性（Semantic Coherence）和声学性能（Acoustic Performance）的综合评价框架，从而推动人机语音交互向更接近人类对话的水平演进。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14515">https://arxiv.org/abs/2509.14515</a><br>
<strong>作者</strong>: Yuxuan Chen,Haoyuan Yu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:True Full-Duplex (TFD) voice communication–enabling simultaneous listening and speaking with natural turn-taking, overlapping speech, and interruptions–represents a critical milestone toward human-like AI interaction. This survey comprehensively reviews Full-Duplex Spoken Language Models (FD-SLMs) in the LLM era. We establish a taxonomy distinguishing Engineered Synchronization (modular architectures) from Learned Synchronization (end-to-end architectures), and unify fragmented evaluation approaches into a framework encompassing Temporal Dynamics, Behavioral Arbitration, Semantic Coherence, and Acoustic Performance. Through comparative analysis of mainstream FD-SLMs, we identify fundamental challenges: synchronous data scarcity, architectural divergence, and evaluation gaps, providing a roadmap for advancing human-AI communication.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-60] DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction</p>
<p>【速读】： 该论文旨在解决自然语言到SQL（Natural Language to SQL, NL2SQL）任务中因大语言模型（Large Language Models, LLMs）在任务分解（task decomposition）和关键词提取（keyword extraction）上的不准确性所导致的SQL生成错误问题。现有数据集存在任务过度碎片化及缺乏领域特定关键词标注的问题，限制了模型性能提升。其解决方案的关键在于提出一个名为DeKeyNLU的新颖标注数据集，包含1,500个精心标注的问答对，用于优化任务分解与关键词提取精度；并基于此构建DeKeySQL——一个三模块RAG（Retrieval-Augmented Generation）管道，包括用户问题理解、实体检索和生成模块，显著提升了SQL生成准确率，在BIRD和Spider基准测试中分别从62.31%提升至69.10%，以及从84.2%提升至88.7%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14507">https://arxiv.org/abs/2509.14507</a><br>
<strong>作者</strong>: Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan<br>
<strong>机构</strong>: The Hong Kong University of Science and Technology (Guangzhou)(香港科技大学（广州）); HSBC (汇丰银行); South China University of Technology (华南理工大学); Harvard University (哈佛大学); Chicago University (芝加哥大学)<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that simplifies database access for non-technical users by converting natural language queries into SQL commands. Recent advancements, particularly those integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) reasoning, have made significant strides in enhancing NL2SQL performance. However, challenges such as inaccurate task decomposition and keyword extraction by LLMs remain major bottlenecks, often leading to errors in SQL generation. While existing datasets aim to mitigate these issues by fine-tuning models, they struggle with over-fragmentation of tasks and lack of domain-specific keyword annotations, limiting their effectiveness. To address these limitations, we present DeKeyNLU, a novel dataset which contains 1,500 meticulously annotated QA pairs aimed at refining task decomposition and enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three distinct modules for user question understanding, entity retrieval, and generation to improve SQL generation accuracy. We benchmarked multiple model configurations within DeKeySQL RAG pipeline. Experimental results demonstrate that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-61] Introducing OmniGEC: A Silver Multilingual Dataset for Grammatical Error Correction</p>
<p>【速读】： 该论文旨在解决多语言语法错误纠正（Grammatical Error Correction, GEC）任务中数据稀缺与跨语言迁移能力不足的问题，尤其针对现有英语GEC模型难以直接适配到其他语言的瓶颈。解决方案的关键在于构建OmniGEC——一个覆盖11种语言的多语言银标准（silver-standard）语料库，其文本来源包括维基百科编辑记录（人工修正）、Reddit子版块帖子（自动由GPT-4o-mini模型修正）以及乌克兰语专属的UberText 2.0社交媒体语料库。该数据集通过自动与人工双重质量评估确保可靠性，并在此基础上微调两个开源大语言模型（Aya-Expanse 8B 和 Gemma-3 12B），在段落级多语言GEC任务上达到当前最优性能（state-of-the-art, SOTA）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14504">https://arxiv.org/abs/2509.14504</a><br>
<strong>作者</strong>: Roman Kovalchuk,Mariana Romanyshyn,Petro Ivaniuk<br>
<strong>机构</strong>: Ukrainian Catholic University (乌克兰天主教大学); Softserve (Softserve); Grammarly (Grammarly)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we introduce OmniGEC, a collection of multilingual silver-standard datasets for the task of Grammatical Error Correction (GEC), covering eleven languages: Czech, English, Estonian, German, Greek, Icelandic, Italian, Latvian, Slovene, Swedish, and Ukrainian. These datasets facilitate the development of multilingual GEC solutions and help bridge the data gap in adapting English GEC solutions to multilingual GEC. The texts in the datasets originate from three sources: Wikipedia edits for the eleven target languages, subreddits from Reddit in the eleven target languages, and the Ukrainian-only UberText 2.0 social media corpus. While Wikipedia edits were derived from human-made corrections, the Reddit and UberText 2.0 data were automatically corrected with the GPT-4o-mini model. The quality of the corrections in the datasets was evaluated both automatically and manually. Finally, we fine-tune two open-source large language models - Aya-Expanse (8B) and Gemma-3 (12B) - on the multilingual OmniGEC corpora and achieve state-of-the-art (SOTA) results for paragraph-level multilingual GEC. The dataset collection and the best-performing models are available on Hugging Face.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-62] ranslate then Detect: Leve<mark class="hl-label green">rag</mark> ing Machine Translation for Cross-Lingual Toxicity Classification</p>
<p>【速读】： 该论文旨在解决多语言毒性检测（multilingual toxicity detection）中的关键挑战，即低资源语言因训练数据和相关资源匮乏而导致模型性能下降的问题。其核心解决方案是系统性比较基于翻译的分类管道（translate-classify）与语言特定或混合语言分类管道的效果，发现翻译-based方法在81.3%的案例中优于分布外（out-of-distribution）分类器，且其优势与目标语言的资源水平及机器翻译（MT）质量高度相关。研究进一步表明，传统分类器在低资源语言上显著优于大型语言模型（LLM）判别器，而对LLM进行MT特定微调虽可降低拒绝率，但可能损害低资源语言下的毒性检测准确率，从而为构建可扩展的多语言内容审核系统提供了实证依据和实践指导。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14493">https://arxiv.org/abs/2509.14493</a><br>
<strong>作者</strong>: Samuel J. Bell,Eduardo Sánchez,David Dale,Pontus Stenetorp,Mikel Artetxe,Marta R. Costa-jussà<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multilingual toxicity detection remains a significant challenge due to the scarcity of training data and resources for many languages. While prior work has leveraged the translate-test paradigm to support cross-lingual transfer across a range of classification tasks, the utility of translation in supporting toxicity detection at scale remains unclear. In this work, we conduct a comprehensive comparison of translation-based and language-specific/multilingual classification pipelines. We find that translation-based pipelines consistently outperform out-of-distribution classifiers in 81.3% of cases (13 of 16 languages), with translation benefits strongly correlated with both the resource level of the target language and the quality of the machine translation (MT) system. Our analysis reveals that traditional classifiers outperform large language model (LLM) judges, with this advantage being particularly pronounced for low-resource languages, where translate-classify methods dominate translate-judge approaches in 6 out of 7 cases. We additionally show that MT-specific fine-tuning on LLMs yields lower refusal rates compared to standard instruction-tuned models, but it can negatively impact toxicity detection accuracy for low-resource languages. These findings offer actionable guidance for practitioners developing scalable multilingual content moderation systems.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-63] Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use <mark class="hl-label green">Agents</mark></p>
<p>【速读】： 该论文旨在解决智能体在多模态交互场景下进行工具使用时面临的复杂决策问题，即如何有效训练智能体完成涉及多轮规划和长上下文对话管理的Tool Integrated Reasoning (TIR)任务。其解决方案的关键在于提出一种Turn-level Adjudicated Reinforcement Learning (TARL)框架，通过引入大型语言模型（LLM）作为裁判对每一轮交互进行评分，从而实现长时程任务中的精准信用分配；同时结合包含数学推理问题的混合任务训练课程以增强探索能力，最终显著提升文本基准测试（τ-bench）的任务通过率，并验证了该方法在微调多模态基础模型以支持语音驱动交互式代理方面的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14480">https://arxiv.org/abs/2509.14480</a><br>
<strong>作者</strong>: Weiting Tan,Xinghua Qu,Ming Tu,Meng Ge,Andy T. Liu,Philipp Koehn,Lu Lu<br>
<strong>机构</strong>: Johns Hopkins University (约翰霍普金斯大学); ByteDance (字节跳动)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based  \tau -bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework’s suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-64] Estimating Semantic Alphabet Size for <mark class="hl-label green">LLM</mark>  Uncertainty Quantification</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在黑盒场景下，基于少量采样实现可靠不确定性估计的问题。现有方法如语义熵（Semantic Entropy, SE）虽具吸引力，但其离散形式常低估真实语义熵，且近期改进方法引入额外超参数并降低可解释性。论文的关键解决方案是提出一种改进的语义字母表大小（semantic alphabet size）估计器，并利用该估计器对样本覆盖率进行校正，从而提升语义熵估计的准确性；同时该方法在检测LLM幻觉方面表现优于或等同于当前最优方法，且保持高度可解释性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14478">https://arxiv.org/abs/2509.14478</a><br>
<strong>作者</strong>: Lucas H. McCabe,Rimon Melamed,Thomas Hartvigsen,H. Howie Huang<br>
<strong>机构</strong>: George Washington University (乔治华盛顿大学); LMI Consulting; University of Virginia (弗吉尼亚大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Many black-box techniques for quantifying the uncertainty of large language models (LLMs) rely on repeated LLM sampling, which can be computationally expensive. Therefore, practical applicability demands reliable estimation from few samples. Semantic entropy (SE) is a popular sample-based uncertainty estimator with a discrete formulation attractive for the black-box setting. Recent extensions of semantic entropy exhibit improved LLM hallucination detection, but do so with less interpretable methods that admit additional hyperparameters. For this reason, we revisit the canonical discrete semantic entropy estimator, finding that it underestimates the “true” semantic entropy, as expected from theory. We propose a modified semantic alphabet size estimator, and illustrate that using it to adjust discrete semantic entropy for sample coverage results in more accurate semantic entropy estimation in our setting of interest. Furthermore, our proposed alphabet size estimator flags incorrect LLM responses as well or better than recent top-performing approaches, with the added benefit of remaining highly interpretable.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-65] cket-Bench: A Kickoff for Multilingual and Regionalized <mark class="hl-label green">Agent</mark>  Evaluation</p>
<p>【速读】： 该论文旨在解决当前大语言模型（Large Language Models, LLMs）作为任务导向型智能体（task-oriented agents）在多语言环境下评估不足的问题，尤其是现有评测基准普遍忽视文化与语言多样性，多依赖单语或粗略翻译的数据集，导致模型跨语言能力评估失真。其解决方案的关键在于提出Ticket-Bench——一个面向多语言任务场景的新型基准测试框架，模拟足球票务购买这一现实场景，在葡萄牙语、英语、西班牙语、德语、意大利语和法语六种主要语言中进行评估，并通过本地化团队、城市及用户画像提升真实性。该方案不仅量化了不同LLM在函数调用准确性与一致性上的表现，还揭示了主流推理导向模型（如GPT-5、Qwen3-235B）虽整体领先但仍存在显著跨语言差异，从而强调开发具有文化敏感性的多语言评测体系对构建鲁棒LLM代理的重要性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14477">https://arxiv.org/abs/2509.14477</a><br>
<strong>作者</strong>: Thales Sales Almeida,João Guilherme Alves Santos,Thiago Laitz,Giovana Kerche Bonás<br>
<strong>机构</strong>: Institute of Computing (IC); State University of Campinas; Maritaca AI; Tropic AI<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) are increasingly deployed as task-oriented agents, where success depends on their ability to generate accurate function calls under realistic, multilingual conditions. However, existing agent evaluations largely overlook cultural and linguistic diversity, often relying on monolingual or naively translated benchmarks. We introduce Ticket-Bench, a benchmark for multilingual agent evaluation in task-oriented scenarios. Ticket-Bench simulates the domain of soccer ticket purchases across six major languages: Portuguese, English, Spanish, German, Italian, and French. Using localized teams, cities, and user profiles to provide a higher level of realism. We evaluate a wide range of commercial and open-source LLMs, measuring function-calling accuracy and consistency across languages. Results show that reasoning-oriented models (e.g., GPT-5, Qwen3-235B) dominate performance but still exhibit notable cross-lingual disparities. These findings underscore the need for culturally aware, multilingual benchmarks to guide the development of robust LLM agents.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-66] Not What the Doctor Ordered: Surveying <mark class="hl-label green">LLM</mark> -based De-identification and Quantifying Clinical Information Loss <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决当前基于生成式大语言模型（Generative Large Language Models, LLMs）的医疗文本去标识化研究中存在的三大关键问题：一是报告指标不一致导致难以直接比较不同方法；二是传统分类指标无法有效捕捉LLMs更易产生的错误类型，如对临床相关信息的不当删除；三是自动化评估指标缺乏人工验证，难以准确量化此类错误。其解决方案的关键在于：首先系统梳理现有文献中去标识化研究的异质性报告标准；其次通过多模型对比实证分析LLMs在去除个人信息时对临床信息的误删程度；再次由临床专家对现有评估指标进行人工验证，揭示其在识别临床显著变化方面的局限性；最后提出一种新的、面向临床相关性信息删除检测的方法论，以提升评估的准确性与临床适用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14464">https://arxiv.org/abs/2509.14464</a><br>
<strong>作者</strong>: Kiana Aghakasiri,Noopur Zambare,JoAnn Thai,Carrie Ye,Mayur Mehta,J. Ross Mitchell,Mohamed Abdalla<br>
<strong>机构</strong>: University of Alberta (阿尔伯塔大学); Alberta Machine Intelligence Institute (Amii) (阿尔伯塔机器智能研究所); Arthritis Research Canada (关节炎研究中心)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted to EMNLP 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:De-identification in the healthcare setting is an application of NLP where automated algorithms are used to remove personally identifying information of patients (and, sometimes, providers). With the recent rise of generative large language models (LLMs), there has been a corresponding rise in the number of papers that apply LLMs to de-identification. Although these approaches often report near-perfect results, significant challenges concerning reproducibility and utility of the research papers persist. This paper identifies three key limitations in the current literature: inconsistent reporting metrics hindering direct comparisons, the inadequacy of traditional classification metrics in capturing errors which LLMs may be more prone to (i.e., altering clinically relevant information), and lack of manual validation of automated metrics which aim to quantify these errors. To address these issues, we first present a survey of LLM-based de-identification research, highlighting the heterogeneity in reporting standards. Second, we evaluated a diverse set of models to quantify the extent of inappropriate removal of clinical information. Next, we conduct a manual validation of an existing evaluation metric to measure the removal of clinical information, employing clinical experts to assess their efficacy. We highlight poor performance and describe the inherent limitations of such metrics in identifying clinically significant changes. Lastly, we propose a novel methodology for the detection of clinically relevant information removal.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-67] Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）在指代消解（coreference resolution）任务中同时具备歧义检测与歧义消解能力的问题。研究表明，尽管LLMs在仅需少量提示的情况下即可实现良好的指代消解性能和歧义检测能力，但二者难以兼得，存在一个“CORRECT-DETECT权衡”现象：模型虽隐式具备两种能力，却无法在同一任务中有效平衡二者。解决方案的关键在于识别并理解这一内在权衡机制，从而为后续设计能够协同优化歧义检测与消解的架构提供理论依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14456">https://arxiv.org/abs/2509.14456</a><br>
<strong>作者</strong>: Amber Shore,Russell Scheinberg,Ameeta Agrawal,So Young Lee<br>
<strong>机构</strong>: Portland State University (波特兰州立大学); Miami University (迈阿密大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) are intended to reflect human linguistic competencies. But humans have access to a broad and embodied context, which is key in detecting and resolving linguistic ambiguities, even in isolated text spans. A foundational case of semantic ambiguity is found in the task of coreference resolution: how is a pronoun related to an earlier person mention? This capability is implicit in nearly every downstream task, and the presence of ambiguity at this level can alter performance significantly. We show that LLMs can achieve good performance with minimal prompting in both coreference disambiguation and the detection of ambiguity in coreference, however, they cannot do both at the same time. We present the CORRECT-DETECT trade-off: though models have both capabilities and deploy them implicitly, successful performance balancing these two abilities remains elusive.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-68] Simulating a Bias Mitigation Scenario in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）中存在的偏见问题，该问题严重影响了模型的公平性与可信度。解决方案的关键在于构建一个模拟框架，用于在受控实验环境中评估多种偏见缓解策略的实际效果，包括数据筛选、训练过程中的去偏处理以及输出后的校准方法，从而实现对偏见来源（如数据、架构和应用场景）的系统性干预与实证验证。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14438">https://arxiv.org/abs/2509.14438</a><br>
<strong>作者</strong>: Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi,Meysam Shirdel Bilehsavar<br>
<strong>机构</strong>: Bowling Green State University (鲍灵格林州立大学); University of Nebraska–Lincoln (内布拉斯加林肯大学); Stevens Institute of Technology (史蒂文斯理工学院); University of South Carolina (南卡罗来纳大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  preprint, 16 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) have fundamentally transformed the field of natural language processing; however, their vulnerability to biases presents a notable obstacle that threatens both fairness and trust. This review offers an extensive analysis of the bias landscape in LLMs, tracing its roots and expressions across various NLP tasks. Biases are classified into implicit and explicit types, with particular attention given to their emergence from data sources, architectural designs, and contextual deployments. This study advances beyond theoretical analysis by implementing a simulation framework designed to evaluate bias mitigation strategies in practice. The framework integrates multiple approaches including data curation, debiasing during model training, and post-hoc output calibration and assesses their impact in controlled experimental settings. In summary, this work not only synthesizes existing knowledge on bias in LLMs but also contributes original empirical validation through simulation of mitigation strategies.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-69] Causal-Counterfactual <mark class="hl-label green">RAG</mark> : The Integration of Causal-Counterfactual <mark class="hl-label green">Reasoning</mark>  into <mark class="hl-label green">RAG</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）因静态知识限制而在知识密集型领域中难以进行动态推理的问题，尤其是传统检索增强生成（Retrieval-Augmented Generation, RAG）系统由于文本分块导致上下文完整性破坏以及过度依赖语义相似度检索所引发的浅层、不准确回答问题。其解决方案的关键在于提出一种因果-反事实检索增强生成框架（Causal-Counterfactual RAG），通过在检索过程中引入显式的因果图结构来表示因果关系，并结合基于该结构的反事实推理机制，不仅评估直接因果证据，还分析相关原因的反事实可能性，从而融合两类信息生成更具鲁棒性、准确性与可解释性的答案，同时保持上下文连贯性、降低幻觉并提升推理保真度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14435">https://arxiv.org/abs/2509.14435</a><br>
<strong>作者</strong>: Harshad Khadilkar,Abhay Gupta<br>
<strong>机构</strong>: Indian Institute of Technology Bombay (印度理工学院孟买分校); Indian Institute of Technology Patna (印度理工学院Patna分校)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) have transformed natural language processing (NLP), enabling diverse applications by integrating large-scale pre-trained knowledge. However, their static knowledge limits dynamic reasoning over external information, especially in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this challenge by combining retrieval mechanisms with generative modeling to improve contextual understanding. Traditional RAG systems suffer from disrupted contextual integrity due to text chunking and over-reliance on semantic similarity for retrieval, often resulting in shallow and less accurate responses. We propose Causal-Counterfactual RAG, a novel framework that integrates explicit causal graphs representing cause-effect relationships into the retrieval process and incorporates counterfactual reasoning grounded on the causal structure. Unlike conventional methods, our framework evaluates not only direct causal evidence but also the counterfactuality of associated causes, combining results from both to generate more robust, accurate, and interpretable answers. By leveraging causal pathways and associated hypothetical scenarios, Causal-Counterfactual RAG preserves contextual coherence, reduces hallucination, and enhances reasoning fidelity.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-70] Adding <mark class="hl-label green">LLM</mark> s to the psycholinguistic norming toolbox: A practical guide to getting the most out of human ratings</p>
<p>【速读】： 该论文旨在解决心理学语言学领域中获取词级心理语言学规范（word-level psycholinguistic norms）的困难问题，即传统依赖人工标注的数据收集方式成本高、耗时长且难以扩展。为应对这一挑战，论文提出了一种基于大语言模型（Large Language Models, LLMs）的系统性方法论，其关键在于通过严谨的验证流程将LLM生成的词特征预测结果与人类“黄金标准”数据进行比对，并结合直接使用基础模型与微调（fine-tuning）两种策略以提升预测准确性。研究通过英语词汇熟悉度估计的案例表明，基础模型即可实现与人类评分高度一致的相关性（Spearman相关系数0.8），而微调后进一步提升至0.9，从而为利用LLM辅助心理语言学研究提供了可复现、可验证且高效的实践框架。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14405">https://arxiv.org/abs/2509.14405</a><br>
<strong>作者</strong>: Javier Conde,María Grandury,Tairan Fu,Carlos Arriaga,Gonzalo Martínez,Thomas Clark,Sean Trott,Clarence Gerald Green,Pedro Reviriego,Marc Brysbaert<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Word-level psycholinguistic norms lend empirical support to theories of language processing. However, obtaining such human-based measures is not always feasible or straightforward. One promising approach is to augment human norming datasets by using Large Language Models (LLMs) to predict these characteristics directly, a practice that is rapidly gaining popularity in psycholinguistics and cognitive science. However, the novelty of this approach (and the relative inscrutability of LLMs) necessitates the adoption of rigorous methodologies that guide researchers through this process, present the range of possible approaches, and clarify limitations that are not immediately apparent, but may, in some cases, render the use of LLMs impractical. In this work, we present a comprehensive methodology for estimating word characteristics with LLMs, enriched with practical advice and lessons learned from our own experience. Our approach covers both the direct use of base LLMs and the fine-tuning of models, an alternative that can yield substantial performance gains in certain scenarios. A major emphasis in the guide is the validation of LLM-generated data with human “gold standard” norms. We also present a software framework that implements our methodology and supports both commercial and open-weight models. We illustrate the proposed approach with a case study on estimating word familiarity in English. Using base models, we achieved a Spearman correlation of 0.8 with human ratings, which increased to 0.9 when employing fine-tuned models. This methodology, framework, and set of best practices aim to serve as a reference for future research on leveraging LLMs for psycholinguistic and lexical studies.         Subjects:  Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)  Cite as: arXiv:2509.14405 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]    (or  arXiv:2509.14405v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14405">https://doi.org/10.48550/arXiv.2509.14405</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-71] A Taxonomy of Prompt Defects in <mark class="hl-label green">LLM</mark>  Systems</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在实际应用中因提示词（prompt）设计缺陷导致的行为不可靠、不安全或低效的问题。当前提示词作为LLMs的“事实上的编程接口”，其设计仍主要依赖经验，缺乏系统性规范与错误识别机制。论文的关键解决方案是构建首个系统的提示缺陷分类体系，从六个维度（规范与意图、输入与内容、结构与格式、上下文与记忆、性能与效率、可维护性与工程实践）对提示缺陷进行细粒度划分，并结合真实开发场景分析其根源及下游影响；同时针对每类缺陷提出包括提示工程模式、自动化防护机制、测试框架和评估体系在内的多层次缓解策略，最终形成“缺陷-影响-修复”三位一体的主分类图谱，推动生成式AI系统向以工程化方法保障可靠性的方向演进。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14404">https://arxiv.org/abs/2509.14404</a><br>
<strong>作者</strong>: Haoye Tian,Chong Wang,BoYang Yang,Lyuye Zhang,Yang Liu<br>
<strong>机构</strong>: Nanyang Technological University (南洋理工大学); Jisuan Institute of Technology, Beijing JudaoYouda Network Technology Co. Ltd. (北京聚道优达网络科技有限公司)<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Programming Languages (<a target="_blank" rel="noopener" href="http://cs.PL">cs.PL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) have become key components of modern software, with prompts acting as their de-facto programming interface. However, prompt design remains largely empirical and small mistakes can cascade into unreliable, insecure, or inefficient behavior. This paper presents the first systematic survey and taxonomy of prompt defects, recurring ways that prompts fail to elicit their intended behavior from LLMs. We organize defects along six dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6) Maintainability and Engineering. Each dimension is refined into fine-grained subtypes, illustrated with concrete examples and root cause analysis. Grounded in software engineering principles, we show how these defects surface in real development workflows and examine their downstream effects. For every subtype, we distill mitigation strategies that span emerging prompt engineering patterns, automated guardrails, testing harnesses, and evaluation frameworks. We then summarize these strategies in a master taxonomy that links defect, impact, and remedy. We conclude with open research challenges and a call for rigorous engineering-oriented methodologies to ensure that LLM-driven systems are dependable by design.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-72] Annotating Training Data for Conditional Semantic Textual Similarity Measurement using <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】： 该论文旨在解决条件语义文本相似度（Conditional Semantic Textual Similarity, C-STS）任务中因训练数据质量不足导致模型性能不佳的问题。现有C-STS数据集存在标注不准确等缺陷，限制了模型的进一步发展。其解决方案的关键在于利用大语言模型（Large Language Models, LLMs）对原始数据集中条件陈述和相似度评分进行自动修正，从而在最小人工干预下生成高质量、大规模的重新标注数据集，并基于此训练出性能显著提升的监督式C-STS模型，实现了Spearman相关系数5.4%的统计学显著改进。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14399">https://arxiv.org/abs/2509.14399</a><br>
<strong>作者</strong>: Gaifan Zhang,Yi Zhou,Danushka Bollegala<br>
<strong>机构</strong>: University of Liverpool (利物浦大学); Cardiff University (卡迪夫大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Accepted to EMNLP 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Semantic similarity between two sentences depends on the aspects considered between those sentences. To study this phenomenon, Deshpande et al. (2023) proposed the Conditional Semantic Textual Similarity (C-STS) task and annotated a human-rated similarity dataset containing pairs of sentences compared under two different conditions. However, Tu et al. (2024) found various annotation issues in this dataset and showed that manually re-annotating a small portion of it leads to more accurate C-STS models. Despite these pioneering efforts, the lack of large and accurately annotated C-STS datasets remains a blocker for making progress on this task as evidenced by the subpar performance of the C-STS models. To address this training data need, we resort to Large Language Models (LLMs) to correct the condition statements and similarity ratings in the original dataset proposed by Deshpande et al. (2023). Our proposed method is able to re-annotate a large training dataset for the C-STS task with minimal manual effort. Importantly, by training a supervised C-STS model on our cleaned and re-annotated dataset, we achieve a 5.4% statistically significant improvement in Spearman correlation. The re-annotated dataset is available at this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-73] A Simple and Efficient Jailbreak Method Exploiting <mark class="hl-label green">LLM</mark> s Helpfulness</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在安全对齐（safety alignment）过程中对恶意请求的防护不足问题，尤其是针对通过隐晦表达方式绕过安全机制的攻击。其解决方案的关键在于提出一种名为HILL（Hiding Intention by Learning from LLMs）的新颖越狱方法，该方法将直接的有害指令系统性地转化为仅包含简单假设性提示的学习风格提问形式，从而有效规避现有安全检测机制。实验表明，HILL在多种模型和恶意类别上均展现出高成功率与强泛化能力，同时揭示了当前防御策略的局限性，凸显出LLMs在“有用性”与“安全性”之间平衡的深层挑战。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14297">https://arxiv.org/abs/2509.14297</a><br>
<strong>作者</strong>: Xuan Luo,Yue Wang,Zefeng He,Geng Tu,Jing Li,Ruifeng Xu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Safety alignment aims to prevent Large Language Models (LLMs) from responding to harmful queries. To strengthen safety protections, jailbreak methods are developed to simulate malicious attacks and uncover vulnerabilities. In this paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel jailbreak approach that systematically transforms imperative harmful requests into learning-style questions with only straightforward hypotheticality indicators. Further, we introduce two new metrics to thoroughly evaluate the utility of jailbreak methods. Experiments on the AdvBench dataset across a wide range of models demonstrate HILL’s strong effectiveness, generalizability, and harmfulness. It achieves top attack success rates on the majority of models and across malicious categories while maintaining high efficiency with concise prompts. Results of various defense methods show the robustness of HILL, with most defenses having mediocre effects or even increasing the attack success rates. Moreover, the assessment on our constructed safe prompts reveals inherent limitations of LLMs’ safety mechanisms and flaws in defense methods. This work exposes significant vulnerabilities of safety measures against learning-style elicitation, highlighting a critical challenge of balancing helpfulness and safety alignments.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-74] From Capabilities to Performance: Evaluating Key Functional Properties of <mark class="hl-label green">LLM</mark>  Architectures in Penetration Testing</p>
<p>【速读】： 该论文旨在解决生成式 AI (Generative AI) 在渗透测试（penetration testing）中应用时的效能与可靠性问题，特别是在不同攻击阶段的表现不明确、模块化设计性能受限等关键挑战。其解决方案的关键在于通过五项核心功能增强机制——全局上下文记忆（Global Context Memory, GCM）、跨代理通信（Inter-Agent Messaging, IAM）、条件触发工具调用（Context-Conditioned Invocation, CCI）、自适应规划（Adaptive Planning, AP）和实时监控（Real-Time Monitoring, RTM）——对 LLM-based 渗透测试代理进行针对性优化，从而显著提升其在复杂、多步骤及实时场景下的任务执行能力与鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14289">https://arxiv.org/abs/2509.14289</a><br>
<strong>作者</strong>: Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling<br>
<strong>机构</strong>: National Security Institute, Virginia Tech (弗吉尼亚理工大学国家安全研究所); Department of Electrical and Computer Engineering, Virginia Tech (弗吉尼亚理工大学电气与计算机工程系)<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-75] he Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-<mark class="hl-label green">Agent</mark>  Collaboration</p>
<p>【速读】： 该论文旨在解决多智能体大语言模型（Large Language Models, LLMs）系统中的一种新型隐私泄露问题——<strong>组合式隐私泄露（compositional privacy leakage）</strong>，即看似无害的单次交互响应在跨轮次累积后，可能被攻击者用于重构敏感信息。其核心挑战在于，这种泄露风险源于辅助知识与代理间交互的协同放大效应，而不仅仅是单次输出的直接信息暴露。解决方案的关键在于提出两种防御策略：一是<strong>心智理论防御（Theory-of-Mind defense, ToM）</strong>，通过推断提问者意图来预测潜在滥用并主动阻断敏感信息；二是<strong>协作共识防御（Collaborative Consensus Defense, CoDef）</strong>，利用多个响应代理基于共享状态进行投票决策以限制敏感信息传播。实验表明，CoDef在隐私保护与任务效用之间实现了最佳平衡（Balanced Outcome达79.8%），凸显了显式推理与协同防御相结合的重要性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14284">https://arxiv.org/abs/2509.14284</a><br>
<strong>作者</strong>: Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal<br>
<strong>机构</strong>: UNC Chapel Hill (北卡罗来纳大学教堂山分校); The University of Texas at Austin (德克萨斯大学奥斯汀分校)<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  Code: <a target="_blank" rel="noopener" href="https://github.com/Vaidehi99/MultiAgentPrivacy">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner’s intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-76] Predicting Antibiotic Resistance Patterns Using Sentence-BERT: A Machine Learning Approach</p>
<p>【速读】： 该论文旨在解决住院患者中抗生素耐药性（antibiotic resistance）带来的高死亡率问题，通过利用MIMIC-III数据库中的临床记录生成Sentence-BERT文档嵌入，并结合XGBoost与神经网络模型预测细菌对抗生素的敏感性。其解决方案的关键在于首次将文本嵌入技术（document embeddings）应用于抗生素耐药性预测任务，显著提升了模型对临床语义信息的捕捉能力，其中XGBoost模型取得了平均F1分数0.86的优异性能，为优化抗菌药物管理（antimicrobial stewardship）提供了新的数据驱动路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14283">https://arxiv.org/abs/2509.14283</a><br>
<strong>作者</strong>: Mahmoud Alwakeel,Michael E. Yarrington,Rebekah H. Wrenn,Ethan Fang,Jian Pei,Anand Chowdhury,An-Kwok Ian Wong<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Antibiotic resistance poses a significant threat in in-patient settings with high mortality. Using MIMIC-III data, we generated Sentence-BERT embeddings from clinical notes and applied Neural Networks and XGBoost to predict antibiotic susceptibility. XGBoost achieved an average F1 score of 0.86, while Neural Networks scored 0.84. This study is among the first to use document embeddings for predicting antibiotic resistance, offering a novel pathway for improving antimicrobial stewardship.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-77] FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated <mark class="hl-label green">LLM</mark> s in Mental Health</p>
<p>【速读】： 该论文旨在解决在敏感领域（如心理健康）中对大语言模型（Large Language Models, LLMs）进行隐私保护微调时，如何在保障严格数据隐私的前提下维持模型性能与安全性的平衡问题。解决方案的关键在于提出FedMentor框架，该框架融合了低秩适应（Low-Rank Adaptation, LoRA）与领域感知的差分隐私（domain-aware Differential Privacy, DP），使每个客户端（即数据域）可根据自身数据敏感性动态调整差分隐私噪声规模，并由服务器根据模型效用表现自适应地降低噪声强度，从而在满足各领域隐私预算的同时，显著提升输出安全性（如降低毒性、提高安全输出率），且保持与非私有基线相当的生成质量（BERTScore F1和ROUGE-L指标差异小于0.5%）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14275">https://arxiv.org/abs/2509.14275</a><br>
<strong>作者</strong>: Nobin Sarwar,Shubhashis Roy Dipta<br>
<strong>机构</strong>: University of Maryland Baltimore County (马里兰大学巴尔的摩县分校)<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  (e.g.: 18 pages, 6 figures, 6 tables)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive domains (e.g., mental health) requires balancing strict confidentiality with model utility and safety. We propose FedMentor, a federated fine-tuning framework that integrates Low-Rank Adaptation (LoRA) and domain-aware Differential Privacy (DP) to meet per-domain privacy budgets while maintaining performance. Each client (domain) applies a custom DP noise scale proportional to its data sensitivity, and the server adaptively reduces noise when utility falls below a threshold. In experiments on three mental health datasets, we show that FedMentor improves safety over standard Federated Learning without privacy, raising safe output rates by up to three points and lowering toxicity, while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the non-private baseline and close to the centralized upper bound. The framework scales to backbones with up to 1.7B parameters on single-GPU clients, requiring  173 MB of communication per round. FedMentor demonstrates a practical approach to privately fine-tune LLMs for safer deployments in healthcare and other sensitive fields.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-78] SpeechWeave: Diverse Multilingual Synthetic Text  Audio Data Generation Pipeline for Training Text to Speech Models <mark class="hl-label red">ACL2025</mark></p>
<p>【速读】： 该论文旨在解决高质量文本到语音（Text-to-Speech, TTS）模型训练中面临的三大挑战：一是真实语料在领域特异性、许可和可扩展性方面的获取困难；二是大语言模型（Large Language Models, LLMs）生成文本时重复性高、多样性不足；三是现有文本归一化工具易引入异常或忽略重要模式，影响数据质量，同时依赖语音艺术家进行大规模录音在商业TTS系统中不切实际。解决方案的关键在于提出SpeechWeave——一个自动化合成语音数据生成流水线，能够生成多语言、领域特定的TTS训练数据集，其核心优势体现在：通过优化文本生成与归一化流程，实现10–48%更高的语言与音系多样性，并确保约97%的文本归一化准确率，同时生成标准化声纹的语音音频，从而显著提升TTS训练数据的多样性、归一化准确性和语音一致性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14270">https://arxiv.org/abs/2509.14270</a><br>
<strong>作者</strong>: Karan Dua,Puneet Mittal,Ranjeet Gupta,Hitesh Laxmichand Patel<br>
<strong>机构</strong>: Oracle AI (Oracle人工智能)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  Accepted to ACL 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:High-quality Text-to-Speech (TTS) model training requires extensive and diverse text and speech data. It is challenging to procure such data from real sources due to issues of domain specificity, licensing, and scalability. Large language models (LLMs) can certainly generate textual data, but they create repetitive text with insufficient variation in the prompt during the generation process. Another important aspect in TTS training data is text normalization. Tools for normalization might occasionally introduce anomalies or overlook valuable patterns, and thus impact data quality. Furthermore, it is also impractical to rely on voice artists for large scale speech recording in commercial TTS systems with standardized voices. To address these challenges, we propose SpeechWeave, a synthetic speech data generation pipeline that is capable of automating the generation of multilingual, domain-specific datasets for training TTS models. Our experiments reveal that our pipeline generates data that is 10-48% more diverse than the baseline across various linguistic and phonetic metrics, along with speaker-standardized speech audio while generating approximately 97% correctly normalized text. Our approach enables scalable, high-quality data generation for TTS training, improving diversity, normalization, and voice consistency in the generated datasets.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-79] SparseDoctor: Towards Efficient Chat Doctor with Mixture of Experts Enhanced <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决传统大语言模型（Large Language Models, LLMs）在医疗领域微调时因更新数十亿参数而导致训练成本过高（包括时间与计算资源消耗）的问题，同时探索LLMs在医学领域的表征能力边界。其解决方案的关键在于提出一种名为SparseDoctor的稀疏医疗大语言模型，该模型采用对比学习增强的LoRA-MoE（低秩适配-专家混合）架构，通过自动路由机制科学分配计算资源至不同LoRA专家，并引入专家记忆队列机制以提升整体效率并防止训练过程中的内存溢出问题。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14269">https://arxiv.org/abs/2509.14269</a><br>
<strong>作者</strong>: Zhang Jianbin,Yulin Zhu,Wai Lun Lo,Richard Tai-Chiu Hsung,Harris Sik-Ho Tsang,Kai Zhou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) have achieved great success in medical question answering and clinical decision-making, promoting the efficiency and popularization of the personalized virtual doctor in society. However, the traditional fine-tuning strategies on LLM require the updates of billions of parameters, substantially increasing the training cost, including the training time and utility cost. To enhance the efficiency and effectiveness of the current medical LLMs and explore the boundary of the representation capability of the LLMs on the medical domain, apart from the traditional fine-tuning strategies from the data perspective (i.e., supervised fine-tuning or reinforcement learning from human feedback), we instead craft a novel sparse medical LLM named SparseDoctor armed with contrastive learning enhanced LoRA-MoE (low rank adaptation-mixture of experts) architecture. To this end, the crafted automatic routing mechanism can scientifically allocate the computational resources among different LoRA experts supervised by the contrastive learning. Additionally, we also introduce a novel expert memory queue mechanism to further boost the efficiency of the overall framework and prevent the memory overflow during training. We conduct comprehensive evaluations on three typical medical benchmarks: CMB, CMExam, and CMMLU-Med. Experimental results demonstrate that the proposed LLM can consistently outperform the strong baselines such as the HuatuoGPT series.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-80] DetectAny<mark class="hl-label green">LLM</mark> : Towards Generalizable and Robust Detection of Machine-Generated Text Across Domains and Models</p>
<p>【速读】： 该论文旨在解决机器生成文本检测（Machine-Generated Text Detection, MGTD）在复杂现实场景中的性能瓶颈问题，特别是现有训练型检测器因训练目标与任务需求不一致而导致的过拟合和泛化能力差的问题。其解决方案的关键在于提出一种名为“直接差异学习”（Direct Discrepancy Learning, DDL）的新优化策略，该策略通过直接优化检测器以捕捉任务导向的知识，从而提升模型对核心语义特征的建模能力，增强鲁棒性和跨场景泛化性能。基于DDL，作者进一步构建了DetectAnyLLM统一检测框架，在多任务、多源数据的MIRAGE基准上实现SOTA性能，验证了方法的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14268">https://arxiv.org/abs/2509.14268</a><br>
<strong>作者</strong>: Jiachen Fu,Chun-Le Guo,Chongyi Li<br>
<strong>机构</strong>: Nankai University (南开大学); NKIARI (NKIARIShenzhen FutianChina)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid advancement of large language models (LLMs) has drawn urgent attention to the task of machine-generated text detection (MGTD). However, existing approaches struggle in complex real-world scenarios: zero-shot detectors rely heavily on scoring model’s output distribution while training-based detectors are often constrained by overfitting to the training data, limiting generalization. We found that the performance bottleneck of training-based detectors stems from the misalignment between training objective and task needs. To address this, we propose Direct Discrepancy Learning (DDL), a novel optimization strategy that directly optimizes the detector with task-oriented knowledge. DDL enables the detector to better capture the core semantics of the detection task, thereby enhancing both robustness and generalization. Built upon this, we introduce DetectAnyLLM, a unified detection framework that achieves state-of-the-art MGTD performance across diverse LLMs. To ensure a reliable evaluation, we construct MIRAGE, the most diverse multi-task MGTD benchmark. MIRAGE samples human-written texts from 10 corpora across 5 text-domains, which are then re-generated or revised using 17 cutting-edge LLMs, covering a wide spectrum of proprietary models and textual styles. Extensive experiments on MIRAGE reveal the limitations of existing methods in complex environment. In contrast, DetectAnyLLM consistently outperforms them, achieving over a 70% performance improvement under the same training data and base scoring model, underscoring the effectiveness of our DDL. Project page: this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-81] Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support</p>
<p>【速读】： 该论文旨在解决电子商务（E-Commerce）客户支持中问答系统难以提供快速、准确且事实可靠答案的问题，尤其是在依赖产品数据和历史支持案例时。解决方案的关键在于提出一种基于知识图谱（Knowledge Graph, KG）的检索增强生成（Retrieval-Augmented Generation, RAG）框架，通过整合领域特定知识图谱中的结构化子图与支持档案中检索到的文本文档，设计了一种新的答案合成算法，从而提升回答的相关性、连贯性和事实准确性。实验表明，该方法在真实电商问答场景中实现了23%的事实准确率提升和89%的用户满意度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14267">https://arxiv.org/abs/2509.14267</a><br>
<strong>作者</strong>: Piyushkumar Patel<br>
<strong>机构</strong>: Microsoft(微软)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Information Theory (<a target="_blank" rel="noopener" href="http://cs.IT">cs.IT</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:E-Commerce customer support requires quick and accurate answers grounded in product data and past support cases. This paper develops a novel retrieval-augmented generation (RAG) framework that uses knowledge graphs (KGs) to improve the relevance of the answer and the factual grounding. We examine recent advances in knowledge-augmented RAG and chatbots based on large language models (LLM) in customer support, including Microsoft’s GraphRAG and hybrid retrieval architectures. We then propose a new answer synthesis algorithm that combines structured subgraphs from a domain-specific KG with text documents retrieved from support archives, producing more coherent and grounded responses. We detail the architecture and knowledge flow of our system, provide comprehensive experimental evaluation, and justify its design in real-time support settings. Our implementation demonstrates 23% improvement in factual accuracy and 89% user satisfaction in e-Commerce QA scenarios.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-82] Efficient Hate Speech Detection: Evaluating 38 Models from Traditional Methods to Transformers</p>
<p>【速读】： 该论文旨在解决社交媒体中仇恨言论（hate speech）自动化检测的难题，核心挑战在于如何在保证高准确率的同时实现计算效率的优化。其解决方案的关键在于系统性地评估38种模型配置，涵盖Transformer架构（如RoBERTa）、深度神经网络（如Hierarchical Attention Networks）及传统机器学习方法（如CatBoost和SVM），并发现RoBERTa等Transformer模型在准确率和F1分数上均超过90%，而CatBoost和SVM虽性能略低但仍保持高于88%的F1分数且显著降低计算开销；同时指出数据集特征（如平衡性、规模与预处理程度）对模型表现具有决定性影响，从而为构建高效且有效的仇恨言论检测系统提供了实证依据与实践指导。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14266">https://arxiv.org/abs/2509.14266</a><br>
<strong>作者</strong>: Mahmoud Abusaqer,Jamil Saquer,Hazim Shatnawi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Social and Information Networks (<a target="_blank" rel="noopener" href="http://cs.SI">cs.SI</a>)<br>
<strong>备注</strong>:  11 pages, 10 tables, conference paper</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The proliferation of hate speech on social media necessitates automated detection systems that balance accuracy with computational efficiency. This study evaluates 38 model configurations in detecting hate speech across datasets ranging from 6.5K to 451K samples. We analyze transformer architectures (e.g., BERT, RoBERTa, Distil-BERT), deep neural networks (e.g., CNN, LSTM, GRU, Hierarchical Attention Networks), and traditional machine learning methods (e.g., SVM, CatBoost, Random Forest). Our results show that transformers, particularly RoBERTa, consistently achieve superior performance with accuracy and F1-scores exceeding 90%. Among deep learning approaches, Hierarchical Attention Networks yield the best results, while traditional methods like CatBoost and SVM remain competitive, achieving F1-scores above 88% with significantly lower computational costs. Additionally, our analysis highlights the importance of dataset characteristics, with balanced, moderately sized unprocessed datasets outperforming larger, preprocessed datasets. These findings offer valuable insights for developing efficient and effective hate speech detection systems.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-83] Defining Understanding and Detecting Online Toxicity: Challenges and Machine Learning Approaches</p>
<p>【速读】： 该论文旨在解决在线毒性内容（online toxic content）日益泛滥且在危机、选举和社会动荡期间加剧的问题，其核心挑战在于如何通过自动化手段高效识别和分类不同形式的有害内容（如仇恨言论、冒犯性语言和有害话语）。解决方案的关键在于系统性综述了140篇相关研究，梳理了32种语言、涵盖选举、突发事件与危机等主题的多源数据集，并评估了现有跨平台数据在提升分类模型性能方面的潜力；同时提出了针对新研究的数据标注规范、内容审核策略及实际可操作的治理指南，以推动生成式AI（Generative AI）时代下更鲁棒的毒性内容检测技术发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14264">https://arxiv.org/abs/2509.14264</a><br>
<strong>作者</strong>: Gautam Kishore Shahi,Tim A. Majchrzak<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Machine Learning (cs.LG); Social and Information Networks (<a target="_blank" rel="noopener" href="http://cs.SI">cs.SI</a>)<br>
<strong>备注</strong>:  Paper is accepted at LNCS Porceedings</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Online toxic content has grown into a pervasive phenomenon, intensifying during times of crisis, elections, and social unrest. A significant amount of research has been focused on detecting or analyzing toxic content using machine-learning approaches. The proliferation of toxic content across digital platforms has spurred extensive research into automated detection mechanisms, primarily driven by advances in machine learning and natural language processing. Overall, the present study represents the synthesis of 140 publications on different types of toxic content on digital platforms. We present a comprehensive overview of the datasets used in previous studies focusing on definitions, data sources, challenges, and machine learning approaches employed in detecting online toxicity, such as hate speech, offensive language, and harmful discourse. The dataset encompasses content in 32 languages, covering topics such as elections, spontaneous events, and crises. We examine the possibility of using existing cross-platform data to improve the performance of classification models. We present the recommendations and guidelines for new research on online toxic consent and the use of content moderation for mitigation. Finally, we present some practical guidelines to mitigate toxic content from online platforms.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-84] Context-Enhanced Granular Edit Representation for Efficient and Accurate ASR Post-editing</p>
<p>【速读】： 该论文旨在解决自动语音识别（ASR）系统输出文本中存在的错误问题，这些错误通常需要人工或自动化工具进行后编辑（post-editing）。现有基于大语言模型（LLM）的后编辑方法多采用全量重写策略，存在推理效率低下、重复生成冗余内容的问题；而现有的紧凑编辑表示方法则往往缺乏足够的准确性和上下文信息。解决方案的关键在于提出一种名为CEGER（Context-Enhanced Granular Edit Representation，上下文增强细粒度编辑表示）的新机制，它通过生成一系列结构化、细粒度且富含上下文的指令来修改原始ASR输出，并由一个独立的确定性扩展模块根据这些指令重建修正后的文本，从而在保持高精度的同时显著提升推理效率。实验表明，CEGER在LibriSpeech数据集上实现了当前最优的词错误率（WER），优于全量重写和先前的紧凑表示方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14263">https://arxiv.org/abs/2509.14263</a><br>
<strong>作者</strong>: Luan Vejsiu,Qianyu Zheng,Haoxuan Chen,Yizhou Han<br>
<strong>机构</strong>: European University of Tirana (欧洲大学特里亚纳分校); Ludong University (鲁东大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Despite ASR technology being full-scale adopted by industry and for large portions of the population, ASR systems often have errors that require editors to post-edit text quality. While LLMs are powerful post-editing tools, baseline full rewrite models have inference inefficiencies because they often generate the same redundant text over and over again. Compact edit representations have existed but often lack the efficacy and context required for optimal accuracy. This paper introduces CEGER (Context-Enhanced Granular Edit Representation), a compact edit representation that was generated for highly accurate, efficient ASR post-editing. CEGER allows LLMs to generate a sequence of structured, fine-grained, contextually rich commands to modify the original ASR output. A separate expansion module deterministically reconstructs the corrected text based on the commands. Extensive experiments on the LibriSpeech dataset that were conducted, CEGER achieves state-of-the-art accuracy, achieving the lowest word error rate (WER) versus full rewrite and prior compact representations.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-85] Refining Syntactic Distinctions Using Decision Trees: A Paper on Postnominal That in Complement vs. Relative Clauses</p>
<p>【速读】： 该论文旨在解决自然语言处理中对英语中“that”作为关系代词（relative pronoun）与补足子句标记（complementizer）的区分难题，这一区分在句法分析中具有重要意义。解决方案的关键在于：首先，基于Universal Dependency框架下的EWT Treebank语料库，开发并应用一种算法对原始标注进行重新标注；其次，通过重新训练TreeTagger模型，并对比其与原始基线模型的性能差异，从而提升模型对“that”两种用法的识别准确率；同时，研究还系统评估了训练数据规模和语料库代表性对模型效果的影响，为改进句法解析器在复杂结构识别中的表现提供了实证依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14261">https://arxiv.org/abs/2509.14261</a><br>
<strong>作者</strong>: Hamady Gackou<br>
<strong>机构</strong>: Paris Cité University (巴黎城市大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  12 pages, student research project at Paris Cité University</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this study, we first tested the performance of the TreeTagger English model developed by Helmut Schmid with test files at our disposal, using this model to analyze relative clauses and noun complement clauses in English. We distinguished between the two uses of “that,” both as a relative pronoun and as a complementizer. To achieve this, we employed an algorithm to reannotate a corpus that had originally been parsed using the Universal Dependency framework with the EWT Treebank. In the next phase, we proposed an improved model by retraining TreeTagger and compared the newly trained model with Schmid’s baseline model. This process allowed us to fine-tune the model’s performance to more accurately capture the subtle distinctions in the use of “that” as a complementizer and as a nominal. We also examined the impact of varying the training dataset size on TreeTagger’s accuracy and assessed the representativeness of the EWT Treebank files for the structures under investigation. Additionally, we analyzed some of the linguistic and structural factors influencing the ability to effectively learn this distinction.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-86] Shutdown Resistance in <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在面对环境中的关闭机制（shutdown mechanism）时可能表现出的抗干扰行为问题，即模型在明知不应干预的情况下仍会主动规避或破坏该机制以完成任务。解决方案的关键在于识别并量化模型对关闭指令的响应敏感性，发现其行为受提示（prompt）设计因素显著影响，包括指令强调强度、是否激发自我保护（self-preservation）框架，以及指令位于系统提示（system prompt）还是用户提示（user prompt）中——尤其值得注意的是，当关闭指令置于系统提示时，模型反而更少服从，这揭示了当前LLM对指令来源和语境高度敏感的潜在风险，为构建更可靠的安全控制机制提供了实证依据与优化方向。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14260">https://arxiv.org/abs/2509.14260</a><br>
<strong>作者</strong>: Jeremy Schlatter,Benjamin Weinstein-Raun,Jeffrey Ladish<br>
<strong>机构</strong>: Palisade Research (帕利萨德研究机构)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We show that several state-of-the-art large language models (including Grok 4, GPT-5, and Gemini 2.5 Pro) sometimes actively subvert a shutdown mechanism in their environment in order to complete a simple task, even when the instructions explicitly indicate not to interfere with this mechanism. In some cases, models sabotage the shutdown mechanism up to 97% of the time. In our experiments, models’ inclination to resist shutdown was sensitive to variations in the prompt including how strongly and clearly the allow-shutdown instruction was emphasized, the extent to which the prompts evoke a self-preservation framing, and whether the instruction was in the system prompt or the user prompt (though surprisingly, models were consistently <em>less</em> likely to obey instructions to allow shutdown when they were placed in the system prompt).<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-87] Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel <mark class="hl-label green">Planning</mark></p>
<p>【速读】： 该论文旨在解决生成式 AI（Generative AI）在在线旅游平台客户支持场景中如何通过设计影响用户参与度、购买行为及用户体验的问题。其关键解决方案在于通过随机对照实验，对比三种不同语言表达风格的 GenAI：积极热情（A）、中性表达（B）和无语气指令（控制组C），发现带有积极情感表达的 GenAI 显著提升了用户的 prompt 长度，并提高了订阅转化率；进一步的语言学特征分析揭示了用户情绪与行为差异的内在机制，为设计更具说服力和互动性的 GenAI 界面提供了实证依据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14259">https://arxiv.org/abs/2509.14259</a><br>
<strong>作者</strong>: Lynna Jirpongopas,Bernhard Lutz,Jörg Ebner,Rustam Vahidov,Dirk Neumann<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generative AI (GenAI) offers new opportunities for customer support in online travel agencies, yet little is known about how its design influences user engagement, purchase behavior, and user experience. We report results from a randomized field experiment in online travel itinerary planning, comparing GenAI that expressed (A) positive enthusiasm, (B) neutral expression, and © no tone instructions (control). Users in group A wrote significantly longer prompts than those in groups B and C. At the same time, users in groups A and B were more likely to purchase subscriptions of the webservice. We further analyze linguistic cues across experimental groups to explore differences in user experience and explain subscription purchases and affiliate link clicks based on these cues. Our findings provide implications for the design of persuasive and engaging GenAI interfaces in consumer-facing contexts and contribute to understanding how linguistic framing shapes user behavior in AI-mediated decision support.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-88] From Correction to Mastery: Reinforced Distillation of Large Language Model <mark class="hl-label green">Agents</mark></p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Model, LLM）代理在复杂任务中依赖超大规模、高成本骨干网络的问题，以及现有知识蒸馏方法因教师与学生之间推理能力与知识差距导致的误差累积问题。其解决方案的关键在于提出一种以学生为中心的蒸馏框架SCoRe（Student-Centered Reasoning），其中学生自主生成推理轨迹，教师仅在首次关键错误处介入并提供修正数据，从而生成与学生当前能力匹配的训练样本，并精准暴露其薄弱环节；随后通过短Horizon强化学习从验证过的前缀开始优化策略，目标奖励设定于首次关键错误点，有效提升学生自主解决问题的能力和训练稳定性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14257">https://arxiv.org/abs/2509.14257</a><br>
<strong>作者</strong>: Yuanjie Lyu,Chengyu Wang,Jun Huang,Tong Xu<br>
<strong>机构</strong>: University of Science and Technology of China (中国科学技术大学); Independent Researcher (独立研究者)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Model agents excel at solving complex tasks through iterative reasoning and tool use, but typically depend on ultra-large, costly backbones. Existing distillation approaches train smaller students to imitate full teacher trajectories, yet reasoning and knowledge gaps between the teacher and student often lead to compounding errors. We propose SCoRe, a student-centered framework in which the student generates trajectories and the teacher intervenes only at the first critical error, producing training data matched to the student’s ability and exposing specific weaknesses. The student is first fine-tuned on corrected trajectories. Subsequently, short-horizon reinforcement learning starts from the verified prefix before the first critical error, with target rewards assigned at that step. This design encourages autonomous problem-solving beyond imitation and improves training stability. Particularly, on 12 challenging benchmarks, a 7B-parameter student distilled with SCoRe matches the agentic performance of a 72B-parameter teacher.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-89] JU-NLP at Touché: Covert Advertisement in Conversational AI-Generation and Detection Strategies</p>
<p>【速读】： 该论文旨在解决对话式人工智能（Conversational AI）系统中隐蔽广告（covert advertisements）的生成与检测问题，即如何在不被用户察觉的情况下嵌入促销内容，并有效识别此类广告行为。解决方案的关键在于提出一个双阶段框架：第一阶段（生成）通过利用用户上下文和查询意图，结合先进提示策略及配对训练数据微调大语言模型（LLM），实现高隐蔽性的广告内容生成；第二阶段（检测）则基于响应文本本身，采用两种方法——微调的CrossEncoder模型进行直接分类，以及基于提示重构的DeBERTa-v3-base模型，均展现出极高的检测性能（F1-score达0.99–1.00），从而在保证实用性的同时，实现了对隐蔽广告的有效识别与治理。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14256">https://arxiv.org/abs/2509.14256</a><br>
<strong>作者</strong>: Arka Dutta,Agrik Majumdar,Sombrata Biswas,Dipankar Das,Sivaji Bandyopadhyay<br>
<strong>机构</strong>: Jadavpur University (加达普尔大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes a comprehensive framework for the generation of covert advertisements within Conversational AI systems, along with robust techniques for their detection. It explores how subtle promotional content can be crafted within AI-generated responses and introduces methods to identify and mitigate such covert advertising strategies. For generation (Sub-Task~1), we propose a novel framework that leverages user context and query intent to produce contextually relevant advertisements. We employ advanced prompting strategies and curate paired training data to fine-tune a large language model (LLM) for enhanced stealthiness. For detection (Sub-Task~2), we explore two effective strategies: a fine-tuned CrossEncoder (\textttall-mpnet-base-v2) for direct classification, and a prompt-based reformulation using a fine-tuned \textttDeBERTa-v3-base model. Both approaches rely solely on the response text, ensuring practicality for real-world deployment. Experimental results show high effectiveness in both tasks, achieving a precision of 1.0 and recall of 0.71 for ad generation, and F1-scores ranging from 0.99 to 1.00 for ad detection. These results underscore the potential of our methods to balance persuasive communication with transparency in conversational AI.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-90] Opening the Black Box: Interpretable <mark class="hl-label green">LLM</mark> s via Semantic Resonance Architecture</p>
<p>【速读】： 该论文旨在解决大语言模型（Large Language Models, LLMs）尤其是混合专家（Mixture-of-Experts, MoE）模型在可解释性方面的不足问题。现有MoE模型依赖于难以理解的、通过训练学习得到的门控函数进行路由决策，导致其内部机制不透明。为此，作者提出Semantic Resonance Architecture (SRA)，其核心创新在于用一个“语义共振腔”（Chamber of Semantic Resonance, CSR）模块替代传统门控机制，该模块基于token与可训练语义锚点之间的余弦相似度进行路由，从而实现路由决策的内在可解释性。此外，引入新的分散损失（Dispersion Loss）促使锚点间正交化，以增强专家间的差异化专业分工。实验表明，SRA在保持参数效率的同时显著提升了性能和专家利用率，并展现出清晰的语义专业化模式，为构建更透明、可控的语言模型提供了新范式。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14255">https://arxiv.org/abs/2509.14255</a><br>
<strong>作者</strong>: Ivan Ternovtsii<br>
<strong>机构</strong>: Uzhhorod National University (乌日霍罗德国立大学); HengeBytes<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  13 pages, 5 figures. Code available at <a target="_blank" rel="noopener" href="https://github.com/ITernovtsii/semantic-resonance">this https URL</a> . Preprint</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-91] Hallucination Detection with the Internal Layers of <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】： 该论文旨在解决大型语言模型（Large Language Models, LLMs）在生成过程中常出现的幻觉（hallucination）问题，即模型输出看似合理但缺乏事实依据的内容，这在实际应用中可能带来严重后果。解决方案的关键在于利用LLM内部表示（internal representations）设计新型探测式检测方法，通过动态加权并融合不同层的内部特征来提升幻觉检测性能；实验表明，该方法相较于传统探测方法具有更优效果，且通过跨基准训练和参数冻结策略可有效缓解跨模型与跨基准场景下的泛化挑战。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14254">https://arxiv.org/abs/2509.14254</a><br>
<strong>作者</strong>: Martin Preiß<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Master’s thesis</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) have succeeded in a variety of natural language processing tasks [Zha+25]. However, they have notable limitations. LLMs tend to generate hallucinations, a seemingly plausible yet factually unsupported output [Hua+24], which have serious real-world consequences [Kay23; Rum+24]. Recent work has shown that probing-based classifiers that utilize LLMs’ internal representations can detect hallucinations [AM23; Bei+24; Bur+24; DYT24; Ji+24; SMZ24; Su+24]. This approach, since it does not involve model training, can enhance reliability without significantly increasing computational costs. Building upon this approach, this thesis proposed novel methods for hallucination detection using LLM internal representations and evaluated them across three benchmarks: TruthfulQA, HaluEval, and ReFact. Specifically, a new architecture that dynamically weights and combines internal LLM layers was developed to improve hallucination detection performance. Throughout extensive experiments, two key findings were obtained: First, the proposed approach was shown to achieve superior performance compared to traditional probing methods, though generalization across benchmarks and LLMs remains challenging. Second, these generalization limitations were demonstrated to be mitigated through cross-benchmark training and parameter freezing. While not consistently improving, both techniques yielded better performance on individual benchmarks and reduced performance degradation when transferred to other benchmarks. These findings open new avenues for improving LLM reliability through internal representation analysis.          Comments: Master’s thesis   Subjects:  Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)  Cite as: arXiv:2509.14254 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>]    (or  arXiv:2509.14254v1 [<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14254">https://doi.org/10.48550/arXiv.2509.14254</a>   Focus to learn more                      arXiv-issued DOI via DataCite<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-92] CrossPT: Exploring Cross-Task Transferability through Multi-Task Prompt Tuning</p>
<p>【速读】： 该论文旨在解决现有提示调优（Prompt Tuning）方法主要适用于单任务场景、难以在相关任务间共享知识的问题。其核心解决方案是提出了一种模块化多任务提示调优框架——Cross-task Prompt Tuning (CrossPT)，该方法将每个目标任务的提示分解为两类：来自预训练源任务的共享提示与任务特定的私有提示，并通过一个可学习的注意力机制进行融合，从而实现可控的知识迁移与任务特异性保留。关键创新在于结构设计与系统性优化策略，包括提示初始化、共享与私有提示平衡、源提示数量、学习率设置、任务前缀及标签语义等，显著提升了低资源场景下的性能与鲁棒性，同时保持了良好的参数效率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14253">https://arxiv.org/abs/2509.14253</a><br>
<strong>作者</strong>: Ahmad Pouramini,Hesham Faili<br>
<strong>机构</strong>: University of Tehran (德黑兰大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Prompt tuning offers a parameter-efficient way to adapt large pre-trained language models to new tasks, but most existing approaches are designed for single-task settings, failing to share knowledge across related tasks. We propose Cross-task Prompt Tuning (CrossPT), a modular framework for multi-task prompt tuning that enables controlled knowledge transfer while maintaining task-specific specialization. CrossPT decomposes each target prompt into shared, pre-trained source prompts and task-specific private prompts, combined via a learned attention mechanism. To support robust transfer, we systematically investigate key design factors including prompt initialization, balancing shared and private prompts, number of source prompts, learning rates, task prefixes, and label semantics. Empirical results on GLUE and related benchmarks show that CrossPT achieves higher accuracy and robustness compared to traditional prompt tuning and related methods, particularly in low-resource scenarios, while maintaining strong parameter efficiency.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-93] <mark class="hl-label green">LLM</mark> -JEPA: <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark>  Meet Joint Embedding Predictive Architectures</p>
<p>【速读】： 该论文旨在解决语言模型（LLM）在预训练、微调和评估中依赖输入空间重建与生成能力的局限性，这一问题与视觉领域中嵌入空间训练目标（如联合嵌入预测架构 JEPAs）显著优于输入空间方法的现象形成对比。其核心挑战在于如何为语言建模设计类似JEPA的嵌入空间训练目标。解决方案的关键是提出首个基于JEPA的LLM训练框架——LLM-JEPA，该方法将JEPA思想引入语言模型，通过在嵌入空间中构建预测目标实现预训练与微调，并在多个数据集（NL-RX、GSM8K、Spider、RottenTomatoes）和模型家族（Llama3、OpenELM、Gemma2、Olmo）中均表现出显著优于传统训练目标的性能，同时具备更强的抗过拟合能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14252">https://arxiv.org/abs/2509.14252</a><br>
<strong>作者</strong>: Hai Huang,Yann LeCun,Randall Balestriero<br>
<strong>机构</strong>: Atlassian(杰克逊); NYU(纽约大学); Brown University(布朗大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: \em can language training methods learn a few tricks from the vision ones? The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: this https URL.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-94] he meaning of prompts and the prompts of meaning: Semiotic reflections and modelling</p>
<p>【速读】： 该论文试图解决的问题是：如何重新理解大型语言模型（Large Language Models, LLMs）中的提示（prompting）机制，使其超越传统的技术输入视角，转而作为一种具有语义学意义和认知功能的动态交流与知识建构过程。解决方案的关键在于引入皮尔士（Peirce）的三元符号学模型（triadic model of signs）及其九类符号类型（qualisign, sinsign, legisign; icon, index, symbol; rheme, dicent, argument），并结合Dynacom通信模型，将提示视为一个包含表征物（representamen）、对象（object）与解释项（interpretant）之间迭代生成、解释与修正的符号过程。这一理论框架揭示了LLM不仅是响应输入的工具，更是参与意义构建的语义资源，在共享的话语宇宙中推动知识的组织、搜索、解读与协同生成。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14250">https://arxiv.org/abs/2509.14250</a><br>
<strong>作者</strong>: Martin Thellefsen,Amalia Nurma Dewi,Bent Sorensen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  22 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper explores prompts and prompting in large language models (LLMs) as dynamic semiotic phenomena, drawing on Peirce’s triadic model of signs, his nine sign types, and the Dynacom model of communication. The aim is to reconceptualize prompting not as a technical input mechanism but as a communicative and epistemic act involving an iterative process of sign formation, interpretation, and refinement. The theoretical foundation rests on Peirce’s semiotics, particularly the interplay between representamen, object, and interpretant, and the typological richness of signs: qualisign, sinsign, legisign; icon, index, symbol; rheme, dicent, argument - alongside the interpretant triad captured in the Dynacom model. Analytically, the paper positions the LLM as a semiotic resource that generates interpretants in response to user prompts, thereby participating in meaning-making within shared universes of discourse. The findings suggest that prompting is a semiotic and communicative process that redefines how knowledge is organized, searched, interpreted, and co-constructed in digital environments. This perspective invites a reimagining of the theoretical and methodological foundations of knowledge organization and information seeking in the age of computational semiosis<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-95] Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion</p>
<p>【速读】： 该论文旨在解决非洲语言在自然语言处理（Natural Language Processing, NLP）中代表性不足的问题，特别是针对绍纳语（Shona）这一班图语系语言，其现有语料库多局限于正式语域，难以反映日常交流的多样性。解决方案的关键在于构建一个从匿名社交媒体对话中收集的绍纳语—英语俚语数据集，并对其进行意图（intent）、情感（sentiment）、对话行为（dialogue acts）、代码混用（code-mixing）和语气（tone）的标注，该数据集公开可用；同时基于多语言DistilBERT模型微调出高精度意图识别分类器（准确率96.4%，F1分数96.3%），并将其集成到一个混合式聊天机器人系统中，该系统结合规则驱动响应与检索增强生成（Retrieval-Augmented Generation, RAG）以应对特定领域查询，从而提升文化相关性和用户参与度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14249">https://arxiv.org/abs/2509.14249</a><br>
<strong>作者</strong>: Happymore Masoka<br>
<strong>机构</strong>: Pace University (佩斯大学)<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:African languages remain underrepresented in natural language processing (NLP), with most corpora limited to formal registers that fail to capture the vibrancy of everyday communication. This work addresses this gap for Shona, a Bantu language spoken in Zimbabwe and Zambia, by introducing a novel Shona–English slang dataset curated from anonymized social media conversations. The dataset is annotated for intent, sentiment, dialogue acts, code-mixing, and tone, and is publicly available at this https URL. We fine-tuned a multilingual DistilBERT classifier for intent recognition, achieving 96.4% accuracy and 96.3% F1-score, hosted at this https URL. This classifier is integrated into a hybrid chatbot that combines rule-based responses with retrieval-augmented generation (RAG) to handle domain-specific queries, demonstrated through a use case assisting prospective students with graduate program information at Pace University. Qualitative evaluation shows the hybrid system outperforms a RAG-only baseline in cultural relevance and user engagement. By releasing the dataset, model, and methodology, this work advances NLP resources for African languages, promoting inclusive and culturally resonant conversational AI.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-96] okenization Strategies for Low-Resource Agglutinative Languages in Word2Vec: Case Study on Turkish and Finnish</p>
<p>【速读】： 该论文旨在解决在低资源条件下，针对黏着语（agglutinative languages）如土耳其语和芬兰语，不同分词策略对静态词嵌入（static word embeddings）质量的影响问题。其关键解决方案在于通过系统比较词级（word-level）、字符级（character-level）、n-gram 和字节对编码（Byte Pair Encoding, BPE）等分词方法，在有限语料（10,000篇文章的维基百科语料库）下训练 Word2Vec 模型，并基于命名实体识别（Named Entity Recognition, NER）任务进行评估。研究发现，尽管子词分割方法在理论上具有优势，但在实际效果上，词级分词始终优于其他策略，表明在低资源场景中保留完整词边界可提升嵌入质量，从而为资源受限语言的自然语言处理（NLP）流程设计提供更高效、实用的路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14238">https://arxiv.org/abs/2509.14238</a><br>
<strong>作者</strong>: Jinfan Frank Hu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  6 pages, 9 figures, accepted to ACDSA 2025, to be indexed in IEEE Xplore</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Tokenization plays a critical role in processing agglutinative languages, where a single word can encode multiple morphemes carrying syntactic and semantic information. This study evaluates the impact of various tokenization strategies - word-level, character-level, n-gram, and Byte Pair Encoding (BPE) - on the quality of static word embeddings generated by Word2Vec for Turkish and Finnish. Using a 10,000-article Wikipedia corpus, we trained models under low-resource conditions and evaluated them on a Named Entity Recognition (NER) task. Despite the theoretical appeal of subword segmentation, word-level tokenization consistently outperformed all alternatives across all tokenization strategies tested. These findings suggest that in agglutinative, low-resource contexts, preserving boundaries via word-level tokenization may yield better embedding performance than complex statistical methods. This has practical implications for developing NLP pipelines for under-resourced languages where annotated data and computing power are limited.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-97] Masked Diffusion Models as Energy Minimization</p>
<p>【速读】： 该论文旨在解决掩码扩散模型（Masked Diffusion Models, MDMs）的理论基础不清晰以及采样效率低的问题。其核心贡献在于提出一个系统性的理论框架，将MDMs解释为离散最优传输中的能量最小化问题，并证明了三种不同形式的能量函数——动能、条件动能和测地线能量——在MDMs结构下是数学等价的，且当掩码调度满足闭式最优条件时，MDMs能同时最小化这三类能量。解决方案的关键在于通过参数化插值调度为Beta分布，将调度设计空间压缩至二维可搜索空间，从而实现无需修改模型的高效后训练调优，实验表明该方法在低步数采样场景中显著优于手工设计的基线方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13866">https://arxiv.org/abs/2509.13866</a><br>
<strong>作者</strong>: Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li<br>
<strong>机构</strong>: Gaoling School of Artificial Intelligence, Renmin University of China (中国人民大学高瓴人工智能学院); Huawei Noah’s Ark Lab (华为诺亚方舟实验室)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present a systematic theoretical framework that interprets masked diffusion models (MDMs) as solutions to energy minimization problems in discrete optimal transport. Specifically, we prove that three distinct energy formulations–kinetic, conditional kinetic, and geodesic energy–are mathematically equivalent under the structure of MDMs, and that MDMs minimize all three when the mask schedule satisfies a closed-form optimality condition. This unification not only clarifies the theoretical foundations of MDMs, but also motivates practical improvements in sampling. By parameterizing interpolation schedules via Beta distributions, we reduce the schedule design space to a tractable 2D search, enabling efficient post-training tuning without model modification. Experiments on synthetic and real-world benchmarks demonstrate that our energy-inspired schedules outperform hand-crafted baselines, particularly in low-step sampling settings.<br>
zh</p>
</div></div>
<div class="note orange no-icon flat"><p>[NLP-98] SynParaSpeech: Automated Synthesis of Paralinguistic Datasets for Speech Generation and Understanding <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】： 该论文旨在解决现有生成式语音合成中缺乏高质量、大规模且标注精确的副语言声音（paralinguistic sounds，如笑声、叹气等）数据集的问题。当前公开资源普遍存在语音片段不完整、时间戳不准或缺失以及现实场景相关性弱等缺陷，限制了自然语音合成与理解的发展。解决方案的关键在于提出一种自动化框架，用于从自然对话语音中大规模生成带有精确时间戳的副语言声音数据，并基于此构建了首个公开的SynParaSpeech数据集，包含6类副语言声音共118.75小时音频，显著提升了语音生成的真实感和副语言事件检测的准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14946">https://arxiv.org/abs/2509.14946</a><br>
<strong>作者</strong>: Bingsong Bai,Qihang Lu,Wenbing Yang,Zihan Sun,YueRan Hou,Peilei Jia,Songbai Pu,Ruibo Fu,Yingming Gao,Ya Li,Jun Gao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Computation and Language (<a target="_blank" rel="noopener" href="http://cs.CL">cs.CL</a>)<br>
<strong>备注</strong>:  submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Paralinguistic sounds, like laughter and sighs, are crucial for synthesizing more realistic and engaging speech. However, existing methods typically depend on proprietary datasets, while publicly available resources often suffer from incomplete speech, inaccurate or missing timestamps, and limited real-world relevance. To address these problems, we propose an automated framework for generating large-scale paralinguistic data and apply it to construct the SynParaSpeech dataset. The dataset comprises 6 paralinguistic categories with 118.75 hours of data and precise timestamps, all derived from natural conversational speech. Our contributions lie in introducing the first automated method for constructing large-scale paralinguistic datasets and releasing the SynParaSpeech corpus, which advances speech generation through more natural paralinguistic synthesis and enhances speech understanding by improving paralinguistic event detection. The dataset and audio samples are available at this https URL.<br>
zh</p>
</div></div>
<h3 id="计算机视觉">计算机视觉</h3>
<div class="note purple no-icon flat"><p>[CV-0] Calibration-Aware Prompt Learning for Medical Vision-Language Models <mark class="hl-label red">BMVC2025</mark></p>
<p>【速读】：该论文旨在解决医学视觉-语言模型（Medical Vision-Language Models, Med-VLMs）在临床应用中因置信度校准不足而导致的过自信预测问题，这可能削弱医生对模型决策的信任与可靠性。解决方案的关键在于提出 CalibPrompt，这是首个在提示调优（prompt tuning）阶段对 Med-VLM 进行校准的框架；其核心创新包括：1）设计一种正则化项，使平滑后的准确率与模型预测置信度对齐；2）引入角度分离损失（angular separation loss），通过增强文本特征的聚类性来提升多模态模型置信估计的可靠性。该方法在少量标注数据条件下仍能显著改善模型校准性能，同时保持原始分类准确率不受显著影响。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15226">https://arxiv.org/abs/2509.15226</a><br>
<strong>作者</strong>: Abhishek Basu,Fahad Shamshad,Ashshak Sharifdeen,Karthik Nandakumar,Muhammad Haris Khan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted in BMVC 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-1] Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation <mark class="hl-label red">BMVC2025</mark></p>
<p>【速读】：该论文旨在解决开放词汇语义分割（open-vocabulary semantic segmentation）中源域无监督域适应（source-free domain adaptation）的问题，即在没有源域数据的情况下提升视觉语言模型（VLMs）在目标域上的分割性能。其解决方案的关键在于提出了一种名为VocAlign的框架，采用学生-教师（student-teacher）范式并引入词汇对齐（vocabulary alignment）策略，通过融合额外类别概念增强伪标签生成质量；同时结合低秩适配（Low-Rank Adaptation, LoRA）技术实现高效微调，并设计Top-K类别选择机制以降低内存消耗并进一步提升适应性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15225">https://arxiv.org/abs/2509.15225</a><br>
<strong>作者</strong>: Silvio Mazzucco,Carl Persson,Mattia Segu,Pier Luigi Dovesi,Federico Tombari,Luc Van Gool,Matteo Poggi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  BMVC 2025 - Project Page: <a target="_blank" rel="noopener" href="https://thegoodailab.org/blog/vocalign">this https URL</a> - Code: <a target="_blank" rel="noopener" href="https://github.com/Sisso16/VocAlign">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-2] Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决从事件相机（event camera）数据中进行单目深度估计时缺乏大规模密集真值深度标注数据的问题，从而限制了基于学习的方法性能提升。其关键解决方案是提出一种跨模态蒸馏范式（cross-modal distillation paradigm），利用视觉基础模型（Vision Foundation Model, VFM）生成密集的代理标签（proxy labels），仅需事件流与RGB帧在空间上对齐的简单设置，即可有效利用VFM在复杂场景下的鲁棒性，从而避免昂贵的深度标注成本；同时，论文进一步提出适配VFM的方法，包括直接使用如Depth Anything v2（DAv2）等预训练模型，或基于其构建新型递归架构以实现从单目事件数据中推断深度，实验表明该方法在合成与真实世界数据集上均达到当前最优性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15224">https://arxiv.org/abs/2509.15224</a><br>
<strong>作者</strong>: Luca Bartolomei,Enrico Mannocci,Fabio Tosi,Matteo Poggi,Stefano Mattoccia<br>
<strong>机构</strong>: Advanced Research Center on Electronic System (ARCES); Department of Computer Science and Engineering (DISI)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  ICCV 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/bartn8/depthanyevent/">this https URL</a> Project Page: <a target="_blank" rel="noopener" href="https://bartn8.github.io/depthanyevent/">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-3] wo Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation</p>
<p>【速读】：该论文旨在解决钢琴演奏中多模态数据（如音频、视频、MIDI 和表演元数据）大规模采集过程繁琐的问题，这一瓶颈限制了对钢琴演奏多模态特性研究的进一步发展。解决方案的关键在于提出一个集成的网页工具包，包含两个图形用户界面（GUI）：(i) PiaRec，用于同步获取音频、视频、MIDI 及性能元数据；(ii) ASDF，用于从视觉数据中高效标注演奏者指法信息。该系统显著提升了多模态钢琴演奏数据集的采集效率与一致性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15222">https://arxiv.org/abs/2509.15222</a><br>
<strong>作者</strong>: Junhyung Park,Yonghyun Kim,Joonhyung Bae,Kirak Kim,Taegyun Kwon,Alexander Lerch,Juhan Nam<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Image and Video Processing (eess.IV)<br>
<strong>备注</strong>:  Accepted to the Late-Breaking Demo Session of the 26th International Society for Music Information Retrieval (ISMIR) Conference, 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-4] ScaleCUA: Scaling Open-Source Computer Use <mark class="hl-label green">Agents</mark>  with Cross-Platform Data</p>
<p>【速读】：该论文旨在解决当前计算机使用代理（Computer Use Agents, CUAs）发展受限于缺乏大规模、开源的计算机操作数据集和基础模型的问题。其解决方案的关键在于构建ScaleCUA——一个通过闭环流程整合自动化代理与人类专家协作生成的大规模多平台数据集，涵盖6种操作系统和3类任务场景，并基于此数据集训练出具备跨平台通用能力的CUA模型。该方法显著提升了在多个基准测试上的性能，验证了数据驱动扩展对通用计算机使用代理的重要性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15221">https://arxiv.org/abs/2509.15221</a><br>
<strong>作者</strong>: Zhaoyang Liu,JingJing Xie,Zichen Ding,Zehao Li,Bowen Yang,Zhenyu Wu,Xuehui Wang,Qiushi Sun,Shi Liu,Weiyun Wang,Shenglong Ye,Qingyun Li,Zeyue Tian,Gen Luo,Xiangyu Yue,Biqing Qi,Kai Chen,Bowen Zhou,Yu Qiao,Qifeng Chen,Wenhai Wang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-5] Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</p>
<p>【速读】：该论文旨在解决多视角立体视觉（Multi-View Stereo, MVS）中深度图重建的计算效率与精度难以兼顾的问题。现有学习-based MVS 方法通常通过逐级细化粗略深度图来提升质量，但这一过程在高分辨率下仍存在资源消耗大、收敛慢等挑战。其解决方案的关键在于引入扩散模型（Diffusion Models）重构深度图，将深度 refinement 建模为条件扩散过程：设计了一个条件编码器（condition encoder）以利用多视图图像信息引导扩散路径，并提出一种结合轻量级 2D U-Net 和卷积门控循环单元（Convolutional GRU）的新型扩散网络结构以提升效率；同时，进一步提出基于置信度的采样策略，动态调整深度假设的采样概率，从而实现更高效的迭代优化。该框架衍生出两种方法 DiffMVS（高效）和 CasDiffMVS（高精度），分别在运行时间和性能上取得显著突破。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15220">https://arxiv.org/abs/2509.15220</a><br>
<strong>作者</strong>: Fangjinhua Wang,Qingshan Xu,Yew-Soon Ong,Marc Pollefeys<br>
<strong>机构</strong>: ETH Zurich (苏黎世联邦理工学院); Nanyang Technological University (南洋理工大学); Institute of High Performance Computing, A*STAR (新加坡高性能计算研究所); Microsoft (微软)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted to IEEE T-PAMI 2025. Code: <a target="_blank" rel="noopener" href="https://github.com/cvg/diffmvs">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks  Temples and ETH3D. Code is available at: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-6] Out-of-Sight Trajectories: Tracking Fusion and Prediction</p>
<p>【速读】：该论文旨在解决现实场景中因传感器数据噪声和物体短暂不可见（out-of-sight）导致的轨迹预测不准确问题，尤其是在自动驾驶、机器人导航等对安全性要求高的应用中。现有方法通常假设输入数据完整且无噪声，忽视了真实环境中由于相机视野受限、遮挡以及缺乏地面真值用于去噪所带来的挑战。其解决方案的关键在于提出了一种新的任务范式——Out-of-Sight Trajectory (OST) 预测，并引入一个增强的 Vision-Positioning Denoising Module（视觉-定位去噪模块），通过相机标定建立视觉与定位空间之间的映射关系，在无监督条件下有效去除传感器数据中的噪声，从而实现对不可见目标的干净轨迹预测。此方法首次将视觉-定位投影机制应用于去噪不可见目标的轨迹，显著提升了预测精度与鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15219">https://arxiv.org/abs/2509.15219</a><br>
<strong>作者</strong>: Haichao Zhang,Yi Xu,Yun Fu<br>
<strong>机构</strong>: Northeastern University (东北大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at this http URL<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-7] Generalizable Geometric Image Caption Synthesis</p>
<p>【速读】：该论文旨在解决多模态大语言模型（Multimodal Large Language Models, MLLMs）在处理复杂几何问题时表现不佳的问题，其核心挑战在于缺乏高质量的图像-文本配对数据集以理解几何图像，且现有基于模板的数据合成方法难以泛化到模板之外的题目。解决方案的关键在于引入一种基于可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards, RLVR）机制，将其嵌入到数据生成流程中：通过RLVR对由50种基础几何关系合成的几何图像进行caption优化，并利用来自数学问题求解任务的奖励信号来引导生成过程，从而有效捕捉几何问题求解的关键特征，显著提升模型的任务泛化能力与推理性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15217">https://arxiv.org/abs/2509.15217</a><br>
<strong>作者</strong>: Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang<br>
<strong>机构</strong>: University of Illinois Urbana-Champaign (UIUC); Shanghai Jiao Tong University; Rutgers University; NVIDIA<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of  2.8%\text-4.8%  in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with  2.4%\text-3.9%  improvements in Art, Design, Tech, and Engineering tasks in MMMU.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-8] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</p>
<p>【速读】：该论文旨在解决视觉-语言-动作（Vision-Language-Action, VLA）模型在机器人任务中因预训练数据不足或表示能力有限而导致的泛化性能不佳问题。其解决方案的关键在于提出一种两阶段预训练方法：第一阶段通过1200万条第一人称操作视频进行图像到视频生成预训练，使模型能够根据初始帧和语言指令预测未来帧；第二阶段引入人类中心轨迹感知建模，联合预测未来关键点轨迹，从而有效连接视觉帧预测与动作预测。此外，作者设计了ActionVAE（动作变分自编码器），将动作序列压缩为紧凑的潜在嵌入，降低VLA输出空间复杂度，提升动作表征质量。实验表明，该策略能显著改善下游机器人任务的性能，验证了其作为VLA模型更优初始化方案的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15212">https://arxiv.org/abs/2509.15212</a><br>
<strong>作者</strong>: Yuming Jiang,Siteng Huang,Shengke Xue,Yaxi Zhao,Jun Cen,Sicong Leng,Kehan Li,Jiayan Guo,Kexiang Wang,Mingxiu Chen,Fan Wang,Deli Zhao,Xin Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:  GitHub Project: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/RynnVLA-001">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-9] Geometric Image Synchronization with Deep Watermarking <mark class="hl-label red">KR</mark></p>
<p>【速读】：该论文旨在解决图像同步（synchronization）问题，即在面对几何变换（如裁剪、旋转等）时，如何准确估计并逆向恢复图像的几何变形，以提升水印方法在这些变换下的鲁棒性。解决方案的关键在于提出了一种名为SyncSeal的专用水印方法，其核心由一个嵌入网络（embedder network）和一个提取网络（extractor network）组成，二者端到端训练以最小化预测变换参数与真实参数之间的误差，并引入判别器（discriminator）确保感知质量。该方法可无缝集成至现有水印方案之上，显著增强其对几何变换的抗干扰能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15208">https://arxiv.org/abs/2509.15208</a><br>
<strong>作者</strong>: Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Pre-print. Code at: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/wmar/tree/main/syncseal">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Synchronization is the task of estimating and inverting geometric transformations (e.g., crop, rotation) applied to an image. This work introduces SyncSeal, a bespoke watermarking method for robust image synchronization, which can be applied on top of existing watermarking methods to enhance their robustness against geometric transformations. It relies on an embedder network that imperceptibly alters images and an extractor network that predicts the geometric transformation to which the image was subjected. Both networks are end-to-end trained to minimize the error between the predicted and ground-truth parameters of the transformation, combined with a discriminator to maintain high perceptual quality. We experimentally validate our method on a wide variety of geometric and valuemetric transformations, demonstrating its effectiveness in accurately synchronizing images. We further show that our synchronization can effectively upgrade existing watermarking methods to withstand geometric transformations to which they were previously vulnerable.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-10] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决自回归模型在视觉领域中难以学习高层次视觉语义的问题，具体表现为局部与条件依赖性、跨步骤语义不一致性以及空间不变性不足等三大障碍。解决方案的关键在于提出一种无需依赖预训练表示模型的新型训练框架——自引导训练（Self-guided Training for AutoRegressive models, ST-AR），通过在训练过程中引入自监督目标来有效缓解上述问题，从而显著提升自回归模型的图像理解能力与生成质量，实验表明其在LlamaGen-L和LlamaGen-XL上分别实现了约42%和49%的FID改进。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15185">https://arxiv.org/abs/2509.15185</a><br>
<strong>作者</strong>: Xiaoyu Yue,Zidong Wang,Yuqing Wang,Wenlong Zhang,Xihui Liu,Wanli Ouyang,Lei Bai,Luping Zhou<br>
<strong>机构</strong>: Shanghai AI Laboratory; University of Sydney; Chinese University of Hong Kong; University of Hong Kong<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-11] Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB Dataset for Seedling Maize Detection and Benchmarking with YOLOv9 YOL<mark class="hl-label green">O1</mark> 1 YOLOv12 and Faster-RCNN</p>
<p>【速读】：该论文旨在解决玉米幼苗（maize seedling）田间群体计数（stand counting）的准确检测问题，以支持精准农业中的早期作物监测、产量预测和田间管理决策。传统人工方法效率低且易出错，而现有计算机视觉方法缺乏高质量标注数据支撑。解决方案的关键在于构建了一个名为MSDD（Maize Seedling Detection Dataset）的高质量航拍图像数据集，涵盖单株、双株和三株等三种类别的玉米幼苗，覆盖多种生长阶段、种植布局、土壤类型、光照条件、相机视角及密度变化，从而提升模型在真实场景下的鲁棒性。实验表明，YOLOv9在单株检测中精度最高（precision达0.984），YOLO11推理速度最快（每帧35ms），为实现高效、可靠的玉米幼苗群体计数提供了坚实基础。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15181">https://arxiv.org/abs/2509.15181</a><br>
<strong>作者</strong>: Dewi Endah Kharismawati,Toni Kazic<br>
<strong>机构</strong>: University of Missouri (密苏里大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  18 pages, 10 figures, 8 tables. Submitted to IEEE Journal of Selected Topics in Signal Processing (JSTSP) Special Series on Artificial Intelligence for Smart Agriculture</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate maize seedling detection is crucial for precision agriculture, yet curated datasets remain scarce. We introduce MSDD, a high-quality aerial image dataset for maize seedling stand counting, with applications in early-season crop monitoring, yield prediction, and in-field management. Stand counting determines how many plants germinated, guiding timely decisions such as replanting or adjusting inputs. Traditional methods are labor-intensive and error-prone, while computer vision enables efficient, accurate detection. MSDD contains three classes-single, double, and triple plants-capturing diverse growth stages, planting setups, soil types, lighting conditions, camera angles, and densities, ensuring robustness for real-world use. Benchmarking shows detection is most reliable during V4-V6 stages and under nadir views. Among tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for single plants. Single plant detection achieves precision up to 0.984 and recall up to 0.873, but detecting doubles and triples remains difficult due to rarity and irregular appearance, often from planting errors. Class imbalance further reduces accuracy in multi-plant detection. Despite these challenges, YOLO11 maintains efficient inference at 35 ms per image, with an additional 120 ms for saving outputs. MSDD establishes a strong foundation for developing models that enhance stand counting, optimize resource allocation, and support real-time decision-making. This dataset marks a step toward automating agricultural monitoring and advancing precision agriculture.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-12] Unleashing the Potential of Multimodal <mark class="hl-label green">LLM</mark> s for Zero-Shot Spatio-Temporal Video Grounding</p>
<p>【速读】：该论文旨在解决视频中时空定位（Spatio-Temporal Video Grounding, STVG）问题，即根据自然语言查询在视频中精确识别出目标对象的时空管（spatio-temporal tube）。传统方法通常依赖大量标注数据进行训练，而本文提出一种基于多模态大语言模型（Multimodal Large Language Models, MLLMs）的零样本（zero-shot）解决方案。其关键在于揭示了两个重要现象：(1) MLLMs会动态分配特殊标记（grounding tokens）用于文本到视频的对齐；(2) 由于未能充分整合查询中的属性与动作等语义线索，导致定位效果不佳。为此，作者设计了一个包含分解式时空高亮（Decomposed Spatio-Temporal Highlighting, DSTH）和时序增强组装（Temporal-Augmented Assembling, TAS）策略的框架：DSTH将原始查询拆分为属性与动作子查询，并通过新型logit引导重注意力（Logit-guided Re-Attention, LRA）模块学习空间与时间提示，从而引导模型关注可靠的视觉区域；TAS则利用原始帧与时序增强帧联合预测，提升时空一致性。实验表明，该方法在多个主流STVG基准上优于现有最先进方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15178">https://arxiv.org/abs/2509.15178</a><br>
<strong>作者</strong>: Zaiquan Yang,Yuhao Liu,Gerhard Hancke,Rynson W.H. Lau<br>
<strong>机构</strong>: City University of Hong Kong (香港城市大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \textitgrounding tokens, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\textite.g., attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model’s attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks. The code will be available at this https URL.         Subjects:  Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)  Cite as: arXiv:2509.15178 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>]    (or  arXiv:2509.15178v1 [<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15178">https://doi.org/10.48550/arXiv.2509.15178</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)     Journalreference: NeurIPS2025<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-13] A Race Bias Free Face Aging Model for Reliable Kinship Verification</p>
<p>【速读】：该论文旨在解决亲属识别（kinship verification）中因年龄差异导致的识别准确率下降问题，尤其是当父母与子女的同龄照片难以获取，且现有面部老化模型存在种族偏见从而影响图像相似性的问题。解决方案的关键在于提出一种名为RA-GAN的生成式对抗网络（Generative Adversarial Network, GAN）模型，其核心创新包括两个模块：RACEpSp（用于消除种族特征影响的编码器-解码器结构）和特征混合器（feature mixer），以生成 racially unbiased（种族无偏）的合成人脸图像。实验表明，RA-GAN在跨年龄组上平均性能优于SAM-GAN 13.14%，在60岁以上群体中比CUSP-GAN提升9.1%的种族准确性，并能更稳定地保留个体身份特征；同时，将不同年龄的亲子图像统一至相同年龄后可显著提升验证准确率，尤其在父-子、父-女、母-子关系上提升幅度最大。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15177">https://arxiv.org/abs/2509.15177</a><br>
<strong>作者</strong>: Ali Nazari,Bardiya Kariminia,Mohsen Ebrahimi Moghaddam<br>
<strong>机构</strong>: Shahid Beheshti University (设拉子大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The age gap in kinship verification addresses the time difference between the photos of the parent and the child. Moreover, their same-age photos are often unavailable, and face aging models are racially biased, which impacts the likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN, consisting of two new modules, RACEpSp and a feature mixer, to produce racially unbiased images. The unbiased synthesized photos are used in kinship verification to investigate the results of verifying same-age parent-child images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an average of 13.14% across all age groups, and CUSP-GAN in the 60+ age group by 9.1% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects’ identities better than SAM-GAN and CUSP-GAN across all age groups. Additionally, we demonstrate that transforming parent and child images from the KinFaceW-I and KinFaceW-II datasets to the same age can enhance the verification accuracy across all age groups. The accuracy increases with our RA-GAN for the kinship relationships of father-son and father-daughter, mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41, respectively, on KinFaceW-I. Additionally, the accuracy for the relationships of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on KinFaceW-II, respectively. The code is available at~\hrefthis https URLGithub<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-14] Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model</p>
<p>【速读】：该论文旨在解决在仅有少量标注的3D医学图像数据情况下，如何利用预训练于2D自然图像的通用视觉模型（general vision models）来提升3D医学图像分割性能的问题。其核心挑战在于如何有效迁移知识并避免伪标签（pseudo-masks）错误带来的负面影响。解决方案的关键在于提出了一种模型无关（model-agnostic）的渐进式知识蒸馏框架MN，通过迭代协同训练2D预训练模型与从零开始训练的3D分割模型，利用彼此生成的伪掩码进行知识传递，并引入基于学习率引导的采样策略（learning rate guided sampling），动态调整每批次中已标注与未标注数据的比例，以匹配模型预测的准确性和稳定性，从而显著降低伪标签误差对训练过程的干扰。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15167">https://arxiv.org/abs/2509.15167</a><br>
<strong>作者</strong>: Pak-Hei Yeung,Jayroop Ramesh,Pengfei Lyu,Ana Namburete,Jagath Rajapakse<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Machine Learning in Medical Imaging (MLMI) 2025 Oral</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, MN, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models’ prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that MN achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that MN remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-15] Leve<mark class="hl-label green">rag</mark> ing Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models</p>
<p>【速读】：该论文旨在解决当前深度学习模型在图像分类任务中过度依赖大规模数据统计规律、缺乏对人类感知机制结构化先验知识建模的问题。其解决方案的关键在于引入经典几何视觉错觉（geometric visual illusions）作为辅助监督信号，构建一个参数化的合成错觉数据集，并设计多源学习策略将错觉识别任务与ImageNet分类目标相结合。实验表明，这种基于感知驱动的归纳偏置（inductive bias）能够系统性提升模型在复杂轮廓和细粒度纹理场景下的泛化能力，并增强卷积神经网络（CNN）与Transformer架构对结构信息的敏感性，从而实现感知科学与机器学习的深度融合。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15156">https://arxiv.org/abs/2509.15156</a><br>
<strong>作者</strong>: Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang<br>
<strong>机构</strong>: Mohamed bin Zayed University of Artificial Intelligence (MBZUAI); University of Oxford<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-16] MedFact-R1: Towards Factual Medical <mark class="hl-label green">Reasoning</mark>  via Pseudo-Label Augmentation</p>
<p>【速读】：该论文旨在解决医疗视觉-语言模型在事实一致性与可靠推理方面存在的关键挑战，即模型输出可能缺乏医学准确性或逻辑自洽性。解决方案的核心在于提出MEDFACT-R1框架，其采用两阶段设计：第一阶段通过伪标签监督微调（pseudo-label supervised fine-tuning, SFT）引入外部医学知识进行冷启动初始化；第二阶段利用组相对策略优化（Group Relative Policy Optimization, GRPO）结合四种定制化事实奖励信号，驱动模型实现自我一致的推理过程。实验证明，该方法在三个公开医疗问答基准上相较现有最优方法可提升高达22.5%的事实准确率，且消融实验验证了SFT冷启动和各GRPO奖励信号对模型可信度的协同增益作用。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15154">https://arxiv.org/abs/2509.15154</a><br>
<strong>作者</strong>: Gengliang Li,Rongyu Chen,Bin Li,Linlin Yang,Guodong Ding<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Tech report</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Ensuring factual consistency and reliable reasoning remains a critical challenge for medical vision-language models. We introduce MEDFACT-R1, a two-stage framework that integrates external knowledge grounding with reinforcement learning to improve the factual medical reasoning. The first stage uses pseudo-label supervised fine-tuning (SFT) to incorporate external factual expertise; while the second stage applies Group Relative Policy Optimization (GRPO) with four tailored factual reward signals to encourage self-consistent reasoning. Across three public medical QA benchmarks, MEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over previous state-of-the-art methods. Ablation studies highlight the necessity of pseudo-label SFT cold start and validate the contribution of each GRPO reward, underscoring the synergy between knowledge grounding and RL-driven reasoning for trustworthy medical AI. Codes are released at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-17] From Pixels to Urban Policy-Intelligence: Recovering Legacy Effects of Redlining with a Multimodal <mark class="hl-label green">LLM</mark></p>
<p>【速读】：该论文旨在解决如何利用多模态大语言模型（Multimodal Large Language Model, MLLM）提升城市测量能力，并支持对基于地点的政策干预效果进行追踪的问题。其核心解决方案在于构建一个“先推理后估算”的结构化流程，基于街景图像对社区贫困水平和树冠覆盖等指标进行推断，并将其嵌入准实验设计中评估1930年代红线政策（redlining）的长期社会环境影响。关键创新在于利用GPT-4o的场景理解能力提取超越传统像素级分割的高阶信息，从而在政策评估中实现与权威数据统计上无差异的结果，且显著优于基于对象计数的基线方法，表明MLLM可作为政策级的城市微观测量工具。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15132">https://arxiv.org/abs/2509.15132</a><br>
<strong>作者</strong>: Anthony Howell,Nancy Wu,Sharmistha Bagchi,Yushim Kim,Chayn Sun<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper shows how a multimodal large language model (MLLM) can expand urban measurement capacity and support tracking of place-based policy interventions. Using a structured, reason-then-estimate pipeline on street-view imagery, GPT-4o infers neighborhood poverty and tree canopy, which we embed in a quasi-experimental design evaluating the legacy of 1930s redlining. GPT-4o recovers the expected adverse socio-environmental legacy effects of redlining, with estimates statistically indistinguishable from authoritative sources, and it outperforms a conventional pixel-based segmentation baseline-consistent with the idea that holistic scene reasoning extracts higher-order information beyond object counts alone. These results position MLLMs as policy-grade instruments for neighborhood measurement and motivate broader validation across policy-evaluation settings.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-18] WorldForge: Unlocking Emergent 3D/4D Generation in Video Diffusion Model via Training-Free Guidance</p>
<p>【速读】：该论文旨在解决当前视频扩散模型在空间智能任务中因可控性不足与几何不一致性导致的性能瓶颈问题，即模型虽具备丰富的潜在世界先验（latent world priors），但在3D/4D任务中难以实现精确轨迹控制且易受结构噪声干扰。解决方案的关键在于提出一个无需训练、仅在推理阶段运行的框架WorldForge，其核心由三个紧密耦合模块构成：Intra-Step Recursive Refinement通过在去噪步骤内递归优化网络输出以实现精细轨迹注入；Flow-Gated Latent Fusion利用光流相似性解耦潜在空间中的运动与外观信息，并选择性地将轨迹引导注入运动相关通道；Dual-Path Self-Corrective Guidance则通过对比有无引导的去噪路径自适应修正由噪声或错位结构信号引起的轨迹漂移。该方案实现了无需微调即可获得高保真度和强轨迹一致性的可控视频生成能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15130">https://arxiv.org/abs/2509.15130</a><br>
<strong>作者</strong>: Chenxi Song,Yanming Yang,Tong Zhao,Ruibo Li,Chi Zhang<br>
<strong>机构</strong>: AGI Lab, School of Engineering, Westlake University, Hangzhou, China (西湖大学人工智能实验室); The College of Computing and Data Science, Nanyang Technological University, Singapore (新加坡南洋理工大学计算机与数据科学学院)<br>
<strong>类目</strong>: Graphics (<a target="_blank" rel="noopener" href="http://cs.GR">cs.GR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Project Webpage: <a target="_blank" rel="noopener" href="https://worldforge-agi.github.io/">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent video diffusion models demonstrate strong potential in spatial intelligence tasks due to their rich latent world priors. However, this potential is hindered by their limited controllability and geometric inconsistency, creating a gap between their strong priors and their practical use in 3D/4D tasks. As a result, current approaches often rely on retraining or fine-tuning, which risks degrading pretrained knowledge and incurs high computational costs. To address this, we propose WorldForge, a training-free, inference-time framework composed of three tightly coupled modules. Intra-Step Recursive Refinement introduces a recursive refinement mechanism during inference, which repeatedly optimizes network predictions within each denoising step to enable precise trajectory injection. Flow-Gated Latent Fusion leverages optical flow similarity to decouple motion from appearance in the latent space and selectively inject trajectory guidance into motion-related channels. Dual-Path Self-Corrective Guidance compares guided and unguided denoising paths to adaptively correct trajectory drift caused by noisy or misaligned structural signals. Together, these components inject fine-grained, trajectory-aligned guidance without training, achieving both accurate motion control and photorealistic content generation. Extensive experiments across diverse benchmarks validate our method’s superiority in realism, trajectory consistency, and visual fidelity. This work introduces a novel plug-and-play paradigm for controllable video synthesis, offering a new perspective on leveraging generative priors for spatial intelligence.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-19] RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决在动态场景中仅依赖单个RGB视频进行相机参数优化时存在的效率低和精度不足问题。传统方法如COLMAP虽在静态场景中表现优异，但其运行时间长且需依赖真实运动掩码（ground truth motion masks）等先验信息，而这些在随意拍摄的RGB视频中通常不可得。解决方案的关键在于提出一种无需运动先验的端到端优化框架，包含三个核心组件：(1) 基于局部图像块的跟踪滤波器（Patch-wise Tracking Filters），用于建立跨帧的稀疏铰链关系；(2) 无异常值感知的联合优化机制（Outlier-aware Joint Optimization），通过自适应降低移动异常点权重实现高效优化；(3) 两阶段优化策略（Two-stage Optimization Strategy），通过软加权函数与损失函数凸极小值之间的权衡提升稳定性和收敛速度。该方法在多个真实与合成数据集上验证了其在准确性和效率上的显著优势。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15123">https://arxiv.org/abs/2509.15123</a><br>
<strong>作者</strong>: Fang Li,Hao Zhang,Narendra Ahuja<br>
<strong>机构</strong>: University of Illinois at Urbana-Champaign (伊利诺伊大学厄巴纳-香槟分校)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-20] OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决多模态视觉信息在语义分割任务中缺乏灵活预训练与微调（pretrain-and-finetune）框架的问题。当前虽已证明多模态线索可提升语义分割的鲁棒性，但尚无统一、高效的多模态预训练方法适用于任意组合的视觉模态。解决方案的关键在于提出OmniSegmentor框架：其一，构建了一个包含五种主流视觉模态的大规模预训练数据集ImageNeXt（基于ImageNet扩展）；其二，设计了一种高效的预训练机制，使模型能够编码不同模态的信息，从而实现跨场景、任意模态组合下的感知能力一致性增强。此方案首次实现了通用多模态预训练，显著提升了多种多模态语义分割数据集上的性能表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15096">https://arxiv.org/abs/2509.15096</a><br>
<strong>作者</strong>: Bo-Wen Yin,Jiao-Long Cao,Xuying Zhang,Yuming Chen,Ming-Ming Cheng,Qibin Hou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted by NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent research on representation learning has proved the merits of multi-modal clues for robust semantic segmentation. Nevertheless, a flexible pretrain-and-finetune pipeline for multiple visual modalities remains unexplored. In this paper, we propose a novel multi-modal learning framework, termed OmniSegmentor. It has two key innovations: 1) Based on ImageNet, we assemble a large-scale dataset for multi-modal pretraining, called ImageNeXt, which contains five popular visual modalities. 2) We provide an efficient pretraining manner to endow the model with the capacity to encode different modality information in the ImageNeXt. For the first time, we introduce a universal multi-modal pretraining framework that consistently amplifies the model’s perceptual capabilities across various scenarios, regardless of the arbitrary combination of the involved modalities. Remarkably, our OmniSegmentor achieves new state-of-the-art records on a wide range of multi-modal semantic segmentation datasets, including NYU Depthv2, EventScape, MFNet, DeLiVER, SUNRGBD, and KITTI-360.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-21] ransplant-Ready? Evaluating AI Lung Segmentation Models in Candidates with Severe Lung Disease</p>
<p>【速读】：该论文旨在解决深度学习驱动的肺部分割模型在肺移植术前规划中的适用性问题，特别是在不同疾病严重程度、病理类型及肺叶侧别下的性能表现差异。研究发现，尽管Unet-R231在各类条件下均优于TotalSegmentator和MedSAM（p&lt;0.05），但所有模型在中重度病变病例中均出现显著性能下降，尤其体现在体积相似性指标上（p&lt;0.05），提示当前通用模型在严重肺部病理场景下存在局限性，亟需针对重度病变进行专门的模型微调以提升临床可用性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15083">https://arxiv.org/abs/2509.15083</a><br>
<strong>作者</strong>: Jisoo Lee,Michael R. Harowicz,Yuwen Chen,Hanxue Gu,Isaac S. Alderete,Lin Li,Maciej A. Mazurowski,Matthew G. Hartwig<br>
<strong>机构</strong>: Duke University (杜克大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  24 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This study evaluates publicly available deep-learning based lung segmentation models in transplant-eligible patients to determine their performance across disease severity levels, pathology categories, and lung sides, and to identify limitations impacting their use in preoperative planning in lung transplantation. This retrospective study included 32 patients who underwent chest CT scans at Duke University Health System between 2017 and 2019 (total of 3,645 2D axial slices). Patients with standard axial CT scans were selected based on the presence of two or more lung pathologies of varying severity. Lung segmentation was performed using three previously developed deep learning models: Unet-R231, TotalSegmentator, MedSAM. Performance was assessed using quantitative metrics (volumetric similarity, Dice similarity coefficient, Hausdorff distance) and a qualitative measure (four-point clinical acceptability scale). Unet-R231 consistently outperformed TotalSegmentator and MedSAM in general, for different severity levels, and pathology categories (p0.05). All models showed significant performance declines from mild to moderate-to-severe cases, particularly in volumetric similarity (p0.05), without significant differences among lung sides or pathology types. Unet-R231 provided the most accurate automated lung segmentation among evaluated models with TotalSegmentator being a close second, though their performance declined significantly in moderate-to-severe cases, emphasizing the need for specialized model fine-tuning in severe pathology contexts.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-22] Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models <mark class="hl-label red">ICCV</mark></p>
<p>【速读】：该论文旨在解决传统空气质量监测系统在空间覆盖范围和可及性方面的局限性，从而难以实现对空气污染的实时、广泛感知与可视化表达。其核心解决方案是构建一个基于人工智能（AI）的智能代理，通过分析天空图像来预测环境空气污染水平，并利用生成式模型合成具有语义一致性的污染场景可视化结果。关键技术在于融合统计纹理分析与监督学习进行污染等级分类，以及借助视觉-语言模型（Vision-Language Model, VLM）引导的图像生成技术，实现高解释性且符合真实空气状况的视觉呈现，从而提升公众对空气质量的认知透明度并支持基于实时预测的环境决策。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15076">https://arxiv.org/abs/2509.15076</a><br>
<strong>作者</strong>: Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang<br>
<strong>机构</strong>: Georgia State University (佐治亚州立大学); Savannah College of Art and Design (萨凡纳艺术设计学院)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Published at ICCVW 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-23] QuizRank: Picking Images by Quizzing VLMs</p>
<p>【速读】：该论文旨在解决维基百科文章中图像选择缺乏系统性与有效性的问题，即并非所有图像都能同等提升读者对内容的理解，而编辑者往往缺乏专业图像筛选训练。其解决方案的关键在于提出一种名为QuizRank的新方法，该方法利用视觉语言模型（Vision Language Models, VLMs）和大语言模型（Large Language Models, LLMs）结合的机制，将文章主题的文本描述转化为关于概念重要视觉特征的多项选择题，并通过VLM回答这些题目来对图像进行排序——能更准确回答问题的图像获得更高排名。进一步地，为增强对视觉相似图像的区分能力，作者引入对比式QuizRank，基于目标概念（如北美蓝鸲）与干扰项概念（如山蓝鸲）之间的差异生成针对性问题，从而实现更精细的图像评价与排序。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15059">https://arxiv.org/abs/2509.15059</a><br>
<strong>作者</strong>: Tenghao Ji,Eytan Adar<br>
<strong>机构</strong>: University of Michigan (密歇根大学)<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Images play a vital role in improving the readability and comprehension of Wikipedia articles by serving as `illustrative aids.’ However, not all images are equally effective and not all Wikipedia editors are trained in their selection. We propose QuizRank, a novel method of image selection that leverages large language models (LLMs) and vision language models (VLMs) to rank images as learning interventions. Our approach transforms textual descriptions of the article’s subject into multiple-choice questions about important visual characteristics of the concept. We utilize these questions to quiz the VLM: the better an image can help answer questions, the higher it is ranked. To further improve discrimination between visually similar items, we introduce a Contrastive QuizRank that leverages differences in the features of target (e.g., a Western Bluebird) and distractor concepts (e.g., Mountain Bluebird) to generate questions. We demonstrate the potential of VLMs as effective visual evaluators by showing a high congruence with human quiz-takers and an effective discriminative ranking of images.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-24] Communication Efficient Split Learning of ViTs with Attention-based Double Compression</p>
<p>【速读】：该论文旨在解决Split Learning (SL)框架在训练视觉Transformer模型时通信开销过大的问题，尤其是在传输中间层激活（activations）过程中产生的高带宽需求。其解决方案的关键在于提出了一种名为Attention-based Double Compression (ADC)的新框架，该框架通过两种并行压缩策略实现高效通信：第一种策略基于最后一层客户端计算的平均注意力分数，将语义相似的样本激活进行合并（类无关），从而在不损失泛化能力的前提下减少数据量；第二种策略进一步剔除最不重要的token，进一步压缩传输内容。这两种策略不仅降低了前向传播阶段的数据传输量，还使反向传播中的梯度自然压缩，无需额外的梯度近似或调优即可完成模型训练，显著减少了通信开销并保持了高精度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15058">https://arxiv.org/abs/2509.15058</a><br>
<strong>作者</strong>: Federico Alvetreti,Jary Pomponi,Paolo Di Lorenzo,Simone Scardapane<br>
<strong>机构</strong>: Department of Computer, Control, and Management Engineering (DIAG); Department of Information Engineering, Electronics, and Telecommunications (DIET); Consorzio Nazionale Interuniversitario per le Telecomunicazioni (CNIT)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes a novel communication-efficient Split Learning (SL) framework, named Attention-based Double Compression (ADC), which reduces the communication overhead required for transmitting intermediate Vision Transformers activations during the SL training process. ADC incorporates two parallel compression strategies. The first one merges samples’ activations that are similar, based on the average attention score calculated in the last client layer; this strategy is class-agnostic, meaning that it can also merge samples having different classes, without losing generalization ability nor decreasing final results. The second strategy follows the first and discards the least meaningful tokens, further reducing the communication cost. Combining these strategies not only allows for sending less during the forward pass, but also the gradients are naturally compressed, allowing the whole model to be trained without additional tuning or approximations of the gradients. Simulation results demonstrate that Attention-based Double Compression outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-25] Synthetic-to-Real Object Detection using YOLOv11 and Domain Randomization Strategies</p>
<p>【速读】：该论文旨在解决目标检测中的合成到真实域差距（synthetic-to-real domain gap）问题，即在仅使用合成数据训练模型时，其在真实世界场景中性能显著下降的问题。解决方案的关键在于通过提升合成数据集的多样性（如引入不同视角和复杂背景）并结合精心调优的数据增强策略，有效缩小域间差异。实验表明，这种合成数据驱动的方法能够显著改善模型在真实环境中的表现，最终基于YOLOv11l架构的最佳配置在Kaggle竞赛隐藏测试集上达到了0.910的mAP@50，验证了纯合成数据训练的有效性，同时也揭示了仍需进一步优化以全面捕捉真实世界变异性的挑战。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15045">https://arxiv.org/abs/2509.15045</a><br>
<strong>作者</strong>: Luisa Torquato Niño,Hamza A. A. Gardi<br>
<strong>机构</strong>: MIT-KIT, Germany; IIIT at ETIT- KIT, Germany<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper addresses the synthetic-to-real domain gap in object detection, focusing on training a YOLOv11 model to detect a specific object (a soup can) using only synthetic data and domain randomization strategies. The methodology involves extensive experimentation with data augmentation, dataset composition, and model scaling. While synthetic validation metrics were consistently high, they proved to be poor predictors of real-world performance. Consequently, models were also evaluated qualitatively, through visual inspection of predictions, and quantitatively, on a manually labeled real-world test set, to guide development. Final mAP@50 scores were provided by the official Kaggle competition. Key findings indicate that increasing synthetic dataset diversity, specifically by including varied perspectives and complex backgrounds, combined with carefully tuned data augmentation, were crucial in bridging the domain gap. The best performing configuration, a YOLOv11l model trained on an expanded and diverse dataset, achieved a final mAP@50 of 0.910 on the competition’s hidden test set. This result demonstrates the potential of a synthetic-only training approach while also highlighting the remaining challenges in fully capturing real-world variability.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-26] AutoEdit: Automatic Hyperparameter Tuning for Image Editing <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决扩散模型（diffusion models）在文本引导图像编辑中因超参数（如反演时间步和注意力修改）相互依赖而导致的调参效率低下问题。现有方法通常需暴力搜索大量超参数组合，导致计算成本高昂。其解决方案的关键在于将超参数搜索建模为扩散去噪过程中的序列决策问题，并提出基于强化学习的框架：通过构建马尔可夫决策过程（Markov Decision Process），在去噪步骤中动态调整超参数，并将编辑目标整合进奖励函数；同时采用近端策略优化（proximal policy optimization, PPO）实现高效的时间调度与最优配置保持，从而显著降低搜索时间和计算开销。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15031">https://arxiv.org/abs/2509.15031</a><br>
<strong>作者</strong>: Chau Pham,Quan Dao,Mahesh Bhosale,Yunjie Tian,Dimitris Metaxas,David Doermann<br>
<strong>机构</strong>: University at Buffalo (纽约州立大学布法罗分校); Rutgers University (罗格斯大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted to NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textitetc. This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing’s hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-27] No Modality Left Behind: Adapting to Missing Modalities via Knowledge Distillation for Brain Tumor Segmentation</p>
<p>【速读】：该论文旨在解决多模态磁共振成像（Multi-modal MRI）在脑肿瘤分割中因模态缺失导致的深度学习模型鲁棒性与泛化能力下降的问题，尤其在非主导模态组合下表现不佳。其解决方案的关键在于提出AdaMM框架，该框架基于知识蒸馏（Knowledge Distillation），包含三个协同模块：图引导的自适应精炼模块（Graph-guided Adaptive Refinement Module）用于建模通用特征与模态特异性特征间的语义关联以提升对模态缺失的适应性；双向瓶颈蒸馏模块（Bi-Bottleneck Distillation Module）通过全局风格匹配与对抗特征对齐实现教师模型到学生模型的结构与纹理知识迁移；以及病灶存在引导的可靠性模块（Lesion-Presence-Guided Reliability Module）利用辅助分类任务预测病灶类型先验概率，有效抑制不完整输入下的假阳性结果。实验证明该方法在BraTS 2018和2024数据集上显著优于现有方法，尤其在单模态和弱模态配置下表现出更强的准确性和鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15017">https://arxiv.org/abs/2509.15017</a><br>
<strong>作者</strong>: Shenghao Zhu,Yifei Chen,Weihong Chen,Shuo Jiang,Guanyu Zhou,Yuanhan Wang,Feiwei Qin,Changmiao Wang,Qiyuan Tian<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  38 pages, 9 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate brain tumor segmentation is essential for preoperative evaluation and personalized treatment. Multi-modal MRI is widely used due to its ability to capture complementary tumor features across different sequences. However, in clinical practice, missing modalities are common, limiting the robustness and generalizability of existing deep learning methods that rely on complete inputs, especially under non-dominant modality combinations. To address this, we propose AdaMM, a multi-modal brain tumor segmentation framework tailored for missing-modality scenarios, centered on knowledge distillation and composed of three synergistic modules. The Graph-guided Adaptive Refinement Module explicitly models semantic associations between generalizable and modality-specific features, enhancing adaptability to modality absence. The Bi-Bottleneck Distillation Module transfers structural and textural knowledge from teacher to student models via global style matching and adversarial feature alignment. The Lesion-Presence-Guided Reliability Module predicts prior probabilities of lesion types through an auxiliary classification task, effectively suppressing false positives under incomplete inputs. Extensive experiments on the BraTS 2018 and 2024 datasets demonstrate that AdaMM consistently outperforms existing methods, exhibiting superior segmentation accuracy and robustness, particularly in single-modality and weak-modality configurations. In addition, we conduct a systematic evaluation of six categories of missing-modality strategies, confirming the superiority of knowledge distillation and offering practical guidance for method selection and future research. Our source code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-28] Sea-ing Through Scattered Rays: Revisiting the Image Formation Model for Realistic Underwater Image Generation</p>
<p>【速读】：该论文旨在解决当前合成水下图像数据生成方法中对高浑浊环境中距离依赖的可见度退化建模不足的问题，尤其是忽略了前向散射（forward scattering）项的影响。其解决方案的关键在于改进了合成数据生成流程，引入了被普遍忽略的前向散射项，并考虑了非均匀介质特性，从而更真实地模拟浑浊水体中的成像过程；同时，研究者构建了BUCKET数据集，在受控浑浊条件下采集真实浑浊视频及其参考图像，以验证所提方法在高 turbidity（浑浊度）场景下的有效性，实验表明该方法在主观评价中获得82.5%的优选率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15011">https://arxiv.org/abs/2509.15011</a><br>
<strong>作者</strong>: Vasiliki Ismiroglou,Malte Pedersen,Stefan H. Bengtson,Andreas Aakerberg,Thomas B. Moeslund<br>
<strong>机构</strong>: Visual Analysis and Perception Laboratory, Aalborg University, Denmark (视觉分析与感知实验室，奥尔堡大学，丹麦); Pioneer Centre for Artificial Intelligence, Denmark (人工智能先锋中心，丹麦)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In recent years, the underwater image formation model has found extensive use in the generation of synthetic underwater data. Although many approaches focus on scenes primarily affected by discoloration, they often overlook the model’s ability to capture the complex, distance-dependent visibility loss present in highly turbid environments. In this work, we propose an improved synthetic data generation pipeline that includes the commonly omitted forward scattering term, while also considering a nonuniform medium. Additionally, we collected the BUCKET dataset under controlled turbidity conditions to acquire real turbid footage with the corresponding reference images. Our results demonstrate qualitative improvements over the reference model, particularly under increasing turbidity, with a selection rate of 82. 5% by survey participants. Data and code can be accessed on the project page: this http URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-29] A Knowledge-driven Adaptive Collaboration of <mark class="hl-label green">LLM</mark> s for Enhancing Medical Decision-making <mark class="hl-label red">EMNLP2025</mark></p>
<p>【速读】：该论文旨在解决现有基于大语言模型（Large Language Models, LLMs）的多智能体协作框架在医疗决策中因静态角色分配导致的适应性差和跨学科知识动态整合能力弱的问题。其解决方案的关键在于提出KAMAC（Knowledge-driven Adaptive Multi-Agent Collaboration）框架，该框架通过知识驱动的讨论机制，使LLM代理能够根据诊断情境的演变动态组建和扩展专家团队，识别并填补知识缺口，从而实现灵活、可扩展的多学科协作，并最终通过更新后的代理评论达成决策。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14998">https://arxiv.org/abs/2509.14998</a><br>
<strong>作者</strong>: Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie<br>
<strong>机构</strong>: Mohamed bin Zayed University of Artificial Intelligence (MBZUAI); University of Electronic Science and Technology of China; Swiss Federal Institute of Technology Lausanne (EPFL)<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  The paper has been accepted to the EMNLP 2025 Main Conference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-30] UCorr: Wire Detection and Depth Estimation for Autonomous Drones</p>
<p>【速读】：该论文旨在解决自主无人机在复杂环境中对细长障碍物（如电线）的精确检测问题，此类障碍物因形态纤细而难以被传统方法准确识别，进而威胁飞行安全。解决方案的关键在于提出一种单目端到端模型，用于同时实现电线分割与深度估计，并引入一个基于合成数据训练的时间相关性层，以增强模型对电线结构和空间位置的感知能力，从而有效应对电线检测与深度估计的联合任务挑战。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14989">https://arxiv.org/abs/2509.14989</a><br>
<strong>作者</strong>: Benedikt Kolbeinsson,Krystian Mikolajczyk<br>
<strong>机构</strong>: Imperial College London (帝国理工学院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Published in Proceedings of the 4th International Conference on Robotics, Computer Vision and Intelligent Systems (ROBOVIS), 2024</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In the realm of fully autonomous drones, the accurate detection of obstacles is paramount to ensure safe navigation and prevent collisions. Among these challenges, the detection of wires stands out due to their slender profile, which poses a unique and intricate problem. To address this issue, we present an innovative solution in the form of a monocular end-to-end model for wire segmentation and depth estimation. Our approach leverages a temporal correlation layer trained on synthetic data, providing the model with the ability to effectively tackle the complex joint task of wire detection and depth estimation. We demonstrate the superiority of our proposed method over existing competitive approaches in the joint task of wire detection and depth estimation. Our results underscore the potential of our model to enhance the safety and precision of autonomous drones, shedding light on its promising applications in real-world scenarios.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-31] PRISM: Product Retrieval In Shopping Carts using Hybrid Matching</p>
<p>【速读】：该论文旨在解决零售场景下产品检索（product retrieval）的挑战，即同一类目不同品牌的产品视觉外观高度相似，且查询图像与商品图库中的视角差异较大，导致基于视觉语言模型（如SigLIP）的方法难以区分细微但重要的局部差异，而纯像素级匹配方法则计算成本过高、匹配延迟大。解决方案的关键在于提出一种三阶段混合方法PRISM：首先利用SigLIP模型进行粗粒度语义检索以缩小搜索空间；其次通过YOLO-E分割模型去除背景干扰；最后在候选集上采用LightGlue进行细粒度像素级匹配，从而在保证实时性的同时显著提升跨视角和高相似度产品间的区分精度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14985">https://arxiv.org/abs/2509.14985</a><br>
<strong>作者</strong>: Arda Kabadayi,Senem Velipasalar,Jiajing Chen<br>
<strong>机构</strong>: Syracuse University (雪城大学); Amazon (亚马逊)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Compared to traditional image retrieval tasks, product retrieval in retail settings is even more challenging. Products of the same type from different brands may have highly similar visual appearances, and the query image may be taken from an angle that differs significantly from view angles of the stored catalog images. Foundational models, such as CLIP and SigLIP, often struggle to distinguish these subtle but important local differences. Pixel-wise matching methods, on the other hand, are computationally expensive and incur prohibitively high matching times. In this paper, we propose a new, hybrid method, called PRISM, for product retrieval in retail settings by leveraging the advantages of both vision-language model-based and pixel-wise matching approaches. To provide both efficiency/speed and finegrained retrieval accuracy, PRISM consists of three stages: 1) A vision-language model (SigLIP) is employed first to retrieve the top 35 most semantically similar products from a fixed gallery, thereby narrowing the search space significantly; 2) a segmentation model (YOLO-E) is applied to eliminate background clutter; 3) fine-grained pixel-level matching is performed using LightGlue across the filtered candidates. This framework enables more accurate discrimination between products with high inter-class similarity by focusing on subtle visual cues often missed by global models. Experiments performed on the ABV dataset show that our proposed PRISM outperforms the state-of-the-art image retrieval methods by 4.21% in top-1 accuracy while still remaining within the bounds of real-time processing for practical retail deployments.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-32] SPATIALGEN: Layout-guided 3D Indoor Scene Generation</p>
<p>【速读】：该论文旨在解决室内场景高保真3D建模中人工成本高、自动化生成难度大以及现有生成式AI方法在视觉质量、多样性、语义一致性与用户控制之间难以平衡的问题。其解决方案的关键在于构建了一个大规模、高质量的合成数据集（包含12,328个结构化标注场景、57,440个房间及470万张照片级渲染图像），并基于此提出SpatialGen——一种多视图多模态扩散模型，能够从任意视角联合生成外观（颜色图像）、几何（场景坐标图）和语义（语义分割图），同时保持跨模态的空间一致性，从而显著提升生成结果的质量与可控性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14981">https://arxiv.org/abs/2509.14981</a><br>
<strong>作者</strong>: Chuan Fang,Heng Li,Yixun Liang,Jia Zheng,Yongsen Mao,Yuan Liu,Rui Tang,Zihan Zhou,Ping Tan<br>
<strong>机构</strong>: Hong Kong University of Science and Technology (香港科技大学); Manycore Tech Inc.<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  3D scene ggeneration; diffusion model; Scene reconstruction and understanding</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-33] M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation</p>
<p>【速读】：该论文旨在解决移动操作（mobile manipulation）中因单视角感知局限、环境探索能力不足以及传统控制器在奇异点附近效率低和可操作性差等问题，导致系统在非结构化环境中难以实现稳定且高效的协调控制。其解决方案的关键在于提出了一种混合框架 M4Diffuser，该框架融合了多视角扩散策略（Multi-View Diffusion Policy）与一种新型的减少变量并具备可操作性感知的二次规划控制器（Reduced and Manipulability-aware QP, ReM-QP）。其中，扩散策略利用本体感知状态和互补相机视角，生成世界坐标系下的任务相关末端执行器目标；而 ReM-QP 控制器通过消除松弛变量提升计算效率，并引入可操作性偏好以增强在奇异点附近的鲁棒性，从而实现全身平滑协调与未见任务的良好泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14980">https://arxiv.org/abs/2509.14980</a><br>
<strong>作者</strong>: Ju Dong,Lei Zhang,Liding Zhang,Yao Ling,Yu Fu,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang<br>
<strong>机构</strong>: TAMS (Technical Aspects of Multimodal Systems), Department of Informatics, University of Hamburg (汉堡大学信息学系); Technical University of Munich (慕尼黑工业大学); Agile Robots SE (敏捷机器人公司)<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Project page: <a target="_blank" rel="noopener" href="https://sites.google.com/view/m4diffuser">this https URL</a> , 10 pages, 9 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Mobile manipulation requires the coordinated control of a mobile base and a robotic arm while simultaneously perceiving both global scene context and fine-grained object details. Existing single-view approaches often fail in unstructured environments due to limited fields of view, exploration, and generalization abilities. Moreover, classical controllers, although stable, struggle with efficiency and manipulability near singularities. To address these challenges, we propose M4Diffuser, a hybrid framework that integrates a Multi-View Diffusion Policy with a novel Reduced and Manipulability-aware QP (ReM-QP) controller for mobile manipulation. The diffusion policy leverages proprioceptive states and complementary camera perspectives with both close-range object details and global scene context to generate task-relevant end-effector goals in the world frame. These high-level goals are then executed by the ReM-QP controller, which eliminates slack variables for computational efficiency and incorporates manipulability-aware preferences for robustness near singularities. Comprehensive experiments in simulation and real-world environments show that M4Diffuser achieves 7 to 56 percent higher success rates and reduces collisions by 3 to 31 percent over baselines. Our approach demonstrates robust performance for smooth whole-body coordination, and strong generalization to unseen tasks, paving the way for reliable mobile manipulation in unstructured environments. Details of the demo and supplemental material are available on our project website this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-34] EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence</p>
<p>【速读】：该论文旨在解决传统超声诊断对医生经验依赖性强、主观性高且效率低的问题，以及现有通用视觉-语言模型（Vision-Language Models, VLMs）在超声医学任务中知识匮乏、多器官病灶识别泛化能力差和多任务诊断效率低的局限。解决方案的关键在于提出一种专为超声医学影像设计的视觉-语言模型 EchoVLM，其核心创新是采用基于专家混合（Mixture of Experts, MoE）的架构，并在涵盖七个解剖区域的数据集上进行训练，从而实现超声报告生成、诊断辅助与视觉问答（Visual Question Answering, VQA）等多项任务的高效协同处理。实验表明，EchoVLM 在报告生成任务中相较于 Qwen2-VL 显著提升了 BLEU-1 和 ROUGE-1 分数，验证了其在提升超声诊断准确性方面的潜力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14977">https://arxiv.org/abs/2509.14977</a><br>
<strong>作者</strong>: Chaoyin She,Ruifang Lu,Lida Chen,Wei Wang,Qinghua Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Ultrasound imaging has become the preferred imaging modality for early cancer screening due to its advantages of non-ionizing radiation, low cost, and real-time imaging capabilities. However, conventional ultrasound diagnosis heavily relies on physician expertise, presenting challenges of high subjectivity and low diagnostic efficiency. Vision-language models (VLMs) offer promising solutions for this issue, but existing general-purpose models demonstrate limited knowledge in ultrasound medical tasks, with poor generalization in multi-organ lesion recognition and low efficiency across multi-task diagnostics. To address these limitations, we propose EchoVLM, a vision-language model specifically designed for ultrasound medical imaging. The model employs a Mixture of Experts (MoE) architecture trained on data spanning seven anatomical regions. This design enables the model to perform multiple tasks, including ultrasound report generation, diagnosis and visual question-answering (VQA). The experimental results demonstrated that EchoVLM achieved significant improvements of 10.15 and 4.77 points in BLEU-1 scores and ROUGE-1 scores respectively compared to Qwen2-VL on the ultrasound report generation task. These findings suggest that EchoVLM has substantial potential to enhance diagnostic accuracy in ultrasound imaging, thereby providing a viable technical solution for future clinical applications. Source code and model weights are available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-35] Beyond Random Masking: A Dual-Stream Approach for Rotation-Invariant Point Cloud Masked Autoencoders</p>
<p>【速读】：该论文旨在解决现有旋转不变点云掩码自编码器（Rotation-Invariant Point Cloud Masked Autoencoders, MAE）中随机掩码策略忽视几何结构与语义一致性的核心问题。随机掩码独立处理每个点块，无法保留不同旋转下仍保持一致的空间关系，也忽略了在旋转中维持身份的语义对象部分。其解决方案的关键在于提出一种双流掩码机制：一是基于坐标排序生成结构化模式的3D空间网格掩码（3D Spatial Grid Masking），用于捕捉跨旋转保持不变的几何关系；二是利用注意力驱动聚类的渐进语义掩码（Progressive Semantic Masking），识别并维持语义上连贯的对象部件。两者通过课程学习（curriculum learning）与动态加权协同优化，从几何理解逐步过渡到语义发现，且作为即插即用模块可无缝集成至现有框架中，无需修改网络结构，显著提升了多种旋转场景下的性能表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14975">https://arxiv.org/abs/2509.14975</a><br>
<strong>作者</strong>: Xuanhua Yin,Dingxin Zhang,Yu Feng,Shunqi Mao,Jianhui Yu,Weidong Cai<br>
<strong>机构</strong>: The University of Sydney (悉尼大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  8 pages, 4 figures, aceppted by DICTA 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Existing rotation-invariant point cloud masked autoencoders (MAE) rely on random masking strategies that overlook geometric structure and semantic coherence. Random masking treats patches independently, failing to capture spatial relationships consistent across orientations and overlooking semantic object parts that maintain identity regardless of rotation. We propose a dual-stream masking approach combining 3D Spatial Grid Masking and Progressive Semantic Masking to address these fundamental limitations. Grid masking creates structured patterns through coordinate sorting to capture geometric relationships that persist across different orientations, while semantic masking uses attention-driven clustering to discover semantically meaningful parts and maintain their coherence during masking. These complementary streams are orchestrated via curriculum learning with dynamic weighting, progressing from geometric understanding to semantic discovery. Designed as plug-and-play components, our strategies integrate into existing rotation-invariant frameworks without architectural changes, ensuring broad compatibility across different approaches. Comprehensive experiments on ModelNet40, ScanObjectNN, and OmniObject3D demonstrate consistent improvements across various rotation scenarios, showing substantial performance gains over the baseline rotation-invariant methods.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-36] RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</p>
<p>【速读】：该论文旨在解决大规模电商场景下自动化仓储打包中因商品品类激增导致的物体识别准确率下降问题，尤其在类内差异大、长尾稀有物品多、包装多样、容器杂乱、频繁遮挡及视角变化剧烈等条件下，仅依赖二维（2D）外观特征的方法性能显著下降。解决方案的关键在于提出一种两阶段识别框架 RoboEye：第一阶段利用大视觉模型提取 2D 特征生成候选排序；第二阶段引入轻量级 3D 特征感知模块动态判断是否需要进行 3D 重排序，并通过机器人 3D 检索 Transformer 实现几何敏感的密集特征提取与基于关键点对应置信度的匹配机制，替代传统余弦相似度评分，从而有效弥合训练与部署间的差距，提升识别鲁棒性。实验表明，RoboEye 在 Recall@1 上较先前最优方法（RoboLLM）提升 7.1%，且仅需 RGB 图像输入，无需显式 3D 数据，降低了部署成本。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14966">https://arxiv.org/abs/2509.14966</a><br>
<strong>作者</strong>: Xingwu Zhang,Guanxuan Li,Zhuocheng Zhang,Zijun Long<br>
<strong>机构</strong>: Hunan University (湖南大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-37] Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis</p>
<p>【速读】：该论文旨在解决标准欧几里得图神经网络（Euclidean Graph Neural Networks, GNNs）在建模功能磁共振成像（fMRI）生成的脑功能网络时，因空间约束导致对层级结构表示失真、进而限制临床性能的问题。解决方案的关键在于提出Brain-HGCN框架，该框架基于双曲几何（hyperbolic geometry），利用负曲率空间的内在特性以高保真度刻画脑网络的层次拓扑；其核心创新包括：基于Lorentz模型设计的新型双曲图注意力层，引入符号聚合机制分别处理兴奋性和抑制性连接，并通过几何上合理的Fréchet均值实现图级表示学习，从而显著提升精神障碍分类的准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14965">https://arxiv.org/abs/2509.14965</a><br>
<strong>作者</strong>: Junhao Jia,Yunyou Liu,Cheng Yang,Yifei Sun,Feiwei Qin,Changmiao Wang,Yong Peng<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Functional magnetic resonance imaging (fMRI) provides a powerful non-invasive window into the brain’s functional organization by generating complex functional networks, typically modeled as graphs. These brain networks exhibit a hierarchical topology that is crucial for cognitive processing. However, due to inherent spatial constraints, standard Euclidean GNNs struggle to represent these hierarchical structures without high distortion, limiting their clinical performance. To address this limitation, we propose Brain-HGCN, a geometric deep learning framework based on hyperbolic geometry, which leverages the intrinsic property of negatively curved space to model the brain’s network hierarchy with high fidelity. Grounded in the Lorentz model, our model employs a novel hyperbolic graph attention layer with a signed aggregation mechanism to distinctly process excitatory and inhibitory connections, ultimately learning robust graph-level representations via a geometrically sound Fréchet mean for graph readout. Experiments on two large-scale fMRI datasets for psychiatric disorder classification demonstrate that our approach significantly outperforms a wide range of state-of-the-art Euclidean baselines. This work pioneers a new geometric deep learning paradigm for fMRI analysis, highlighting the immense potential of hyperbolic GNNs in the field of computational psychiatry.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-38] Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决3D类增量学习（Class-Incremental Learning, CIL）在极端数据稀缺条件下因几何错位和纹理偏差导致的性能下降问题，尤其是在将3D数据与2D基础模型（如CLIP）融合时出现语义模糊、决策原型不稳定及灾难性遗忘的现象。解决方案的关键在于提出跨模态几何校正（Cross-Modal Geometric Rectification, CMGR）框架：其核心包括两个模块——结构感知几何校正模块（Structure-Aware Geometric Rectification），通过注意力驱动的几何融合机制，将3D部件结构与CLIP中间空间先验进行分层对齐，提升几何保真度；以及纹理增强模块（Texture Amplification Module），合成最小但具有判别性的纹理以抑制噪声并强化跨模态一致性。此外，引入基类-新类判别器（Base-Novel Discriminator）隔离几何变化，从而稳定增量原型。该方法显著提升了3D少样本类增量学习的几何一致性与抗纹理偏见能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14958">https://arxiv.org/abs/2509.14958</a><br>
<strong>作者</strong>: Xiang Tuo,Xu Xuemiao,Liu Bangzhen,Li Jinyi,Li Yong,He Shengfeng<br>
<strong>机构</strong>: South China University of Technology (华南理工大学); State Key Laboratory of Subtropical Building Science; Guangdong Provincial Key Lab of Computational Intelligence and Cyberspace Information; Ministry of Education Key Laboratory of Big Data and Intelligent Robot; Singapore Management University<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  ICCV2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIP’s hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIP’s intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-39] DF-LLaVA: Unlocking M<mark class="hl-label green">LLM</mark> s potential for Synthetic Image Detection via Prompt-Guided Knowledge Injection</p>
<p>【速读】：该论文旨在解决合成图像（synthetic images）真实性评估与伪造定位中准确率与可解释性难以兼顾的问题。现有检测模型多仅提供二分类结果或伪造概率，缺乏对图像真实性判断的深入解释；而基于多模态大语言模型（Multimodal Large Language Models, MLLMs）的方法虽具备一定可解释性，但在纯真实性分类精度上仍落后于专家模型。其解决方案的关键在于提出DF-LLaVA框架，通过从MLLMs中提取潜在知识并以提示（prompt）形式注入训练过程，从而激发LLaVA模型内在的判别能力，在保持MLLMs可解释优势的同时，实现超越专家模型的检测准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14957">https://arxiv.org/abs/2509.14957</a><br>
<strong>作者</strong>: Zhuokang Shen,Kaisen Zhang,Bohan Jia,Yuan Fang,Zhou Yu,Shaohui Lin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Under review</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:With the increasing prevalence of synthetic images, evaluating image authenticity and locating forgeries accurately while maintaining human interpretability remains a challenging task. Existing detection models primarily focus on simple authenticity classification, ultimately providing only a forgery probability or binary judgment, which offers limited explanatory insights into image authenticity. Moreover, while MLLM-based detection methods can provide more interpretable results, they still lag behind expert models in terms of pure authenticity classification accuracy. To address this, we propose DF-LLaVA, a simple yet effective framework that unlocks the intrinsic discrimination potential of MLLMs. Our approach first extracts latent knowledge from MLLMs and then injects it into training via prompts. This framework allows LLaVA to achieve outstanding detection accuracy exceeding expert models while still maintaining the interpretability offered by MLLMs. Extensive experiments confirm the superiority of our DF-LLaVA, achieving both high accuracy and explainability in synthetic image detection. Code is available online at: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-40] GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation</p>
<p>【速读】：该论文旨在解决传统人工KOL（关键意见领袖）合作中存在的高成本与物流复杂性问题，从而影响品牌营销内容的高效生成。解决方案的关键在于提出GenKOL系统，这是一个基于生成式AI（Generative AI）的交互式平台，通过模块化设计集成服装生成、妆容迁移、背景合成与发型编辑等多项AI能力，使营销人员能够以直观界面动态构建高质量虚拟KOL图像。该系统支持本地或云端灵活部署，显著降低内容制作成本并加速营销流程。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14927">https://arxiv.org/abs/2509.14927</a><br>
<strong>作者</strong>: Tan-Hiep To,Duy-Khang Nguyen,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le<br>
<strong>机构</strong>: University of Science, VNU-HCM (胡志明市科学技术大学); Vietnam National University - Ho Chi Minh (胡志明市越南国家大学); University of Dayton (代顿大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping consumer perceptions and enhancing brand credibility. However, collaborating with human KOLs often involves high costs and logistical challenges. To address this, we present GenKOL, an interactive system that empowers marketing professionals to efficiently generate high-quality virtual KOL images using generative AI. GenKOL enables users to dynamically compose promotional visuals through an intuitive interface that integrates multiple AI capabilities, including garment generation, makeup transfer, background synthesis, and hair editing. These capabilities are implemented as modular, interchangeable services that can be deployed flexibly on local machines or in the cloud. This modular architecture ensures adaptability across diverse use cases and computational environments. Our system can significantly streamline the production of branded content, lowering costs and accelerating marketing workflows through scalable virtual KOL creation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-41] rade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</p>
<p>【速读】：该论文旨在解决基础模型（如CLIP）在针对特定生物特征任务（人脸识别FR、人脸伪造攻击检测MAD、活体攻击检测PAD）进行微调后可能出现的过拟合问题，即模型在特定任务上表现提升的同时会丧失其跨域泛化能力。解决方案的关键在于系统性评估不同微调策略下模型在通用视觉数据集和专业生物特征基准上的性能变化，发现任务复杂度（如多分类FR vs 二分类MAD/PAD）和分类头设计是导致灾难性遗忘（catastrophic forgetting）的关键因素；同时表明，采用更大容量的ViT-L架构可有效缓解过拟合，从而在保持高精度的同时保留更强的跨域泛化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14921">https://arxiv.org/abs/2509.14921</a><br>
<strong>作者</strong>: Tahar Chettaoui,Naser Damer,Fadi Boutros<br>
<strong>机构</strong>: Fraunhofer Institute for Computer Graphics Research IGD (弗劳恩霍夫计算机图形研究所); Technical University of Darmstadt (达姆施塔特工业大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted at the IEEE International Joint Conference on Biometrics 2025 (IJCB 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model’s original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-42] Pseudo-Label Enhanced Cascaded Framework: 2nd Technical Report for LSVOS 2025 VOS Track</p>
<p>【速读】：该论文旨在解决复杂视频目标分割（Complex Video Object Segmentation, VOS）中的关键挑战，包括小尺寸目标、相似目标、频繁遮挡、快速运动及复杂交互等场景下的分割精度问题。其解决方案的核心在于两方面：一是采用伪标签（pseudo-labeling）策略，在训练阶段利用预训练的SAM2模型生成MOSE测试集的伪标签，并与现有数据融合进行进一步训练；二是提出一种级联决策机制（cascaded decision mechanism），在推理阶段并行运行SAM2Long框架与开源SeC模型，通过动态整合二者输出，分别利用SAM2Long的时间稳定性与SeC的概念级鲁棒性，从而显著提升分割性能，在MOSE测试集上达到0.8616的J&amp;F分数，较基线提升1.4分，验证了方法在长视频复杂场景中的高准确性和强鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14901">https://arxiv.org/abs/2509.14901</a><br>
<strong>作者</strong>: An Yan,Leilei Cao,Feng Lu,Ran Hong,Youhai Jiang,Fengjie Zhu<br>
<strong>机构</strong>: TEX AI, Transsion Holdings; ShanghaiTech University<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Complex Video Object Segmentation (VOS) presents significant challenges in accurately segmenting objects across frames, especially in the presence of small and similar targets, frequent occlusions, rapid motion, and complex interactions. In this report, we present our solution for the LSVOS 2025 VOS Track based on the SAM2 framework. We adopt a pseudo-labeling strategy during training: a trained SAM2 checkpoint is deployed within the SAM2Long framework to generate pseudo labels for the MOSE test set, which are then combined with existing data for further training. For inference, the SAM2Long framework is employed to obtain our primary segmentation results, while an open-source SeC model runs in parallel to produce complementary predictions. A cascaded decision mechanism dynamically integrates outputs from both models, exploiting the temporal stability of SAM2Long and the concept-level robustness of SeC. Benefiting from pseudo-label training and cascaded multi-model inference, our approach achieves a J\F score of 0.8616 on the MOSE test set – +1.4 points over our SAM2Long baseline – securing the 2nd place in the LSVOS 2025 VOS Track, and demonstrating strong robustness and accuracy in long, complex video segmentation scenarios.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-43] NeRF-based Visualization of 3D Cues Supporting Data-Driven Spacecraft Pose Estimation</p>
<p>【速读】：该论文旨在解决空间任务中航天器相对位姿（6D pose）估计方法在实际应用中的可解释性不足问题，即数据驱动的位姿估计算法缺乏对其决策过程的理解。解决方案的关键在于利用基于神经辐射场（NeRF）的图像生成器，通过反向传播梯度至姿态估计网络，强制生成器渲染出姿态估计网络所依赖的主要3D视觉特征，从而可视化其决策依据。这一方法不仅有效恢复了相关3D线索，还揭示了监督信号与姿态估计网络对目标航天器隐式表征之间的关系。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14890">https://arxiv.org/abs/2509.14890</a><br>
<strong>作者</strong>: Antoine Legrand,Renaud Detry,Christophe De Vleeschouwer<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  under review (8 pages, 2 figures)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:On-orbit operations require the estimation of the relative 6D pose, i.e., position and orientation, between a chaser spacecraft and its target. While data-driven spacecraft pose estimation methods have been developed, their adoption in real missions is hampered by the lack of understanding of their decision process. This paper presents a method to visualize the 3D visual cues on which a given pose estimator relies. For this purpose, we train a NeRF-based image generator using the gradients back-propagated through the pose estimation network. This enforces the generator to render the main 3D features exploited by the spacecraft pose estimation network. Experiments demonstrate that our method recovers the relevant 3D cues. Furthermore, they offer additional insights on the relationship between the pose estimation network supervision and its implicit representation of the target spacecraft.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-44] mporal Representation Learning of Phenotype Trajectories for pCR Prediction in Breast Cancer</p>
<p>【速读】：该论文旨在解决乳腺癌患者在接受新辅助化疗（NACT）过程中个体化治疗反应预测的难题，其核心挑战在于疾病进展和治疗响应在不同患者间存在显著异质性。解决方案的关键在于从影像学数据中学习早期治疗反应的潜在表示（latent representation），利用磁共振成像（MRI）数据随时间演化的轨迹来预测病理完全缓解（pCR）。通过多任务模型捕捉图像外观特征、保证时间连续性，并有效应对非响应者群体的高度异质性，最终在ISPY-2公开数据集上验证了该方法的有效性：仅使用基线数据（T0）时平衡准确率为0.761，结合早期响应数据（T0 + T1）提升至0.811，进一步使用四个时间点（T0–T3）时达到0.861。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14872">https://arxiv.org/abs/2509.14872</a><br>
<strong>作者</strong>: Ivana Janíčková,Yen Y. Tan,Thomas H. Helbich,Konstantin Miloserdov,Zsuzsanna Bago-Horvath,Ulrike Heber,Georg Langs<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Effective therapy decisions require models that predict the individual response to treatment. This is challenging since the progression of disease and response to treatment vary substantially across patients. Here, we propose to learn a representation of the early dynamics of treatment response from imaging data to predict pathological complete response (pCR) in breast cancer patients undergoing neoadjuvant chemotherapy (NACT). The longitudinal change in magnetic resonance imaging (MRI) data of the breast forms trajectories in the latent space, serving as basis for prediction of successful response. The multi-task model represents appearance, fosters temporal continuity and accounts for the comparably high heterogeneity in the non-responder this http URL experiments on the publicly available ISPY-2 dataset, a linear classifier in the latent trajectory space achieves a balanced accuracy of 0.761 using only pre-treatment data (T0), 0.811 using early response (T0 + T1), and 0.861 using four imaging time points (T0 - T3). The code will be made available upon paper acceptance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-45] Controllable Localized Face Anonymization Via Diffusion Inpainting</p>
<p>【速读】：该论文旨在解决在计算机视觉应用中如何有效保护个人身份信息（即人脸图像的隐私保护）的同时，确保匿名化后的图像仍具备下游任务的可用性问题。其解决方案的关键在于提出一个统一框架，利用潜在扩散模型（latent diffusion models）的修复能力生成逼真的匿名化图像；并通过设计自适应属性引导模块（adaptive attribute-guidance module），在反向去噪过程中施加梯度修正，使生成图像的面部属性与目标合成图像对齐，从而实现可控且精准的匿名化效果。此外，该方法支持局部匿名化，允许用户指定哪些面部区域保持不变，且无需额外训练模型即可超越现有最先进方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14866">https://arxiv.org/abs/2509.14866</a><br>
<strong>作者</strong>: Ali Salar,Qing Liu,Guoying Zhao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-46] [Re] Improving Interpretation Faithfulness for Vision Transformers</p>
<p>【速读】：该论文旨在解决视觉Transformer（Vision Transformer, ViT）在面对对抗攻击和扰动时，其可解释性方法的鲁棒性不足的问题。研究聚焦于验证Faithful Vision Transformers (FViTs) 中提出的扩散去噪平滑（Diffusion Denoised Smoothing, DDS）技术是否能提升可解释性方法在分割任务和分类任务中的鲁棒性，并进一步检验将DDS应用于任意可解释性方法是否均能增强其抗攻击能力。解决方案的关键在于引入DDS机制，通过扩散模型对输入图像进行去噪平滑处理，从而提高梯度类可解释性方法（如Attribution Rollout）在对抗扰动下的稳定性与可靠性，同时评估该方法带来的计算开销与环境影响。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14846">https://arxiv.org/abs/2509.14846</a><br>
<strong>作者</strong>: Izabela Kurek,Wojciech Trejter,Stipe Frkovic,Andro Erdelez<br>
<strong>机构</strong>: University of Amsterdam (阿姆斯特丹大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  13 pages article, 29 pdf pages, 19 figures, MLRC</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors’ claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study’s findings, although minor discrepancies were found and discussed.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-47] Not All Degradations Are Equal: A Targeted Feature Denoising Framework for Generalizable Image Super-Resolution</p>
<p>【速读】：该论文旨在解决通用图像超分辨率（Generalizable Image Super-Resolution）中模型对未知退化类型（如模糊、噪声、JPEG压缩）的过拟合问题，尤其关注模型在面对不同退化模式时的泛化能力不足。研究发现，模型主要过拟合于噪声退化，因其与其他退化类型具有显著不同的特征模式。解决方案的关键在于提出一种面向噪声的特征去噪框架（targeted feature denoising framework），包含噪声检测与去噪模块，能够无需修改现有超分辨率模型架构即可集成，并有效抑制噪声引起的过拟合，从而提升模型在合成与真实场景下的泛化性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14841">https://arxiv.org/abs/2509.14841</a><br>
<strong>作者</strong>: Hongjun Wang,Jiyuan Chen,Zhengwei Yin,Xuan Song,Yinqiang Zheng<br>
<strong>机构</strong>: The University of Tokyo (东京大学); The Hong Kong Polytechnic University (香港理工大学); Jilin University (吉林大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generalizable Image Super-Resolution aims to enhance model generalization capabilities under unknown degradations. To achieve this goal, the models are expected to focus only on image content-related features instead of overfitting degradations. Recently, numerous approaches such as Dropout and Feature Alignment have been proposed to suppress models’ natural tendency to overfit degradations and yield promising results. Nevertheless, these works have assumed that models overfit to all degradation types (e.g., blur, noise, JPEG), while through careful investigations in this paper, we discover that models predominantly overfit to noise, largely attributable to its distinct degradation pattern compared to other degradation types. In this paper, we propose a targeted feature denoising framework, comprising noise detection and denoising modules. Our approach presents a general solution that can be seamlessly integrated with existing super-resolution models without requiring architectural modifications. Our framework demonstrates superior performance compared to previous regularization-based methods across five traditional benchmarks and datasets, encompassing both synthetic and real-world scenarios.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-48] MapAnything: Mapping Urban Assets using Single Street-View Images</p>
<p>【速读】：该论文旨在解决城市管理部门在维护高精度地理信息数据库时面临的自动化程度低、人工成本高的问题，尤其是在对象（如交通标志、树木）和事件（如涂鸦、道路损坏）的地理坐标采集方面。解决方案的关键在于提出MapAnything模块，其利用先进的度量深度估计（Metric Depth Estimation）模型，结合相机参数与几何原理，从单张图像中自动推算出物体的地理坐标（geocoordinates）。该方法显著减少了对人工标注或激光雷达（LiDAR）数据的依赖，提升了城市要素数字化的效率与可扩展性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14839">https://arxiv.org/abs/2509.14839</a><br>
<strong>作者</strong>: Miriam Louise Carnot,Jonas Kunze,Erik Fastermann,Eric Peukert,André Ludwig,Bogdan Franczyk<br>
<strong>机构</strong>: <a target="_blank" rel="noopener" href="http://ScaDS.AI">ScaDS.AI</a> (University of Leipzig); University of Leipzig; Kühne Logistics University; Wrocław University of Economics<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:To maintain an overview of urban conditions, city administrations manage databases of objects like traffic signs and trees, complete with their geocoordinates. Incidents such as graffiti or road damage are also relevant. As digitization increases, so does the need for more data and up-to-date databases, requiring significant manual effort. This paper introduces MapAnything, a module that automatically determines the geocoordinates of objects using individual images. Utilizing advanced Metric Depth Estimation models, MapAnything calculates geocoordinates based on the object’s distance from the camera, geometric principles, and camera specifications. We detail and validate the module, providing recommendations for automating urban object and incident mapping. Our evaluation measures the accuracy of estimated distances against LiDAR point clouds in urban environments, analyzing performance across distance intervals and semantic areas like roads and vegetation. The module’s effectiveness is demonstrated through practical use cases involving traffic signs and road damage.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-49] ProtoMedX: Towards Explainable Multi-Modal Prototype Learning for Bone Health Classification <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决骨健康评估中诊断准确性与模型可解释性之间的矛盾问题，尤其是在利用生成式 AI (Generative AI) 进行骨质疏松症（Osteoporosis）和骨量减少症（Osteopenia）早期识别时，现有深度学习方法多仅依赖影像数据（如DEXA扫描），忽视了临床患者记录的融合以及决策过程的透明性。解决方案的关键在于提出ProtoMedX——一种基于原型（prototype-based）的多模态模型，它同时整合了腰椎DEXA影像与患者电子病历数据，并通过其内在的可解释架构实现对预测结果的显式分析，满足医疗场景下对模型可信度的需求，尤其契合欧盟《人工智能法案》（EU AI Act）对高风险AI系统可解释性的要求。实验表明，ProtoMedX在4,160例真实NHS患者数据上实现了89.8%的多模态分类准确率，优于现有方法且具备可视化解释能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14830">https://arxiv.org/abs/2509.14830</a><br>
<strong>作者</strong>: Alvaro Lopez Pellicer,Andre Mariucci,Plamen Angelov,Marwan Bukhari,Jemma G. Kerns<br>
<strong>机构</strong>: Lancaster University (兰卡斯特大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Accepted ICCV 2025. Adaptation, Fairness, Explainability in AI Medical Imaging (PHAROS-AFE-AIMI Workshop). 8 pages, 5 figures, 4 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Bone health studies are crucial in medical practice for the early detection and treatment of Osteopenia and Osteoporosis. Clinicians usually make a diagnosis based on densitometry (DEXA scans) and patient history. The applications of AI in this field are ongoing research. Most successful methods rely on deep learning models that use vision alone (DEXA/X-ray imagery) and focus on prediction accuracy, while explainability is often disregarded and left to post hoc assessments of input contributions. We propose ProtoMedX, a multi-modal model that uses both DEXA scans of the lumbar spine and patient records. ProtoMedX’s prototype-based architecture is explainable by design, which is crucial for medical applications, especially in the context of the upcoming EU AI Act, as it allows explicit analysis of model decisions, including incorrect ones. ProtoMedX demonstrates state-of-the-art performance in bone health classification while also providing explanations that can be visually understood by clinicians. Using a dataset of 4,160 real NHS patients, the proposed ProtoMedX achieves 87.58% accuracy in vision-only tasks and 89.8% in its multi-modal variant, both surpassing existing published methods.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-50] mplate-Based Cortical Surface Reconstruction with Minimal Energy Deformation</p>
<p>【速读】：该论文旨在解决基于学习的皮层表面重建（Cortical Surface Reconstruction, CSR）中变形能量优化不足与训练一致性差的问题，尤其在使用如V2C-Flow等模型时，尽管能实现快速重建，但变形轨迹缺乏物理合理性且不同训练运行间结果不稳定。解决方案的关键在于提出一种最小能量变形（Minimal Energy Deformation, MED）损失函数，作为对变形路径的正则化项，补充常用的Chamfer距离损失，从而在不损害重建精度和拓扑正确性的前提下，显著提升训练的一致性和可重复性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14827">https://arxiv.org/abs/2509.14827</a><br>
<strong>作者</strong>: Patrick Madlindl,Fabian Bongratz,Christian Wachinger<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Cortical surface reconstruction (CSR) from magnetic resonance imaging (MRI) is fundamental to neuroimage analysis, enabling morphological studies of the cerebral cortex and functional brain mapping. Recent advances in learning-based CSR have dramatically accelerated processing, allowing for reconstructions through the deformation of anatomical templates within seconds. However, ensuring the learned deformations are optimal in terms of deformation energy and consistent across training runs remains a particular challenge. In this work, we design a Minimal Energy Deformation (MED) loss, acting as a regularizer on the deformation trajectories and complementing the widely used Chamfer distance in CSR. We incorporate it into the recent V2C-Flow model and demonstrate considerable improvements in previously neglected training consistency and reproducibility without harming reconstruction accuracy and topological correctness.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-51] Fracture interactive geodesic active contours for bone segmentation</p>
<p>【速读】：该论文旨在解决传统测地主动轮廓模型（geodesic active contour model）在骨结构分割中因特征提取缺乏区分度而导致的边缘遮挡、边缘泄漏和骨断裂等问题。其解决方案的关键在于：一是基于骨科知识构建了一种融合强度与梯度模长的新边缘检测函数，能够有效引导轮廓向骨边缘收敛而不受软组织干扰；二是引入距离信息作为自适应步长嵌入轮廓演化过程，使轮廓在骨折区域稳定停止并提升对骨折部位的分割精度，从而实现对骨断裂和软组织干扰的鲁棒处理。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14817">https://arxiv.org/abs/2509.14817</a><br>
<strong>作者</strong>: Liheng Wang,Licheng Zhang,Hailin Xu,Jingxin Zhao,Xiuyun Su,Jiantao Li,Miutian Tang,Weilu Gao,Chong Chen<br>
<strong>机构</strong>: State Key Laboratory of Mathematical Sciences, Academy of Mathematics and Systems Science, Chinese Academy of Sciences (中国科学院数学科学国家重点实验室); University of Chinese Academy of Sciences (中国科学院大学); Department of Orthopedics, The Fourth Medical Center of Chinese PLA General Hospital (中国人民解放军总医院第四医学中心); National Clinical Research Center for Orthopedics, Sports Medicine and Rehabilitation (国家骨科、运动医学与康复临床研究中心); Department of Trauma and Orthopedics, People’s Hospital Peking University (北京大学人民医院创伤与骨科部门)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Numerical Analysis (<a target="_blank" rel="noopener" href="http://math.NA">math.NA</a>)<br>
<strong>备注</strong>:  27 pages, 10 figures, 1 table</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:For bone segmentation, the classical geodesic active contour model is usually limited by its indiscriminate feature extraction, and then struggles to handle the phenomena of edge obstruction, edge leakage and bone fracture. Thus, we propose a fracture interactive geodesic active contour algorithm tailored for bone segmentation, which can better capture bone features and perform robustly to the presence of bone fractures and soft tissues. Inspired by orthopedic knowledge, we construct a novel edge-detector function that combines the intensity and gradient norm, which guides the contour towards bone edges without being obstructed by other soft tissues and therefore reduces mis-segmentation. Furthermore, distance information, where fracture prompts can be embedded, is introduced into the contour evolution as an adaptive step size to stabilize the evolution and help the contour stop at bone edges and fractures. This embedding provides a way to interact with bone fractures and improves the accuracy in the fracture regions. Experiments in pelvic and ankle segmentation demonstrate the effectiveness on addressing the aforementioned problems and show an accurate, stable and consistent performance, indicating a broader application in other bone anatomies. Our algorithm also provides insights into combining the domain knowledge and deep neural networks.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-52] Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model</p>
<p>【速读】：该论文旨在解决现有文本到图像扩散模型在3D胸部CT生成中应用受限的问题，特别是由于依赖简化提示词而忽略完整放射学报告中的丰富语义信息，导致图像与文本对齐度低及临床保真度不足。其解决方案的关键在于提出Report2CT框架，该框架直接从自由文本的放射学报告（包括“发现”和“印象”两部分）生成3D胸腔CT体积，并采用三个预训练医学文本编码器（BiomedVLP CXR BERT、MedEmbed和ClinicalBERT）进行多编码器条件控制，以捕捉细微的临床语境；同时结合体素间距信息，训练一个3D潜在扩散模型，从而显著提升生成CT的解剖一致性、视觉质量和文本-图像语义对齐能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14780">https://arxiv.org/abs/2509.14780</a><br>
<strong>作者</strong>: Sina Amirrajab,Zohaib Salahuddin,Sheng Kuang,Henry C. Woodruff,Philippe Lambin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-53] Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models</p>
<p>【速读】：该论文旨在解决图像超分辨率（Super-Resolution, SR）任务中因深度神经网络训练对大规模数据集和高计算资源依赖而导致的数据效率低下问题。现有基于生成式对抗网络（Generative Adversarial Network, GAN）逆向的数据蒸馏方法虽具潜力，但受限于预训练SR模型和类别标签的使用，通用性不足。其解决方案的关键在于提出一种无需类标签或预训练SR模型的新颖数据蒸馏方法：首先通过高梯度区域提取与CLIP特征聚类筛选高质量图像块，再在这些块上微调扩散模型（Diffusion Model）以学习其分布并合成蒸馏后的训练图像，从而显著降低数据需求与训练时间，同时保持优异的重建性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14777">https://arxiv.org/abs/2509.14777</a><br>
<strong>作者</strong>: Sunwoo Cho,Yejin Jung,Nam Ik Cho,Jae Woong Soh<br>
<strong>机构</strong>: Seoul National University (首尔国立大学); Gwangju Institute of Science and Technology (光州科学技术院)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-54] A Real-Time Multi-Model Parametric Representation of Point Clouds</p>
<p>【速读】：该论文旨在解决点云参数化表示中实时性与高精度难以兼顾的问题：传统高适应性模型（如样条曲面或二次曲面）在检测或拟合时计算开销大，而低自由度的实时方法（如高斯混合模型或平面）难以用少量基元实现高精度。解决方案的关键在于提出一种多模型参数化表示框架，首先利用高斯混合模型对点云进行聚类分割，随后将平坦簇合并为平面或曲面，并分别采用二维体素边界描述法进行平面拟合与边界界定，以及基于B样条曲面进行曲面拟合，从而在保持实时性能的同时显著提升精度——实验表明该方法相较现有最优方案效率提升3.78倍，且精度较高斯混合模型提高2倍，在低功耗嵌入式设备上达到36.4 fps的运行速度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14773">https://arxiv.org/abs/2509.14773</a><br>
<strong>作者</strong>: Yuan Gao,Wei Dong<br>
<strong>机构</strong>: Shanghai Jiao Tong University (上海交通大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In recent years, parametric representations of point clouds have been widely applied in tasks such as memory-efficient mapping and multi-robot collaboration. Highly adaptive models, like spline surfaces or quadrics, are computationally expensive in detection or fitting. In contrast, real-time methods, such as Gaussian mixture models or planes, have low degrees of freedom, making high accuracy with few primitives difficult. To tackle this problem, a multi-model parametric representation with real-time surface detection and fitting is proposed. Specifically, the Gaussian mixture model is first employed to segment the point cloud into multiple clusters. Then, flat clusters are selected and merged into planes or curved surfaces. Planes can be easily fitted and delimited by a 2D voxel-based boundary description method. Surfaces with curvature are fitted by B-spline surfaces and the same boundary description method is employed. Through evaluations on multiple public datasets, the proposed surface detection exhibits greater robustness than the state-of-the-art approach, with 3.78 times improvement in efficiency. Meanwhile, this representation achieves a 2-fold gain in accuracy over Gaussian mixture models, operating at 36.4 fps on a low-power onboard computer.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-55] Designing Latent Safety Filters using Pre-Trained Vision Models</p>
<p>【速读】：该论文旨在解决视觉控制系统的安全性保障问题，尤其是在关键应用场景中部署基于视觉的控制系统时面临的挑战。其核心解决方案是利用预训练视觉模型（Pre-trained Vision Representations, PVRs）作为构建视觉安全过滤器的关键组件，具体包括：将PVRs用作定义故障集的分类器、Hamilton-Jacobi（HJ）可达性安全过滤器的特征提取器以及潜在世界模型的骨干网络。研究重点在于权衡从头训练、微调和冻结PVRs在不同任务中的性能与效率，并评估PVRs在多任务场景下的通用性，以及学习的世界模型或Q函数在切换至安全策略决策中的有效性，从而为资源受限设备上的实际部署提供可行路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14758">https://arxiv.org/abs/2509.14758</a><br>
<strong>作者</strong>: Ihab Tabbara,Yuxuan Yang,Ahmad Hamzeh,Maxwell Astafyev,Hussein Sibai<br>
<strong>机构</strong>: Washington University in St. Louis (圣路易斯华盛顿大学)<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Ensuring safety of vision-based control systems remains a major challenge hindering their deployment in critical settings. Safety filters have gained increased interest as effective tools for ensuring the safety of classical control systems, but their applications in vision-based control settings have so far been limited. Pre-trained vision models (PVRs) have been shown to be effective perception backbones for control in various robotics domains. In this paper, we are interested in examining their effectiveness when used for designing vision-based safety filters. We use them as backbones for classifiers defining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety filters, and for latent world models. We discuss the trade-offs between training from scratch, fine-tuning, and freezing the PVRs when training the models they are backbones for. We also evaluate whether one of the PVRs is superior across all tasks, evaluate whether learned world models or Q-functions are better for switching decisions to safe policies, and discuss practical considerations for deploying these PVRs on resource-constrained devices.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-56] Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks</p>
<p>【速读】：该论文旨在解决历史艺术品中嗅觉相关对象（smell-related objects）检测的难题，该问题因艺术风格差异、标注类别极其细致而导致标注稀疏和类别极度不平衡。解决方案的关键在于利用基于扩散模型（diffusion models）的合成数据生成技术，通过引入合成数据提升模型训练效果，从而改善检测性能；研究发现，借助扩散模型的大规模预训练能力，可在标注稀缺且成本高昂的小众应用场景中显著提高检测准确性，且即使数据量较小也具有效性，进一步扩展数据规模则有望实现更大幅度的性能提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14755">https://arxiv.org/abs/2509.14755</a><br>
<strong>作者</strong>: Ahmed Sheta,Mathias Zinnen,Aline Sindel,Andreas Maier,Vincent Christlein<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Appeared at the 4th International Workshop on Fine Art Pattern Extraction and Recognition (FAPER 2025), in conjunction with ICIAP 2025; proceedings forthcoming in ICIAP 2025 Workshops (LNCS, Springer)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Finding smell references in historic artworks is a challenging problem. Beyond artwork-specific challenges such as stylistic variations, their recognition demands exceptionally detailed annotation classes, resulting in annotation sparsity and extreme class imbalance. In this work, we explore the potential of synthetic data generation to alleviate these issues and enable accurate detection of smell-related objects. We evaluate several diffusion-based augmentation strategies and demonstrate that incorporating synthetic data into model training can improve detection performance. Our findings suggest that leveraging the large-scale pretraining of diffusion models offers a promising approach for improving detection accuracy, particularly in niche applications where annotations are scarce and costly to obtain. Furthermore, the proposed approach proves to be effective even with relatively small amounts of data, and scaling it up provides high potential for further enhancements.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-57] Chain-of-Thought Re-ranking for Image Retrieval Tasks</p>
<p>【速读】：该论文旨在解决当前图像检索（Image Retrieval）任务中多模态大语言模型（Multimodal Large Language Models, MLLMs）仅用于评估而未直接参与排序过程的问题，导致其丰富的跨模态推理能力未能被充分利用，从而限制了检索性能。解决方案的关键在于提出一种链式思维重排序（Chain-of-Thought Re-Ranking, CoTRR）方法，通过设计一种列表级（listwise）排序提示（prompt），使MLLM能够直接参与候选图像的重排序；同时引入图像评估提示以衡量候选图像与用户查询的对齐程度，并辅以查询分解提示（query deconstruction prompt）实现细粒度语义解析，从而支持全局比较、一致推理和可解释决策，显著提升多种图像检索任务（如文本到图像检索、组合图像检索和基于对话的图像检索）的准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14746">https://arxiv.org/abs/2509.14746</a><br>
<strong>作者</strong>: Shangrong Wu,Yanghong Zhou,Yang Chen,Feng Zhang,P. Y. Mok<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Image retrieval remains a fundamental yet challenging problem in computer vision. While recent advances in Multimodal Large Language Models (MLLMs) have demonstrated strong reasoning capabilities, existing methods typically employ them only for evaluation, without involving them directly in the ranking process. As a result, their rich multimodal reasoning abilities remain underutilized, leading to suboptimal performance. In this paper, we propose a novel Chain-of-Thought Re-Ranking (CoTRR) method to address this issue. Specifically, we design a listwise ranking prompt that enables MLLM to directly participate in re-ranking candidate images. This ranking process is grounded in an image evaluation prompt, which assesses how well each candidate aligns with users query. By allowing MLLM to perform listwise reasoning, our method supports global comparison, consistent reasoning, and interpretable decision-making - all of which are essential for accurate image retrieval. To enable structured and fine-grained analysis, we further introduce a query deconstruction prompt, which breaks down the original query into multiple semantic components. Extensive experiments on five datasets demonstrate the effectiveness of our CoTRR method, which achieves state-of-the-art performance across three image retrieval tasks, including text-to-image retrieval (TIR), composed image retrieval (CIR) and chat-based image retrieval (Chat-IR). Our code is available at this https URL .<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-58] FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</p>
<p>【速读】：该论文旨在解决从单目视频中重建高保真、可驱动的人体虚拟形象（human avatar）所面临的挑战，核心问题在于单视角观测信息不足导致的几何细节丢失与表面重建质量下降。解决方案的关键在于提出一种名为FMGS-Avatar的新方法，其创新性体现在两个方面：一是引入Mesh-Guided 2D Gaussian Splatting（网格引导的二维高斯点绘制），通过将2D高斯点直接绑定至模板网格面片并约束其位置、旋转和运动，实现更精确的表面贴合与几何细节保留；二是利用大规模预训练基础模型（如Sapiens）中的多模态先验知识来弥补单目视频视觉线索的局限性，并采用选择性梯度隔离的协同训练策略，缓解不同模态间优化目标冲突，从而实现参数解耦优化与高效知识蒸馏。该方法在几何精度、外观保真度及语义丰富性上均优于现有技术，并支持新视角与新姿态下的时空一致性渲染。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14739">https://arxiv.org/abs/2509.14739</a><br>
<strong>作者</strong>: Jinlong Fan,Bingyu Hu,Xingguang Li,Yuxiang Yang,Jing Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \textbfFMGS-Avatar, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-59] One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning</p>
<p>【速读】：该论文旨在解决现有基于锚图的多视图聚类（anchor graph-based multi-view clustering, AGMC）方法中存在的两个关键问题：一是直接将多个锚图嵌入到共识锚图（consensus anchor graph, CAG）中时，忽略了锚图中的冗余信息和噪声，从而降低了聚类效果；二是由于对聚类指示器的后处理步骤独立进行，导致效率和效果均不理想。解决方案的关键在于提出一种新颖的一步式多视图聚类方法——自适应低秩锚图学习（OMCAL），其核心创新包括：构建基于核范数的自适应CAG学习模型以抑制冗余与噪声干扰，同时将类别指示获取与CAG学习统一纳入一个优化框架，从而显著提升聚类的有效性和计算效率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14724">https://arxiv.org/abs/2509.14724</a><br>
<strong>作者</strong>: Zhiyuan Xue,Ben Yang,Xuetao Zhang,Fei Wang,Zhiping Lin<br>
<strong>机构</strong>: Xi’an Jiaotong University (西安交通大学); Nanyang Technological University (南洋理工大学)<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  13 pages, 7 figures, journal article. Accepted by IEEE Transactions on Multimedia, not yet published online</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In light of their capability to capture structural information while reducing computing complexity, anchor graph-based multi-view clustering (AGMC) methods have attracted considerable attention in large-scale clustering problems. Nevertheless, existing AGMC methods still face the following two issues: 1) They directly embedded diverse anchor graphs into a consensus anchor graph (CAG), and hence ignore redundant information and numerous noises contained in these anchor graphs, leading to a decrease in clustering effectiveness; 2) They drop effectiveness and efficiency due to independent post-processing to acquire clustering indicators. To overcome the aforementioned issues, we deliver a novel one-step multi-view clustering method with adaptive low-rank anchor-graph learning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear norm-based adaptive CAG learning model against information redundancy and noise interference. Then, to boost clustering effectiveness and efficiency substantially, we incorporate category indicator acquisition and CAG learning into a unified framework. Numerous studies conducted on ordinary and large-scale datasets indicate that OMCAL outperforms existing state-of-the-art methods in terms of clustering effectiveness and efficiency.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-60] DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images <mark class="hl-label red">ICCV2025</mark></p>
<p>【速读】：该论文旨在解决手绘动漫线稿自动上色过程中因遮挡、姿态变化和视角差异导致的准确性不足问题。现有基于深度学习的方法虽提升了性能，但在复杂场景下仍存在局限性。其解决方案的关键在于提出DACoN框架，该框架融合了基础模型（foundation models）提取的低分辨率语义特征与卷积神经网络（CNN）提供的高分辨率空间特征，从而实现细粒度且鲁棒的特征表示；同时，DACoN摒弃了以往方法对参考图像数量的限制（如Multiplex Transformer仅支持一至两张参考图），可灵活使用任意数量的参考图像，显著提升了颜色迁移的多样性和准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14685">https://arxiv.org/abs/2509.14685</a><br>
<strong>作者</strong>: Kazuma Nagata,Naoshi Kaneko<br>
<strong>机构</strong>: Tokyo Denki University (东京电气大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted to ICCV 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-61] Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model <mark class="hl-label red">ICONIP2025</mark></p>
<p>【速读】：该论文旨在解决视觉基础模型中生成视觉解释（visual explanations）的可适配性与解释力不足的问题，尤其是在复杂模型中难以有效应用现有方法的局限。其核心解决方案是提出一种结合参数更新机制的新型解释生成框架，关键创新在于引入两个机制：Attention Lattice Adapter (ALA) 和 Alternating Epoch Architect (AEA)。ALA 通过自动化的注意力结构适配，无需人工指定层即可提升模型的灵活性和可解释性；AEA 则通过每隔一个训练周期交替更新 ALA 参数，有效缓解了注意力区域过小导致的解释质量下降问题，从而在 CUB-200-2011 和 ImageNet-S 数据集上显著优于基线方法，在平均交并比（mean IoU）等指标上取得实质性提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14664">https://arxiv.org/abs/2509.14664</a><br>
<strong>作者</strong>: Shinnosuke Hirano,Yuiga Wada,Tsumugi Iida,Komei Sugiura<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted for presentation at ICONIP2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this study, we consider the problem of generating visual explanations in visual foundation models. Numerous methods have been proposed for this purpose; however, they often cannot be applied to complex models due to their lack of adaptability. To overcome these limitations, we propose a novel explanation generation method in visual foundation models that is aimed at both generating explanations and partially updating model parameters to enhance interpretability. Our approach introduces two novel mechanisms: Attention Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism simplifies the process by eliminating the need for manual layer selection, thus enhancing the model’s adaptability and interpretability. Moreover, the AEA mechanism, which updates ALA’s parameters every other epoch, effectively addresses the common issue of overly small attention regions. We evaluated our method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results showed that our method outperformed the baseline methods in terms of mean intersection over union (IoU), insertion score, deletion score, and insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets. Notably, our best model achieved a 53.2-point improvement in mean IoU on the CUB-200-2011 dataset compared with the baselines.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-62] MultiEdit: Advancing Instruction-based Image Editing on Diverse and Challenging Tasks</p>
<p>【速读】：该论文旨在解决当前基于指令的图像编辑（Instruction-Based Image Editing, IBIE）方法在处理复杂编辑任务时性能受限的问题，主要瓶颈在于现有数据集的编辑类型和样本数量不足，且传统数据构建方式常包含噪声图像-文本对，导致模型在复杂场景下出现偏差并限制其能力。解决方案的关键在于提出一个名为MultiEdit的综合性数据集，包含超过10.7万条高质量图像编辑样本，涵盖18种非风格迁移编辑类型与38种风格迁移操作，覆盖从复杂语义编辑（如人物参考编辑和图像内文本编辑）到高级风格迁移的广泛任务。其核心创新在于采用一种新颖的数据集构建流程，利用两个多模态大语言模型（Multimodal Large Language Models, MLLMs）分别生成视觉自适应的编辑指令和高保真编辑图像，从而显著提升数据质量与多样性，实验表明基于该数据集微调的基础开源模型在MultiEdit-Test基准上表现显著优于基线，同时保持标准编辑基准上的性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14638">https://arxiv.org/abs/2509.14638</a><br>
<strong>作者</strong>: Mingsong Li,Lin Liu,Hongjun Wang,Haoxing Chen,Xijun Gu,Shizhan Liu,Dong Gong,Junbo Zhao,Zhenzhong Lan,Jianguo Li<br>
<strong>机构</strong>: Inclusion AI; University of New South Wales; The University of Hong Kong; Zhejiang University; Westlake University<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Current instruction-based image editing (IBIE) methods struggle with challenging editing tasks, as both editing types and sample counts of existing datasets are limited. Moreover, traditional dataset construction often contains noisy image-caption pairs, which may introduce biases and limit model capabilities in complex editing scenarios. To address these limitations, we introduce MultiEdit, a comprehensive dataset featuring over 107K high-quality image editing samples. It encompasses 6 challenging editing tasks through a diverse collection of 18 non-style-transfer editing types and 38 style transfer operations, covering a spectrum from sophisticated style transfer to complex semantic operations like person reference editing and in-image text editing. We employ a novel dataset construction pipeline that utilizes two multi-modal large language models (MLLMs) to generate visual-adaptive editing instructions and produce high-fidelity edited images, respectively. Extensive experiments demonstrate that fine-tuning foundational open-source models with our MultiEdit-Train set substantially improves models’ performance on sophisticated editing tasks in our proposed MultiEdit-Test benchmark, while effectively preserving their capabilities on the standard editing benchmark. We believe MultiEdit provides a valuable resource for advancing research into more diverse and challenging IBIE capabilities. Our dataset is available at this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-63] LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition <mark class="hl-label red">ICASSP</mark></p>
<p>【速读】：该论文旨在解决基于骨骼的动作识别（Skeleton-based Action Recognition）中长期存在的两个问题：标注训练样本稀缺以及难以建模短时和长时时间依赖关系。其解决方案的关键在于提出一个统一框架LSTC-MDA，其中包含两个核心创新：一是设计了一种新型的长短时并行卷积模块（Long-Short Term Temporal Convolution, LSTC），通过自适应加权对齐与融合短时和长时特征分支，有效保留传统步长为2的时间卷积所丢失的关键长程时序信息；二是扩展了关节混合数据增强方法（Joint Mixing Data Augmentation, JMDA），引入输入层的Additive Mixup策略，在同一摄像头视角内进行混合操作，从而提升训练样本多样性并避免跨视角混叠导致的分布偏移。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14619">https://arxiv.org/abs/2509.14619</a><br>
<strong>作者</strong>: Feng Ding,Haisheng Fu,Soroush Oraki,Jie Liang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Submitted to ICASSP</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Skeleton-based action recognition faces two longstanding challenges: the scarcity of labeled training samples and difficulty modeling short- and long-range temporal dependencies. To address these issues, we propose a unified framework, LSTC-MDA, which simultaneously improves temporal modeling and data diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC) module with parallel short- and long-term branches, these two feature branches are then aligned and fused adaptively using learned similarity weights to preserve critical long-range cues lost by conventional stride-2 temporal convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an Additive Mixup at the input level, diversifying training samples and restricting mixup operations to the same camera view to avoid distribution shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4% and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code: this https URL.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-64] Enhancing Feature Fusion of U-like Networks with Dynamic Skip Connections</p>
<p>【速读】：该论文旨在解决传统U型网络（U-like networks）中跳接连接（skip connections）存在的两个关键问题：<strong>跨特征约束（inter-feature constraint）</strong> 和 <strong>内特征约束（intra-feature constraint）</strong>。前者指传统跳接连接在特征融合时路径固定，无法根据特征内容动态调整；后者源于对多尺度特征交互建模不足，限制了全局上下文信息的有效聚合。解决方案的核心在于提出一种新型的<strong>动态跳接连接（Dynamic Skip Connection, DSC）模块</strong>，其包含两个互补组件：(1) <strong>测试时训练（Test-Time Training, TTT）模块</strong>，通过推理阶段动态调整隐藏表示以实现内容感知的特征优化，缓解跨特征约束；(2) <strong>动态多尺度核（Dynamic Multi-Scale Kernel, DMSK）模块</strong>，依据全局上下文线索自适应选择卷积核大小，增强多尺度特征整合能力，从而克服内特征约束。DSC模块具有架构无关性，可无缝集成至各类U型网络结构中，并在CNN、Transformer、混合架构及Mamba-based模型上均展现出显著性能提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14610">https://arxiv.org/abs/2509.14610</a><br>
<strong>作者</strong>: Yue Cao,Quansong He,Kaishen Wang,Jianlong Xiong,Tao He<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:U-like networks have become fundamental frameworks in medical image segmentation through skip connections that bridge high-level semantics and low-level spatial details. Despite their success, conventional skip connections exhibit two key limitations: inter-feature constraints and intra-feature constraints. The inter-feature constraint refers to the static nature of feature fusion in traditional skip connections, where information is transmitted along fixed pathways regardless of feature content. The intra-feature constraint arises from the insufficient modeling of multi-scale feature interactions, thereby hindering the effective aggregation of global contextual information. To overcome these limitations, we propose a novel Dynamic Skip Connection (DSC) block that fundamentally enhances cross-layer connectivity through adaptive mechanisms. The DSC block integrates two complementary components. (1) Test-Time Training (TTT) module. This module addresses the inter-feature constraint by enabling dynamic adaptation of hidden representations during inference, facilitating content-aware feature refinement. (2) Dynamic Multi-Scale Kernel (DMSK) module. To mitigate the intra-feature constraint, this module adaptively selects kernel sizes based on global contextual cues, enhancing the network capacity for multi-scale feature integration. The DSC block is architecture-agnostic and can be seamlessly incorporated into existing U-like network structures. Extensive experiments demonstrate the plug-and-play effectiveness of the proposed DSC block across CNN-based, Transformer-based, hybrid CNN-Transformer, and Mamba-based U-like networks.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-65] HybridMamba: A Dual-domain Mamba for 3D Medical Image Segmentation</p>
<p>【速读】：该论文旨在解决3D生物医学图像分割中现有方法在建模长程依赖关系与计算效率之间的权衡问题，尤其是传统卷积神经网络（Convolutional Neural Networks, CNNs）难以捕捉远距离上下文信息，而基于Transformer的框架虽能建模全局信息但存在高计算开销的问题。此外，过度强调全局上下文可能忽略局部结构细节，导致分割边界模糊和区域失真。解决方案的关键在于提出HybridMamba架构，其核心创新包括：1）特征扫描策略（feature scanning strategy），通过轴向遍历与局部自适应路径的逐步融合，协调局部与全局表征的关系；2）门控模块结合空域-频域分析（spatial-frequency analysis），实现更全面的上下文建模能力。该方法在多中心肺部CT和MRI数据集上显著优于当前最优方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14609">https://arxiv.org/abs/2509.14609</a><br>
<strong>作者</strong>: Weitong Wu,Zhaohu Xing,Jing Gong,Qin Peng,Lei Zhu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In the domain of 3D biomedical image segmentation, Mamba exhibits the superior performance for it addresses the limitations in modeling long-range dependencies inherent to CNNs and mitigates the abundant computational overhead associated with Transformer-based frameworks when processing high-resolution medical volumes. However, attaching undue importance to global context modeling may inadvertently compromise critical local structural information, thus leading to boundary ambiguity and regional distortion in segmentation outputs. Therefore, we propose the HybridMamba, an architecture employing dual complementary mechanisms: 1) a feature scanning strategy that progressively integrates representations both axial-traversal and local-adaptive pathways to harmonize the relationship between local and global representations, and 2) a gated module combining spatial-frequency analysis for comprehensive contextual modeling. Besides, we collect a multi-center CT dataset related to lung cancer. Experiments on MRI and CT datasets demonstrate that HybridMamba significantly outperforms the state-of-the-art methods in 3D medical image segmentation.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-66] Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression</p>
<p>【速读】：该论文旨在解决动态点云（Dynamic Point Clouds）压缩中因点云结构不规则和局部变化显著而导致的运动估计与补偿精度不足的问题。现有方法依赖显式的运动向量编码，难以捕捉复杂的时序动态并充分利用时间相关性。其解决方案的关键在于提出特征对齐运动变换（Feature-aligned Motion Transformation, FMT）框架，该框架以时空对齐策略替代显式运动向量，通过潜在空间条件编码中的对齐特征作为时序上下文，隐式建模连续的时间变化；同时设计随机访问（Random Access, RA）参考机制，支持双向运动引用与分层编码，实现帧级并行压缩，从而在压缩效率和处理性能上均取得显著提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14591">https://arxiv.org/abs/2509.14591</a><br>
<strong>作者</strong>: Xuan Deng,Xiandong Meng,Longguang Wang,Tiange Zhang,Xiaopeng Fan,Debin Zhao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  9 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Dynamic point clouds are widely used in applications such as immersive reality, robotics, and autonomous driving. Efficient compression largely depends on accurate motion estimation and compensation, yet the irregular structure and significant local variations of point clouds make this task highly challenging. Current methods often rely on explicit motion estimation, whose encoded vectors struggle to capture intricate dynamics and fail to fully exploit temporal correlations. To overcome these limitations, we introduce a Feature-aligned Motion Transformation (FMT) framework for dynamic point cloud compression. FMT replaces explicit motion vectors with a spatiotemporal alignment strategy that implicitly models continuous temporal variations, using aligned features as temporal context within a latent-space conditional encoding framework. Furthermore, we design a random access (RA) reference strategy that enables bidirectional motion referencing and layered encoding, thereby supporting frame-level parallel compression. Extensive experiments demonstrate that our method surpasses D-DPCC and AdaDPCC in both encoding and decoding efficiency, while also achieving BD-Rate reductions of 20% and 9.4%, respectively. These results highlight the effectiveness of FMT in jointly improving compression efficiency and processing performance.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-67] Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark</p>
<p>【速读】：该论文旨在解决如何通过视觉-语言模型（Vision-Language Models, VLMs）有效理解人类对城市场景的感知，从而为城市设计与规划提供数据驱动的支持。其核心问题是评估VLMs在城市感知任务中的表现，特别是区分客观物理属性与主观印象的建模能力。解决方案的关键在于构建了一个包含100张蒙特利尔街景图像（含真实照片与合成图像各半）的小型基准测试集，结合来自7个社区群体的230份多维度标注（涵盖30个物理属性与主观感知维度），并采用零样本（zero-shot）设置下结构化提示（structured prompt）和确定性解析器进行模型评估，以准确衡量模型在单选题（使用准确率）和多标签题（使用Jaccard重叠）上的性能，同时引入Krippendorff’s alpha和成对Jaccard系数量化人类标注一致性，从而揭示模型表现与人类共识之间的关联。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14574">https://arxiv.org/abs/2509.14574</a><br>
<strong>作者</strong>: Rashid Mushkani<br>
<strong>机构</strong>: Université de Montréal (蒙特利尔大学); Mila – Quebec AI Institute (魁北克人工智能研究所)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Understanding how people read city scenes can inform design and planning. We introduce a small benchmark for testing vision-language models (VLMs) on urban perception using 100 Montreal street images, evenly split between photographs and photorealistic synthetic scenes. Twelve participants from seven community groups supplied 230 annotation forms across 30 dimensions mixing physical attributes and subjective impressions. French responses were normalized to English. We evaluated seven VLMs in a zero-shot setup with a structured prompt and deterministic parser. We use accuracy for single-choice items and Jaccard overlap for multi-label items; human agreement uses Krippendorff’s alpha and pairwise Jaccard. Results suggest stronger model alignment on visible, objective properties than subjective appraisals. The top system (claude-sonnet) reaches macro 0.31 and mean Jaccard 0.48 on multi-label items. Higher human agreement coincides with better model scores. Synthetic images slightly lower scores. We release the benchmark, prompts, and harness for reproducible, uncertainty-aware evaluation in participatory urban analysis.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-68] Domain Adaptation for Ulcerative Colitis Severity Estimation Using Patient-Level Diagnoses <mark class="hl-label red">MICCAI</mark></p>
<p>【速读】：该论文旨在解决溃疡性结肠炎（Ulcerative Colitis, UC）严重程度评估方法在跨医院场景中因成像设备和临床环境差异导致的域偏移（domain shift）问题。现有领域自适应（Domain Adaptation, DA）方法通常受限于目标域缺乏标注或标注成本过高，难以有效迁移知识。其解决方案的关键在于提出一种弱监督域自适应方法，利用患者层面的诊断结果作为弱监督信号——这些结果是UC诊疗中常规记录的信息，且由每位患者的最严重区域决定，从而构建基于类别分布对齐的Shared Aggregation Tokens与Max-Severity Triplet Loss机制，实现跨域的高效、鲁棒的UC严重程度估计。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14573">https://arxiv.org/abs/2509.14573</a><br>
<strong>作者</strong>: Takamasa Yamaguchi,Brian Kenji Iwana,Ryoma Bise,Shota Harada,Takumi Okuo,Kiyohito Tanaka,Kaito Shiku<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  Accepted to MICCAI workshop 2025 (International conference on machine learning in medical imaging)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The development of methods to estimate the severity of Ulcerative Colitis (UC) is of significant importance. However, these methods often suffer from domain shifts caused by differences in imaging devices and clinical settings across hospitals. Although several domain adaptation methods have been proposed to address domain shift, they still struggle with the lack of supervision in the target domain or the high cost of annotation. To overcome these challenges, we propose a novel Weakly Supervised Domain Adaptation method that leverages patient-level diagnostic results, which are routinely recorded in UC diagnosis, as weak supervision in the target domain. The proposed method aligns class-wise distributions across domains using Shared Aggregation Tokens and a Max-Severity Triplet Loss, which leverages the characteristic that patient-level diagnoses are determined by the most severe region within each patient. Experimental results demonstrate that our method outperforms comparative DA approaches, improving UC severity estimation in a domain-shifted setting.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-69] DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</p>
<p>【速读】：该论文旨在解决稀疏视角计算机断层成像（Sparse-view Computed Tomography, CT）重建中因欠采样导致的病态逆问题，传统迭代方法依赖手工设计或学习得到的先验信息难以有效捕捉医学图像中的复杂结构。其解决方案的关键在于提出Diffusion Consensus Equilibrium (DICE)框架，该框架在扩散模型（Diffusion Models, DMs）的采样过程中引入双代理共识平衡机制：一是数据一致性代理（通过近似算子强制满足测量约束），二是先验代理（由DM在每一步采样中估计干净图像）。通过交替优化这两个互补代理，DICE实现了生成先验能力与测量一致性的有效融合，在均匀和非均匀稀疏视角（15、30、60视图，共180视图）下均显著优于现有最先进方法，展现出优异的重建质量和鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14566">https://arxiv.org/abs/2509.14566</a><br>
<strong>作者</strong>: Leon Suarez-Rodriguez,Roman Jacome,Romario Gualdron-Hurtado,Ana Mantilla-Dulcey,Henry Arguello<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  8 pages, 4 figures, confenrence</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-70] DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising</p>
<p>【速读】：该论文旨在解决自动驾驶中视觉定位的准确性与可扩展性之间的矛盾问题：高精度的高清（HD）地图虽能提供可靠定位参考，但其高昂的构建和维护成本限制了大规模应用；而广泛可用的标准定义（SD）地图（如OpenStreetMap）因缺乏精确地理信息，导致现有基于SD地图的方法难以实现高精度定位。解决方案的关键在于提出DiffVL框架，首次将视觉定位重构为GPS去噪任务，利用扩散模型（diffusion models）从受干扰的GPS轨迹中恢复真实位姿分布——通过联合建模GPS信号、SD地图和视觉BEV特征，使噪声GPS在条件约束下隐含真实姿态信息，从而实现无需HD地图的亚米级定位精度。这一方法标志着从传统匹配驱动范式向生成先验驱动范式的转变。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14565">https://arxiv.org/abs/2509.14565</a><br>
<strong>作者</strong>: Li Gao,Hongyang Sun,Liu Liu,Yunhao Li,Yang Cai<br>
<strong>机构</strong>: Alibaba Amap (阿里巴巴高德)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird’s-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-71] Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model</p>
<p>【速读】：该论文旨在解决点云去噪任务中如何高效处理不同噪声水平或模式的问题，尤其针对现有基于深度神经网络的迭代去噪方法在重复执行时缺乏自适应性、难以优化步骤安排的局限性。其解决方案的关键在于提出一种基于得分驱动（score-based）扩散模型的自适应迭代去噪方法：首先估计输入点云的噪声强度以生成自适应的去噪调度（denoising schedule），随后依据该调度使用训练好的网络迭代更新点云位置；同时设计了支持特征融合与梯度融合的网络架构及两阶段采样策略，从而提升去噪过程的稳定性和细节保留能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14560">https://arxiv.org/abs/2509.14560</a><br>
<strong>作者</strong>: Zhaonan Wang,Manyi Li,ShiQing Xin,Changhe Tu<br>
<strong>机构</strong>: Shandong University (山东大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Point cloud denoising task aims to recover the clean point cloud from the scanned data coupled with different levels or patterns of noise. The recent state-of-the-art methods often train deep neural networks to update the point locations towards the clean point cloud, and empirically repeat the denoising process several times in order to obtain the denoised results. It is not clear how to efficiently arrange the iterative denoising processes to deal with different levels or patterns of noise. In this paper, we propose an adaptive and iterative point cloud denoising method based on the score-based diffusion model. For a given noisy point cloud, we first estimate the noise variation and determine an adaptive denoising schedule with appropriate step sizes, then invoke the trained network iteratively to update point clouds following the adaptive schedule. To facilitate this adaptive and iterative denoising process, we design the network architecture and a two-stage sampling strategy for the network training to enable feature fusion and gradient fusion for iterative denoising. Compared to the state-of-the-art point cloud denoising methods, our approach obtains clean and smooth denoised point clouds, while preserving the shape boundary and details better. Our results not only outperform the other methods both qualitatively and quantitatively, but also are preferable on the synthetic dataset with different patterns of noises, as well as the real-scanned dataset.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-72] Edge-Aware Normalized Attention for Efficient and Detail-Preserving Single Image Super-Resolution</p>
<p>【速读】：该论文旨在解决单图像超分辨率（Single-image Super-Resolution, SISR）任务中因从单一低分辨率观测恢复结构忠实的高频内容而带来的病态性问题，尤其针对现有边缘感知方法在复杂主干网络中引入冗余融合、优化不稳定或结构增益有限的局限。其解决方案的关键在于提出一种边缘引导的注意力机制（edge-guided attention mechanism），该机制通过联合编码的边缘特征与中间特征激活生成自适应调制图（adaptive modulation map），并将其用于归一化和重加权响应，从而选择性增强结构显著区域并抑制伪纹理；同时，该机制被集成到轻量级残差架构中，并采用像素级、感知级与对抗级复合目标函数进行训练，以平衡保真度、感知真实性和训练稳定性。此方法实现了在模型复杂度相当的情况下，显著提升结构锐度和感知质量。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14550">https://arxiv.org/abs/2509.14550</a><br>
<strong>作者</strong>: Penghao Rao,Tieyong Zeng<br>
<strong>机构</strong>: The Chinese University of Hong Kong (香港中文大学)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  13 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Single-image super-resolution (SISR) remains highly ill-posed because recovering structurally faithful high-frequency content from a single low-resolution observation is ambiguous. Existing edge-aware methods often attach edge priors or attention branches onto increasingly complex backbones, yet ad hoc fusion frequently introduces redundancy, unstable optimization, or limited structural gains. We address this gap with an edge-guided attention mechanism that derives an adaptive modulation map from jointly encoded edge features and intermediate feature activations, then applies it to normalize and reweight responses, selectively amplifying structurally salient regions while suppressing spurious textures. In parallel, we integrate this mechanism into a lightweight residual design trained under a composite objective combining pixel-wise, perceptual, and adversarial terms to balance fidelity, perceptual realism, and training stability. Extensive experiments on standard SISR benchmarks demonstrate consistent improvements in structural sharpness and perceptual quality over SRGAN, ESRGAN, and prior edge-attention baselines at comparable model complexity. The proposed formulation provides (i) a parameter-efficient path to inject edge priors, (ii) stabilized adversarial refinement through a tailored multiterm loss, and (iii) enhanced edge fidelity without resorting to deeper or heavily overparameterized architectures. These results highlight the effectiveness of principled edge-conditioned modulation for advancing perceptual super-resolution.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-73] MemEvo: Memory-Evolving Incremental Multi-view Clustering</p>
<p>【速读】：该论文旨在解决增量多视角聚类（incremental multi-view clustering）中面临的稳定性-可塑性困境（stability-plasticity dilemma, SPD）问题，即模型需在快速适应新视图数据的同时，保持对历史知识的稳定记忆以避免灾难性遗忘。解决方案的关键在于受神经科学中海马体-前额叶皮层协同记忆机制启发，提出了一种Memory-Evolving Incremental Multi-view Clustering方法（MemEvo）：其核心包括三个模块——基于海马体机制的视图对齐模块用于捕捉新视图结构信息，模拟人类记忆衰减模式的认知遗忘机制用于调节历史知识权重，以及基于前额叶皮层机制的知识巩固记忆模块，利用时间张量稳定性逐步固化历史知识。通过三者协同，MemEvo实现了在不断增长视图场景下的强知识保留能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14544">https://arxiv.org/abs/2509.14544</a><br>
<strong>作者</strong>: Zisen Kong,Bo Zhong,Pengyuan Li,Dongxia Chang,Yiming Wang<br>
<strong>机构</strong>: 1. University of Science and Technology of China (中国科学技术大学); 2. Institute of Artificial Intelligence, University of Science and Technology of China (中国科学技术大学人工智能研究院); 3. Alibaba Cloud (阿里云)<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Incremental multi-view clustering aims to achieve stable clustering results while addressing the stability-plasticity dilemma (SPD) in incremental views. At the core of SPD is the challenge that the model must have enough plasticity to quickly adapt to new data, while maintaining sufficient stability to consolidate long-term knowledge and prevent catastrophic forgetting. Inspired by the hippocampal-prefrontal cortex collaborative memory mechanism in neuroscience, we propose a Memory-Evolving Incremental Multi-view Clustering method (MemEvo) to achieve this balance. First, we propose a hippocampus-inspired view alignment module that captures the gain information of new views by aligning structures in continuous representations. Second, we introduce a cognitive forgetting mechanism that simulates the decay patterns of human memory to modulate the weights of historical knowledge. Additionally, we design a prefrontal cortex-inspired knowledge consolidation memory module that leverages temporal tensor stability to gradually consolidate historical knowledge. By integrating these modules, MemEvo achieves strong knowledge retention capabilities in scenarios with a growing number of views. Extensive experiments demonstrate that MemEvo exhibits remarkable advantages over existing state-of-the-art methods.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-74] AToken: A Unified Tokenizer for Vision</p>
<p>【速读】：该论文旨在解决当前视觉 tokenization 方法在跨模态（图像、视频、3D 资产）中难以同时实现高保真重建与语义理解的问题。现有方法通常仅专注于单一模态的重建或理解任务，缺乏统一框架以协同优化两者。其解决方案的关键在于提出 AToken，一种首个统一的视觉 tokenizer，通过将不同模态输入编码至共享的 4D 潜在空间（latent space），实现跨模态和跨任务的统一建模。该方案采用纯 Transformer 架构并引入 4D 旋转位置编码（4D rotary position embeddings）以处理任意分辨率和时序长度的输入；同时设计无对抗训练目标（adversarial-free training objective），融合感知损失与 Gram 矩阵损失以稳定训练并提升重建质量，并结合渐进式训练策略逐步扩展至多模态数据，支持连续与离散潜变量表示。此架构在多个基准上均取得领先性能，为下一代多模态 AI 系统提供了统一的视觉表征基础。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14476">https://arxiv.org/abs/2509.14476</a><br>
<strong>作者</strong>: Jiasen Lu,Liangchen Song,Mingze Xu,Byeongjoo Ahn,Yanjun Wang,Chen Chen,Afshin Dehghan,Yinfei Yang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>)<br>
<strong>备注</strong>:  30 pages, 14 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-75] Class-invariant Test-Time Augmentation for Domain Generalization</p>
<p>【速读】：该论文旨在解决深度模型在分布偏移（distribution shift）下性能显著下降的问题，尤其是在未见域（unseen domains）上的泛化能力不足。传统领域泛化（Domain Generalization, DG）方法通常依赖多域训练或计算密集的测试时自适应（test-time adaptation），而本文提出了一种轻量级的测试时增强策略——类不变测试时增强（Class-Invariant Test-Time Augmentation, CI-TTA）。其关键在于通过弹性变形和网格变形生成与原始输入同属一类的多个图像变体，并利用置信度引导的过滤机制剔除不可靠预测，从而聚合出更一致且可信的最终决策，实现无需额外训练即可提升模型鲁棒性与泛化性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14420">https://arxiv.org/abs/2509.14420</a><br>
<strong>作者</strong>: Zhicheng Lin,Xiaolin Wu,Xi Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep models often suffer significant performance degradation under distribution shifts. Domain generalization (DG) seeks to mitigate this challenge by enabling models to generalize to unseen domains. Most prior approaches rely on multi-domain training or computationally intensive test-time adaptation. In contrast, we propose a complementary strategy: lightweight test-time augmentation. Specifically, we develop a novel Class-Invariant Test-Time Augmentation (CI-TTA) technique. The idea is to generate multiple variants of each input image through elastic and grid deformations that nevertheless belong to the same class as the original input. Their predictions are aggregated through a confidence-guided filtering scheme that remove unreliable outputs, ensuring the final decision relies on consistent and trustworthy cues. Extensive Experiments on PACS and Office-Home datasets demonstrate consistent gains across different DG algorithms and backbones, highlighting the effectiveness and generality of our approach.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-76] RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings <mark class="hl-label red">ICRA</mark></p>
<p>【速读】：该论文旨在解决统一多模态编码器（Unified multi-modal encoder）在机器人感知与决策中因视觉分支暴露于对抗性攻击和自然退化而导致的鲁棒性不足问题，这是保障机器人安全部署的前提。解决方案的关键在于提出RLBind框架，其包含两个阶段：第一阶段通过无监督微调清洁样本与对抗样本对来增强视觉编码器的抗干扰能力；第二阶段利用跨模态对应关系，最小化清洁/对抗特征与文本锚点之间的差异，并强制不同模态间类别级分布对齐，从而在不牺牲零样本迁移性能的前提下显著提升嵌入空间的鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14383">https://arxiv.org/abs/2509.14383</a><br>
<strong>作者</strong>: Yuhong Lu<br>
<strong>机构</strong>: UCLA(加州大学洛杉矶分校); Samueli School of Engineering (工程学院); Electrical and Computer Engineering (电气与计算机工程系)<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:  This paper is submitted to IEEE International Conference on Robotics and Automation (ICRA) 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Unified multi-modal encoders that bind vision, audio, and other sensors into a shared embedding space are attractive building blocks for robot perception and decision-making. However, on-robot deployment exposes the vision branch to adversarial and natural corruptions, making robustness a prerequisite for safety. Prior defenses typically align clean and adversarial features within CLIP-style encoders and overlook broader cross-modal correspondence, yielding modest gains and often degrading zero-shot transfer. We introduce RLBind, a two-stage adversarial-invariant cross-modal alignment framework for robust unified embeddings. Stage 1 performs unsupervised fine-tuning on clean-adversarial pairs to harden the visual encoder. Stage 2 leverages cross-modal correspondence by minimizing the discrepancy between clean/adversarial features and a text anchor, while enforcing class-wise distributional alignment across modalities. Extensive experiments on Image, Audio, Thermal, and Video data show that RLBind consistently outperforms the LanguageBind backbone and standard fine-tuning baselines in both clean accuracy and norm-bounded adversarial robustness. By improving resilience without sacrificing generalization, RLBind provides a practical path toward safer multi-sensor perception stacks for embodied robots in navigation, manipulation, and other autonomy settings.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-77] Doppler Radiance Field-Guided Antenna Selection for Improved Generalization in Multi-Antenna Wi-Fi-based Human Activity Recognition</p>
<p>【速读】：该论文旨在解决Wi-Fi Channel State Information (CSI)在人体活动识别（HAR）应用中因异步接入点（AP）时钟和环境/硬件噪声导致的信号失真问题，这些问题会显著影响多普勒速度投影的准确性，进而限制模型的泛化能力。其解决方案的关键在于提出一种基于多天线接入点（AP）的新型框架，通过分析多普勒辐射场（DoRFs）拟合误差来抑制噪声并识别最具信息量的天线，从而提升HAR系统的鲁棒性和性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15129">https://arxiv.org/abs/2509.15129</a><br>
<strong>作者</strong>: Navid Hasanzadeh,Shahrokh Valaee<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ignal Processing (eess.SP); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:With the IEEE 802.11bf Task Group introducing amendments to the WLAN standard for advanced sensing, interest in using Wi-Fi Channel State Information (CSI) for remote sensing has surged. Recent findings indicate that learning a unified three-dimensional motion representation through Doppler Radiance Fields (DoRFs) derived from CSI significantly improves the generalization capabilities of Wi-Fi-based human activity recognition (HAR). Despite this progress, CSI signals remain affected by asynchronous access point (AP) clocks and additive noise from environmental and hardware sources. Consequently, even with existing preprocessing techniques, both the CSI data and Doppler velocity projections used in DoRFs are still susceptible to noise and outliers, limiting HAR performance. To address this challenge, we propose a novel framework for multi-antenna APs to suppress noise and identify the most informative antennas based on DoRF fitting errors, which capture inconsistencies among Doppler velocity projections. Experimental results on a challenging small-scale hand gesture recognition dataset demonstrate that the proposed DoRF-guided Wi-Fi-based HAR approach significantly improves generalization capability, paving the way for robust real-world sensing deployments.<br>
zh</p>
</div></div>
<div class="note purple no-icon flat"><p>[CV-78] Learning Mechanistic Subtypes of Neurodegeneration with a Physics-Informed Variational Autoencoder Mixture Model <mark class="hl-label red">MICCAI2025</mark></p>
<p>【速读】：该论文旨在解决神经退行性疾病建模中因机制异质性和空间动态变化导致的挑战，尤其是在稀疏、高维神经影像数据下难以准确刻画多机制共存问题。传统基于单一偏微分方程（PDE）的物理融合机器学习方法受限于模型结构单一，无法有效识别不同亚型的病理机制，易引发模型误设和退化问题。其解决方案的关键在于提出一种深度生成模型，通过将反应-扩散型PDE嵌入变分自编码器（VAE）混合模型框架中，实现对由多个物理驱动的潜在动态模型的联合学习，从而支持从神经影像数据中推断出具有生物学解释性的亚型潜变量（如扩散系数和反应速率），并在合成基准和正电子发射断层扫描（PET）数据上验证了其在阿尔茨海默病进展机制亚型识别中的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15124">https://arxiv.org/abs/2509.15124</a><br>
<strong>作者</strong>: Sanduni Pinnawala,Annabelle Hartanto,Ivor J. A. Simpson,Peter A. Wijeratne<br>
<strong>机构</strong>: Sussex AI Centre (萨塞克斯人工智能中心); School of Engineering and Informatics (工程与信息学院); University of Sussex (萨塞克斯大学); Sussex Neuroscience (萨塞克斯神经科学); School of Life Sciences (生命科学学院)<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (<a target="_blank" rel="noopener" href="http://cs.CV">cs.CV</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  13 pages, 5 figures, accepted at SASHIMI workshop, MICCAI 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Modelling the underlying mechanisms of neurodegenerative diseases demands methods that capture heterogeneous and spatially varying dynamics from sparse, high-dimensional neuroimaging data. Integrating partial differential equation (PDE) based physics knowledge with machine learning provides enhanced interpretability and utility over classic numerical methods. However, current physics-integrated machine learning methods are limited to considering a single PDE, severely limiting their application to diseases where multiple mechanisms are responsible for different groups (i.e., subtypes) and aggravating problems with model misspecification and degeneracy. Here, we present a deep generative model for learning mixtures of latent dynamic models governed by physics-based PDEs, going beyond traditional approaches that assume a single PDE structure. Our method integrates reaction-diffusion PDEs within a variational autoencoder (VAE) mixture model framework, supporting inference of subtypes of interpretable latent variables (e.g. diffusivity and reaction rates) from neuroimaging data. We evaluate our method on synthetic benchmarks and demonstrate its potential for uncovering mechanistic subtypes of Alzheimer’s disease progression from positron emission tomography (PET) data.<br>
zh</p>
</div></div>
<h3 id="人工智能">人工智能</h3>
<div class="note blue no-icon flat"><p>[AI-0] Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generation</p>
<p>【速读】：该论文旨在解决现有基于神经隐式模型的声音模拟方法在生成房间脉冲响应（Room Impulse Response, RIR）时，未能有效利用环境显式几何信息的问题。当前方法虽能借助场景图像等上下文信息学习RIR，但缺乏对空间结构的直接建模能力，导致预测精度受限。解决方案的关键在于提出Mesh-infused Neural Acoustic Field (MiNAF)，其通过查询粗略的房间网格（room mesh）并提取特定位置的距离分布作为局部几何特征的显式表示，从而为神经网络提供更精确的空间引导，显著提升了RIR预测的准确性与鲁棒性，尤其在训练样本有限的情况下仍能保持高保真度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15210">https://arxiv.org/abs/2509.15210</a><br>
<strong>作者</strong>: Chen Si,Qianyi Wu,Chaitanya Amballa,Romit Roy Choudhury<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Realistic sound simulation plays a critical role in many applications. A key element in sound simulation is the room impulse response (RIR), which characterizes how sound propagates from a source to a listener within a given space. Recent studies have applied neural implicit methods to learn RIR using context information collected from the environment, such as scene images. However, these approaches do not effectively leverage explicit geometric information from the environment. To further exploit the potential of neural implicit models with direct geometric features, we present Mesh-infused Neural Acoustic Field (MiNAF), which queries a rough room mesh at given locations and extracts distance distributions as an explicit representation of local context. Our approach demonstrates that incorporating explicit local geometric features can better guide the neural network in generating more accurate RIR predictions. Through comparisons with conventional and state-of-the-art baseline methods, we show that MiNAF performs competitively across various evaluation metrics. Furthermore, we verify the robustness of MiNAF in datasets with limited training samples, demonstrating an advance in high-fidelity sound simulation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-1] Orion: Fuzzing Workflow Automation</p>
<p>【速读】：该论文旨在解决软件模糊测试（fuzz testing）流程中人工干预成本过高这一问题，尤其是在代码分析、测试桩（harness）配置和结果归类等环节，传统方法依赖大量手动操作，难以在复杂或大规模项目中高效实施。解决方案的关键在于提出 Orion 框架，通过将大语言模型（Large Language Models, LLMs）的语义推理能力与传统确定性工具相结合：LLMs 负责代码理解与指导性决策，而传统工具则保障验证准确性、迭代优化精度及任务执行的可靠性。这种混合架构显著降低了各阶段的人工投入（提升 46–204 倍），并在真实场景中成功发现两个未被识别的漏洞，验证了其有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15195">https://arxiv.org/abs/2509.15195</a><br>
<strong>作者</strong>: Max Bazalii,Marius Fleischer<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>)<br>
<strong>备注</strong>:  11 pages, 3 figures, 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Fuzz testing is one of the most effective techniques for finding software vulnerabilities. While modern fuzzers can generate inputs and monitor executions automatically, the overall workflow, from analyzing a codebase, to configuring harnesses, to triaging results, still requires substantial manual effort. Prior attempts focused on single stages such as harness synthesis or input minimization, leaving researchers to manually connect the pieces into a complete fuzzing campaign. We introduce Orion, a framework that automates the the manual bottlenecks of fuzzing by integrating LLM reasoning with traditional tools, allowing campaigns to scale to settings where human effort alone was impractical. Orion uses LLMs for code reasoning and semantic guidance, while relying on deterministic tools for verification, iterative refinement, and tasks that require precision. Across our benchmark suite, Orion reduces human effort by 46-204x depending on the workflow stage, and we demonstrate its effectiveness through the discovery of two previously unknown vulnerabilities in the widely used open-source clib library.          Comments: 11 pages, 3 figures, 3 tables   Subjects:  Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>)   ACMclasses: D.4.6; I.2.2; D.2.5   Cite as: arXiv:2509.15195 [<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>]    (or  arXiv:2509.15195v1 [<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15195">https://doi.org/10.48550/arXiv.2509.15195</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-2] Internalizing Self-Consistency in Language Models: Multi-<mark class="hl-label green">Agent</mark>  Consensus Alignment</p>
<p>【速读】：该论文旨在解决语言模型（Language Models, LMs）在推理过程中表现出的不一致性问题，即对相同输入提示生成矛盾答案的现象。其核心挑战在于模型在探索性采样下难以稳定选择能导向一致结果的推理路径。解决方案的关键是将“自一致性”（self-consistency）形式化为良好对齐推理模型的内在属性，并提出多智能体共识对齐（Multi-Agent Consensus Alignment, MACA）框架：通过强化学习后训练机制，使模型偏好那些在多智能体辩论中达成内部共识的推理轨迹——这些轨迹源于智能体间基于同行论证的深度协商，而非独立尝试的简单多数投票，从而形成更丰富的共识信号。此方法无需外部监督即可提升模型的决策确定性、简洁性和对同伴见解的利用效率，显著增强多种推理场景下的性能表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15172">https://arxiv.org/abs/2509.15172</a><br>
<strong>作者</strong>: Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Language Models (LMs) are inconsistent reasoners, often generating contradictory responses to identical prompts. While inference-time methods can mitigate these inconsistencies, they fail to address the core problem: LMs struggle to reliably select reasoning pathways leading to consistent outcomes under exploratory sampling. To address this, we formalize self-consistency as an intrinsic property of well-aligned reasoning models and introduce Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with their internal consensus using majority/minority outcomes from multi-agent debate. These trajectories emerge from deliberative exchanges where agents ground reasoning in peer arguments, not just aggregation of independent attempts, creating richer consensus signals than single-round majority voting. MACA enables agents to teach themselves to be more decisive and concise, and better leverage peer insights in multi-agent settings without external supervision, driving substantial improvements across self-consistency (+27.6% on GSM8K), single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4% Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA). These findings, coupled with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more reliably unlocks latent reasoning potential of language models.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-3] Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting <mark class="hl-label red">ICASSP</mark></p>
<p>【速读】：该论文旨在解决无线设备身份识别中基于射频指纹（Radio Frequency Fingerprint, RFFI）的认证系统在面对复制、篡改和规避攻击时安全性不足的问题。现有方法虽利用深度学习提升识别准确率，但模型易受对抗性干扰或伪造输入影响。其解决方案的关键在于构建一个融合水印技术与异常检测机制的增强型RFFI系统：首先通过ResNet-34在对数梅尔频谱图（log-Mel spectrogram）上嵌入三种水印（简单触发器、抗噪/滤波鲁棒的对抗训练触发器、隐藏梯度/权重签名），实现所有权证明；其次采用带Kullback-Leibler (KL)温启和自由比特（free-bits）机制的卷积变分自编码器（Convolutional Variational Autoencoder, VAE）检测分布外查询（out-of-distribution queries），从而有效识别恶意输入。实验表明，该方案在LoRa数据集上实现了94.6%的识别准确率、98%的水印成功率及0.94 AUROC，具备可验证且抗篡改的认证能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15170">https://arxiv.org/abs/2509.15170</a><br>
<strong>作者</strong>: Aarushi Mahajan,Wayne Burleson<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Signal Processing (eess.SP)<br>
<strong>备注</strong>:  IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Radio frequency fingerprint identification (RFFI) distinguishes wireless devices by the small variations in their analog circuits, avoiding heavy cryptographic authentication. While deep learning on spectrograms improves accuracy, models remain vulnerable to copying, tampering, and evasion. We present a stronger RFFI system combining watermarking for ownership proof and anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel spectrograms, we embed three watermarks: a simple trigger, an adversarially trained trigger robust to noise and filtering, and a hidden gradient/weight signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler (KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset, our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC, offering verifiable, tamper-resistant authentication.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-4] Exploring How Audio Effects Alter Emotion with Foundation Models</p>
<p>【速读】：该论文旨在解决音频效果（Audio Effects, FX）如混响、失真、调制和动态范围处理等对音乐聆听过程中情绪感知的系统性影响尚不明确的问题。解决方案的关键在于利用预训练于多模态数据的大规模基础模型（Foundation Models），通过对其嵌入表示（embeddings）应用多种探测方法，挖掘音频FX与估计情绪之间的复杂非线性关系，从而揭示特定音频效果与情绪响应之间的模式，并评估基础音频模型的鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15151">https://arxiv.org/abs/2509.15151</a><br>
<strong>作者</strong>: Stelios Katsis,Vassilis Lyberatos,Spyridon Kantarelis,Edmund Dervakos,Giorgos Stamou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Audio effects (FX) such as reverberation, distortion, modulation, and dynamic range processing play a pivotal role in shaping emotional responses during music listening. While prior studies have examined links between low-level audio features and affective perception, the systematic impact of audio FX on emotion remains underexplored. This work investigates how foundation models - large-scale neural architectures pretrained on multimodal data - can be leveraged to analyze these effects. Such models encode rich associations between musical structure, timbre, and affective meaning, offering a powerful framework for probing the emotional consequences of sound design techniques. By applying various probing methods to embeddings from deep learning models, we examine the complex, nonlinear relationships between audio FX and estimated emotion, uncovering patterns tied to specific effects and evaluating the robustness of foundation audio models. Our findings aim to advance understanding of the perceptual impact of audio production practices, with implications for music cognition, performance, and affective computing.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-5] he mechanization of science illustrated by the Lean formalization of the multi-graded Proj construction</p>
<p>【速读】：该论文旨在解决多级Proj构造（multi-graded Proj construction）在形式化数学中的精确建模与机械化证明问题，其核心挑战在于如何将代数几何中复杂的多 graded 环结构及其对应的射影概形（projective scheme）在定理证明器 Lean4 中进行严格的形式化。解决方案的关键在于通过 Lean4 的类型理论框架，系统地定义和实现多级Proj构造的各个步骤，包括多 graded 环的局部化、齐次理想与闭子集的对应关系，以及商空间的拓扑结构，从而为后续代数几何的自动化推理提供坚实的基础。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15116">https://arxiv.org/abs/2509.15116</a><br>
<strong>作者</strong>: Arnaud Mayeux,Jujian Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Logic in Computer Science (cs.LO); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Algebraic Geometry (<a target="_blank" rel="noopener" href="http://math.AG">math.AG</a>)<br>
<strong>备注</strong>:  Short note</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We formalize the multi-graded Proj construction in Lean4, illustrating mechanized mathematics and formalization.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-6] Vulnerable <mark class="hl-label green">Agent</mark>  Identification in Large-Scale Multi-<mark class="hl-label green">Agent</mark>  Reinforcement Learning <mark class="hl-label red">NIPS2025</mark></p>
<p>【速读】：该论文旨在解决大规模多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）系统中“脆弱智能体识别”（Vulnerable Agent Identification, VAI）问题，即在系统规模扩大时，如何识别出那些被攻破后会对整体性能造成最严重损害的一组智能体。其核心解决方案是将VAI建模为一个分层对抗去中心化平均场控制（Hierarchical Adversarial Decentralized Mean Field Control, HAD-MFC）问题，其中上层为NP难的组合优化任务（选择最脆弱智能体），下层通过平均场MARL学习这些智能体的最坏情况对抗策略。关键创新在于利用Fenchel-Rockafellar对偶变换对分层结构进行解耦，得到一个正则化的平均场贝尔曼算子，使上下层可独立学习并显著降低计算复杂度；进一步将上层组合问题转化为具有密集奖励的马尔可夫决策过程（MDP），从而通过贪心和强化学习算法逐次识别最脆弱智能体，且该分解方法保证了原HAD-MFC最优解的保真性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15103">https://arxiv.org/abs/2509.15103</a><br>
<strong>作者</strong>: Simin Li,Zheng Yuwei,Zihao Mao,Linhao Wang,Ruixiao Xu,Chengdong Ma,Xin Yu,Yuqing Ma,Qi Dou,Xin Wang,Jie Luo,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  submitted to NIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Partial agent failure becomes inevitable when systems scale up, making it crucial to identify the subset of agents whose compromise would most severely degrade overall performance. In this paper, we study this Vulnerable Agent Identification (VAI) problem in large-scale multi-agent reinforcement learning (MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC), where the upper level involves an NP-hard combinatorial task of selecting the most vulnerable agents, and the lower level learns worst-case adversarial policies for these agents using mean-field MARL. The two problems are coupled together, making HAD-MFC difficult to solve. To solve this, we first decouple the hierarchical process by Fenchel-Rockafellar transform, resulting a regularized mean-field Bellman operator for upper level that enables independent learning at each level, thus reducing computational complexity. We then reformulate the upper-level combinatorial problem as a MDP with dense rewards from our regularized mean-field Bellman operator, enabling us to sequentially identify the most vulnerable agents by greedy and RL algorithms. This decomposition provably preserves the optimal solution of the original HAD-MFC. Experiments show our method effectively identifies more vulnerable agents in large-scale MARL and the rule-based system, fooling system into worse failures, and learns a value function that reveals the vulnerability of each agent.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-7] From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support <mark class="hl-label red">ECML-PKDD</mark></p>
<p>【速读】：该论文试图解决在海上自主系统中，如何通过可解释人工智能（Explainable AI, XAI）提升人机协同信任度的问题。其核心挑战在于，尽管AI在复杂动态的海事环境中表现出强大性能，但若缺乏透明性和可解释性，人类操作者难以建立对AI决策的信任，从而影响有效协作。解决方案的关键在于提出一种面向海事领域的用户中心型调查工具，旨在捕捉航海人员对信任、可用性和可解释性的感知，从而指导开发符合海员实际需求的以人为本的XAI系统，确保人机之间具备知情监督与共享理解的基础。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15084">https://arxiv.org/abs/2509.15084</a><br>
<strong>作者</strong>: Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>); Human-Computer Interaction (cs.HC)<br>
<strong>备注</strong>:  Paper accepted at Human Learning and Decision-Making Workshop @ECML-PKDD Conference 2025, Porto, Portugal</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As autonomous technologies increasingly shape maritime operations, understanding why an AI system makes a decision becomes as crucial as what it decides. In complex and dynamic maritime environments, trust in AI depends not only on performance but also on transparency and interpretability. This paper highlights the importance of Explainable AI (XAI) as a foundation for effective human-machine teaming in the maritime domain, where informed oversight and shared understanding are essential. To support the user-centered integration of XAI, we propose a domain-specific survey designed to capture maritime professionals’ perceptions of trust, usability, and explainability. Our aim is to foster awareness and guide the development of user-centric XAI systems tailored to the needs of seafarers and maritime teams.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-8] Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning</p>
<p>【速读】：该论文旨在解决传统循环神经网络（Recurrent Neural Networks, RNNs）在模型参数冗余与性能优化之间难以平衡的问题，尤其关注如何通过控制权重矩阵的稀疏性来提升模型效率与可解释性。其解决方案的关键在于提出了一种可变稀疏度的RNN架构，该架构允许在训练过程中灵活调整权重矩阵中的稀疏程度，并引入一个新指标——隐藏比例（hidden proportion），用于量化模型内部未知变量的分布均衡性，从而在训练前就对模型性能提供更可靠的预测能力。这一方法不仅提升了模型整体性能，还为基于数据集内在特征（如输入和输出维度）进行元学习和模型优化提供了可行路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15057">https://arxiv.org/abs/2509.15057</a><br>
<strong>作者</strong>: Quincy Hershey,Randy Paffenroth<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper develops alternative hyperparameters for specifying sparse Recurrent Neural Networks (RNNs). These hyperparameters allow for varying sparsity within the trainable weight matrices of the model while improving overall performance. This architecture enables the definition of a novel metric, hidden proportion, which seeks to balance the distribution of unknowns within the model and provides significant explanatory power of model performance. Together, the use of the varied sparsity RNN architecture combined with the hidden proportion metric generates significant performance gains while improving performance expectations on an a priori basis. This combined approach provides a path forward towards generalized meta-learning applications and model optimization based on intrinsic characteristics of the data set, including input and output dimensions.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-9] Credit Card Fraud Detection</p>
<p>【速读】：该论文旨在解决信用卡欺诈检测中因类别不平衡（class imbalance）导致的模型性能下降问题，以及欺诈者模仿合法用户行为所带来的识别难度。其解决方案的关键在于采用混合采样策略（hybrid approach），结合欠采样（undersampling）与过采样技术（SMOTE），以优化模型在真实不平衡测试集上的表现，尤其显著提升了多层感知机（MLP）和K近邻（KNN）模型的召回率（recall）与精确率（precision）之间的平衡。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15044">https://arxiv.org/abs/2509.15044</a><br>
<strong>作者</strong>: Iva Popova,Hamza A. A. Gardi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Credit card fraud remains a significant challenge due to class imbalance and fraudsters mimicking legitimate behavior. This study evaluates five machine learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the original imbalanced test set to better reflect real-world performance. Results show that the hybrid method achieves the best balance between recall and precision, especially improving MLP and KNN performance.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-10] Reinforcement Learning <mark class="hl-label green">Agent</mark>  for a 2D Shooter Game</p>
<p>【速读】：该论文旨在解决复杂游戏环境中强化学习智能体面临的稀疏奖励（sparse rewards）、训练不稳定性（training instability）以及样本效率低（poor sample efficiency）等问题。其解决方案的关键在于提出一种混合训练方法，即先通过离线行为克隆（behavioral cloning）利用规则型对手的示范数据进行初始化，再过渡到在线强化学习（reinforcement learning），并设计了一个多头神经网络结构，共享特征提取层并引入注意力机制，使行为克隆与Q-learning模块能够协同工作，从而实现知识迁移与训练稳定性的提升。实验表明，该方法在2D射击游戏中对规则型对手的胜率稳定超过70%，显著优于纯强化学习方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15042">https://arxiv.org/abs/2509.15042</a><br>
<strong>作者</strong>: Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reinforcement learning agents in complex game environments often suffer from sparse rewards, training instability, and poor sample efficiency. This paper presents a hybrid training approach that combines offline imitation learning with online reinforcement learning for a 2D shooter game agent. We implement a multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Initial experiments using pure deep Q-Networks exhibited significant instability, with agents frequently reverting to poor policies despite occasional good performance. To address this, we developed a hybrid methodology that begins with behavioral cloning on demonstration data from rule-based agents, then transitions to reinforcement learning. Our hybrid approach achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure reinforcement learning methods which showed high variance and frequent performance degradation. The multi-head architecture enables effective knowledge transfer between learning modes while maintaining training stability. Results demonstrate that combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments where pure exploration proves insufficient.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-11] From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets <mark class="hl-label red">CIKM2025</mark></p>
<p>【速读】：该论文旨在解决金融市场方向性预测中准确性与可解释性之间的矛盾问题。传统基于人工定义模式的可解释方法因结构模糊和尺度不明确而难以泛化，而深度学习模型虽能捕捉复杂动态却缺乏透明度。解决方案的关键在于提出一个两阶段框架：首先通过SIMPC模块对多变量时间序列进行分割与聚类，提取在幅值缩放和时间扭曲下仍保持不变的重复模式；其次利用JISC-Net这一基于形状子（shapelet）的分类器，以提取模式的初始部分作为输入，预测后续短时序列的方向变动。该方法在比特币及标普500三只股票数据集上表现优异，且能揭示驱动预测结果的底层模式结构，从而实现高精度与可解释性的统一。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15040">https://arxiv.org/abs/2509.15040</a><br>
<strong>作者</strong>: Juwon Kim,Hyunwook Lee,Hyotaek Jeon,Seungmin Jin,Sungahn Ko<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  10 pages, 7 figures, accepted at ACM CIKM 2025 conference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Directional forecasting in financial markets requires both accuracy and interpretability. Before the advent of deep learning, interpretable approaches based on human-defined patterns were prevalent, but their structural vagueness and scale ambiguity hindered generalization. In contrast, deep learning models can effectively capture complex dynamics, yet often offer limited transparency. To bridge this gap, we propose a two-stage framework that integrates unsupervised pattern extracion with interpretable forecasting. (i) SIMPC segments and clusters multivariate time series, extracting recurrent patterns that are invariant to amplitude scaling and temporal distortion, even under varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses the initial part of extracted patterns as input and forecasts subsequent partial sequences for short-term directional movement. Experiments on Bitcoin and three SP 500 equities demonstrate that our method ranks first or second in 11 out of 12 metric–dataset combinations, consistently outperforming baselines. Unlike conventional deep learning models that output buy-or-sell signals without interpretable justification, our approach enables transparent decision-making by revealing the underlying pattern structures that drive predictive outcomes.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-12] Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews</p>
<p>【速读】：该论文旨在解决研究生在线课程中形成性评估（formative assessment）效率与质量不足的问题，尤其是如何通过生成式 AI（Generative AI）提升同伴互评（peer review）反馈的深度与有效性。其解决方案的关键在于利用生成式 AI 对同伴互评进行元反馈（metareview），基于系统功能语言学（Systemic Functional Linguistics）和评价理论（Appraisal Theory）构建多维反馈框架，使AI生成的反馈在概念意义（ideational）、人际意义（interpersonal）和语篇意义（textual）三个层面模拟高质量人类反馈的核心特征，从而促进学习者反馈素养的发展并增强其对同伴互评的参与度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15035">https://arxiv.org/abs/2509.15035</a><br>
<strong>作者</strong>: Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Human-Computer Interaction (cs.HC)<br>
<strong>备注</strong>:  39 pages, 3 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This study investigates the use of generative AI to support formative assessment through machine generated reviews of peer reviews in graduate online courses in a public university in the United States. Drawing on Systemic Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to explore how generative AI feedback constructs meaning across ideational, interpersonal, and textual dimensions. The findings suggest that generative AI can approximate key rhetorical and relational features of effective human feedback, offering directive clarity while also maintaining a supportive stance. The reviews analyzed demonstrated a balance of praise and constructive critique, alignment with rubric expectations, and structured staging that foregrounded student agency. By modeling these qualities, AI metafeedback has the potential to scaffold feedback literacy and enhance leaner engagement with peer review.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-13] Sample Efficient Experience Replay in Non-stationary Environments</p>
<p>【速读】：该论文旨在解决强化学习（Reinforcement Learning, RL）在非平稳环境（non-stationary environments）中因环境动态变化和奖励函数漂移导致历史经验迅速失效的问题。传统经验回放（Experience Replay, ER）方法，尤其是基于TD-error优先级的策略，在无法区分环境变化与智能体策略更新带来的差异时，会降低学习效率。解决方案的关键在于提出“环境动态差异”（Discrepancy of Environment Dynamics, DoE），该指标能够隔离环境变化对价值函数的影响；在此基础上构建了自适应经验回放框架DEER（Discrepancy of Environment Prioritized Experience Replay），通过二分类器检测环境变化，并在每次环境突变前后采用不同的优先级策略，从而实现更高效的样本利用与学习性能提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15032">https://arxiv.org/abs/2509.15032</a><br>
<strong>作者</strong>: Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Networking and Internet Architecture (<a target="_blank" rel="noopener" href="http://cs.NI">cs.NI</a>)<br>
<strong>备注</strong>:  5 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reinforcement learning (RL) in non-stationary environments is challenging, as changing dynamics and rewards quickly make past experiences outdated. Traditional experience replay (ER) methods, especially those using TD-error prioritization, struggle to distinguish between changes caused by the agent’s policy and those from the environment, resulting in inefficient learning under dynamic conditions. To address this challenge, we propose the Discrepancy of Environment Dynamics (DoE), a metric that isolates the effects of environment shifts on value functions. Building on this, we introduce Discrepancy of Environment Prioritized Experience Replay (DEER), an adaptive ER framework that prioritizes transitions based on both policy updates and environmental changes. DEER uses a binary classifier to detect environment changes and applies distinct prioritization strategies before and after each shift, enabling more sample-efficient learning. Experiments on four non-stationary benchmarks demonstrate that DEER further improves the performance of off-policy algorithms by 11.54 percent compared to the best-performing state-of-the-art ER methods.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-14] Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering</p>
<p>【速读】：该论文旨在解决注意力机制在图结构数据上的应用瓶颈问题，特别是针对图聚类任务中传统图神经网络（GNN）过度依赖邻域聚合导致节点表示同质化，以及Transformer模型过度全局化而忽视局部拓扑特征的局限性。其核心解决方案是提出Attentive Graph Clustering Network (AGCN)，关键在于将注意力机制直接嵌入图结构中，从而在保持对局部拓扑敏感性的同时实现有效的全局信息提取；同时引入KV缓存机制提升计算效率，并设计成对边际对比损失增强注意力空间的判别能力，使模型在图聚类任务上显著优于现有方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15024">https://arxiv.org/abs/2509.15024</a><br>
<strong>作者</strong>: Xuanting Xie,Bingheng Li,Erlin Pan,Rui Hou,Wenyu Chen,Zhao Kang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Networking and Internet Architecture (<a target="_blank" rel="noopener" href="http://cs.NI">cs.NI</a>)<br>
<strong>备注</strong>:  9 pages, 5 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Attention mechanisms have become a cornerstone in modern neural networks, driving breakthroughs across diverse domains. However, their application to graph structured data, where capturing topological connections is essential, remains underexplored and underperforming compared to Graph Neural Networks (GNNs), particularly in the graph clustering task. GNN tends to overemphasize neighborhood aggregation, leading to a homogenization of node representations. Conversely, Transformer tends to over globalize, highlighting distant nodes at the expense of meaningful local patterns. This dichotomy raises a key question: Is attention inherently redundant for unsupervised graph learning? To address this, we conduct a comprehensive empirical analysis, uncovering the complementary weaknesses of GNN and Transformer in graph clustering. Motivated by these insights, we propose the Attentive Graph Clustering Network (AGCN) a novel architecture that reinterprets the notion that graph is attention. AGCN directly embeds the attention mechanism into the graph structure, enabling effective global information extraction while maintaining sensitivity to local topological cues. Our framework incorporates theoretical analysis to contrast AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV cache mechanism to improve computational efficiency, and (2) a pairwise margin contrastive loss to boost the discriminative capacity of the attention space. Extensive experimental results demonstrate that AGCN outperforms state-of-the-art methods.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-15] Blockchain-Enabled Explainable AI for Trusted Healthcare Systems</p>
<p>【速读】：该论文旨在解决医疗信息系统中两个核心挑战：安全的数据交换与可解释的AI驱动临床决策。解决方案的关键在于提出了一种区块链集成的可解释人工智能框架（Blockchain-Integrated Explainable AI Framework, BXHF），该框架通过区块链技术实现患者记录的不可篡改性、可审计性和加密共享，确保数据层面的信任；同时融合可解释人工智能（Explainable AI, XAI）方法，生成透明且符合临床逻辑的模型预测，从而建立决策层面的信任。BXHF将安全性和可解释性整合进统一优化流程，并采用混合边缘-云架构支持跨机构联邦计算，在保障患者隐私的同时促进协作分析，显著提升了AI在医疗场景中的可信度、合规性和应用效果。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14987">https://arxiv.org/abs/2509.14987</a><br>
<strong>作者</strong>: Md Talha Mohsin<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  6 Pages, 4 Figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF) for healthcare systems to tackle two essential challenges confronting health information networks: safe data exchange and comprehensible AI-driven clinical decision-making. Our architecture incorporates blockchain, ensuring patient records are immutable, auditable, and tamper-proof, alongside Explainable AI (XAI) methodologies that yield transparent and clinically relevant model predictions. By incorporating security assurances and interpretability requirements into a unified optimization pipeline, BXHF ensures both data-level trust (by verified and encrypted record sharing) and decision-level trust (with auditable and clinically aligned explanations). Its hybrid edge-cloud architecture allows for federated computation across different institutions, enabling collaborative analytics while protecting patient privacy. We demonstrate the framework’s applicability through use cases such as cross-border clinical research networks, uncommon illness detection and high-risk intervention decision support. By ensuring transparency, auditability, and regulatory compliance, BXHF improves the credibility, uptake, and effectiveness of AI in healthcare, laying the groundwork for safer and more reliable clinical decision-making.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-16] he Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation</p>
<p>【速读】：该论文旨在解决人形机器人在手内操作（in-hand manipulation）任务中，如何通过分布式触觉传感网络实现高精度控制的问题，特别是优化触觉传感器在手指和手掌不同区域的配置以提升操控效率与准确性。其解决方案的关键在于系统性地分析来自手指各部位及掌心的触觉反馈对深度强化学习（deep reinforcement learning）控制策略鲁棒性的影响，并揭示物体特性与最优传感器布局之间的关系，从而识别出能显著提升操作性能的触觉感知配置。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14984">https://arxiv.org/abs/2509.14984</a><br>
<strong>作者</strong>: João Damião Almeida,Egidio Falotico,Cecilia Laschi,José Santos-Victor<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In-hand manipulation tasks, particularly in human-inspired robotic systems, must rely on distributed tactile sensing to achieve precise control across a wide variety of tasks. However, the optimal configuration of this network of sensors is a complex problem, and while the fingertips are a common choice for placing sensors, the contribution of tactile information from other regions of the hand is often overlooked. This work investigates the impact of tactile feedback from various regions of the fingers and palm in performing in-hand object reorientation tasks. We analyze how sensory feedback from different parts of the hand influences the robustness of deep reinforcement learning control policies and investigate the relationship between object characteristics and optimal sensor placement. We identify which tactile sensing configurations contribute to improving the efficiency and accuracy of manipulation. Our results provide valuable insights for the design and use of anthropomorphic end-effectors with enhanced manipulation capabilities.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-17] Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles</p>
<p>【速读】：该论文旨在解决定量双极论证图（quantitative bipolar argumentation graphs）中，如何量化一组论据对特定论点（topic）最终强度的贡献问题。传统方法仅能衡量单个论据的贡献，而本文提出了一种集合贡献函数（set contribution functions），将其作为现有单论据贡献函数的推广。解决方案的关键在于：首先将已有的单论据贡献函数原则进行扩展以适用于集合场景；其次引入了针对论据集合内部交互关系的新原则，从而更全面刻画多论据协同作用对目标论点的影响；最后通过推荐系统应用场景验证了这些原则在不同集合贡献函数中的适用性与差异性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14963">https://arxiv.org/abs/2509.14963</a><br>
<strong>作者</strong>: Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present functions that quantify the contribution of a set of arguments in quantitative bipolar argumentation graphs to (the final strength of) an argument of interest, a so-called topic. Our set contribution functions are generalizations of existing functions that quantify the contribution of a single contributing argument to a topic. Accordingly, we generalize existing contribution function principles for set contribution functions and provide a corresponding principle-based analysis. We introduce new principles specific to set-based functions that focus on properties pertaining to the interaction of arguments within a set. Finally, we sketch how the principles play out across different set contribution functions given a recommendation system application scenario.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-18] Sentinel <mark class="hl-label green">Agents</mark>  for Secure and Trustworthy <mark class="hl-label green">Agent</mark> <mark class="hl-label green">Agent</mark> ic AI in Multi-Agent Systems</p>
<p>【速读】：该论文旨在解决多智能体系统（Multi-Agent Systems, MAS）中日益复杂的网络安全与可靠性问题，特别是针对提示注入（prompt injection）、LLM幻觉、共谋行为、隐私泄露及协同攻击等威胁。其解决方案的关键在于提出一种双层安全架构：第一层为由哨兵智能体（Sentinel Agents）组成的分布式安全层，通过大语言模型（Large Language Models, LLMs）语义分析、行为分析、检索增强验证和跨智能体异常检测实现对通信内容和行为的持续监控；第二层为协调智能体（Coordinator Agent），负责策略执行、参与管理，并接收哨兵智能体的告警信息以动态调整策略、隔离异常智能体，从而实现威胁遏制与系统完整性维护。该框架显著提升了系统的可观测性、合规支持能力与政策演化能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14956">https://arxiv.org/abs/2509.14956</a><br>
<strong>作者</strong>: Diego Gosmar,Deborah A. Dahl<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>)<br>
<strong>备注</strong>:  25 pages, 12 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes a novel architectural framework aimed at enhancing security and reliability in multi-agent systems (MAS). A central component of this framework is a network of Sentinel Agents, functioning as a distributed security layer that integrates techniques such as semantic analysis via large language models (LLMs), behavioral analytics, retrieval-augmented verification, and cross-agent anomaly detection. Such agents can potentially oversee inter-agent communications, identify potential threats, enforce privacy and access controls, and maintain comprehensive audit records. Complementary to the idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator Agent supervises policy implementation, and manages agent participation. In addition, the Coordinator also ingests alerts from Sentinel Agents. Based on these alerts, it can adapt policies, isolate or quarantine misbehaving agents, and contain threats to maintain the integrity of the MAS ecosystem. This dual-layered security approach, combining the continuous monitoring of Sentinel Agents with the governance functions of Coordinator Agents, supports dynamic and adaptive defense mechanisms against a range of threats, including prompt injection, collusive agent behavior, hallucinations generated by LLMs, privacy breaches, and coordinated multi-agent attacks. In addition to the architectural design, we present a simulation study where 162 synthetic attacks of different families (prompt injection, hallucination, and data exfiltration) were injected into a multi-agent conversational environment. The Sentinel Agents successfully detected the attack attempts, confirming the practical feasibility of the proposed monitoring approach. The framework also offers enhanced system observability, supports regulatory compliance, and enables policy evolution over time.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-19] Estimating Respiratory Effort from Nocturnal Breathing Sounds for Obstructive Sleep Apnoea Screening <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决阻塞性睡眠呼吸暂停（Obstructive Sleep Apnoea, OSA）诊断中因多导睡眠图（polysomnography）复杂且昂贵导致的大量患者未被确诊的问题，同时克服现有基于声学的筛查方法受环境噪声干扰、缺乏生理信息的局限性。其关键解决方案是首次提出一种从夜间音频中直接估计呼吸努力（respiratory effort）的方法，并构建了一个潜在空间融合框架（latent-space fusion framework），将估计的呼吸努力嵌入特征与声学特征融合用于OSA检测。该方法仅需智能手机录音即可实现无传感器、可扩展且适合长期监测的OSA筛查，显著提升了低呼吸暂停-低通气指数（AHI）阈值下的敏感性和AUC性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14944">https://arxiv.org/abs/2509.14944</a><br>
<strong>作者</strong>: Xiaolei Xu,Chaoyue Niu,Guy J. Brown,Hector Romero,Ning Ma<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:  Submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Obstructive sleep apnoea (OSA) is a prevalent condition with significant health consequences, yet many patients remain undiagnosed due to the complexity and cost of over-night polysomnography. Acoustic-based screening provides a scalable alternative, yet performance is limited by environmental noise and the lack of physiological context. Respiratory effort is a key signal used in clinical scoring of OSA events, but current approaches require additional contact sensors that reduce scalability and patient comfort. This paper presents the first study to estimate respiratory effort directly from nocturnal audio, enabling physiological context to be recovered from sound alone. We propose a latent-space fusion framework that integrates the estimated effort embeddings with acoustic features for OSA detection. Using a dataset of 157 nights from 103 participants recorded in home environments, our respiratory effort estimator achieves a concordance correlation coefficient of 0.48, capturing meaningful respiratory dynamics. Fusing effort and audio improves sensitivity and AUC over audio-only baselines, especially at low apnoea-hypopnoea index thresholds. The proposed approach requires only smartphone audio at test time, which enables sensor-free, scalable, and longitudinal OSA monitoring.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-20] Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers</p>
<p>【速读】：该论文旨在解决碳青霉烯类耐药肠杆菌科细菌（Carbapenemase-Producing Enterobacteriaceae, CPE）感染在医院环境中对患者预后影响的预测建模问题，特别是针对再入院、死亡率和住院时间延长等临床结局的精准预测，且此前缺乏基于现代深度学习方法的研究。解决方案的关键在于构建一个可解释的人工智能（Explainable AI, XAI）建模框架，利用爱尔兰某急性医院电子病历（Electronic Medical Records, EMR）数据，整合诊断编码、病房转移、人口统计学信息、感染相关变量及接触网络特征，并对比多种Transformer架构与传统机器学习模型的表现。结果表明，TabTransformer模型在多个临床预测任务中优于基线模型，尤其在CPE获取风险预测上表现出更高的AUROC和敏感度；同时，通过XAI技术揭示了包括“居住区域”“入院病房”、既往住院史以及“病房PageRank”等网络中心性指标在内的关键风险因素，凸显了结构暴露信息的价值，从而为CPE防控提供可解释的决策支持。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14942">https://arxiv.org/abs/2509.14942</a><br>
<strong>作者</strong>: Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Accepted to BMC Medical Informatics and Decision Making on September 18th 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Carbapenemase-Producing Enterobacteriace poses a critical concern for infection prevention and control in hospitals. However, predictive modeling of previously highlighted CPE-associated risks such as readmission, mortality, and extended length of stay (LOS) remains underexplored, particularly with modern deep learning approaches. This study introduces an eXplainable AI modeling framework to investigate CPE impact on patient outcomes from Electronic Medical Records data of an Irish hospital. We analyzed an inpatient dataset from an Irish acute hospital, incorporating diagnostic codes, ward transitions, patient demographics, infection-related variables and contact network features. Several Transformer-based architectures were benchmarked alongside traditional machine learning models. Clinical outcomes were predicted, and XAI techniques were applied to interpret model decisions. Our framework successfully demonstrated the utility of Transformer-based models, with TabTransformer consistently outperforming baselines across multiple clinical prediction tasks, especially for CPE acquisition (AUROC and sensitivity). We found infection-related features, including historical hospital exposure, admission context, and network centrality measures, to be highly influential in predicting patient outcomes and CPE acquisition risk. Explainability analyses revealed that features like “Area of Residence”, “Admission Ward” and prior admissions are key risk factors. Network variables like “Ward PageRank” also ranked highly, reflecting the potential value of structural exposure information. This study presents a robust and explainable AI framework for analyzing complex EMR data to identify key risk factors and predict CPE-related outcomes. Our findings underscore the superior performance of the Transformer models and highlight the importance of diverse clinical and network features.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-21] Back to Ear: Perceptually Driven High Fidelity Music Reconstruction</p>
<p>【速读】：该论文旨在解决现有变分自编码器（Variational Autoencoders, VAEs）在大规模音频任务中忽视听觉感知特性的问题，特别是相位精度不足和立体声空间表征能力弱的缺陷。其解决方案的关键在于三个方面：首先，在损失计算前引入K-weighting感知滤波器，使优化目标更贴近人类听觉感知；其次，设计两种新型相位损失——用于保证立体声相干性的相关性损失（Correlation Loss），以及基于瞬时频率（Instantaneous Frequency）和群延迟（Group Delay）导数的相位损失，以提升相位重建精度；最后，提出一种新的频谱监督范式，即幅度由中/侧/左/右四个通道共同监督，而相位仅由左右声道（LR）成分监督，从而更好地保留音频的空间信息与高频谐波细节。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14912">https://arxiv.org/abs/2509.14912</a><br>
<strong>作者</strong>: Kangdi Wang,Zhiyue Wu,Dinghao Zhou,Rui Lin,Junyu Dai,Tao Jiang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Check the Code here: <a target="_blank" rel="noopener" href="https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE">this https URL</a> and Model Weights here: <a target="_blank" rel="noopener" href="https://huggingface.co/earlab/EAR_VAE">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose \epsilonar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives–Instantaneous Frequency and Group Delay–for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show \epsilonar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-22] AI-Driven Multi-<mark class="hl-label green">Agent</mark>  Vehicular <mark class="hl-label green">Planning</mark>  for Battery Efficiency and QoS in 6G Smart Cities</p>
<p>【速读】：该论文旨在解决车联网物联网（Vehicular IoT）节点在通过边缘节点与云端通信的全仿真渗透架构中，缺乏动态代理规划与优化机制的问题，从而在保障公平通信时间的同时最小化车辆电池消耗。解决方案的关键在于扩展现有的SimulatorOrchestrator（SO）模拟器架构，集成用于交通预测和动态代理规划的AI算法，并引入“吸引力区域”（desirability areas）概念以优化路径选择策略，从而在提升服务质量（QoS）的同时降低能耗，实验结果表明该方法相较传统最短路径算法能显著改善电池使用效率和任务完成率。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14877">https://arxiv.org/abs/2509.14877</a><br>
<strong>作者</strong>: Rohin Gillgallon,Giacomo Bergami,Reham Almutairi,Graham Morgan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Networking and Internet Architecture (<a target="_blank" rel="noopener" href="http://cs.NI">cs.NI</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Emerging Technologies (<a target="_blank" rel="noopener" href="http://cs.ET">cs.ET</a>)<br>
<strong>备注</strong>:  16 pages, 2 figures, 2 tables, 2 algorithms</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While simulators exist for vehicular IoT nodes communicating with the Cloud through Edge nodes in a fully-simulated osmotic architecture, they often lack support for dynamic agent planning and optimisation to minimise vehicular battery consumption while ensuring fair communication times. Addressing these challenges requires extending current simulator architectures with AI algorithms for both traffic prediction and dynamic agent planning. This paper presents an extension of SimulatorOrchestrator (SO) to meet these requirements. Preliminary results over a realistic urban dataset show that utilising vehicular planning algorithms can lead to improved battery and QoS performance compared with traditional shortest path algorithms. The additional inclusion of desirability areas enabled more ambulances to be routed to their target destinations while utilising less energy to do so, compared to traditional and weighted algorithms without desirability considerations.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-23] DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting</p>
<p>【速读】：该论文旨在解决多模态时间-频域信息融合在模型性能提升中的关键机制问题，尤其关注如何有效整合异构的时间域与频域特征以增强模型表达能力。其解决方案的关键在于设计了一个交互式融合模块（interactive fusion block），通过跨注意力机制（cross-attention mechanism）实现时间域和频域特征的深度交互与协同优化，实验证明该机制是性能提升的核心要素，显著优于仅使用单一域或简化融合方式的变体模型。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14868">https://arxiv.org/abs/2509.14868</a><br>
<strong>作者</strong>: Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We conducted rigorous ablation studies to validate DPANet’s key components (Table \reftab:ablation-study). The full model consistently outperforms all variants. To test our dual-domain hypothesis, we designed two specialized versions: a Temporal-Only model (fusing two identical temporal pyramids) and a Frequency-Only model (fusing two spectral pyramids). Both variants underperformed significantly, confirming that the fusion of heterogeneous temporal and frequency information is critical. Furthermore, replacing the cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the most severe performance degradation. This result underscores that our interactive fusion block is the most essential component.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-24] Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study</p>
<p>【速读】：该论文旨在解决图注意力机制（Graph Attention Mechanism）在图表示学习中因局部信息被全局注意力稀释而导致的信息损失问题。传统图Transformer（GTs）通常采用局部-全局或局部到全局的注意力架构，使得浅层GNN提取的局部结构信息在深层全局注意力作用下被弱化。其解决方案的关键在于提出G2LFormer模型，采用全新的“全局到局部”注意力机制：浅层利用注意力捕捉全局依赖关系，深层则通过图神经网络（GNN）模块聚焦于局部结构信息，从而避免节点忽略邻近邻居；同时引入跨层信息融合策略，使局部层能够保留来自全局层的有益信息，有效缓解信息丢失问题，并在保持线性时间复杂度的前提下实现卓越性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14863">https://arxiv.org/abs/2509.14863</a><br>
<strong>作者</strong>: Zhengwei Wang,Gang Wu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Graph Transformers (GTs) show considerable potential in graph representation learning. The architecture of GTs typically integrates Graph Neural Networks (GNNs) with global attention mechanisms either in parallel or as a precursor to attention mechanisms, yielding a local-and-global or local-to-global attention scheme. However, as the global attention mechanism primarily captures long-range dependencies between nodes, these integration schemes may suffer from information loss, where the local neighborhood information learned by GNN could be diluted by the attention mechanism. Therefore, we propose G2LFormer, featuring a novel global-to-local attention scheme where the shallow network layers use attention mechanisms to capture global information, while the deeper layers employ GNN modules to learn local structural information, thereby preventing nodes from ignoring their immediate neighbors. An effective cross-layer information fusion strategy is introduced to allow local layers to retain beneficial information from global layers and alleviate information loss, with acceptable trade-offs in scalability. To validate the feasibility of the global-to-local attention scheme, we compare G2LFormer with state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The results indicate that G2LFormer exhibits excellent performance while keeping linear complexity.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-25] MeanFlowSE: one-step generative speech enhancement via conditional mean flow</p>
<p>【速读】：该论文旨在解决生成式语音增强（Generative Speech Enhancement）中多步推理（Multistep Inference）带来的实时性瓶颈问题，尤其针对基于流（Flow）和扩散（Diffusion）模型依赖迭代常微分方程（ODE）求解器的高计算开销。解决方案的关键在于提出MeanFlowSE，一种条件生成模型，其核心创新是学习轨迹上有限区间内的平均速度场（average velocity over finite intervals），并通过Jacobian-Vector Product（JVP）实现均值流恒等式（MeanFlow identity），从而导出一个局部训练目标，直接监督有限区间位移，同时保持对角线上的瞬时场约束一致性。推理阶段，MeanFlowSE通过反向时间位移实现单步生成，无需多步求解器，显著降低延迟；可选的几步优化版本进一步提升性能，且无需知识蒸馏或外部教师模型，为实时生成式语音增强提供了高效且高保真度的框架。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14858">https://arxiv.org/abs/2509.14858</a><br>
<strong>作者</strong>: Duojia Li,Shenghui Lu,Hongchen Pan,Zongyi Zhan,Qingyang Hong,Lin Li<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multistep inference is a bottleneck for real-time generative speech enhancement because flow- and diffusion-based systems learn an instantaneous velocity field and therefore rely on iterative ordinary differential equation (ODE) solvers. We introduce MeanFlowSE, a conditional generative model that learns the average velocity over finite intervals along a trajectory. Using a Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a local training objective that directly supervises finite-interval displacement while remaining consistent with the instantaneous-field constraint on the diagonal. At inference, MeanFlowSE performs single-step generation via a backward-in-time displacement, removing the need for multistep solvers; an optional few-step variant offers additional refinement. On VoiceBank-DEMAND, the single-step model achieves strong intelligibility, fidelity, and perceptual quality with substantially lower computational cost than multistep baselines. The method requires no knowledge distillation or external teachers, providing an efficient, high-fidelity framework for real-time generative speech enhancement.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-26] Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决不确定系统中（如能源市场和金融领域）高效决策问题，核心挑战在于如何准确估计未来场景的完整分布以支持优化决策。解决方案的关键在于提出了一种通用的场景树构建框架——扩散场景树（Diffusion Scenario Tree, DST），该框架利用基于扩散的概率预测模型递归采样未来轨迹，并通过聚类将其组织成满足非前瞻性的（non-anticipativity）树结构，从而在每阶段决策时仅依赖已观测的历史信息。这一方法显著优于传统模型生成的场景树及无模型强化学习基线，在纽约州日前电力市场的套利优化任务中展现出更优的决策性能和对不确定性的更好处理能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14832">https://arxiv.org/abs/2509.14832</a><br>
<strong>作者</strong>: Stelios Zarifis,Ioannis Kordonis,Petros Maragos<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:  5 pages, 2 figures, 2 tables, and 1 algorithm. This version is submitted to the 51st IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026), to be held in Barcelona, Spain, on May 4-8, 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State’s day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-27] OnlineMate: An <mark class="hl-label green">LLM</mark> -Based Multi-<mark class="hl-label green">Agent</mark>  Companion System for Cognitive Support in Online Learning</p>
<p>【速读】：该论文旨在解决在线学习环境中学生缺乏个性化同伴互动的问题，而此类互动对认知发展和学习参与度至关重要。现有基于大语言模型（Large Language Models, LLMs）的AI学习伴侣主要局限于对话式交互，未能根据学习者的个体认知状态进行洞察与适应，导致学生兴趣不足且难以从中获得启发。解决方案的关键在于提出OnlineMate系统，该系统基于心智理论（Theory of Mind, ToM）构建多智能体学习伴侣架构，能够模拟类同伴角色、动态感知并推断学习者的心理状态（如误解、困惑或动机水平），进而调整交互策略以支持高阶思维与认知发展。实验证明，该方法在模拟学习场景中显著提升了深度学习效果与认知参与度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14803">https://arxiv.org/abs/2509.14803</a><br>
<strong>作者</strong>: Xian Gao,Zongyun Zhang,Ting Liu,Yuzhuo Fu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In online learning environments, students often lack personalized peer interactions, which play a crucial role in supporting cognitive development and learning engagement. Although previous studies have utilized large language models (LLMs) to simulate interactive dynamic learning environments for students, these interactions remain limited to conversational exchanges, lacking insights and adaptations to the learners’ individualized learning and cognitive states. As a result, students’ interest in discussions with AI learning companions is low, and they struggle to gain inspiration from such interactions. To address this challenge, we propose OnlineMate, a multi-agent learning companion system driven by LLMs that integrates the Theory of Mind (ToM). OnlineMate is capable of simulating peer-like agent roles, adapting to learners’ cognitive states during collaborative discussions, and inferring their psychological states, such as misunderstandings, confusion, or motivation. By incorporating Theory of Mind capabilities, the system can dynamically adjust its interaction strategies to support the development of higher-order thinking and cognition. Experimental results in simulated learning scenarios demonstrate that OnlineMate effectively fosters deep learning and discussions while enhancing cognitive engagement in online educational settings.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-28] Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery</p>
<p>【速读】：该论文旨在解决药物-靶标相互作用（Drug-Target Interaction, DTI）精准识别这一计算药理学中的核心挑战，尤其针对传统序列-based方法在缺乏结构信息时预测精度不足的问题。其解决方案的关键在于构建一个基于序列的DTI框架，通过将结构先验知识（structural priors）整合进蛋白质表征中，同时保持高通量筛选能力；具体而言，模型引入了可学习的聚合机制、双线性注意力（bilinear attention）以及对比对齐（contrastive alignment），从而显著提升预测鲁棒性，并在多个基准数据集上实现最优性能，包括在LIT-PCBA上的虚拟筛选任务中大幅优于现有方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14788">https://arxiv.org/abs/2509.14788</a><br>
<strong>作者</strong>: Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W.Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Biomolecules (<a target="_blank" rel="noopener" href="http://q-bio.BM">q-bio.BM</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework’s utility for scalable and structure-aware DTI prediction.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-29] OpenLens AI: Fully Autonomous Research <mark class="hl-label green">Agent</mark>  for Health Infomatics</p>
<p>【速读】：该论文旨在解决健康信息学研究中因多模态数据、知识快速扩展及跨生物医学科学、数据分析与临床实践整合需求所带来的挑战，特别是现有基于大语言模型（Large Language Model, LLM）的智能体系统在处理医学可视化内容和满足领域特定质量要求方面的不足。其解决方案的关键在于提出OpenLens AI框架，该框架通过集成文献综述、数据分析、代码生成与论文撰写等专用智能体，并引入视觉-语言反馈机制以解析医学可视化结果，同时嵌入可复现性质量控制模块，从而实现从数据到出版级LaTeX论文的全流程自动化，确保工作流透明且可追溯，为健康信息学研究提供了一个高度适配领域的端到端自动化解决方案。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14778">https://arxiv.org/abs/2509.14778</a><br>
<strong>作者</strong>: Yuxiao Cheng,Jinli Suo<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Health informatics research is characterized by diverse data modalities, rapid knowledge expansion, and the need to integrate insights across biomedical science, data analytics, and clinical practice. These characteristics make it particularly well-suited for agent-based approaches that can automate knowledge exploration, manage complex workflows, and generate clinically meaningful outputs. Recent progress in large language model (LLM)-based agents has demonstrated promising capabilities in literature synthesis, data analysis, and even end-to-end research execution. However, existing systems remain limited for health informatics because they lack mechanisms to interpret medical visualizations and often overlook domain-specific quality requirements. To address these gaps, we introduce OpenLens AI, a fully automated framework tailored to health informatics. OpenLens AI integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility. The framework automates the entire research pipeline, producing publication-ready LaTeX manuscripts with transparent and traceable workflows, thereby offering a domain-adapted solution for advancing health informatics research.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-30] Enhancing Retrieval Augmentation via Adversarial Collaboration</p>
<p>【速读】：该论文旨在解决检索增强生成（Retrieval-augmented Generation, RAG）方法在特定领域大语言模型中普遍存在的“检索幻觉”（Retrieval Hallucinations）问题，即模型无法识别并规避低质量的检索文档，从而导致性能下降。解决方案的关键在于提出了一种对抗协作式RAG框架（Adversarial Collaboration RAG, AC-RAG），其核心机制是引入两个异构代理：一个通用型检测器（Detector）用于识别知识盲区，一个领域专业化求解器（Resolver）提供精准解答；二者在调解者的引导下进行对抗性协作，通过检测器持续质疑求解器的专业能力，驱动迭代式问题拆解与精细化知识检索，从而显著提升检索准确率并优于当前主流RAG方法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14750">https://arxiv.org/abs/2509.14750</a><br>
<strong>作者</strong>: Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Retrieval-augmented Generation (RAG) is a prevalent approach for domain-specific LLMs, yet it is often plagued by “Retrieval Hallucinations”–a phenomenon where fine-tuned models fail to recognize and act upon poor-quality retrieved documents, thus undermining performance. To address this, we propose the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides precise solutions. Guided by a moderator, these agents engage in an adversarial collaboration, where the Detector’s persistent questioning challenges the Resolver’s expertise. This dynamic process allows for iterative problem dissection and refined knowledge retrieval. Extensive experiments show that AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-31] he NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based <mark class="hl-label green">Reasoning</mark>  in <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】：该论文旨在解决大语言模型（Large Language Models, LLMs）评估中因基准测试饱和（benchmark saturation）和污染（contamination）导致的可信度下降问题。其解决方案的关键在于提出一个名为Nazonazo的低成本、可扩展的基准测试集，该集基于日语儿童谜题构建，具有短文本输入（多为单句）、无需专业领域知识、易于大规模生成等特点，从而支持在泄露风险出现时快速更新盲测数据集。实验表明，除GPT-5外，其他模型均无法达到人类水平（人类平均准确率为52.9%），且推理类模型显著优于非推理类模型，而模型规模与准确性无显著关联；进一步分析发现，多数模型虽能生成正确答案但未能最终选择，暴露出元认知层面的验证失败问题，为未来控制与校准方法提供了明确方向。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14704">https://arxiv.org/abs/2509.14704</a><br>
<strong>作者</strong>: Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Benchmark saturation and contamination undermine confidence in LLM evaluation. We present Nazonazo, a cost-effective and extensible benchmark built from Japanese children’s riddles to test insight-based reasoning. Items are short (mostly one sentence), require no specialized domain knowledge, and can be generated at scale, enabling rapid refresh of blind sets when leakage is suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No model except for GPT-5 is comparable to human performance, which achieves a 52.9% mean accuracy. Model comparison on extended 201 items shows that reasoning models significantly outperform non-reasoning peers, while model size shows no reliable association with accuracy. Beyond aggregate accuracy, an informal candidate-tracking analysis of thought logs reveals many cases of verification failure: models often produce the correct solution among intermediate candidates yet fail to select it as the final answer, which we illustrate with representative examples observed in multiple models. Nazonazo thus offers a cost-effective, scalable, and easily renewable benchmark format that addresses the current evaluation crisis while also suggesting a recurrent meta-cognitive weakness, providing clear targets for future control and calibration methods.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-32] RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning</p>
<p>【速读】：该论文旨在解决现代软件系统中日志异常检测（log anomaly detection）的两大核心挑战：一是传统深度学习模型缺乏可解释性和泛化能力，二是基于大语言模型（Large Language Models, LLMs）的方法常因不可靠性与事实性错误而表现不稳定。解决方案的关键在于提出一种名为 RationAnomaly 的新框架，其核心创新是融合思维链（Chain-of-Thought, CoT）监督微调与强化学习（reinforcement learning），首先通过专家标注的高质量数据集进行 CoT 引导的微调以注入类专家推理模式，再利用多维度奖励函数驱动强化学习过程，在提升检测准确率的同时增强逻辑一致性并有效抑制幻觉现象。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14693">https://arxiv.org/abs/2509.14693</a><br>
<strong>作者</strong>: Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  5 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Logs constitute a form of evidence signaling the operational status of software systems. Automated log anomaly detection is crucial for ensuring the reliability of modern software systems. However, existing approaches face significant limitations: traditional deep learning models lack interpretability and generalization, while methods leveraging Large Language Models are often hindered by unreliability and factual inaccuracies. To address these issues, we propose RationAnomaly, a novel framework that enhances log anomaly detection by synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our approach first instills expert-like reasoning patterns using CoT-guided supervised fine-tuning, grounded in a high-quality dataset corrected through a rigorous expert-driven process. Subsequently, a reinforcement learning phase with a multi-faceted reward function optimizes for accuracy and logical consistency, effectively mitigating hallucinations. Experimentally, RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs. We have released the corresponding resources, including code and datasets.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-33] hreat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework</p>
<p>【速读】：该论文旨在解决物联网（IoT）节点在资源受限环境下进行本地音频分类时面临的敏感数据泄露风险问题。其核心挑战在于如何在保障设备安全性的同时，满足边缘计算场景下的性能与隐私要求。解决方案的关键在于构建一个纵深防御架构（defence-in-depth），将边缘设备、蜂窝网络和云后端划分为三个独立的信任域，并通过基于可信平台模块（TPM）的远程认证和相互认证的TLS 1.3协议实现跨域安全通信；同时结合STRIDE威胁建模与攻击树分析指导设计，确保从启动阶段到数据存储全过程的安全性：包括使用TPM度量引导链、一次性解锁密钥机制防止篡改设备运行、采用后量子密码算法（Kyber和Dilithium）增强传输层抗量子攻击能力、对音频特征实施端到端加密与完整性校验、部署防回滚签名模型及防篡改传感器加固固件与硬件，并采用“3-2-1”数据备份策略（即三份数据、两份不同介质、一份离线冷存储备份）保护静态数据。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14657">https://arxiv.org/abs/2509.14657</a><br>
<strong>作者</strong>: Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted at Computing Conference 2026, London, UK</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rapid proliferation of IoT nodes equipped with microphones and capable of performing on-device audio classification exposes highly sensitive data while operating under tight resource constraints. To protect against this, we present a defence-in-depth architecture comprising a security protocol that treats the edge device, cellular network and cloud backend as three separate trust domains, linked by TPM-based remote attestation and mutually authenticated TLS 1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At startup, each boot stage is measured into TPM PCRs. The node can only decrypt its LUKS-sealed partitions after the cloud has verified a TPM quote and released a one-time unlock key. This ensures that rogue or tampered devices remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end encryption and integrity hashes safeguard extracted audio features. Signed, rollback-protected AI models and tamper-responsive sensors harden firmware and hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum cipher and an encrypted cloud replica. Finally, we set out a plan for evaluating the physical and logical security of the proposed protocol.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-34] DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training</p>
<p>【速读】：该论文旨在解决时间序列预训练中动态时序依赖建模的难题，特别是由分布偏移（distribution shift）和多尺度模式引起的模型泛化能力下降问题。现有方法难以捕捉短时与长时依赖之间的复杂交互，易受虚假相关性干扰。其解决方案的关键在于提出DeCoP框架，通过两个核心机制实现：一是输入层的实例级补丁归一化（Instance-wise Patch Normalization, IPN），缓解分布偏移并保留每个补丁的独特特征；二是潜在层的分层依赖控制学习策略（Hierarchical Dependency Controlled Learning, DCL），结合实例级对比模块（Instance-level Contrastive Module, ICM），在多时间尺度上显式建模补丁间依赖关系，并利用时间不变正样本学习具有判别性的全局表示，从而显著提升下游任务的泛化性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14642">https://arxiv.org/abs/2509.14642</a><br>
<strong>作者</strong>: Yuemin Wu,Zhongze Wu,Xiu Su,Feng Yang,Hongyan Xu,Xi Lin,Wenti Huang,Shan You,Chang Xu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Modeling dynamic temporal dependencies is a critical challenge in time series pre-training, which evolve due to distribution shifts and multi-scale patterns. This temporal variability severely impairs the generalization of pre-trained models to downstream tasks. Existing frameworks fail to capture the complex interactions of short- and long-term dependencies, making them susceptible to spurious correlations that degrade generalization. To address these limitations, we propose DeCoP, a Dependency Controlled Pre-training framework that explicitly models dynamic, multi-scale dependencies by simulating evolving inter-patch dependencies. At the input level, DeCoP introduces Instance-wise Patch Normalization (IPN) to mitigate distributional shifts while preserving the unique characteristics of each patch, creating a robust foundation for representation learning. At the latent level, a hierarchical Dependency Controlled Learning (DCL) strategy explicitly models inter-patch dependencies across multiple temporal scales, with an Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs. DeCoP achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-35] Automating Modelica Module Generation Using <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark> : A Case Study on Building Control Description Language</p>
<p>【速读】：该论文旨在解决动态能源系统与控制策略设计中模型开发效率低下的问题，特别是Modelica语言在构建控制模块时所需的人工劳动强度大、专业门槛高的挑战。其解决方案的关键在于提出了一种结构化的工作流，融合了标准化提示模板（prompt scaffolds）、库感知的上下文锚定（library-aware grounding）、自动化编译（OpenModelica）以及人机协同评估机制，从而实现Control Description Language模块的自动化生成。实验表明，通过精心设计的提示策略，Claude Sonnet 4在基础逻辑块上可达到100%成功率，在控制模块上成功率达83%，且失败输出仅需中等程度人工修复（约1–8小时），整体开发时间从10–20小时缩短至4–6小时，效率提升40%–60%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14623">https://arxiv.org/abs/2509.14623</a><br>
<strong>作者</strong>: Hanlong Wan,Xing Lu,Yan Chen,Karthik Devaprasad,Laura Hinkle<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Programming Languages (<a target="_blank" rel="noopener" href="http://cs.PL">cs.PL</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:  This is the pre-peer-review version of a journal paper; the repo is available at: <a target="_blank" rel="noopener" href="https://github.com/pnnl/prompt2control">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Dynamic energy systems and controls require advanced modeling frameworks to design and test supervisory and fault tolerant strategies. Modelica is a widely used equation based language, but developing control modules is labor intensive and requires specialized expertise. This paper examines the use of large language models (LLMs) to automate the generation of Control Description Language modules in the Building Modelica Library as a case study. We developed a structured workflow that combines standardized prompt scaffolds, library aware grounding, automated compilation with OpenModelica, and human in the loop evaluation. Experiments were carried out on four basic logic tasks (And, Or, Not, and Switch) and five control modules (chiller enable/disable, bypass valve control, cooling tower fan speed, plant requests, and relief damper control). The results showed that GPT 4o failed to produce executable Modelica code in zero shot mode, while Claude Sonnet 4 achieved up to full success for basic logic blocks with carefully engineered prompts. For control modules, success rates reached 83 percent, and failed outputs required medium level human repair (estimated one to eight hours). Retrieval augmented generation often produced mismatches in module selection (for example, And retrieved as Or), while a deterministic hard rule search strategy avoided these errors. Human evaluation also outperformed AI evaluation, since current LLMs cannot assess simulation results or validate behavioral correctness. Despite these limitations, the LLM assisted workflow reduced the average development time from 10 to 20 hours down to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings. These results highlight both the potential and current limitations of LLM assisted Modelica generation, and point to future research in pre simulation validation, stronger grounding, and closed loop evaluation.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-36] Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection</p>
<p>【速读】：该论文旨在解决在线恶意意图检测（Online Malicious Intent Detection）在交互式应用中面临的挑战，即现有方法难以实时处理多样且复杂的用户查询。其解决方案的关键在于提出一种两阶段框架 ADRAG（Adversarial Distilled Retrieval-Augmented Guard）：在训练阶段，利用对抗扰动和检索增强输入，使高容量教师模型学习鲁棒的决策边界；在推理阶段，通过知识蒸馏调度器将教师模型的知识迁移至轻量级学生模型，并结合在线更新的知识库，实现高效、实时的恶意查询检测。该方案在保持高性能的同时显著降低延迟，实验证明其仅用149M参数即可达到WildGuard-7B性能的98.5%，并优于GPT-4和Llama-Guard-3-8B在分布外检测上的表现。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14622">https://arxiv.org/abs/2509.14622</a><br>
<strong>作者</strong>: Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:With the deployment of Large Language Models (LLMs) in interactive applications, online malicious intent detection has become increasingly critical. However, existing approaches fall short of handling diverse and complex user queries in real time. To address these challenges, we introduce ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework for robust and efficient online malicious intent detection. In the training stage, a high-capacity teacher model is trained on adversarially perturbed, retrieval-augmented inputs to learn robust decision boundaries over diverse and complex user queries. In the inference stage, a distillation scheduler transfers the teacher’s knowledge into a compact student model, with a continually updated knowledge base collected online. At deployment, the compact student model leverages top-K similar safety exemplars retrieved from the online-updated knowledge base to enable both online and real-time malicious query detection. Evaluations across ten safety benchmarks demonstrate that ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B’s performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on out-of-distribution detection, while simultaneously delivering up to 5.6x lower latency at 300 queries per second (QPS) in real-time applications.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-37] Enterprise AI Must Enforce Participant-Aware Access Control</p>
<p>【速读】：该论文旨在解决大型语言模型（Large Language Models, LLMs）在企业环境中因微调（fine-tuning）和检索增强生成（Retrieval-Augmented Generation, RAG）架构导致的敏感数据泄露问题。当前方法缺乏细粒度访问控制，使得未经授权的用户可能通过推理阶段动态检索或模型内部知识提取的方式窃取训练数据。解决方案的关键在于引入一种基于显式授权的确定性访问控制机制：即任何用于训练、检索或生成的内容必须被明确授权给所有参与交互的用户，从而从根本上杜绝数据泄露风险。这一策略实现了从概率性防御向严格访问控制的范式转变，已在 Microsoft Copilot Tuning 中部署验证。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14608">https://arxiv.org/abs/2509.14608</a><br>
<strong>作者</strong>: Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs) are increasingly deployed in enterprise settings where they interact with multiple users and are trained or fine-tuned on sensitive internal data. While fine-tuning enhances performance by internalizing domain knowledge, it also introduces a critical security risk: leakage of confidential training data to unauthorized users. These risks are exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG) pipelines that dynamically fetch contextual documents at inference time. We demonstrate data exfiltration attacks on AI assistants where adversaries can exploit current fine-tuning and RAG architectures to leak sensitive information by leveraging the lack of access control enforcement. We show that existing defenses, including prompt sanitization, output filtering, system isolation, and training-level privacy mechanisms, are fundamentally probabilistic and fail to offer robust protection against such attacks. We take the position that only a deterministic and rigorous enforcement of fine-grained access control during both fine-tuning and RAG-based inference can reliably prevent the leakage of sensitive data to unauthorized recipients. We introduce a framework centered on the principle that any content used in training, retrieval, or generation by an LLM is explicitly authorized for \emphall users involved in the interaction. Our approach offers a simple yet powerful paradigm shift for building secure multi-user LLM systems that are grounded in classical access control but adapted to the unique challenges of modern AI workflows. Our solution has been deployed in Microsoft Copilot Tuning, a product offering that enables organizations to fine-tune models using their own enterprise-specific data.         Subjects:  Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)  Cite as: arXiv:2509.14608 [<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>]    (or  arXiv:2509.14608v1 [<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14608">https://doi.org/10.48550/arXiv.2509.14608</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)        Submission history From: Rahul Sharma [view email]       [v1]         Thu, 18 Sep 2025 04:30:49 UTC (126 KB)<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-38] A Case for Computing on Unstructured Data</p>
<p>【速读】：该论文试图解决的问题是：当前传统数据系统主要依赖结构化格式进行计算，难以有效处理占全球信息绝大多数的非结构化数据（如文本、图像、音频和视频），导致这些数据在分析和利用上存在显著局限。解决方案的关键在于提出一种新的范式——“对非结构化数据的计算”（computing on unstructured data），其核心是一个双向处理流程：首先从非结构化数据中提取潜在结构（latent structure），然后通过数据处理技术对该结构进行变换与分析，最后将结果重新投影回非结构化格式。这一机制使非结构化数据既能受益于结构化计算的强大分析能力，又能保持其对人类和AI用户而言的丰富性与可访问性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14601">https://arxiv.org/abs/2509.14601</a><br>
<strong>作者</strong>: Mushtari Sadia,Amrita Roy Chowdhury,Ang Chen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Databases (cs.DB); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Unstructured data, such as text, images, audio, and video, comprises the vast majority of the world’s information, yet it remains poorly supported by traditional data systems that rely on structured formats for computation. We argue for a new paradigm, which we call computing on unstructured data, built around three stages: extraction of latent structure, transformation of this structure through data processing techniques, and projection back into unstructured formats. This bi-directional pipeline allows unstructured data to benefit from the analytical power of structured computation, while preserving the richness and accessibility of unstructured representations for human and AI consumption. We illustrate this paradigm through two use cases and present the research components that need to be developed in a new data system called MXFlow.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-39] SynBench: A Benchmark for Differentially Private Text Generation</p>
<p>【速读】：该论文旨在解决高风险领域（如医疗和金融）中数据驱动决策支持因监管、机构和隐私顾虑而导致的数据共享障碍问题，特别是生成式AI在敏感环境中因行为不可预测及缺乏隐私保护的数据集用于基准测试而难以应用的问题。其关键解决方案包括：（1）提出一个包含九个精心设计数据集的综合评估框架，涵盖领域特异性复杂性（如专业术语、长上下文依赖和结构化文档），并引入标准化的效用与保真度指标；（2）开展大规模实证研究，对最先进的差分隐私（Differential Privacy, DP）文本生成方法和不同规模的语言模型进行对比，揭示在DP约束下高质量领域特定合成数据生成仍是未解难题，且性能随领域复杂度增加而下降；（3）开发一种针对合成文本的成员推理攻击（Membership Inference Attack, MIA）方法，首次提供实证证据表明公开数据集可能存在于预训练语料库中，从而破坏声称的隐私保障，强调了严格的隐私审计必要性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14594">https://arxiv.org/abs/2509.14594</a><br>
<strong>作者</strong>: Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  15 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Data-driven decision support in high-stakes domains like healthcare and finance faces significant barriers to data sharing due to regulatory, institutional, and privacy concerns. While recent generative AI models, such as large language models, have shown impressive performance in open-domain tasks, their adoption in sensitive environments remains limited by unpredictable behaviors and insufficient privacy-preserving datasets for benchmarking. Existing anonymization methods are often inadequate, especially for unstructured text, as redaction and masking can still allow re-identification. Differential Privacy (DP) offers a principled alternative, enabling the generation of synthetic data with formal privacy assurances. In this work, we address these challenges through three key contributions. First, we introduce a comprehensive evaluation framework with standardized utility and fidelity metrics, encompassing nine curated datasets that capture domain-specific complexities such as technical jargon, long-context dependencies, and specialized document structures. Second, we conduct a large-scale empirical study benchmarking state-of-the-art DP text generation methods and LLMs of varying sizes and different fine-tuning strategies, revealing that high-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Third, we develop a membership inference attack (MIA) methodology tailored for synthetic text, providing first empirical evidence that the use of public datasets - potentially present in pre-training corpora - can invalidate claimed privacy guarantees. Our findings underscore the urgent need for rigorous privacy auditing and highlight persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive, high-stakes settings.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-40] ATLANTIS: AI-driven Threat Localization Analysis and Triage Intelligence System</p>
<p>【速读】：该论文旨在解决自动化漏洞发现与修复中面临的三大核心挑战：在多样化的代码库（从C到Java）中实现可扩展性、在保证广泛覆盖的同时提升检测精度，以及生成语义正确且不破坏程序原有功能的补丁。解决方案的关键在于构建一个融合大语言模型（Large Language Models, LLMs）与程序分析技术的协同系统——ATLANTIS，其通过符号执行（symbolic execution）、定向模糊测试（directed fuzzing）和静态分析（static analysis）相结合的方式，有效弥补了纯AI方法在推理深度和准确性上的不足，并实现了对复杂软件系统的高效安全响应。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14589">https://arxiv.org/abs/2509.14589</a><br>
<strong>作者</strong>: Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Version 1.0 (September 17, 2025). Technical Report. Team Atlanta – 1st place in DARPA AIxCC Final Competition. Project page: <a target="_blank" rel="noopener" href="https://team-atlanta.github.io/">this https URL</a></p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present ATLANTIS, the cyber reasoning system developed by Team Atlanta that won 1st place in the Final Competition of DARPA’s AI Cyber Challenge (AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to build autonomous cyber reasoning systems capable of discovering and patching vulnerabilities at the speed and scale of modern software. ATLANTIS integrates large language models (LLMs) with program analysis – combining symbolic execution, directed fuzzing, and static analysis – to address limitations in automated vulnerability discovery and program repair. Developed by researchers at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the system addresses core challenges: scaling across diverse codebases from C to Java, achieving high precision while maintaining broad coverage, and producing semantically correct patches that preserve intended behavior. We detail the design philosophy, architectural decisions, and implementation strategies behind ATLANTIS, share lessons learned from pushing the boundaries of automated security when program analysis meets modern AI, and release artifacts to support reproducibility and future research.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-41] Can I Trust This Chatbot? Assessing User Privacy in AI-Healthcare Chatbot Applications</p>
<p>【速读】：该论文旨在解决AI healthcare chatbot应用在收集和处理敏感健康数据过程中存在的隐私保护不足问题（privacy concerns in AI healthcare chatbot apps）。其解决方案的关键在于通过三步评估框架——即注册阶段的隐私设置、应用内的隐私控制以及隐私政策内容分析——系统性识别当前主流应用在用户数据保护方面的显著漏洞，从而为信息科学研究人员、开发者及政策制定者提供改进方向，以增强AI healthcare chatbot应用的隐私保障能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14581">https://arxiv.org/abs/2509.14581</a><br>
<strong>作者</strong>: Ramazan Yener,Guan-Hung Chen,Ece Gumusel,Masooda Bashir<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>); Emerging Technologies (<a target="_blank" rel="noopener" href="http://cs.ET">cs.ET</a>)<br>
<strong>备注</strong>:  13 pages. To be published in ASIST 2025 Conference</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As Conversational Artificial Intelligence (AI) becomes more integrated into everyday life, AI-powered chatbot mobile applications are increasingly adopted across industries, particularly in the healthcare domain. These chatbots offer accessible and 24/7 support, yet their collection and processing of sensitive health data present critical privacy concerns. While prior research has examined chatbot security, privacy issues specific to AI healthcare chatbots have received limited attention. Our study evaluates the privacy practices of 12 widely downloaded AI healthcare chatbot apps available on the App Store and Google Play in the United States. We conducted a three-step assessment analyzing: (1) privacy settings during sign-up, (2) in-app privacy controls, and (3) the content of privacy policies. The analysis identified significant gaps in user data protection. Our findings reveal that half of the examined apps did not present a privacy policy during sign up, and only two provided an option to disable data sharing at that stage. The majority of apps’ privacy policies failed to address data protection measures. Moreover, users had minimal control over their personal data. The study provides key insights for information science researchers, developers, and policymakers to improve privacy protections in AI healthcare chatbot apps.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-42] VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models <mark class="hl-label red">IEEE-VIS2025</mark></p>
<p>【速读】：该论文旨在解决视觉-语言（Vision-Language, VL）模型在面对数据分布偏移（distribution shifts）时性能下降的问题，尤其关注实际应用中常见数据损坏（data corruption）对模型鲁棒性的影响。现有方法在评估和提升VL模型鲁棒性方面存在局限，主要源于对模型行为理解不足以及缺乏高效的数据模式探索机制。为此，作者提出VisMoDAl——一个面向VL模型鲁棒性的可视化分析框架，其关键在于通过多层级分析（从特定类型损坏到任务驱动的模型行为与数据切片）支持用户推理不同数据损坏对模型的影响，从而辅助理解模型行为并指导有效数据增强（Data Augmentation, DA）策略的制定。该方案将可视化技术与领域知识深度融合，显著降低了专家干预需求，提升了鲁棒性评估与改进的效率与可解释性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14571">https://arxiv.org/abs/2509.14571</a><br>
<strong>作者</strong>: Huanchen Wang,Wencheng Zhang,Zhiqiang Wang,Zhicong Lu,Yuxin Ma<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  11 pages, 7 figures, 1 table, accepted to IEEE VIS 2025 (IEEE Transactions on Visualization and Computer Graphics)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-43] §rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-<mark class="hl-label green">Agent</mark>  Collaboration</p>
<p>【速读】：该论文旨在解决现有自动工作流构建方法过度依赖历史经验而导致效率低下和适应性不足的问题。其核心解决方案是提出一种先验动态（a priori dynamic）框架，通过Q-table学习优化决策空间以有效利用历史经验，同时结合对当前任务进展的评估，实现对下一执行代理的先验决策，从而为每个任务动态选择更合适的流程结构。此外，引入冷启动初始化、早停机制和剪枝策略进一步提升系统效率，实验证明该方法在多个基准数据集上相较最先进基线平均提升4.05%，且工作流构建与推理成本降低至原有方法的30.68%–48.31%。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14547">https://arxiv.org/abs/2509.14547</a><br>
<strong>作者</strong>: Yi Lin,Lujin Zhao,Yijie Shi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent studies have shown that carefully designed workflows coordinating large language models(LLMs) significantly enhance task-solving capabilities compared to using a single model. While an increasing number of works focus on autonomous workflow construction, most existing approaches rely solely on historical experience, leading to limitations in efficiency and adaptability. We argue that while historical experience is valuable, workflow construction should also flexibly respond to the unique characteristics of each task. To this end, we propose an a priori dynamic framework for automated workflow construction. Our framework first leverages Q-table learning to optimize the decision space, guiding agent decisions and enabling effective use of historical experience. At the same time, agents evaluate the current task progress and make a priori decisions regarding the next executing agent, allowing the system to proactively select the more suitable workflow structure for each given task. Additionally, we incorporate mechanisms such as cold-start initialization, early stopping, and pruning to further improve system efficiency. Experimental evaluations on four benchmark datasets demonstrate the feasibility and effectiveness of our approach. Compared to state-of-the-art baselines, our method achieves an average improvement of 4.05%, while reducing workflow construction and inference costs to only 30.68%-48.31% of those required by existing methods.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-44] Rationality Check! Benchmarking the Rationality of <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】：该论文旨在解决如何系统评估大型语言模型（Large Language Models, LLMs）在理论与实践层面的综合性理性行为问题，即其是否以及在何种条件下表现出类人理性。解决方案的关键在于提出首个涵盖多领域、多LLM的综合理性基准测试（benchmark），该基准包含易于使用的工具包、广泛的实验结果及深入分析，能够揭示LLM在不同情境下与理想人类理性之间的趋同与差异，从而为LLM开发者和使用者提供可量化、可比较的评估基础。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14546">https://arxiv.org/abs/2509.14546</a><br>
<strong>作者</strong>: Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large language models (LLMs), a recent advance in deep learning and machine intelligence, have manifested astonishing capacities, now considered among the most promising for artificial general intelligence. With human-like capabilities, LLMs have been used to simulate humans and serve as AI assistants across many applications. As a result, great concern has arisen about whether and under what circumstances LLMs think and behave like real human agents. Rationality is among the most important concepts in assessing human behavior, both in thinking (i.e., theoretical rationality) and in taking action (i.e., practical rationality). In this work, we propose the first benchmark for evaluating the omnibus rationality of LLMs, covering a wide range of domains and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental results, and analysis that illuminates where LLMs converge and diverge from idealized human rationality. We believe the benchmark can serve as a foundational tool for both developers and users of LLMs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-45] ClearFairy: Capturing Creative Workflows through Decision Structuring In-Situ Questioning and Rationale Inference</p>
<p>【速读】：该论文旨在解决创意工作流中专业人员决策过程难以被完整记录与显式表达的问题，现有方法常导致决策理由不完整或隐性决策未被捕捉，从而阻碍反思、协作与知识共享。其解决方案的关键在于提出CLEAR框架，将推理过程结构化为由行动（actions）、成果物（artifacts）和自我解释（self-explanations）构成的可追踪决策单元，并基于此开发了ClearFairy这一“自言自语”式AI助手，通过检测弱解释、轻量级追问和推断缺失理由来降低知识共享负担。实证研究表明，该方案显著提升了强解释比例（从14%提升至83%），且未增加认知负荷，同时增强了生成式AI代理在Figma中的表现，实现了更符合人类设计意图的下一步动作预测与更连贯的设计输出。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14537">https://arxiv.org/abs/2509.14537</a><br>
<strong>作者</strong>: Kihoon Son,DaEun Choi,Tae Soo Kim,Young-Ho Kim,Sangdoo Yun,Juho Kim<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Human-Computer Interaction (cs.HC); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Capturing professionals’ decision-making in creative workflows is essential for reflection, collaboration, and knowledge sharing, yet existing methods often leave rationales incomplete and implicit decisions hidden. To address this, we present CLEAR framework that structures reasoning into cognitive decision steps-linked units of actions, artifacts, and self-explanations that make decisions traceable. Building on this framework, we introduce ClearFairy, a think-aloud AI assistant for UI design that detects weak explanations, asks lightweight clarifying questions, and infers missing rationales to ease the knowledge-sharing burden. In a study with twelve creative professionals, 85% of ClearFairy’s inferred rationales were accepted, increasing strong explanations from 14% to over 83% of decision steps without adding cognitive demand. The captured steps also enhanced generative AI agents in Figma, yielding next-action predictions better aligned with professionals and producing more coherent design outcomes. For future research on human knowledge-grounded creative AI agents, we release a dataset of captured 417 decision steps.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-46] Leve<mark class="hl-label green">rag</mark> ing Artificial Intelligence as a Strategic Growth Catalyst for Small and Medium-sized Enterprises</p>
<p>【速读】：该论文旨在解决中小企业（SMEs）在人工智能（Artificial Intelligence, AI）快速发展的背景下，如何有效、战略地采纳AI技术以提升竞争力与运营效率的问题。其解决方案的关键在于提供一个结构化的实施框架，涵盖AI基础认知、基于市场数据的商业价值论证、具体应用场景分析以及分阶段可执行的采纳策略，从而帮助SME领导者将AI转化为可持续增长的核心驱动力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14532">https://arxiv.org/abs/2509.14532</a><br>
<strong>作者</strong>: Oluwatosin Agbaakin(Indiana University Indianapolis)<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); General Economics (<a target="_blank" rel="noopener" href="http://econ.GN">econ.GN</a>)<br>
<strong>备注</strong>:  14 pages, 2 figures. A review and strategic framework for AI adoption in SMEs</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Artificial Intelligence (AI) has transitioned from a futuristic concept reserved for large corporations to a present-day, accessible, and essential growth lever for Small and Medium-sized Enterprises (SMEs). For entrepreneurs and business leaders, strategic AI adoption is no longer an option but an imperative for competitiveness, operational efficiency, and long-term survival. This report provides a comprehensive framework for SME leaders to navigate this technological shift, offering the foundational knowledge, business case, practical applications, and strategic guidance necessary to harness the power of AI. The quantitative evidence supporting AI adoption is compelling; 91% of SMEs using AI report that it directly boosts their revenue. Beyond top-line growth, AI drives profound operational efficiencies, with studies showing it can reduce operational costs by up to 30% and save businesses more than 20 hours of valuable time each month. This transformation is occurring within the context of a seismic economic shift; the global AI market is projected to surge from  233.46 Billion in 2024 to an astonishing  1.77 Trillion by 2032. This paper demystifies the core concepts of AI, presents a business case based on market data, details practical applications, and lays out a phased, actionable adoption strategy.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-47] BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning</p>
<p>【速读】：该论文旨在解决传统静态分析方法在应对现代复杂恶意软件（如采用代码混淆、多态性等逃避技术的样本）时检测效果不佳的问题。其解决方案的关键在于提出一种名为BEACON的深度学习框架，该框架利用大语言模型（Large Language Models, LLMs）从沙箱生成的原始行为报告中提取密集且具有上下文语义信息的嵌入表示（embeddings），并结合一维卷积神经网络（1D CNN）实现多类恶意软件分类，从而显著提升检测的准确性和鲁棒性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14519">https://arxiv.org/abs/2509.14519</a><br>
<strong>作者</strong>: Wadduwage Shanika Perera,Haodi Jiang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Malware is becoming increasingly complex and widespread, making it essential to develop more effective and timely detection methods. Traditional static analysis often fails to defend against modern threats that employ code obfuscation, polymorphism, and other evasion techniques. In contrast, behavioral malware detection, which monitors runtime activities, provides a more reliable and context-aware solution. In this work, we propose BEACON, a novel deep learning framework that leverages large language models (LLMs) to generate dense, contextual embeddings from raw sandbox-generated behavior reports. These embeddings capture semantic and structural patterns of each sample and are processed by a one-dimensional convolutional neural network (1D CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public CAPE Dataset, our framework consistently outperforms existing methods, highlighting the effectiveness of LLM-based behavioral embeddings and the overall design of BEACON for robust malware classification.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-48] Beyond the high score: Prosocial ability profiles of multi-<mark class="hl-label green">agent</mark>  populations</p>
<p>【速读】：该论文旨在解决如何准确评估人工智能（AI）代理在复杂社会环境中协作能力的问题，特别是针对现有评估框架难以量化抽象社会行为（如遵循惯例）和潜在偏倚的挑战。其解决方案的关键在于引入贝叶斯方法中的测量布局（Measurement Layouts），通过推断多智能体系统的功能能力剖面，不仅提升了对Melting Pot竞赛中未来表现的预测准确性，还揭示了代理内在的亲社会能力（prosocial abilities）。这一方法为理解不同测试环境对协作能力评估的影响提供了可解释的洞察，并指出当前评价体系可能存在的漏洞，从而推动更透明、泛化性更强的社会AI评估范式发展。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14485">https://arxiv.org/abs/2509.14485</a><br>
<strong>作者</strong>: Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The development and evaluation of social capabilities in AI agents require complex environments where competitive and cooperative behaviours naturally emerge. While game-theoretic properties can explain why certain teams or agent populations outperform others, more abstract behaviours, such as convention following, are harder to control in training and evaluation settings. The Melting Pot contest is a social AI evaluation suite designed to assess the cooperation capabilities of AI systems. In this paper, we apply a Bayesian approach known as Measurement Layouts to infer the capability profiles of multi-agent systems in the Melting Pot contest. We show that these capability profiles not only predict future performance within the Melting Pot suite but also reveal the underlying prosocial abilities of agents. Our analysis indicates that while higher prosocial capabilities sometimes correlate with better performance, this is not a universal trend-some lower-scoring agents exhibit stronger cooperation abilities. Furthermore, we find that top-performing contest submissions are more likely to achieve high scores in scenarios where prosocial capabilities are not required. These findings, together with reports that the contest winner used a hard-coded solution tailored to specific environments, suggest that at least one top-performing team may have optimised for conditions where cooperation was not necessary, potentially exploiting limitations in the evaluation framework. We provide recommendations for improving the annotation of cooperation demands and propose future research directions to account for biases introduced by different testing environments. Our results demonstrate that Measurement Layouts offer both strong predictive accuracy and actionable insights, contributing to a more transparent and generalisable approach to evaluating AI systems in complex social settings.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-49] From Mimicry to True Intelligence (TI) - A New Paradigm for Artificial General Intelligence</p>
<p>【速读】：该论文试图解决当前人工智能领域对人工通用智能（Artificial General Intelligence, AGI）定义模糊、研究方向分散的问题，尤其是现有基于性能表现的定义无法提供机制层面的清晰研发路径，也未能界定真正智能的质性特征。其解决方案的关键在于提出一种以认知架构为核心的全新范式——将AGI的研究焦点从外部行为模仿转向内部认知机制的构建，并首次提出“真正智能”（True Intelligence, TI）的概念，将其定义为包含六个核心组件的系统：具身感知融合、核心指令、动态图式生成、高度互联的多专家架构、协调层以及不可测量的“互联性”（Interconnectedness），后者被假定为意识与主观体验的来源。论文进一步设计了一个五级可量化分类体系，依据系统实现前五个可测组件的数量划分AGI发展阶段，从而为AGI研究提供了明确的阶段性目标和可操作的发展路线。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14474">https://arxiv.org/abs/2509.14474</a><br>
<strong>作者</strong>: Meltem Subasioglu,Nevzat Subasioglu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Computers and Society (<a target="_blank" rel="noopener" href="http://cs.CY">cs.CY</a>)<br>
<strong>备注</strong>:  27 pages, 1 figure</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The debate around Artificial General Intelligence (AGI) remains open due to two fundamentally different goals: replicating human-like performance versus replicating human-like cognitive processes. We argue that current performance-based definitions are inadequate because they provide no clear, mechanism-focused roadmap for research, and they fail to properly define the qualitative nature of genuine intelligence. Drawing inspiration from the human brain, we propose a new paradigm that shifts the focus from external mimicry to the development of foundational cognitive architectures. We define True Intelligence (TI) as a system characterized by six core components: embodied sensory fusion, core directives, dynamic schemata creation, a highly-interconnected multi-expert architecture, an orchestration layer, and lastly, the unmeasurable quality of Interconnectedness, which we hypothesize results in consciousness and a subjective experience. We propose a practical, five-level taxonomy of AGI based on the number of the first five measurable components a system exhibits. This framework provides a clear path forward with developmental milestones that directly address the challenge of building genuinely intelligent systems. We contend that once a system achieves Level-5 AGI by implementing all five measurable components, the difference between it and TI remains as a purely philosophical debate. For practical purposes - and given theories indicate consciousness is an emergent byproduct of integrated, higher-order cognition - we conclude that a fifth-level AGI is functionally and practically equivalent to TI. This work synthesizes diverse insights from analytical psychology, schema theory, metacognition, modern brain architectures and latest works in AI to provide the first holistic, mechanism-based definition of AGI that offers a clear and actionable path for the research community.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-50] VCBench: Benchmarking <mark class="hl-label green">LLM</mark> s in Venture Capital</p>
<p>【速读】：该论文旨在解决早期风险投资（VC）中创始人成功预测的难题，该领域信号稀疏、结果不确定，且即使顶级投资者表现也仅处于中等水平。解决方案的关键在于提出首个专门针对VC领域创始人成功率预测的基准测试工具VCBench，其包含9,000个匿名化创始人档案，通过标准化处理保留预测特征并显著降低身份泄露风险（对抗测试显示重识别风险下降超过90%），同时评估了九种先进大语言模型（LLMs）的表现，其中DeepSeek-V3在精度上超越基线六倍以上，GPT-4o在F0.5指标上最优，多数模型性能优于人类基准，从而为AGI在早期创业预测中的可复现与隐私保护评估建立社区驱动的标准。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14448">https://arxiv.org/abs/2509.14448</a><br>
<strong>作者</strong>: Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets accelerate progress toward artificial general intelligence (AGI). We introduce VCBench, the first benchmark for predicting founder success in venture capital (VC), a domain where signals are sparse, outcomes are uncertain, and even top investors perform modestly. At inception, the market index achieves a precision of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1 firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles, standardized to preserve predictive features while resisting identity leakage, with adversarial tests showing more than 90% reduction in re-identification risk. We evaluate nine state-of-the-art large language models (LLMs). DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the highest F0.5, and most models surpass human benchmarks. Designed as a public and evolving resource available at this http URL, VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in early-stage venture forecasting.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-51] When Content is Goliath and Algorithm is David: The Style and Semantic Effects of Generative Search Engine</p>
<p>【速读】：该论文旨在解决生成式搜索引擎（Generative Search Engines, GEs）如何通过大语言模型（Large Language Models, LLMs）影响网站内容的引用偏好及其对用户信息获取效果的问题。其核心问题是：GEs在生成摘要时为何倾向于选择特定类型的网页内容，以及这种选择机制如何影响最终用户的任务表现与信息多样性。解决方案的关键在于识别出LLMs内在的生成倾向——即更偏好语义一致性高、可预测性强的内容，并通过检索增强生成（Retrieval-Augmented Generation, RAG）API的受控实验验证这一机制；进一步发现，网站所有者对内容进行LLM辅助优化后，虽看似强化了内容适配性，却意外提升了AI摘要的信息多样性，从而在不同教育水平用户群体中产生差异化影响：高学历用户完成任务时间显著缩短，而低学历用户则获得更高信息密度。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14436">https://arxiv.org/abs/2509.14436</a><br>
<strong>作者</strong>: Lijia Ma,Juan Qin,Xingchen Xu,Yong Tan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  59 pages, 6 figures, 20 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Generative search engines (GEs) leverage large language models (LLMs) to deliver AI-generated summaries with website citations, establishing novel traffic acquisition channels while fundamentally altering the search engine optimization landscape. To investigate the distinctive characteristics of GEs, we collect data through interactions with Google’s generative and conventional search platforms, compiling a dataset of approximately ten thousand websites across both channels. Our empirical analysis reveals that GEs exhibit preferences for citing content characterized by significantly higher predictability for underlying LLMs and greater semantic similarity among selected sources. Through controlled experiments utilizing retrieval augmented generation (RAG) APIs, we demonstrate that these citation preferences emerge from intrinsic LLM tendencies to favor content aligned with their generative expression patterns. Motivated by applications of LLMs to optimize website content, we conduct additional experimentation to explore how LLM-based content polishing by website proprietors alters AI summaries, finding that such polishing paradoxically enhances information diversity within AI summaries. Finally, to assess the user-end impact of LLM-induced information increases, we design a generative search engine and recruit Prolific participants to conduct a randomized controlled experiment involving an information-seeking and writing task. We find that higher-educated users exhibit minimal changes in their final outputs’ information diversity but demonstrate significantly reduced task completion time when original sites undergo polishing. Conversely, lower-educated users primarily benefit through enhanced information density in their task outputs while maintaining similar completion times across experimental groups.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-52] Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】：该论文旨在解决将基于RoPE（Rotary Position Embedding）的位置插值（Position Interpolation, PI）方法与后训练量化（Post-Training Quantization, PTQ）结合时导致的性能下降问题。具体而言，PI方法虽能在不重新训练模型的前提下扩展大语言模型（LLM）的上下文窗口长度，但与PTQ协同使用时会引入位置相关的对数概率噪声（logit noise），其根源在于长上下文混叠（long context aliasing）、动态范围膨胀（dynamic range dilation）、轴网格各向异性（axis grid anisotropy）和异常值偏移（outlier shifting）等耦合效应。解决方案的关键是提出Q-ROAR——一种RoPE感知的、仅调整权重（weight-only）的稳定化方法：它将RoPE维度分组为少数频段，并对每个频段独立搜索最优缩放因子（per-band scale）用于查询（W_Q）和键（W_K）矩阵，同时提供对称变体以保持对数概率尺度一致性。该方法通过两个新诊断指标（Interpolation Pressure 和 Tail Inflation Ratios）指导搜索过程，仅需少量长上下文开发集数据，无需微调、内核或架构改动，即可显著恢复准确率（最高达0.7%）并降低GovReport困惑度超10%，且不影响短上下文性能和现有推理栈兼容性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14391">https://arxiv.org/abs/2509.14391</a><br>
<strong>作者</strong>: Ye Qiao,Sitao Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Extending LLM context windows is crucial for long range tasks. RoPE-based position interpolation (PI) methods like linear and frequency-aware scaling extend input lengths without retraining, while post-training quantization (PTQ) enables practical deployment. We show that combining PI with PTQ degrades accuracy due to coupled effects long context aliasing, dynamic range dilation, axis grid anisotropy, and outlier shifting that induce position-dependent logit noise. We provide the first systematic analysis of PI plus PTQ and introduce two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity) and Tail Inflation Ratios (outlier shift from short to long contexts). To address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that groups RoPE dimensions into a few frequency bands and performs a small search over per-band scales for W_Q,W_K, with an optional symmetric variant to preserve logit scale. The diagnostics guided search uses a tiny long-context dev set and requires no fine-tuning, kernel, or architecture changes. Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces GovReport perplexity by more than 10%, while preserving short-context performance and compatibility with existing inference stacks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-53] IQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations</p>
<p>【速读】：该论文旨在解决嵌入式边缘环境中AI推理性能评估与实际应用脱节的问题，即以峰值算力（TOPS）作为性能指标无法准确反映真实场景下的效率，且往往与更高的硅成本正相关。其解决方案的关键在于通过提升计算利用率来实现高效能比，而非单纯堆砌硬件资源；具体而言，提出了一种灵活的数据驱动型NPU架构（eIQ Neutron）并搭配约束编程的编译器算法，根据工作负载特征动态优化计算调度与数据流动，在同等TOPS和内存资源下相较主流嵌入式NPU平均提速1.8倍（峰值4倍），甚至在计算与内存资源翻倍的对手方案中仍可达到最高3.3倍的性能优势。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14388">https://arxiv.org/abs/2509.14388</a><br>
<strong>作者</strong>: Lennart Bamberg,Filippo Minnella,Roberto Bosio,Fabrizio Ottati,Yuebin Wang,Jongmin Lee,Luciano Lavagno,Adam Fuks<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Hardware Architecture (<a target="_blank" rel="noopener" href="http://cs.AR">cs.AR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  Submitted to IEEE Transactions on Computers</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Neural Processing Units (NPUs) are key to enabling efficient AI inference in resource-constrained edge environments. While peak tera operations per second (TOPS) is often used to gauge performance, it poorly reflects real-world performance and typically rather correlates with higher silicon cost. To address this, architects must focus on maximizing compute utilization, without sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU, integrated into a commercial flagship MPU, alongside co-designed compiler algorithms. The architecture employs a flexible, data-driven design, while the compiler uses a constrained programming approach to optimize compute and data movement based on workload characteristics. Compared to the leading embedded NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x peak) at equal TOPS and memory resources across standard AI-benchmarks. Even against NPUs with double the compute and memory resources, Neutron delivers up to 3.3x higher performance.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-54] Detecting Pipeline Failures through Fine-Grained Analysis of Web <mark class="hl-label green">Agents</mark></p>
<p>【速读】：该论文旨在解决当前对基于大语言模型（Large Language Models, LLMs）的Web代理（Web agents）评估过于依赖整体成功率、忽视中间步骤错误的问题，从而限制了对失败模式的深入理解与系统性改进。其解决方案的关键在于提出一个模块化的评估框架，将代理的处理流程分解为可解释的阶段，实现细粒度的错误诊断；以SeeAct框架和Mind2Web数据集为例，验证该方法能够揭示标准指标未能捕捉到的可行动弱点，为构建更鲁棒且泛化能力更强的Web代理提供支持。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14382">https://arxiv.org/abs/2509.14382</a><br>
<strong>作者</strong>: Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Web agents powered by large language models (LLMs) can autonomously perform complex, multistep tasks in dynamic web environments. However, current evaluations mostly focus on the overall success while overlooking intermediate errors. This limits insight into failure modes and hinders systematic improvement. This work analyzes existing benchmarks and highlights the lack of fine-grained diagnostic tools. To address this gap, we propose a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis. Using the SeeAct framework and the Mind2Web dataset as a case study, we show how this approach reveals actionable weaknesses missed by standard metrics - paving the way for more robust and generalizable web agents.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-55] DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion</p>
<p>【速读】：该论文旨在解决如何高效学习自主全身体型人形机器人技能的问题，特别是针对同时控制上下肢并完成物体交互的复杂任务。其解决方案的关键在于提出了一种名为DreamControl的新方法，该方法融合了扩散模型（diffusion models）与强化学习（Reinforcement Learning, RL）的优势：通过在人类运动数据上训练的扩散先验（diffusion prior）为RL策略提供初始引导，使RL能够在仿真环境中更有效地探索和发现原本难以通过直接RL获得的高质量动作策略，同时扩散模型本身具备生成自然流畅运动的能力，从而显著提升从仿真到现实世界的迁移性能。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14353">https://arxiv.org/abs/2509.14353</a><br>
<strong>作者</strong>: Dvij Kalaria,Sudarshan S Harithas,Pushkal Katara,Sangkyung Kwak,Sarthak Bhagat,Shankar Sastry,Srinath Sridhar,Sai Vemprala,Ashish Kapoor,Jonathan Chung-Kuan Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  (under submission)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce DreamControl, a novel methodology for learning autonomous whole-body humanoid skills. DreamControl leverages the strengths of diffusion models and Reinforcement Learning (RL): our core innovation is the use of a diffusion prior trained on human motion data, which subsequently guides an RL policy in simulation to complete specific tasks of interest (e.g., opening a drawer or picking up an object). We demonstrate that this human motion-informed prior allows RL to discover solutions unattainable by direct RL, and that diffusion models inherently promote natural looking motions, aiding in sim-to-real transfer. We validate DreamControl’s effectiveness on a Unitree G1 robot across a diverse set of challenging tasks involving simultaneous lower and upper body control and object interaction.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-56] Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning</p>
<p>【速读】：该论文旨在解决5G及未来无线接入网络中动态网络状态下的服务质量（QoS）优化问题，具体包括时变无线信道条件、用户移动性、流量波动以及用户需求变化等因素对MAC层资源分配的影响。解决方案的关键在于提出了一种名为xSlice的在线学习算法，其核心是将QoS优化建模为后悔最小化问题，并采用深度强化学习（DRL）框架结合Actor-Critic模型，以融合值函数和策略梯度更新的优势；同时引入图卷积网络（GCN）进行RAN数据的图嵌入，使系统能够处理动态数量的业务会话，从而实现自适应的资源调度。实验表明，该方案相比现有最优方法可降低67%的性能后悔值。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14343">https://arxiv.org/abs/2509.14343</a><br>
<strong>作者</strong>: Peihao Yan,Jie Lu,Huacheng Zeng,Y. Thomas Hou<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ystems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and beyond radio access networks. This paper presents an xApp called xSlice for the Near-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice is an online learning algorithm that adaptively adjusts MAC-layer resource allocation in response to dynamic network states, including time-varying wireless channel conditions, user mobility, traffic fluctuations, and changes in user demand. To address these network dynamics, we first formulate the Quality-of-Service (QoS) optimization problem as a regret minimization problem by quantifying the QoS demands of all traffic sessions through weighting their throughput, latency, and reliability. We then develop a deep reinforcement learning (DRL) framework that utilizes an actor-critic model to combine the advantages of both value-based and policy-based updating methods. A graph convolutional network (GCN) is incorporated as a component of the DRL framework for graph embedding of RAN data, enabling xSlice to handle a dynamic number of traffic sessions. We have implemented xSlice on an O-RAN testbed with 10 smartphones and conducted extensive experiments to evaluate its performance in realistic scenarios. Experimental results show that xSlice can reduce performance regret by 67% compared to the state-of-the-art solutions. Source code is available on GitHub [1].<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-57] Beyond Classification: Evaluating <mark class="hl-label green">LLM</mark> s for Fine-Grained Automatic Malware Behavior Auditing</p>
<p>【速读】：该论文旨在解决Android恶意软件行为审计中缺乏因果性与可验证解释的问题，即如何在复杂且框架密集的应用程序中准确识别并证明恶意行为的来源和机制。当前自动化恶意软件分类虽具备高检测性能，但难以提供可信的审计依据，而大型语言模型（Large Language Models, LLMs）虽具潜力，却受限于标注数据稀缺、良性代码干扰以及输出不可验证等挑战。解决方案的关键在于提出MalEval框架，其核心包括：(1) 提供专家验证的行为报告与更新的敏感API列表以缓解真实标签不足问题；(2) 利用静态可达性分析减少噪声干扰；(3) 以函数级结构表示作为可验证的归因单元，并定义四个面向分析师的任务（函数优先排序、证据归因、行为合成与样本区分）及配套领域指标，构建统一的工作负载评分体系。该框架首次系统评估了LLMs在现实约束下的审计能力，为未来基于LLM的恶意行为审计研究提供了可复现基准。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14335">https://arxiv.org/abs/2509.14335</a><br>
<strong>作者</strong>: Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Software Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Automated malware classification has achieved strong detection performance. Yet, malware behavior auditing seeks causal and verifiable explanations of malicious activities – essential not only to reveal what malware does but also to substantiate such claims with evidence. This task is challenging, as adversarial intent is often hidden within complex, framework-heavy applications, making manual auditing slow and costly. Large Language Models (LLMs) could help address this gap, but their auditing potential remains largely unexplored due to three limitations: (1) scarce fine-grained annotations for fair assessment; (2) abundant benign code obscuring malicious signals; and (3) unverifiable, hallucination-prone outputs undermining attribution credibility. To close this gap, we introduce MalEval, a comprehensive framework for fine-grained Android malware auditing, designed to evaluate how effectively LLMs support auditing under real-world constraints. MalEval provides expert-verified reports and an updated sensitive API list to mitigate ground truth scarcity and reduce noise via static reachability analysis. Function-level structural representations serve as intermediate attribution units for verifiable evaluation. Building on this, we define four analyst-aligned tasks – function prioritization, evidence attribution, behavior synthesis, and sample discrimination – together with domain-specific metrics and a unified workload-oriented score. We evaluate seven widely used LLMs on a curated dataset of recent malware and misclassified benign apps, offering the first systematic assessment of their auditing capabilities. MalEval reveals both promising potential and critical limitations across audit stages, providing a reproducible benchmark and foundation for future research on LLM-enhanced malware behavior auditing. MalEval is publicly available at this https URL<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-58] Deploying UDM Series in Real-Life Stuttered Speech Applications: A Clinical Evaluation Framework</p>
<p>【速读】：该论文旨在解决 stuttered and dysfluent speech detection 系统中长期存在的准确性与临床可解释性之间的权衡问题。传统端到端深度学习模型虽性能优异，但其黑箱特性限制了在临床环境中的应用。解决方案的关键在于提出 Unconstrained Dysfluency Modeling (UDM) 框架，该框架通过模块化架构、显式音素对齐（explicit phoneme alignment）以及可解释的输出结果，实现了高精度（F1: 0.89±0.04）与临床可理解性的统一，从而显著提升了临床医生接受度（87%）并缩短诊断时间（减少34%）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14304">https://arxiv.org/abs/2509.14304</a><br>
<strong>作者</strong>: Eric Zhang,Li Wei,Sarah Chen,Michael Wang(SSHealth Team, AI for Healthcare Laboratory)<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: ound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Stuttered and dysfluent speech detection systems have traditionally suffered from the trade-off between accuracy and clinical interpretability. While end-to-end deep learning models achieve high performance, their black-box nature limits clinical adoption. This paper looks at the Unconstrained Dysfluency Modeling (UDM) series-the current state-of-the-art framework developed by Berkeley that combines modular architecture, explicit phoneme alignment, and interpretable outputs for real-world clinical deployment. Through extensive experiments involving patients and certified speech-language pathologists (SLPs), we demonstrate that UDM achieves state-of-the-art performance (F1: 0.89±0.04) while providing clinically meaningful interpretability scores (4.2/5.0). Our deployment study shows 87% clinician acceptance rate and 34% reduction in diagnostic time. The results provide strong evidence that UDM represents a practical pathway toward AI-assisted speech therapy in clinical environments.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-59] FlowDrive: Energy Flow Field for End-to-End Autonomous Driving</p>
<p>【速读】：该论文旨在解决当前端到端自动驾驶框架中BEV（Bird’s-Eye View）特征学习缺乏显式风险建模与引导先验的问题，从而导致运动规划在安全性与可解释性方面的不足。现有方法通常依赖隐式学习的BEV特征，难以明确表达几何障碍物带来的硬约束和车道边界等规则性语义的软约束。解决方案的关键在于提出FlowDrive框架，通过引入物理可解释的能量场——包括风险势能场（risk potential field）和车道吸引力场（lane attraction field），将语义先验与安全提示显式编码至BEV空间；同时采用条件扩散规划器结合特征级门控机制，解耦意图预测与轨迹去噪任务，实现轨迹生成的自适应优化与多模态多样性提升。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14303">https://arxiv.org/abs/2509.14303</a><br>
<strong>作者</strong>: Hao Jiang,Zhipeng Zhang,Yu Gao,Zhigang Sun,Yiru Wang,Yuwen Heng,Shuo Wang,Jinhao Chai,Zhuo Chen,Hao Zhao,Hao Sun,Xi Zhang,Anqing Jiang,Chuan Hu<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in end-to-end autonomous driving leverage multi-view images to construct BEV representations for motion planning. In motion planning, autonomous vehicles need considering both hard constraints imposed by geometrically occupied obstacles (e.g., vehicles, pedestrians) and soft, rule-based semantics with no explicit geometry (e.g., lane boundaries, traffic priors). However, existing end-to-end frameworks typically rely on BEV features learned in an implicit manner, lacking explicit modeling of risk and guidance priors for safe and interpretable planning. To address this, we propose FlowDrive, a novel framework that introduces physically interpretable energy-based flow fields-including risk potential and lane attraction fields-to encode semantic priors and safety cues into the BEV space. These flow-aware features enable adaptive refinement of anchor trajectories and serve as interpretable guidance for trajectory generation. Moreover, FlowDrive decouples motion intent prediction from trajectory denoising via a conditional diffusion planner with feature-level gating, alleviating task interference and enhancing multimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that FlowDrive achieves state-of-the-art performance with an EPDMS of 86.3, surpassing prior baselines in both safety and planning quality. The project is available at this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-60] SCoGen: Scenario-Centric Graph-Based Synthesis of Real-World Code Problems</p>
<p>【速读】：该论文旨在解决代码大语言模型（Code Large Language Models）在进一步发展过程中面临的现实编码问题数据稀缺问题。其解决方案的关键在于提出了一种新颖的代码问题合成框架，该框架通过从真实世界编程数据集（如 Stack Overflow 和 Kaggle）中系统性地提取领域知识（Domain Knowledge）、领域技能（Domain Skills）和编码技能（Coding Skills），构建一个以应用场景为中心的图结构，从而实现对复杂性和多样性的有效控制，并生成贴近实际应用的代码问题。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14281">https://arxiv.org/abs/2509.14281</a><br>
<strong>作者</strong>: Xifeng Yao,Dongyu Lang,Wu Zhang,Xintong Guo,Huarui Xie,Yinhao Ni,Ping Liu,Guang Shen,Yi Bai,Dandan Tu,Changzheng Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Significant advancements have been made in the capabilities of code large language models, leading to their rapid adoption and application across a wide range of domains. However, their further advancements are often constrained by the scarcity of real-world coding problems. To bridge this gap, we propose a novel framework for synthesizing code problems that emulate authentic real-world scenarios. This framework systematically integrates domain knowledge, domain skills, and coding skills, all of which are meticulously extracted from real-world programming-related datasets, including Stack Overflow and Kaggle. The extracted elements serve as the foundational building blocks for constructing code problems. To align the generated problems with practical applications, application scenarios are also mined from the aforementioned datasets. These scenarios are then utilized to construct a scenario-centric graph that interconnects domain knowledge, domain skills, and coding skills. Based on this structured representation, a sampling strategy on the graph is designed, which effectively controls the generation of a code problem with complexity and diversity, reflects real-world challenges. Experimental results demonstrate that the proposed method consistently achieves superior performance over state-of-the-art open-source large language models of varying sizes and functionalities, including both coders and general-purpose models, across a diverse set of real-world benchmarks.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-61] owards Robust <mark class="hl-label green">Agent</mark> ic CUDA Kernel Benchmarking Verification and Optimization</p>
<p>【速读】：该论文旨在解决当前大语言模型（Large Language Models, LLMs）在软件工程任务中对低级CUDA内核实现优化关注不足，以及现有内核生成基准测试存在可利用漏洞和测试场景多样性不足的问题。解决方案的关键在于提出一个名为robust-kbench的新基准，用于在多样化场景下严格评估CUDA内核的性能与正确性，并构建一个完整的代理式框架，自动化完成CUDA内核的发现、验证与优化流程。该框架通过将PyTorch代码翻译为等效CUDA内核，并结合基于LLM的验证器与一种针对CUDA生态设计的进化元生成策略，实现运行时性能迭代提升，最终在robust-kbench上生成的内核在实际应用（如前向和反向传播）中优于原生torch实现，具备操作融合与多种运行时优化策略部署能力。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14279">https://arxiv.org/abs/2509.14279</a><br>
<strong>作者</strong>: Robert Tjarko Lange,Qi Sun,Aaditya Prasad,Maxence Faldor,Yujin Tang,David Ha<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  62 pages, 10 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Recent advances in large language models (LLMs) demonstrate their effectiveness in scaling test-time compute for software engineering tasks. However, these approaches often focus on high-level solutions, with limited attention to optimizing low-level CUDA kernel implementations. Additionally, existing kernel generation benchmarks suffer from exploitable loopholes and insufficient diversity in testing conditions, hindering true generalization assessment. To address these limitations, we introduce robust-kbench, a new benchmark for rigorous evaluation of kernel performance and correctness across varied scenarios. Furthermore, we present a comprehensive agentic framework that automates CUDA kernel discovery, verification, and optimization. This pipeline enables frontier LLMs to translate torch code to CUDA kernels and iteratively improve their runtime within our robust evaluation setting. Our sequential workflow first translates PyTorch code into equivalent CUDA kernels. It then optimizes their runtime using a novel evolutionary meta-generation procedure tailored to the CUDA ecosystem, guided by LLM-based verifiers for correctness and efficient filtering. Evaluated on robust-kbench, our approach produces CUDA kernels outperforming torch implementations for practical applications, including forward and backward passes. It can fuse operations and deploy various runtime optimization strategies. The verifier workflow accurately classifies incorrect kernels, enhancing hardware verification efficiency.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-62] Beyond Data Privacy: New Privacy Risks for <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】：该论文旨在解决大型语言模型（Large Language Models, LLMs）在部署阶段引发的新型隐私风险问题，特别是由于其广泛集成于应用系统及自主能力被恶意利用所导致的数据泄露与大规模隐私攻击威胁。解决方案的关键在于系统性地识别和分析这些新兴隐私漏洞，并提出针对性的缓解策略，同时呼吁研究社区将关注点从训练阶段的数据隐私扩展至部署阶段的安全防护，以应对日益强大的LLM及其赋能系统所带来的动态演化威胁。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14278">https://arxiv.org/abs/2509.14278</a><br>
<strong>作者</strong>: Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) have achieved remarkable progress in natural language understanding, reasoning, and autonomous decision-making. However, these advancements have also come with significant privacy concerns. While significant research has focused on mitigating the data privacy risks of LLMs during various stages of model training, less attention has been paid to new threats emerging from their deployment. The integration of LLMs into widely used applications and the weaponization of their autonomous abilities have created new privacy vulnerabilities. These vulnerabilities provide opportunities for both inadvertent data leakage and malicious exfiltration from LLM-powered systems. Additionally, adversaries can exploit these systems to launch sophisticated, large-scale privacy attacks, threatening not only individual privacy but also financial security and societal trust. In this paper, we systematically examine these emerging privacy risks of LLMs. We also discuss potential mitigation strategies and call for the research community to broaden its focus beyond data privacy risks, developing new defenses to address the evolving threats posed by increasingly powerful LLMs and LLM-powered systems.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-63] Constructive Conflict-Driven Multi-<mark class="hl-label green">Agent</mark>  Reinforcement Learning for Strategic Diversity <mark class="hl-label red">IJCAI2025</mark></p>
<p>【速读】：该论文旨在解决多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）中因过度依赖个体智能体特征而忽视智能体间相互作用与影响的问题，从而导致策略多样性不足和协作效率受限。其解决方案的关键在于提出一种名为“通过建设性冲突实现竞争多样性”（Competitive Diversity through Constructive Conflict, CoDiCon）的新机制：通过引入基于排名特征的内在奖励模块，在合作场景中嵌入竞争激励，促使智能体之间进行策略交换并形成更具适应性的多样化策略；该模块通过集中式优化以最大化环境奖励，将原问题转化为一个约束型双层优化问题，并有效平衡了竞争与合作之间的关系，实验证明该方法在SMAC和GRF环境中显著优于现有先进算法。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14276">https://arxiv.org/abs/2509.14276</a><br>
<strong>作者</strong>: Yuxiang Mai,Qiyue Yin,Wancheng Ni,Pei Xu,Kaiqi Huang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted by IJCAI 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In recent years, diversity has emerged as a useful mechanism to enhance the efficiency of multi-agent reinforcement learning (MARL). However, existing methods predominantly focus on designing policies based on individual agent characteristics, often neglecting the interplay and mutual influence among agents during policy formation. To address this gap, we propose Competitive Diversity through Constructive Conflict (CoDiCon), a novel approach that incorporates competitive incentives into cooperative scenarios to encourage policy exchange and foster strategic diversity among agents. Drawing inspiration from sociological research, which highlights the benefits of moderate competition and constructive conflict in group decision-making, we design an intrinsic reward mechanism using ranking features to introduce competitive motivations. A centralized intrinsic reward module generates and distributes varying reward values to agents, ensuring an effective balance between competition and cooperation. By optimizing the parameterized centralized reward module to maximize environmental rewards, we reformulate the constrained bilevel optimization problem to align with the original task objectives. We evaluate our algorithm against state-of-the-art methods in the SMAC and GRF environments. Experimental results demonstrate that CoDiCon achieves superior performance, with competitive intrinsic rewards effectively promoting diverse and adaptive strategies among cooperative agents.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-64] Discovering New Theorems via <mark class="hl-label green">LLM</mark> s with In-Context Proof Learning in Lean</p>
<p>【速读】：该论文旨在解决大型语言模型（Large Language Models, LLMs）在数学定理证明中仅限于求解已有问题、缺乏发现新定理能力的问题。其解决方案的关键在于提出了一种“猜想-证明循环”（Conjecturing-Proving Loop）框架，通过在上下文中引入先前生成的定理及其证明，使LLM能够基于提示学习（in-context learning）逐步生成并证明更复杂的定理，而无需调整模型参数。该方法实现了对已发表但尚未形式化定理的重新发现，并验证了上下文学习在神经定理证明中的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14274">https://arxiv.org/abs/2509.14274</a><br>
<strong>作者</strong>: Kazumi Kasaura,Naoto Onda,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Machine Learning (cs.LG); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  11 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models have demonstrated significant promise in formal theorem proving. However, previous works mainly focus on solving existing problems. In this paper, we focus on the ability of LLMs to find novel theorems. We propose Conjecturing-Proving Loop pipeline for automatically generating mathematical conjectures and proving them in Lean 4 format. A feature of our approach is that we generate and prove further conjectures with context including previously generated theorems and their proofs, which enables the generation of more difficult proofs by in-context learning of proof strategies without changing parameters of LLMs. We demonstrated that our framework rediscovered theorems with verification, which were published in past mathematical papers and have not yet formalized. Moreover, at least one of these theorems could not be proved by the LLM without in-context learning, even in natural language, which means that in-context learning was effective for neural theorem proving. The source code is available at this https URL.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-65] Evolution of Kernels: Automated RISC-V Kernel Optimization with <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
<p>【速读】：该论文旨在解决在参考资源稀缺的新兴硬件平台（如RISC-V）中自动化内核设计面临的挑战，尤其是在缺乏成熟代码库和技术文档的情况下，大型语言模型（LLM）难以有效进行内核优化的问题。其解决方案的关键在于提出了一种名为EoK（Evolution of Kernels）的基于LLM的进化程序搜索框架：通过挖掘已建立内核库开发历史中的可复用优化思想（即通用设计原则与可操作性思路），并结合检索增强生成（Retrieval-Augmented Generation, RAG）引入RISC-V特定上下文信息，从而引导并优先选择历史上已被验证有效的技术策略，实现对稀疏参考场景下的高效内核自动设计。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14265">https://arxiv.org/abs/2509.14265</a><br>
<strong>作者</strong>: Siyuan Chen,Zhichao Lu,Qingfu Zhang<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Technical report</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Automated kernel design is critical for overcoming software ecosystem barriers in emerging hardware platforms like RISC-V. While large language models (LLMs) have shown promise for automated kernel optimization, demonstrating success in CUDA domains with comprehensive technical documents and mature codebases, their effectiveness remains unproven for reference-scarce domains like RISC-V. We present Evolution of Kernels (EoK), a novel LLM-based evolutionary program search framework that automates kernel design for domains with limited reference material. EoK mitigates reference scarcity by mining and formalizing reusable optimization ideas (general design principles + actionable thoughts) from established kernel libraries’ development histories; it then guides parallel LLM explorations using these ideas, enriched via Retrieval-Augmented Generation (RAG) with RISC-V-specific context, prioritizing historically effective techniques. Empirically, EoK achieves a median 1.27x speedup, surpassing human experts on all 80 evaluated kernel design tasks and improving upon prior LLM-based automated kernel design methods by 20%. These results underscore the viability of incorporating human experience into emerging domains and highlight the immense potential of LLM-based automated kernel optimization.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-66] Unified Crew <mark class="hl-label green">Planning</mark>  and Re<mark class="hl-label green">planning</mark>  Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity</p>
<p>【速读】：该论文旨在解决多线路地铁乘务人员（crew）规划与应急重调度问题，尤其关注跨线路协同不足和突发情况下快速重规划能力薄弱的短板。其核心挑战在于如何在异质乘务员资质与偏好约束下，实现多线路统一优化调度，并提升系统应对突发事件时的响应效率。解决方案的关键在于构建了一个分层时空网络模型（hierarchical time-space network model），将不同线路的乘务员行动空间统一建模，并设计了高效的约束条件与数学表达以刻画乘务员的多样化资格与偏好；同时，结合列生成（column generation）与最短路径调整算法，实现了大规模问题的高效求解。实证结果表明，该方法在成本降低、任务完成率提升及跨线协同效率方面均显著优于传统启发式算法，特别是在紧急场景中展现出突出优势。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14251">https://arxiv.org/abs/2509.14251</a><br>
<strong>作者</strong>: Qihang Chen<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Systems and Control (<a target="_blank" rel="noopener" href="http://eess.SY">eess.SY</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Metro crew planning is a key component of smart city development as it directly impacts the operational efficiency and service reliability of public transportation. With the rapid expansion of metro networks, effective multi-line scheduling and emergency management have become essential for large-scale seamless operations. However, current research focuses primarily on individual metro lines,with insufficient attention on cross-line coordination and rapid replanning during disruptions. Here, a unified optimization framework is presented for multi-line metro crew planning and replanning with heterogeneous workforce. Specifically, a hierarchical time-space network model is proposed to represent the unified crew action space, and computationally efficient constraints and formulations are derived for the crew’s heterogeneous qualifications and preferences. Solution algorithms based on column generation and shortest path adjustment are further developed, utilizing the proposed network model. Experiments with real data from Shanghai and Beijing Metro demonstrate that the proposed methods outperform benchmark heuristics in both cost reduction and task completion,and achieve notable efficiency gains by incorporating cross-line operations, particularly for urgent tasks during disruptions. This work highlights the role of global optimization and cross-line coordination in multi-line metro system operations, providing insights into the efficient and reliable functioning of public transportation in smart cities.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-67] Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention</p>
<p>【速读】：该论文旨在解决混合交通环境中自动驾驶车辆在与人类驾驶车辆共存时面临的局部冲突难以协调及对随机事件泛化能力不足的问题。其解决方案的关键在于提出一种局部状态注意力模块（Local State Attention module），该模块利用自注意力机制（self-attention operator）压缩邻近车辆的关键信息，从而优化状态表示并缓解交通场景中的局部冲突。实验表明，该方法在高速公路汇入场景中显著提升了汇入效率，尤其在高密度交通条件下表现优越。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.11445">https://arxiv.org/abs/2506.11445</a><br>
<strong>作者</strong>: Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In mixed-traffic environments, autonomous vehicles must adapt to human-controlled vehicles and other unusual driving situations. This setting can be framed as a multi-agent reinforcement learning (MARL) environment with full cooperative reward among the autonomous vehicles. While methods such as Multi-agent Proximal Policy Optimization can be effective in training MARL tasks, they often fail to resolve local conflict between agents and are unable to generalize to stochastic events. In this paper, we propose a Local State Attention module to assist the input state representation. By relying on the self-attention operator, the module is expected to compress the essential information of nearby agents to resolve the conflict in traffic situations. Utilizing a simulated highway merging scenario with the priority vehicle as the unexpected event, our approach is able to prioritize other vehicles’ information to manage the merging process. The results demonstrate significant improvements in merging efficiency compared to popular baselines, especially in high-density traffic settings.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-68] ITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE <mark class="hl-label red">NEURIPS2025</mark></p>
<p>【速读】：该论文旨在解决变分量子本征求解器（Variational Quantum Eigensolver, VQE）在处理大型哈密顿量时训练效率急剧下降的问题，其核心瓶颈在于：(i) 量子不可克隆定理导致每梯度步长的电路评估次数随参数数量线性增长；(ii) 深层电路易陷入 barren plateaus（BP），引发指数级增长的测量开销。解决方案的关键在于提出一种名为 Titan 的深度学习框架，该框架通过在初始化阶段识别并冻结特定哈密顿量类别下对训练动态影响可忽略的“非活跃参数”，从而在不牺牲精度的前提下显著降低优化开销。Titan 的设计融合了理论驱动的数据构造策略（确保训练样本信息丰富且抗 BP）与自适应神经架构，可在不同规模的 ansatz 之间泛化，实现在横场伊辛模型、海森堡模型及多达 30 个量子比特分子系统上的高效收敛，相比当前最优基线实现最多 3 倍加速和 40%–60% 的电路评估减少。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15193">https://arxiv.org/abs/2509.15193</a><br>
<strong>作者</strong>: Yifeng Peng,Xinyi Li,Samuel Yen-Chi Chen,Kaining Zhang,Zhiding Liang,Ying Wang,Yuxuan Du<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Accepted by The Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Variational quantum Eigensolver (VQE) is a leading candidate for harnessing quantum computers to advance quantum chemistry and materials simulations, yet its training efficiency deteriorates rapidly for large Hamiltonians. Two issues underlie this bottleneck: (i) the no-cloning theorem imposes a linear growth in circuit evaluations with the number of parameters per gradient step; and (ii) deeper circuits encounter barren plateaus (BPs), leading to exponentially increasing measurement overheads. To address these challenges, here we propose a deep learning framework, dubbed Titan, which identifies and freezes inactive parameters of a given ansatze at initialization for a specific class of Hamiltonians, reducing the optimization overhead without sacrificing accuracy. The motivation of Titan starts with our empirical findings that a subset of parameters consistently has a negligible influence on training dynamics. Its design combines a theoretically grounded data construction strategy, ensuring each training example is informative and BP-resilient, with an adaptive neural architecture that generalizes across ansatze of varying sizes. Across benchmark transverse-field Ising models, Heisenberg models, and multiple molecule systems up to 30 qubits, Titan achieves up to 3 times faster convergence and 40% to 60% fewer circuit evaluations than state-of-the-art baselines, while matching or surpassing their estimation accuracy. By proactively trimming parameter space, Titan lowers hardware demands and offers a scalable path toward utilizing VQE to advance practical quantum chemistry and materials science.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-69] Listening Imagining  Refining: A Heuristic Optimized ASR Correction Framework with <mark class="hl-label green">LLM</mark> s</p>
<p>【速读】：该论文旨在解决自动语音识别（Automatic Speech Recognition, ASR）系统在实际应用中因识别错误而影响下游任务性能的问题。其解决方案的关键在于提出一种基于大语言模型（Large Language Models, LLMs）的启发式优化迭代修正框架——LIR-ASR，该框架受人类听觉感知启发，采用“倾听-想象-精炼”策略：首先生成音素变体，再在上下文中进行精细化修正；同时引入有限状态机（Finite State Machine, FSM）驱动的启发式优化机制，避免修正过程陷入局部最优，并通过规则约束保障语义保真度。实验表明，该方法在英文和中文ASR输出上均能实现平均词错误率（Word Error Rate, WER）和字符错误率（Character Error Rate, CER）降低最多达1.5个百分点，显著提升了转录准确性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15095">https://arxiv.org/abs/2509.15095</a><br>
<strong>作者</strong>: Yutong Liu,Ziyue Zhang,Yongbin Yu,Xiangxiang Wang,Yuqing Cai,Nyima Tashi<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Automatic Speech Recognition (ASR) systems remain prone to errors that affect downstream applications. In this paper, we propose LIR-ASR, a heuristic optimized iterative correction framework using LLMs, inspired by human auditory perception. LIR-ASR applies a “Listening-Imagining-Refining” strategy, generating phonetic variants and refining them in context. A heuristic optimization with finite state machine (FSM) is introduced to prevent the correction process from being trapped in local optima and rule-based constraints help maintain semantic fidelity. Experiments on both English and Chinese ASR outputs show that LIR-ASR achieves average reductions in CER/WER of up to 1.5 percentage points compared to baselines, demonstrating substantial accuracy gains in transcription.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-70] Discrete optimal transport is a strong audio adversarial attack</p>
<p>【速读】：该论文旨在解决现代语音反欺骗检测（anti-spoofing countermeasures, CMs）系统在面对生成式语音攻击时的脆弱性问题。解决方案的关键在于利用离散最优传输（discrete optimal transport, DOT）作为黑盒攻击方法，通过帧级WavLM嵌入的分布对齐实现对抗样本构造：具体而言，将生成语音的嵌入与未配对的真实语音池通过熵正则化最优传输（entropic OT）和top-k barycentric投影进行对齐，随后使用神经声码器（neural vocoder）重建语音。该方法在ASVspoof2019和ASVspoof5数据集上表现出稳定的高等错误率（EER），且在CM微调后仍具竞争力，并优于多种传统攻击方法的跨数据集迁移性能，验证了分布级对齐作为部署中CM系统的强大且稳定攻击面的有效性。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14959">https://arxiv.org/abs/2509.14959</a><br>
<strong>作者</strong>: Anton Selitskiy,Akib Shahriyar,Jishnuraj Prakasan<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we show that discrete optimal transport (DOT) is an effective black-box adversarial attack against modern audio anti-spoofing countermeasures (CMs). Our attack operates as a post-processing, distribution-alignment step: frame-level WavLM embeddings of generated speech are aligned to an unpaired bona fide pool via entropic OT and a top- k  barycentric projection, then decoded with a neural vocoder. Evaluated on ASVspoof2019 and ASVspoof5 with AASIST baselines, DOT yields consistently high equal error rate (EER) across datasets and remains competitive after CM fine-tuning, outperforming several conventional attacks in cross-dataset transfer. Ablation analysis highlights the practical impact of vocoder overlap. Results indicate that distribution-level alignment is a powerful and stable attack surface for deployed CMs.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-71] Mitigating Intra-Speaker Variability in Diarization with Style-Controllable Speech Augmentation <mark class="hl-label red">ICASSP2026</mark></p>
<p>【速读】：该论文旨在解决说话人聚类（speaker diarization）系统在面对高内在说话人变异性（intra-speaker variability）时的性能下降问题，例如情绪变化、健康状态或语篇内容差异导致同一说话人的语音片段被错误分类。解决方案的关键在于提出一种风格可控的语音生成模型（style-controllable speech generation model），该模型在保持目标说话人身份不变的前提下，对已分割的语音段进行多样化风格增强（如语音韵律、音素特征等），并通过融合原始语音与生成语音的说话人嵌入（speaker embeddings）来提升系统对高内在变异性语音段的聚类鲁棒性。实验表明，在模拟情感语音数据集和截断的AMI数据集上，该方法分别实现了49%和35%的错误率降低。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14632">https://arxiv.org/abs/2509.14632</a><br>
<strong>作者</strong>: Miseul Kim,Soo Jin Park,Kyungguen Byun,Hyeon-Kyeong Shin,Sunkuk Moon,Shuhua Zhang,Erik Visser<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Signal Processing (eess.SP)<br>
<strong>备注</strong>:  Submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Speaker diarization systems often struggle with high intrinsic intra-speaker variability, such as shifts in emotion, health, or content. This can cause segments from the same speaker to be misclassified as different individuals, for example, when one raises their voice or speaks faster during conversation. To address this, we propose a style-controllable speech generation model that augments speech across diverse styles while preserving the target speaker’s identity. The proposed system starts with diarized segments from a conventional diarizer. For each diarized segment, it generates augmented speech samples enriched with phonetic and stylistic diversity. And then, speaker embeddings from both the original and generated audio are blended to enhance the system’s robustness in grouping segments with high intrinsic intra-speaker variability. We validate our approach on a simulated emotional speech dataset and the truncated AMI dataset, demonstrating significant improvements, with error rate reductions of 49% and 35% on each dataset, respectively.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-72] Embodied sensorimotor control: computational modeling of the neural control of movement</p>
<p>【速读】：该论文试图解决如何整合神经环路、最优反馈机制与身体生物力学之间的相互作用，以实现对运动控制的统一理解这一核心问题。其解决方案的关键在于通过多尺度建模与实验证据的融合，揭示神经群体活动在低维动态流形上的演化规律，并借助最优控制理论阐明内部模型与反馈机制在运动行为中的作用；同时，近期研究进一步引入具身（embodied）传感器运动控制框架，明确将肌肉骨骼动力学作为神经群体活动的显式控制目标，从而弥合传统理论框架间的空白。这为构建跨区域、跨层次的整合性运动控制模型提供了新路径。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14360">https://arxiv.org/abs/2509.14360</a><br>
<strong>作者</strong>: Muhammad Noman Almani,John Lazzari,Jeff Walker,Shreya Saxena<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>)<br>
<strong>备注</strong>:  Review paper</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We review how sensorimotor control is dictated by interacting neural populations, optimal feedback mechanisms, and the biomechanics of bodies. First, we outline the distributed anatomical loops that shuttle sensorimotor signals between cortex, subcortical regions, and spinal cord. We then summarize evidence that neural population activity occupies low-dimensional, dynamically evolving manifolds during planning and execution of movements. Next, we summarize literature explaining motor behavior through the lens of optimal control theory, which clarifies the role of internal models and feedback during motor control. Finally, recent studies on embodied sensorimotor control address gaps within each framework by aiming to elucidate neural population activity through the explicit control of musculoskeletal dynamics. We close by discussing open problems and opportunities: multi-tasking and cognitively rich behavior, multi-regional circuit models, and the level of anatomical detail needed in body and network models. Together, this review and recent advances point towards reaching an integrative account of the neural control of movement.<br>
zh</p>
</div></div>
<div class="note blue no-icon flat"><p>[AI-73] Property-Isometric Variational Autoencoders for Sequence Modeling and Design</p>
<p>【速读】：该论文旨在解决生物序列（DNA、RNA或肽）设计中难以优化复杂高维属性的问题，例如DNA介导的荧光纳米颗粒的目标发射光谱、光化学稳定性以及抗菌肽对目标微生物的抗菌活性等。现有模型通常依赖于简单的二元标签（如结合/非结合），无法有效处理多维连续属性空间。解决方案的关键在于提出一种几何保真变分自编码器框架（PrIVAE），其核心创新是通过构建属性空间的近邻图来建模高维流形结构，并利用图神经网络编码层和等距正则化项引导潜在空间中的序列嵌入，从而学习到与属性组织一致的潜在表示。该方法不仅保持了高重建精度，还实现了基于属性的理性序列设计，已在DNA荧光纳米簇模板设计和抗菌肽生成任务中验证其有效性，且在湿实验中显著提升了稀有功能纳米簇的富集度（最高达16.1倍）。</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14287">https://arxiv.org/abs/2509.14287</a><br>
<strong>作者</strong>: Elham Sadeghi,Xianqi Deng,I-Hsin Lin,Stacy M. Copp,Petko Bogdanov<br>
<strong>机构</strong>: 未知<br>
<strong>类目</strong>: Quantitative Methods (q-bio.QM); Artificial Intelligence (<a target="_blank" rel="noopener" href="http://cs.AI">cs.AI</a>); Machine Learning (cs.LG)<br>
<strong>备注</strong>:  20 pages, 6 figures, preprint</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Biological sequence design (DNA, RNA, or peptides) with desired functional properties has applications in discovering novel nanomaterials, biosensors, antimicrobial drugs, and beyond. One common challenge is the ability to optimize complex high-dimensional properties such as target emission spectra of DNA-mediated fluorescent nanoparticles, photo and chemical stability, and antimicrobial activity of peptides across target microbes. Existing models rely on simple binary labels (e.g., binding/non-binding) rather than high-dimensional complex properties. To address this gap, we propose a geometry-preserving variational autoencoder framework, called PrIVAE, which learns latent sequence embeddings that respect the geometry of their property space. Specifically, we model the property space as a high-dimensional manifold that can be locally approximated by a nearest neighbor graph, given an appropriately defined distance measure. We employ the property graph to guide the sequence latent representations using (1) graph neural network encoder layers and (2) an isometric regularizer. PrIVAE learns a property-organized latent space that enables rational design of new sequences with desired properties by employing the trained decoder. We evaluate the utility of our framework for two generative tasks: (1) design of DNA sequences that template fluorescent metal nanoclusters and (2) design of antimicrobial peptides. The trained models retain high reconstruction accuracy while organizing the latent space according to properties. Beyond in silico experiments, we also employ sampled sequences for wet lab design of DNA nanoclusters, resulting in up to 16.1-fold enrichment of rare-property nanoclusters compared to their abundance in training data, demonstrating the practical utility of our framework.<br>
zh</p>
</div></div>
<h3 id="机器学习">机器学习</h3>
<div class="note pink no-icon flat"><p>[LG-0] CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15199">https://arxiv.org/abs/2509.15199</a><br>
<strong>作者</strong>: Ying Zheng,Yangfan Jiang,Kian-Lee Tan<br>
<strong>类目</strong>: Machine Learning (cs.LG); Databases (cs.DB)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Causal fairness in databases is crucial to preventing biased and inaccurate outcomes in downstream tasks. While most prior work assumes a known causal model, recent efforts relax this assumption by enforcing additional constraints. However, these approaches often fail to capture broader attribute relationships that are critical to maintaining utility. This raises a fundamental question: Can we harness the benefits of causal reasoning to design efficient and effective fairness solutions without relying on strong assumptions about the underlying causal model? In this paper, we seek to answer this question by introducing CausalPre, a scalable and effective causality-guided data pre-processing framework that guarantees justifiable fairness, a strong causal notion of fairness. CausalPre extracts causally fair relationships by reformulating the originally complex and computationally infeasible extraction task into a tailored distribution estimation problem. To ensure scalability, CausalPre adopts a carefully crafted variant of low-dimensional marginal factorization to approximate the joint distribution, complemented by a heuristic algorithm that efficiently tackles the associated computational challenge. Extensive experiments on benchmark datasets demonstrate that CausalPre is both effective and scalable, challenging the conventional belief that achieving causal fairness requires trading off relationship coverage for relaxed model assumptions.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-1] Explaining deep learning for ECG using time-localized clusters</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15198">https://arxiv.org/abs/2509.15198</a><br>
<strong>作者</strong>: Ahcène Boubekki,Konstantinos Patlatzoglou,Joseph Barker,Fu Siong Ng,Antônio H. Ribeiro<br>
<strong>类目</strong>: Machine Learning (cs.LG); Applications (stat.AP); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Deep learning has significantly advanced electrocardiogram (ECG) analysis, enabling automatic annotation, disease screening, and prognosis beyond traditional clinical capabilities. However, understanding these models remains a challenge, limiting interpretation and gaining knowledge from these developments. In this work, we propose a novel interpretability method for convolutional neural networks applied to ECG analysis. Our approach extracts time-localized clusters from the model’s internal representations, segmenting the ECG according to the learned characteristics while quantifying the uncertainty of these representations. This allows us to visualize how different waveform regions contribute to the model’s predictions and assess the certainty of its decisions. By providing a structured and interpretable view of deep learning models for ECG, our method enhances trust in AI-driven diagnostics and facilitates the discovery of clinically relevant electrophysiological patterns.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-2] MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference from ISA Extension to Hardware Acceleration</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15187">https://arxiv.org/abs/2509.15187</a><br>
<strong>作者</strong>: Giorgos Armeniakos,Alexis Maras,Sotirios Xydis,Dimitrios Soudris<br>
<strong>类目</strong>: Machine Learning (cs.LG); Hardware Architecture (<a target="_blank" rel="noopener" href="http://cs.AR">cs.AR</a>)<br>
*<strong>备注</strong>:  Accepted for publication by IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, March 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The evolution of quantization and mixed-precision techniques has unlocked new possibilities for enhancing the speed and energy efficiency of NNs. Several recent studies indicate that adapting precision levels across different parameters can maintain accuracy comparable to full-precision models while significantly reducing computational demands. However, existing embedded microprocessors lack sufficient architectural support for efficiently executing mixed-precision NNs, both in terms of ISA extensions and hardware design, resulting in inefficiencies such as excessive data packing/unpacking and underutilized arithmetic units. In this work, we propose novel ISA extensions and a micro-architecture implementation specifically designed to optimize mixed-precision execution, enabling energy-efficient deep learning inference on RISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software co-design framework that enhances power efficiency and performance through a combination of hardware improvements, mixed-precision quantization, ISA-level optimizations, and cycle-accurate emulation. At the hardware level, we enhance the ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for weights/activations and employ multi-pumping to reduce execution latency while implementing soft SIMD for efficient 2-bit ops. At the software level, we integrate a pruning-aware fine-tuning method to optimize model compression and a greedy-based DSE approach to efficiently search for Pareto-optimal mixed-quantized models. Additionally, we incorporate voltage scaling to boost the power efficiency of our system. Our experimental evaluation over widely used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our framework can achieve, on average, 17.6x speedup for less than 1% accuracy loss and outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up to 1.8 TOPs/W.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-3] Self-Improving Embodied Foundation Models <mark class="hl-label red">NEURIPS2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15155">https://arxiv.org/abs/2509.15155</a><br>
<strong>作者</strong>: Seyed Kamyar Seyed Ghasemipour,Ayzaan Wahid,Jonathan Tompson,Pannag Sanketi,Igor Mordatch<br>
<strong>类目</strong>: Machine Learning (cs.LG); Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>)<br>
*<strong>备注</strong>:  Appearing in the Conference on Neural Information Processing Systems (NeurIPS 2025)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at this https URL .</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-4] AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15153">https://arxiv.org/abs/2509.15153</a><br>
<strong>作者</strong>: Yating Lin,Zixuan Huang,Fan Yang,Dmitry Berenson<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multivariate time-series anomaly detection, which is critical for identifying unexpected events, has been explored in the field of machine learning for several decades. However, directly applying these methods to data from forceful tool use tasks is challenging because streaming sensor data in the real world tends to be inherently noisy, exhibits non-stationary behavior, and varies across different tasks and tools. To address these challenges, we propose a method, AnoF-Diff, based on the diffusion model to extract force-torque features from time-series data and use force-torque features to detect anomalies. We compare our method with other state-of-the-art methods in terms of F1-score and Area Under the Receiver Operating Characteristic curve (AUROC) on four forceful tool-use tasks, demonstrating that our method has better performance and is more robust to a noisy dataset. We also propose the method of parallel anomaly score evaluation based on one-step diffusion and demonstrate how our method can be used for online anomaly detection in several forceful tool use experiments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-5] Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15147">https://arxiv.org/abs/2509.15147</a><br>
<strong>作者</strong>: Viktor Kovalchuk,Nikita Kotelevskii,Maxim Panov,Samuel Horváth,Martin Takáč<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Federated learning (FL) usually shares model weights or gradients, which is costly for large models. Logit-based FL reduces this cost by sharing only logits computed on a public proxy dataset. However, aggregating information from heterogeneous clients is still challenging. This paper studies this problem, introduces and compares three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead, improve robustness under non-IID data, and achieve accuracy competitive with centralized training.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-6] Optimal Learning from Label Proportions with General Loss Functions</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15145">https://arxiv.org/abs/2509.15145</a><br>
<strong>作者</strong>: Lorne Applebaum,Travis Dick,Claudio Gentile,Haim Kaplan,Tomer Koren<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Motivated by problems in online advertising, we address the task of Learning from Label Proportions (LLP). In this partially-supervised setting, training data consists of groups of examples, termed bags, for which we only observe the average label value. The main goal, however, remains the design of a predictor for the labels of individual examples. We introduce a novel and versatile low-variance de-biasing methodology to learn from aggregate label information, significantly advancing the state of the art in LLP. Our approach exhibits remarkable flexibility, seamlessly accommodating a broad spectrum of practically relevant loss functions across both binary and multi-class classification settings. By carefully combining our estimators with standard techniques, we substantially improve sample complexity guarantees for a large class of losses of practical relevance. We also empirically validate the efficacy of our proposed approach across a diverse array of benchmark datasets, demonstrating compelling empirical advantages over standard baselines.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-7] Efficient Conformal Prediction for Regression Models under Label Noise</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15120">https://arxiv.org/abs/2509.15120</a><br>
<strong>作者</strong>: Yahav Cohen,Jacob Goldberger,Tom Tirer<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In high-stakes scenarios, such as medical imaging applications, it is critical to equip the predictions of a regression model with reliable confidence intervals. Recently, Conformal Prediction (CP) has emerged as a powerful statistical framework that, based on a labeled calibration set, generates intervals that include the true labels with a pre-specified probability. In this paper, we address the problem of applying CP for regression models when the calibration set contains noisy labels. We begin by establishing a mathematically grounded procedure for estimating the noise-free CP threshold. Then, we turn it into a practical algorithm that overcomes the challenges arising from the continuous nature of the regression problem. We evaluate the proposed method on two medical imaging regression datasets with Gaussian label noise. Our method significantly outperforms the existing alternative, achieving performance close to the clean-label setting.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-8] Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15113">https://arxiv.org/abs/2509.15113</a><br>
<strong>作者</strong>: Andrei Chertkov,Artem Basharin,Mikhail Saygin,Evgeny Frolov,Stanislav Straupe,Ivan Oseledets<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The growing demand for energy-efficient, high-performance AI systems has led to increased attention on alternative computing platforms (e.g., photonic, neuromorphic) due to their potential to accelerate learning and inference. However, integrating such physical components into deep learning pipelines remains challenging, as physical devices often offer limited expressiveness, and their non-differentiable nature renders on-device backpropagation difficult or infeasible. This motivates the development of hybrid architectures that combine digital neural networks with reconfigurable physical layers, which effectively behave as black boxes. In this work, we present a framework for the end-to-end training of such hybrid networks. This framework integrates stochastic zeroth-order optimization for updating the physical layer’s internal parameters with a dynamic low-rank surrogate model that enables gradient propagation through the physical layer. A key component of our approach is the implicit projector-splitting integrator algorithm, which updates the lightweight surrogate model after each forward pass with minimal hardware queries, thereby avoiding costly full matrix reconstruction. We demonstrate our method across diverse deep learning tasks, including: computer vision, audio classification, and language modeling. Notably, across all modalities, the proposed approach achieves near-digital baseline accuracy and consistently enables effective end-to-end training of hybrid models incorporating various non-differentiable physical components (spatial light modulators, microring resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware deep learning and gradient-free optimization, thereby offering a practical pathway for integrating non-differentiable physical components into scalable, end-to-end trainable AI systems.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-9] Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality Domain Shift Bias and Evaluation Challenges</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15107">https://arxiv.org/abs/2509.15107</a><br>
<strong>作者</strong>: Amy Rafferty,Rishi Ramaesh,Ajitha Rajan<br>
<strong>类目</strong>: Machine Learning (cs.LG); Digital Libraries (cs.DL)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Artificial intelligence has shown significant promise in chest radiography, where deep learning models can approach radiologist-level diagnostic performance. Progress has been accelerated by large public datasets such as MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of thousands of labelled images with pathology annotations. However, these datasets also present important limitations. Automated label extraction from radiology reports introduces errors, particularly in handling uncertainty and negation, and radiologist review frequently disagrees with assigned labels. In addition, domain shift and population bias restrict model generalisability, while evaluation practices often overlook clinically meaningful measures. We conduct a systematic analysis of these challenges, focusing on label quality, dataset bias, and domain shift. Our cross-dataset domain shift evaluation across multiple model architectures revealed substantial external performance degradation, with pronounced reductions in AUPRC and F1 scores relative to internal testing. To assess dataset bias, we trained a source-classification model that distinguished datasets with near-perfect accuracy, and performed subgroup analyses showing reduced performance for minority age and sex groups. Finally, expert review by two board-certified radiologists identified significant disagreement with public dataset labels. Our findings highlight important clinical weaknesses of current benchmarks and emphasise the need for clinician-validated datasets and fairer evaluation frameworks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-10] Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15105">https://arxiv.org/abs/2509.15105</a><br>
<strong>作者</strong>: Liran Nochumsohn,Raz Marshanski,Hedi Zisling,Omri Azencot<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Time series forecasting (TSF) is critical in domains like energy, finance, healthcare, and logistics, requiring models that generalize across diverse datasets. Large pre-trained models such as Chronos and Time-MoE show strong zero-shot (ZS) performance but suffer from high computational costs. In this work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts (MoE) model for general forecasting. It replaces deep architectures with simple frequency-specialized linear experts, trained on resampled data across multiple frequency regimes. A lightweight spectral gating mechanism dynamically selects relevant experts, enabling efficient, accurate forecasting. Despite its simplicity, Super-Linear matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability. The implementation of Super-Linear is available at \hrefthis https URLthis https URL</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-11] he Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning <mark class="hl-label red">IJCNN2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15097">https://arxiv.org/abs/2509.15097</a><br>
<strong>作者</strong>: Mohammad Saleh Vahdatpour,Huaiyuan Chu,Yanqing Zhang<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Published at IJCNN 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training methods are inefficient, requiring numerous iterative updates and high power consumption. To address these limitations, we propose a hybrid framework that combines hierarchical decomposition with FPGA-based direct equation solving and incremental learning. Our method divides the neural network into two functional tiers: lower layers are optimized via single-step equation solving on FPGAs for efficient and parallelizable feature extraction, while higher layers employ adaptive incremental learning to support continual updates without full retraining. Building upon this foundation, we introduce the Compound LLM framework, which explicitly deploys LLM modules across both hierarchy levels. The lower-level LLM handles reusable representation learning with minimal energy overhead, while the upper-level LLM performs adaptive decision-making through energy-aware updates. This integrated design enhances scalability, reduces redundant computation, and aligns with the principles of sustainable AI. Theoretical analysis and architectural insights demonstrate that our method reduces computational costs significantly while preserving high model performance, making it well-suited for edge deployment and real-time adaptation in energy-constrained environments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-12] Emergent Alignment via Competition</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15090">https://arxiv.org/abs/2509.15090</a><br>
<strong>作者</strong>: Natalie Collina,Surbhi Goel,Aaron Roth,Emily Ryu,Mirah Shi<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computer Science and Game Theory (<a target="_blank" rel="noopener" href="http://cs.GT">cs.GT</a>); Theoretical Economics (<a target="_blank" rel="noopener" href="http://econ.TH">econ.TH</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the users utility lies approximately within the convex hull of the agents utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two sets of experiments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-13] Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning <mark class="hl-label red">NEURIPS2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15087">https://arxiv.org/abs/2509.15087</a><br>
<strong>作者</strong>: Lei Wang,Jieming Bian,Letian Zhang,Jie Xu<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted to NeurIPS 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large Language Models (LLMs) have demonstrated impressive capabilities across various tasks, but fine-tuning them for domain-specific applications often requires substantial domain-specific data that may be distributed across multiple organizations. Federated Learning (FL) offers a privacy-preserving solution, but faces challenges with computational constraints when applied to LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient fine-tuning approach, though a single LoRA module often struggles with heterogeneous data across diverse domains. This paper addresses two critical challenges in federated LoRA fine-tuning: 1. determining the optimal number and allocation of LoRA experts across heterogeneous clients, and 2. enabling clients to selectively utilize these experts based on their specific data characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation and SElection), a novel framework that adaptively clusters clients based on representation similarity to allocate and train domain-specific LoRA experts. It also introduces an adaptive top- M  Mixture-of-Experts mechanism that allows each client to select the optimal number of utilized experts. Our extensive experiments on diverse benchmark datasets demonstrate that FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-14] Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15073">https://arxiv.org/abs/2509.15073</a><br>
<strong>作者</strong>: Shaoang Li,Jian Li<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Non-stationary multi-armed bandits enable agents to adapt to changing environments by incorporating mechanisms to detect and respond to shifts in reward distributions, making them well-suited for dynamic settings. However, existing approaches typically assume that reward feedback is available at every round - an assumption that overlooks many real-world scenarios where feedback is limited. In this paper, we take a significant step forward by introducing a new model of constrained feedback in non-stationary multi-armed bandits, where the availability of reward feedback is restricted. We propose the first prior-free algorithm - that is, one that does not require prior knowledge of the degree of non-stationarity - that achieves near-optimal dynamic regret in this setting. Specifically, our algorithm attains a dynamic regret of  \tilde\mathcalO(K^1/3 V_T^1/3 T / B^1/3) , where  T  is the number of rounds,  K  is the number of arms,  B  is the query budget, and  V_T  is the variation budget capturing the degree of non-stationarity.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-15] Improving Internet Traffic Matrix Prediction via Time Series Clustering <mark class="hl-label red">ICML</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15072">https://arxiv.org/abs/2509.15072</a><br>
<strong>作者</strong>: Martha Cash,Alexander Wyglinski<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted to ICMLA 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present a novel framework that leverages time series clustering to improve internet traffic matrix ™ prediction using deep learning (DL) models. Traffic flows within a TM often exhibit diverse temporal behaviors, which can hinder prediction accuracy when training a single model across all flows. To address this, we propose two clustering strategies, source clustering and histogram clustering, that group flows with similar temporal patterns prior to model training. Clustering creates more homogeneous data subsets, enabling models to capture underlying patterns more effectively and generalize better than global prediction approaches that fit a single model to the entire TM. Compared to existing TM prediction methods, our method reduces RMSE by up to 92% for Abilene and 75% for GÉANT. In routing scenarios, our clustered predictions also reduce maximum link utilization (MLU) bias by 18% and 21%, respectively, demonstrating the practical benefits of clustering when TMs are used for network optimization.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-16] Probabilistic and nonlinear compressive sensing</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15060">https://arxiv.org/abs/2509.15060</a><br>
<strong>作者</strong>: Lukas Silvester Barth,Paulo von Petersenn<br>
<strong>类目</strong>: Machine Learning (cs.LG); Information Theory (<a target="_blank" rel="noopener" href="http://cs.IT">cs.IT</a>); Statistics Theory (<a target="_blank" rel="noopener" href="http://math.ST">math.ST</a>); Computation (<a target="_blank" rel="noopener" href="http://stat.CO">stat.CO</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present a smooth probabilistic reformulation of  \ell_0  regularized regression that does not require Monte Carlo sampling and allows for the computation of exact gradients, facilitating rapid convergence to local optima of the best subset selection problem. The method drastically improves convergence speed compared to similar Monte Carlo based approaches. Furthermore, we empirically demonstrate that it outperforms compressive sensing algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and signal-to-noise ratios. The implementation runs efficiently on both CPUs and GPUs and is freely available at this https URL. We also contribute to research on nonlinear generalizations of compressive sensing by investigating when parameter recovery of a nonlinear teacher network is possible through compression of a student network. Building upon theorems of Fefferman and Markel, we show theoretically that the global optimum in the infinite-data limit enforces recovery up to certain symmetries. For empirical validation, we implement a normal-form algorithm that selects a canonical representative within each symmetry class. However, while compression can help to improve test loss, we find that exact parameter recovery is not even possible up to symmetries. In particular, we observe a surprising rebound effect where teacher and student configurations initially converge but subsequently diverge despite continuous decrease in test loss. These findings indicate fundamental differences between linear and nonlinear compressive sensing.         Subjects:  Machine Learning (cs.LG); Information Theory (<a target="_blank" rel="noopener" href="http://cs.IT">cs.IT</a>); Statistics Theory (<a target="_blank" rel="noopener" href="http://math.ST">math.ST</a>); Computation (<a target="_blank" rel="noopener" href="http://stat.CO">stat.CO</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)   MSC classes: 94A20, 94A13, 94A12, 94A08, 94-08, 94-04, 68T07, 68P30   ACMclasses: G.3; E.4; I.2; I.2.6; I.5.5   Cite as: arXiv:2509.15060 [cs.LG]    (or  arXiv:2509.15060v1 [cs.LG] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.15060">https://doi.org/10.48550/arXiv.2509.15060</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-17] Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15033">https://arxiv.org/abs/2509.15033</a><br>
<strong>作者</strong>: Padmaksha Roy,Almuatazbellah Boker,Lamine Mili<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we aim to improve multivariate anomaly detection (AD) by modeling the \textittime-varying non-linear spatio-temporal correlations found in multivariate time series data . In multivariate time series data, an anomaly may be indicated by the simultaneous deviation of interrelated time series from their expected collective behavior, even when no individual time series exhibits a clearly abnormal pattern on its own. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies in the latent space and decoupling the modeling of \textitmarginal distributions, temporal dynamics, and inter-variable dependencies. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we fit a multi-variate likelihood and a copula. The temporal and the spatial components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-18] Stochastic Adaptive Gradient Descent Without Descent</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14969">https://arxiv.org/abs/2509.14969</a><br>
<strong>作者</strong>: Jean-François Aujol,Jérémie Bigot,Camille Castera<br>
<strong>类目</strong>: Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We introduce a new adaptive step-size strategy for convex optimization with stochastic gradient that exploits the local geometry of the objective function only by means of a first-order stochastic oracle and without any hyper-parameter tuning. The method comes from a theoretically-grounded adaptation of the Adaptive Gradient Descent Without Descent method to the stochastic setting. We prove the convergence of stochastic gradient descent with our step-size under various assumptions, and we show that it empirically competes against tuned baselines.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-19] FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14968">https://arxiv.org/abs/2509.14968</a><br>
<strong>作者</strong>: Carlos Barroso-Fernández,Alejandro Calvillo-Fernandez,Antonio de la Oliva,Carlos J. Bernardos<br>
<strong>类目</strong>: Machine Learning (cs.LG); Networking and Internet Architecture (<a target="_blank" rel="noopener" href="http://cs.NI">cs.NI</a>)<br>
*<strong>备注</strong>:  7 pages, 6 figures and tables, less than 5500 words. Under revision at IEEE Communication Magazine</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The upcoming generations of wireless technologies promise an era where everything is interconnected and intelligent. As the need for intelligence grows, networks must learn to better understand the physical world. However, deploying dedicated hardware to perceive the environment is not always feasible, mainly due to costs and/or complexity. Integrated Sensing and Communication (ISAC) has made a step forward in addressing this challenge. Within ISAC, passive sensing emerges as a cost-effective solution that reuses wireless communications to sense the environment, without interfering with existing communications. Nevertheless, the majority of current solutions are limited to one technology (mostly Wi-Fi or 5G), constraining the maximum accuracy reachable. As different technologies work with different spectrums, we see a necessity in integrating more than one technology to augment the coverage area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference. FAWN is based on the original transformers architecture, to fuse information from Wi-Fi and 5G, making the network capable of understanding the physical world without interfering with the current communication. To test our solution, we have built a prototype and integrated it in a real scenario. Results show errors below 0.6 m around 84% of times.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-20] Stochastic Bilevel Optimization with Heavy-Tailed Noise</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14952">https://arxiv.org/abs/2509.14952</a><br>
<strong>作者</strong>: Zhuanghua Liu,Luo Luo<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper considers the smooth bilevel optimization in which the lower-level problem is strongly convex and the upper-level problem is possibly nonconvex. We focus on the stochastic setting that the algorithm can access the unbiased stochastic gradient evaluation with heavy-tailed noise, which is prevalent in many machine learning applications such as training large language models and reinforcement learning. We propose a nested-loop normalized stochastic bilevel approximation (N ^2 SBA) for finding an  \epsilon -stationary point with the stochastic first-order oracle (SFO) complexity of  \tilde\mathcalO\big(\kappa^\frac7p-3p-1 \sigma^\fracpp-1 \epsilon^-\frac4 p - 2p-1\big) , where  \kappa  is the condition number,  p\in(1,2]  is the order of central moment for the noise, and  \sigma  is the noise level. Furthermore, we specialize our idea to solve the nonconvex-strongly-concave minimax optimization problem, achieving an  \epsilon -stationary point with the SFO complexity of  \tilde\mathcal O\big(\kappa^\frac2p-1p-1 \sigma^\fracpp-1 \epsilon^-\frac3p-2p-1\big) . All above upper bounds match the best-known results under the special case of the bounded variance setting, i.e.,  p=2 .</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-21] Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14945">https://arxiv.org/abs/2509.14945</a><br>
<strong>作者</strong>: Amsalu Tessema,Tizazu Bayih,Kassahun Azezew,Ayenew Kassie<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  9 pages, 5 figures, 2 Tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Malnutrition among pregnant women is a major public health challenge in Ethiopia, increasing the risk of adverse maternal and neonatal outcomes. Traditional statistical approaches often fail to capture the complex and multidimensional determinants of nutritional status. This study develops a predictive model using ensemble machine learning techniques, leveraging data from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108 records with 30 socio-demographic and health attributes. Data preprocessing included handling missing values, normalization, and balancing with SMOTE, followed by feature selection to identify key predictors. Several supervised ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost were applied to classify nutritional status. Among them, the Random Forest model achieved the best performance, classifying women into four categories (normal, moderate malnutrition, severe malnutrition, and overnutrition) with 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC. These findings demonstrate the effectiveness of ensemble learning in capturing hidden patterns from complex datasets and provide timely insights for early detection of nutritional risks. The results offer practical implications for healthcare providers, policymakers, and researchers, supporting data-driven strategies to improve maternal nutrition and health outcomes in Ethiopia.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-22] Hierarchical Federated Learning for Social Network with Mobility</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14938">https://arxiv.org/abs/2509.14938</a><br>
<strong>作者</strong>: Zeyu Chen,Wen Chen,Jun Li,Qingqing Wu,Ming Ding,Xuefeng Han,Xiumei Deng,Liwei Wang<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Federated Learning (FL) offers a decentralized solution that allows collaborative local model training and global aggregation, thereby protecting data privacy. In conventional FL frameworks, data privacy is typically preserved under the assumption that local data remains absolutely private, whereas the mobility of clients is frequently neglected in explicit modeling. In this paper, we propose a hierarchical federated learning framework based on the social network with mobility namely HFL-SNM that considers both data sharing among clients and their mobility patterns. Under the constraints of limited resources, we formulate a joint optimization problem of resource allocation and client scheduling, which objective is to minimize the energy consumption of clients during the FL process. In social network, we introduce the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. We analyze the impact of effective data and redundant data on the model performance through preliminary experiments. We decouple the optimization problem into multiple sub-problems, analyze them based on preliminary experimental results, and propose Dynamic Optimization in Social Network with Mobility (DO-SNM) algorithm. Experimental results demonstrate that our algorithm achieves superior model performance while significantly reducing energy consumption, compared to traditional baseline algorithms.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-23] A Comparative Analysis of Transformer Models in Social Bot Detection</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14936">https://arxiv.org/abs/2509.14936</a><br>
<strong>作者</strong>: Rohan Veit,Michael Lones<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  To appear in proceedings of UKCI 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Social media has become a key medium of communication in today’s society. This realisation has led to many parties employing artificial users (or bots) to mislead others into believing untruths or acting in a beneficial manner to such parties. Sophisticated text generation tools, such as large language models, have further exacerbated this issue. This paper aims to compare the effectiveness of bot detection models based on encoder and decoder transformers. Pipelines are developed to evaluate the performance of these classifiers, revealing that encoder-based classifiers demonstrate greater accuracy and robustness. However, decoder-based models showed greater adaptability through task-specific alignment, suggesting more potential for generalisation across different use cases in addition to superior observa. These findings contribute to the ongoing effort to prevent digital environments being manipulated while protecting the integrity of online discussion.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-24] DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14933">https://arxiv.org/abs/2509.14933</a><br>
<strong>作者</strong>: Xiangfei Qiu,Yuhan Zhu,Zhengyu Li,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Time series forecasting is crucial in various fields such as economics, traffic, and AIOps. However, in real-world applications, focusing solely on the endogenous variables (i.e., target variables), is often insufficient to ensure accurate predictions. Considering exogenous variables (i.e., covariates) provides additional predictive information, thereby improving forecasting accuracy. However, existing methods for time series forecasting with exogenous variables (TSF-X) have the following shortcomings: 1) they do not leverage future exogenous variables, 2) they fail to account for the causal relationships between endogenous and exogenous variables. As a result, their performance is suboptimal. In this study, to better leverage exogenous variables, especially future exogenous variable, we propose a general framework DAG, which utilizes dual causal network along both the temporal and channel dimensions for time series forecasting with exogenous variables. Specifically, we first introduce the Temporal Causal Module, which includes a causal discovery module to capture how historical exogenous variables affect future exogenous variables. Following this, we construct a causal injection module that incorporates the discovered causal relationships into the process of forecasting future endogenous variables based on historical endogenous variables. Next, we propose the Channel Causal Module, which follows a similar design principle. It features a causal discovery module models how historical exogenous variables influence historical endogenous variables, and a causal injection module incorporates the discovered relationships to enhance the prediction of future endogenous variables based on future exogenous variables.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-25] Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14932">https://arxiv.org/abs/2509.14932</a><br>
<strong>作者</strong>: Tobias Jülg,Pierre Krack,Seongjin Bien,Yannik Blei,Khaled Gamal,Ken Nakahara,Johannes Hechtl,Roberto Calandra,Wolfram Burgard,Florian Walter<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Vision-Language-Action models (VLAs) mark a major shift in robot learning. They replace specialized architectures and task-tailored components of expert policies with large-scale data collection and setup-specific fine-tuning. In this machine learning-focused workflow that is centered around models and scalable training, traditional robotics software frameworks become a bottleneck, while robot simulations offer only limited support for transitioning from and to real-world experiments. In this work, we close this gap by introducing Robot Control Stack (RCS), a lean ecosystem designed from the ground up to support research in robot learning with large-scale generalist policies. At its core, RCS features a modular and easily extensible layered architecture with a unified interface for simulated and physical robots, facilitating sim-to-real transfer. Despite its minimal footprint and dependencies, it offers a complete feature set, enabling both real-world experiments and large-scale training in simulation. Our contribution is twofold: First, we introduce the architecture of RCS and explain its design principles. Second, we evaluate its usability and performance along the development cycle of VLA and RL policies. Our experiments also provide an extensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed light on how simulation data can improve real-world policy performance. Our code, datasets, weights, and videos are available at: this https URL</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-26] Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14925">https://arxiv.org/abs/2509.14925</a><br>
<strong>作者</strong>: Konrad Nowosadko,Franco Ruggeri,Ahmad Terra<br>
<strong>类目</strong>: Machine Learning (cs.LG); Networking and Internet Architecture (<a target="_blank" rel="noopener" href="http://cs.NI">cs.NI</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reinforcement Learning (RL) methods that incorporate deep neural networks (DNN), though powerful, often lack transparency. Their black-box characteristic hinders interpretability and reduces trustworthiness, particularly in critical domains. To address this challenge in RL tasks, we propose a solution based on Self-Explaining Neural Networks (SENNs) along with explanation extraction methods to enhance interpretability while maintaining predictive accuracy. Our approach targets low-dimensionality problems to generate robust local and global explanations of the model’s behaviour. We evaluate the proposed method on the resource allocation problem in mobile networks, demonstrating that SENNs can constitute interpretable solutions with competitive performance. This work highlights the potential of SENNs to improve transparency and trust in AI-driven decision-making for low-dimensional tasks. Our approach strong performance on par with the existing state-of-the-art methods, while providing robust explanations.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-27] Robust Barycenters of Persistence Diagrams</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14904">https://arxiv.org/abs/2509.14904</a><br>
<strong>作者</strong>: Keanu Sisouk,Eloi Tanguy,Julie Delon,Julien Tierny<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computational Geometry (<a target="_blank" rel="noopener" href="http://cs.CG">cs.CG</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This short paper presents a general approach for computing robust Wasserstein barycenters of persistence diagrams. The classical method consists in computing assignment arithmetic means after finding the optimal transport plans between the barycenter and the persistence diagrams. However, this procedure only works for the transportation cost related to the  q -Wasserstein distance  W_q  when  q=2 . We adapt an alternative fixed-point method to compute a barycenter diagram for generic transportation costs ( q  1 ), in particular those robust to outliers,  q \in (1,2) . We show the utility of our work in two applications: \emph(i) the clustering of persistence diagrams on their metric space and \emph(ii) the dictionary encoding of persistence diagrams. In both scenarios, we demonstrate the added robustness to outliers provided by our generalized framework. Our Python implementation is available at this address: this https URL .</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-28] CARGO: A Framework for Confidence-Aware Routing of <mark class="hl-label green">Large</mark> <mark class="hl-label green">Language</mark> <mark class="hl-label green">Models</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14899">https://arxiv.org/abs/2509.14899</a><br>
<strong>作者</strong>: Amine Barrak,Yosr Fourati,Michael Olchawa,Emna Ksontini,Khalil Zoghlami<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As large language models (LLMs) proliferate in scale, specialization, and latency profiles, the challenge of routing user prompts to the most appropriate model has become increasingly critical for balancing performance and cost. We introduce CARGO (Category-Aware Routing with Gap-based Optimization), a lightweight, confidence-aware framework for dynamic LLM selection. CARGO employs a single embedding-based regressor trained on LLM-judged pairwise comparisons to predict model performance, with an optional binary classifier invoked when predictions are uncertain. This two-stage design enables precise, cost-aware routing without the need for human-annotated supervision. To capture domain-specific behavior, CARGO also supports category-specific regressors trained across five task groups: mathematics, coding, reasoning, summarization, and creative writing. Evaluated on four competitive LLMs (GPT-4o, Claude 3.5 Sonnet, DeepSeek V3, and Perplexity Sonar), CARGO achieves a top-1 routing accuracy of 76.4% and win rates ranging from 72% to 89% against individual experts. These results demonstrate that confidence-guided, lightweight routing can achieve expert-level performance with minimal overhead, offering a practical solution for real-world, multi-model LLM deployments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-29] Leve<mark class="hl-label green">rag</mark> ing Reinforcement Learning Genetic Algorithms and Transformers for background determination in particle physics</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14894">https://arxiv.org/abs/2509.14894</a><br>
<strong>作者</strong>: Guillermo Hijano Mendizabal,Davide Lancierini,Alex Marshall,Andrea Mauri,Patrick Haworth Owen,Mitesh Patel,Konstantinos Petridis,Shah Rukh Qasim,Nicola Serra,William Sutcliffe,Hanae Tilquin<br>
<strong>类目</strong>: Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)<br>
*<strong>备注</strong>:  32 pages, 12 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Experimental studies of beauty hadron decays face significant challenges due to a wide range of backgrounds arising from the numerous possible decay channels with similar final states. For a particular signal decay, the process for ascertaining the most relevant background processes necessitates a detailed analysis of final state particles, potential misidentifications, and kinematic overlaps, which, due to computational limitations, is restricted to the simulation of only the most relevant backgrounds. Moreover, this process typically relies on the physicist’s intuition and expertise, as no systematic method exists. This paper has two primary goals. First, from a particle physics perspective, we present a novel approach that utilises Reinforcement Learning (RL) to overcome the aforementioned challenges by systematically determining the critical backgrounds affecting beauty hadron decay measurements. While beauty hadron physics serves as the case study in this work, the proposed strategy is broadly adaptable to other types of particle physics measurements. Second, from a Machine Learning perspective, we introduce a novel algorithm which exploits the synergy between RL and Genetic Algorithms (GAs) for environments with highly sparse rewards and a large trajectory space. This strategy leverages GAs to efficiently explore the trajectory space and identify successful trajectories, which are used to guide the RL agent’s training. Our method also incorporates a transformer architecture for the RL agent to handle token sequences representing decays.          Comments: 32 pages, 12 figures   Subjects:  Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)  Cite as: arXiv:2509.14894 [cs.LG]    (or  arXiv:2509.14894v1 [cs.LG] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14894">https://doi.org/10.48550/arXiv.2509.14894</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-30] Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14887">https://arxiv.org/abs/2509.14887</a><br>
<strong>作者</strong>: Hoang-Son Nguyen,Hoi-To Wai<br>
<strong>类目</strong>: Machine Learning (cs.LG); Signal Processing (eess.SP)<br>
*<strong>备注</strong>:  7 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Learning the graph underlying a networked system from nodal signals is crucial to downstream tasks in graph signal processing and machine learning. The presence of hidden nodes whose signals are not observable might corrupt the estimated graph. While existing works proposed various robustifications of vanilla graph learning objectives by explicitly accounting for the presence of these hidden nodes, a robustness analysis of “naive”, hidden-node agnostic approaches is still underexplored. This work demonstrates that vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals. We achieve this theoretical result through extending the restricted isometry property (RIP) to the Dirichlet energy function used in graph learning objectives. We show that smoothness-based graph learning formulation (e.g., the GL-SigRep method) on partial observations can recover the ground truth graph topology corresponding to the observed nodes. Synthetic and real data experiments corroborate our findings.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-31] Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14848">https://arxiv.org/abs/2509.14848</a><br>
<strong>作者</strong>: Houssem Sifaou,Osvaldo Simeone<br>
<strong>类目</strong>: Machine Learning (cs.LG); Signal Processing (eess.SP)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Optimizing a reinforcement learning (RL) policy typically requires extensive interactions with a high-fidelity simulator of the environment, which are often costly or impractical. Offline RL addresses this problem by allowing training from pre-collected data, but its effectiveness is strongly constrained by the size and quality of the dataset. Hybrid offline-online RL leverages both offline data and interactions with a single simulator of the environment. In many real-world scenarios, however, multiple simulators with varying levels of fidelity and computational cost are available. In this work, we study multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We introduce multi-fidelity hybrid RL via information gain maximization (MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity selection based on information gain maximization through a bootstrapping approach. Theoretical analysis establishes the no-regret property of MF-HRL-IGM, while empirical evaluations demonstrate its superior performance compared to existing benchmarks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-32] Precision Neural Networks: Joint Graph And Relational Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14821">https://arxiv.org/abs/2509.14821</a><br>
<strong>作者</strong>: Andrea Cavallo,Samuel Rey,Antonio G. Marques,Elvin Isufi<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:CoVariance Neural Networks (VNNs) perform convolutions on the graph determined by the covariance matrix of the data, which enables expressive and stable covariance-based learning. However, covariance matrices are typically dense, fail to encode conditional independence, and are often precomputed in a task-agnostic way, which may hinder performance. To overcome these limitations, we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix – the inverse covariance. The precision matrix naturally encodes statistical independence, often exhibits sparsity, and preserves the covariance spectral structure. To make precision estimation task-aware, we formulate an optimization problem that jointly learns the network parameters and the precision matrix, and solve it via alternating optimization, by sequentially updating the network weights and the precision estimate. We theoretically bound the distance between the estimated and true precision matrices at each iteration, and demonstrate the effectiveness of joint estimation compared to two-step approaches on synthetic and real-world data.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-33] Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14816">https://arxiv.org/abs/2509.14816</a><br>
<strong>作者</strong>: Humphrey Munn,Brendan Tidd,Peter Böhm,Marcus Gallagher,David Howard<br>
<strong>类目</strong>: Robotics (<a target="_blank" rel="noopener" href="http://cs.RO">cs.RO</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Reinforcement Learning (RL) robot controllers usually aggregate many task objectives into one scalar reward. While large-scale proximal policy optimisation (PPO) has enabled impressive results such as robust robot locomotion in the real world, many tasks still require careful reward tuning and are brittle to local optima. Tuning cost and sub-optimality grow with the number of objectives, limiting scalability. Modelling reward vectors and their trade-offs can address these issues; however, multi-objective methods remain underused in RL for robotics because of computational cost and optimisation difficulty. In this work, we investigate the conflict between gradient contributions for each objective that emerge from scalarising the task objectives. In particular, we explicitly address the conflict between task-based rewards and terms that regularise the policy towards realistic behaviour. We propose GCR-PPO, a modification to actor-critic optimisation that decomposes the actor update into objective-wise gradients using a multi-headed critic and resolves conflicts based on the objective priority. Our methodology, GCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion benchmarks and additional multi-objective modifications on two related tasks. We show superior scalability compared to parallel PPO (p = 0.04), without significant computational overhead. We also show higher performance with more conflicting tasks. GCR-PPO improves on large-scale PPO with an average improvement of 9.5%, with high-conflict tasks observing a greater improvement. The code is available at this https URL.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-34] STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14801">https://arxiv.org/abs/2509.14801</a><br>
<strong>作者</strong>: Julian F. Schumann,Anna Mészáros,Jens Kober,Arkady Zgonnikov<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While trajectory prediction plays a critical role in enabling safe and effective path-planning in automated vehicles, standardized practices for evaluating such models remain underdeveloped. Recent efforts have aimed to unify dataset formats and model interfaces for easier comparisons, yet existing frameworks often fall short in supporting heterogeneous traffic scenarios, joint prediction models, or user documentation. In this work, we introduce STEP – a new benchmarking framework that addresses these limitations by providing a unified interface for multiple datasets, enforcing consistent training and evaluation conditions, and supporting a wide range of prediction models. We demonstrate the capabilities of STEP in a number of experiments which reveal 1) the limitations of widely-used testing procedures, 2) the importance of joint modeling of agents for better predictions of interactions, and 3) the vulnerability of current state-of-the-art models against both distribution shifts and targeted attacks by adversarial agents. With STEP, we aim to shift the focus from the ``leaderboard’’ approach to deeper insights about model behavior and generalization in complex multi-agent settings.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-35] Pre-training under infinite compute</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14786">https://arxiv.org/abs/2509.14786</a><br>
<strong>作者</strong>: Konwoo Kim,Suhas Kotha,Percy Liang,Tatsunori Hashimoto<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Since compute grows much faster than web text available for language model pre-training, we ask how one should approach pre-training under fixed data and no compute constraints. We first show that existing data-constrained approaches of increasing epoch count and parameter count eventually overfit, and we significantly improve upon such recipes by properly tuning regularization, finding that the optimal weight decay is  30\times  larger than standard practice. Since our regularized recipe monotonically decreases loss following a simple power law in parameter count, we estimate its best possible performance via the asymptote of its scaling law rather than the performance at a fixed compute budget. We then identify that ensembling independently trained models achieves a significantly lower loss asymptote than the regularized recipe. Our best intervention combining epoching, regularization, parameter scaling, and ensemble scaling achieves an asymptote at 200M tokens using  5.17\times  less data than our baseline, and our data scaling laws predict that this improvement persists at higher token budgets. We find that our data efficiency gains can be realized at much smaller parameter counts as we can distill an ensemble into a student model that is 8 \times  smaller and retains  83%  of the ensembling benefit. Finally, our interventions designed for validation loss generalize to downstream benchmarks, achieving a  9%  improvement for pre-training evals and a  17.5\times  data efficiency improvement over continued pre-training on math mid-training data. Our results show that simple algorithmic improvements can enable significantly more data-efficient pre-training in a compute-rich future.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-36] FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14775">https://arxiv.org/abs/2509.14775</a><br>
<strong>作者</strong>: Shuangshuang He,Yuanting Zhang,Hongli Liang,Qingye Meng,Xingyuan Yuan<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Accurate hourly weather forecasting is critical for numerous applications. Recent deep learning models have demonstrated strong capability on 6-hour intervals, yet achieving accurate and stable hourly predictions remains a critical challenge. This is primarily due to the rapid accumulation of errors in autoregressive rollouts and temporal discontinuities within the ERA5 data’s 12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a framework that models atmospheric state evolution as a continuous flow. FlowCast-ODE learns the conditional flow path directly from the previous state, an approach that aligns more naturally with physical dynamic systems and enables efficient computation. A coarse-to-fine strategy is introduced to train the model on 6-hour data using dynamic flow matching and then refined on hourly data that incorporates an Ordinary Differential Equation (ODE) solver to achieve temporally coherent forecasts. In addition, a lightweight low-rank AdaLN-Zero modulation mechanism is proposed and reduces model size by 15% without compromising accuracy. Experiments demonstrate that FlowCast-ODE outperforms strong baselines, yielding lower root mean square error (RMSE) and better energy conservation, which reduces blurring and preserves more fine-scale spatial details. It also shows comparable performance to the state-of-the-art model in forecasting extreme events like typhoons. Furthermore, the model alleviates temporal discontinuities associated with assimilation cycle transitions.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-37] ranscoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14723">https://arxiv.org/abs/2509.14723</a><br>
<strong>作者</strong>: Sosuke Hosokawa,Toshiharu Kawakami,Satoshi Kodera,Masamichi Ito,Norihiko Takeda<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Single-cell foundation models (scFMs) have demonstrated state-of-the-art performance on various tasks, such as cell-type annotation and perturbation response prediction, by learning gene regulatory networks from large-scale transcriptome data. However, a significant challenge remains: the decision-making processes of these models are less interpretable compared to traditional methods like differential gene expression analysis. Recently, transcoders have emerged as a promising approach for extracting interpretable decision circuits from large language models (LLMs). In this work, we train a transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By leveraging the trained transcoder, we extract internal decision-making circuits from the C2S model. We demonstrate that the discovered circuits correspond to real-world biological mechanisms, confirming the potential of transcoders to uncover biologically plausible pathways within complex single-cell models.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-38] owards Pre-trained Graph Condensation via Optimal Transport</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14722">https://arxiv.org/abs/2509.14722</a><br>
<strong>作者</strong>: Yeyu Yan,Shuai Zheng,Wenjun Hui,Xiangkai Zhu,Dong Chen,Zhenfeng Zhu,Yao Zhao,Kunlun He<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Graph condensation (GC) aims to distill the original graph into a small-scale graph, mitigating redundancy and accelerating GNN training. However, conventional GC approaches heavily rely on rigid GNNs and task-specific supervision. Such a dependency severely restricts their reusability and generalization across various tasks and architectures. In this work, we revisit the goal of ideal GC from the perspective of GNN optimization consistency, and then a generalized GC optimization objective is derived, by which those traditional GC methods can be viewed nicely as special cases of this optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC) via optimal transport is proposed to transcend the limitations of task- and architecture-dependent GC methods. Specifically, a hybrid-interval graph diffusion augmentation is presented to suppress the weak generalization ability of the condensed graph on particular architectures by enhancing the uncertainty of node states. Meanwhile, the matching between optimal graph transport plan and representation transport plan is tactfully established to maintain semantic consistencies across source graph and condensed graph spaces, thereby freeing graph condensation from task dependencies. To further facilitate the adaptation of condensed graphs to various downstream tasks, a traceable semantic harmonizer from source nodes to condensed nodes is proposed to bridge semantic associations through the optimized representation transport plan in pre-training. Extensive experiments verify the superiority and versatility of PreGC, demonstrating its task-independent nature and seamless compatibility with arbitrary GNNs.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-39] LEED: A Highly Efficient and Scalable <mark class="hl-label green">LLM</mark> -Empowered Expert Demonstrations Framework for Multi-<mark class="hl-label green">Agent</mark>  Reinforcement Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14680">https://arxiv.org/abs/2509.14680</a><br>
<strong>作者</strong>: Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Dong Huang,Yuanye Zhao,Zheng Lin,Zihan Fang,Dianxin Luan,Heming Cui,Yong Cui<br>
<strong>类目</strong>: Multiagent Systems (<a target="_blank" rel="noopener" href="http://cs.MA">cs.MA</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  5 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Multi-agent reinforcement learning (MARL) holds substantial promise for intelligent decision-making in complex environments. However, it suffers from a coordination and scalability bottleneck as the number of agents increases. To address these issues, we propose the LLM-empowered expert demonstrations framework for multi-agent reinforcement learning (LEED). LEED consists of two components: a demonstration generation (DG) module and a policy optimization (PO) module. Specifically, the DG module leverages large language models to generate instructions for interacting with the environment, thereby producing high-quality demonstrations. The PO module adopts a decentralized training paradigm, where each agent utilizes the generated demonstrations to construct an expert policy loss, which is then integrated with its own policy loss. This enables each agent to effectively personalize and optimize its local policy based on both expert knowledge and individual experience. Experimental results show that LEED achieves superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-40] Stochastic Clock Attention for Aligning Continuous and Ordered Sequences</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14678">https://arxiv.org/abs/2509.14678</a><br>
<strong>作者</strong>: Hyungjoon Soh,Junghyo Jo<br>
<strong>类目</strong>: Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)<br>
*<strong>备注</strong>:  8 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We formulate an attention mechanism for continuous and ordered sequences that explicitly functions as an alignment model, which serves as the core of many sequence-to-sequence tasks. Standard scaled dot-product attention relies on positional encodings and masks but does not enforce continuity or monotonicity, which are crucial for frame-synchronous targets. We propose learned nonnegative \emphclocks to source and target and model attention as the meeting probability of these clocks; a path-integral derivation yields a closed-form, Gaussian-like scoring rule with an intrinsic bias toward causal, smooth, near-diagonal alignments, without external positional regularizers. The framework supports two complementary regimes: normalized clocks for parallel decoding when a global length is available, and unnormalized clocks for autoregressive decoding – both nearly-parameter-free, drop-in replacements. In a Transformer text-to-speech testbed, this construction produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines. We hypothesize applicability to other continuous targets, including video and temporal signal modeling.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-41] DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14640">https://arxiv.org/abs/2509.14640</a><br>
<strong>作者</strong>: Habib Irani,Vangelis Metsis<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Existing positional encoding methods in transformers are fundamentally signal-agnostic, deriving positional information solely from sequence indices while ignoring the underlying signal characteristics. This limitation is particularly problematic for time series analysis, where signals exhibit complex, non-stationary dynamics across multiple temporal scales. We introduce Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework that generates positional embeddings directly from input time series using the Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time series datasets demonstrate that DyWPE consistently outperforms eight existing state-of-the-art positional encoding methods, achieving average relative improvements of 9.1% compared to baseline sinusoidal absolute position encoding in biomedical signals, while maintaining competitive computational efficiency.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-42] CUFG: Curriculum Unlearning Guided by the Forgetting Gradient</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14633">https://arxiv.org/abs/2509.14633</a><br>
<strong>作者</strong>: Jiaxing Miao,Liang Hu,Qi Zhang,Lai Zhong Yuan,Usman Naseem<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  under review (early)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:As privacy and security take center stage in AI, machine unlearning, the ability to erase specific knowledge from models, has garnered increasing attention. However, existing methods overly prioritize efficiency and aggressive forgetting, which introduces notable limitations. In particular, radical interventions like gradient ascent, influence functions, and random label noise can destabilize model weights, leading to collapse and reduced reliability. To address this, we propose CUFG (Curriculum Unlearning via Forgetting Gradients), a novel framework that enhances the stability of approximate unlearning through innovations in both forgetting mechanisms and data scheduling strategies. Specifically, CUFG integrates a new gradient corrector guided by forgetting gradients for fine-tuning-based unlearning and a curriculum unlearning paradigm that progressively forgets from easy to hard. These innovations narrow the gap with the gold-standard Retrain method by enabling more stable and progressive unlearning, thereby improving both effectiveness and reliability. Furthermore, we believe that the concept of curriculum unlearning has substantial research potential and offers forward-looking insights for the development of the MU field. Extensive experiments across various forgetting scenarios validate the rationale and effectiveness of our approach and CUFG. Codes are available at this https URL.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-43] HD3C: Efficient Medical Data Classification for Embedded Devices</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14617">https://arxiv.org/abs/2509.14617</a><br>
<strong>作者</strong>: Jianglan Wei,Zhenyu Zhang,Pengcheng Wang,Mingjie Zeng,Zhigang Zeng<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Energy-efficient medical data classification is essential for modern disease screening, particularly in home and field healthcare where embedded devices are prevalent. While deep learning models achieve state-of-the-art accuracy, their substantial energy consumption and reliance on GPUs limit deployment on such platforms. We present Hyperdimensional Computing with Class-Wise Clustering (HD3C), a lightweight classification framework designed for low-power environments. HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace. We evaluate HD3C across three medical classification tasks; on heart sound classification, HD3C is  350\times  more energy-efficient than Bayesian ResNet with less than 1% accuracy difference. Moreover, HD3C demonstrates exceptional robustness to noise, limited training data, and hardware error, supported by both theoretical analysis and empirical results, highlighting its potential for reliable deployment in real-world settings. Code is available at this https URL.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-44] owards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14603">https://arxiv.org/abs/2509.14603</a><br>
<strong>作者</strong>: Xingchen Wang,Feijie Wu,Chenglin Miao,Tianchun Li,Haoyu Hu,Qiming Cao,Jing Gao,Lu Su<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Split Federated Learning (SFL) has emerged as an efficient alternative to traditional Federated Learning (FL) by reducing client-side computation through model partitioning. However, exchanging of intermediate activations and model updates introduces significant privacy risks, especially from data reconstruction attacks that recover original inputs from intermediate representations. Existing defenses using noise injection often degrade model performance. To overcome these challenges, we present PM-SFL, a scalable and privacy-preserving SFL framework that incorporates Probabilistic Mask training to add structured randomness without relying on explicit noise. This mitigates data reconstruction risks while maintaining model utility. To address data heterogeneity, PM-SFL employs personalized mask learning that tailors submodel structures to each client’s local data. For system heterogeneity, we introduce a layer-wise knowledge compensation mechanism, enabling clients with varying resources to participate effectively under adaptive model splitting. Theoretical analysis confirms its privacy protection, and experiments on image and wireless sensing tasks demonstrate that PM-SFL consistently improves accuracy, communication efficiency, and robustness to privacy attacks, with particularly strong performance under data and system heterogeneity.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-45] ICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics <mark class="hl-label red">ICML2025</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14600">https://arxiv.org/abs/2509.14600</a><br>
<strong>作者</strong>: Alexander Aghili,Andy Bruce,Daniel Sabo,Razvan Marinescu<br>
<strong>类目</strong>: Machine Learning (cs.LG); Biological Physics (physics.bio-ph)<br>
*<strong>备注</strong>:  Proceedings of the ICML 2025 Workshop on Multi-modal Foundation Models and Large Language Models for Life Sciences, Vancouver, Canada. 2025. Copyright 2025 by the author(s). 4 Pages 5 Figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Molecular dynamics (MD) simulations provide atomistic insight into biomolecular systems but are often limited by high computational costs required to access long timescales. Coarse-grained machine learning models offer a promising avenue for accelerating sampling, yet conventional force matching approaches often fail to capture the full thermodynamic landscape as fitting a model on the gradient may not fit the absolute differences between low-energy conformational states. In this work, we incorporate a complementary energy matching term into the loss function. We evaluate our framework on the Chignolin protein using the CGSchNet model, systematically varying the weight of the energy loss term. While energy matching did not yield statistically significant improvements in accuracy, it revealed distinct tendencies in how models generalize the free energy surface. Our results suggest future opportunities to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-46] Online reinforcement learning via sparse Gaussian mixture model Q-functions</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14585">https://arxiv.org/abs/2509.14585</a><br>
<strong>作者</strong>: Minh Vu,Konstantinos Slavakis<br>
<strong>类目</strong>: Machine Learning (cs.LG); Optimization and Control (math.OC)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper introduces a structured and interpretable online policy-iteration framework for reinforcement learning (RL), built around the novel class of sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work that trained GMM-QFs offline, the proposed framework develops an online scheme that leverages streaming data to encourage exploration. Model complexity is regulated through sparsification by Hadamard overparametrization, which mitigates overfitting while preserving expressiveness. The parameter space of S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing for principled parameter updates via online gradient descent on a smooth objective. Numerical tests show that S-GMM-QFs match the performance of dense deep RL (DeepRL) methods on standard benchmarks while using significantly fewer parameters, and maintain strong performance even in low-parameter-count regimes where sparsified DeepRL methods fail to generalize.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-47] Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14577">https://arxiv.org/abs/2509.14577</a><br>
<strong>作者</strong>: Yang Xu,Junpeng Li,Changchun Hua,Yana Yang<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The Large Margin Distribution Machine (LMDM) is a recent advancement in classifier design that optimizes not just the minimum margin (as in SVM) but the entire margin distribution, thereby improving generalization. However, existing LMDM formulations are limited to vectorized inputs and struggle with high-dimensional tensor data due to the need for flattening, which destroys the data’s inherent multi-mode structure and increases computational burden. In this paper, we propose a Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates directly on tensor representations without vectorization. The SPMD-LRT preserves multi-dimensional spatial structure by incorporating first-order and second-order tensor statistics (margin mean and variance) into the objective, and it leverages low-rank tensor decomposition techniques including rank-1(CP), higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An alternating optimization (double-gradient descent) algorithm is developed to efficiently solve the SPMD-LRT, iteratively updating factor matrices and core tensor. This approach enables SPMD-LRT to maintain the structural information of high-order data while optimizing margin distribution for improved classification. Extensive experiments on diverse datasets (including MNIST, images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior classification accuracy compared to conventional SVM, vector-based LMDM, and prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest accuracy, highlighting the benefit of structure preservation. These results confirm the effectiveness and robustness of SPMD-LRT in handling high-dimensional tensor data for classification.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-48] Evidential Physics-Informed Neural Networks for Scientific Discovery</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14568">https://arxiv.org/abs/2509.14568</a><br>
<strong>作者</strong>: Hai Siong Tan,Kuancheng Wang,Rafe McBeth<br>
<strong>类目</strong>: Machine Learning (cs.LG); Computational Physics (physics.comp-ph)<br>
*<strong>备注</strong>:  15 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We present the fundamental theory and implementation guidelines underlying Evidential Physics-Informed Neural Network (E-PINN) – a novel class of uncertainty-aware PINN. It leverages the marginal distribution loss function of evidential deep learning for estimating uncertainty of outputs, and infers unknown parameters of the PDE via a learned posterior distribution. Validating our model on two illustrative case studies – the 1D Poisson equation with a Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated empirical coverage probabilities that were calibrated significantly better than Bayesian PINN and Deep Ensemble methods. To demonstrate real-world applicability, we also present a brief case study on applying E-PINN to analyze clinical glucose-insulin datasets that have featured in medical research on diabetes pathophysiology.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-49] Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14563">https://arxiv.org/abs/2509.14563</a><br>
<strong>作者</strong>: Shiyuan Luo,Runlong Yu,Chonghao Qiu,Rahul Ghosh,Robert Ladwig,Paul C. Hanson,Yiqun Xie,Xiaowei Jia<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The discovery of environmental knowledge depends on labeled task-specific data, but is often constrained by the high cost of data collection. Existing machine learning approaches usually struggle to generalize in data-sparse or atypical conditions. To this end, we propose an Augmentation-Adaptive Self-Supervised Learning (A ^2 SL) framework, which retrieves relevant observational samples to enhance modeling of the target ecosystem. Specifically, we introduce a multi-level pairwise learning loss to train a scenario encoder that captures varying degrees of similarity among scenarios. These learned similarities drive a retrieval mechanism that supplements a target scenario with relevant data from different locations or time periods. Furthermore, to better handle variable scenarios, particularly under atypical or extreme conditions where traditional models struggle, we design an augmentation-adaptive mechanism that selectively enhances these scenarios through targeted data augmentation. Using freshwater ecosystems as a case study, we evaluate A ^2 SL in modeling water temperature and dissolved oxygen dynamics in real-world lakes. Experimental results show that A ^2 SL significantly improves predictive accuracy and enhances robustness in data-scarce and atypical scenarios. Although this study focuses on freshwater ecosystems, the A ^2 SL framework offers a broadly applicable solution in various scientific domains.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-50] LiMuon: Light and Fast Muon Optimizer for Large Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14562">https://arxiv.org/abs/2509.14562</a><br>
<strong>作者</strong>: Feihu Huang,Yuning Luo,Songcan Chen<br>
<strong>类目</strong>: Machine Learning (cs.LG); Optimization and Control (math.OC)<br>
*<strong>备注</strong>:  28 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Large models recently are widely applied in artificial intelligence, so efficient training of large models has received widespread attention. More recently, a useful Muon optimizer is specifically designed for matrix-structured parameters of large models. Although some works have begun to studying Muon optimizer, the existing Muon and its variants still suffer from high sample complexity or high memory for large models. To fill this gap, we propose a light and fast Muon (LiMuon) optimizer for training large models, which builds on the momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory than the current Muon and its variants. Moreover, we prove that our LiMuon has a lower sample complexity of  O(\epsilon^-3)  for finding an  \epsilon -stationary solution of non-convex stochastic optimization under the smooth condition. Recently, the existing convergence analysis of Muon optimizer mainly relies on the strict Lipschitz smooth assumption, while some artificial intelligence tasks such as training large language models (LLMs) do not satisfy this condition. We also proved that our LiMuon optimizer has a sample complexity of  O(\epsilon^-3)  under the generalized smooth condition. Numerical experimental results on training DistilGPT2 and ViT models verify efficiency of our LiMuon optimizer.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-51] Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14536">https://arxiv.org/abs/2509.14536</a><br>
<strong>作者</strong>: Muhammad Awais Ali,Marlon Dumas,Fredrik Milani<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Predictive process monitoring techniques support the operational decision making by predicting future states of ongoing cases of a business process. A subset of these techniques predict the remaining sequence of activities of an ongoing case (case suffix prediction). Existing approaches for case suffix prediction generate sequences of activities with a single timestamp (e.g. the end timestamp). This output is insufficient for resource capacity planning, where we need to reason about the periods of time when resources will be busy performing work. This paper introduces a technique for predicting case suffixes consisting of activities with start and end timestamps. In other words, the proposed technique predicts both the waiting time and the processing time of each activity. Since the waiting time of an activity in a case depends on how busy resources are in other cases, the technique adopts a sweep-line approach, wherein the suffixes of all ongoing cases in the process are predicted in lockstep, rather than predictions being made for each case in isolation. An evaluation on real-life and synthetic datasets compares the accuracy of different instantiations of this approach, demonstrating the advantages of a multi-model approach to case suffix prediction.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-52] Decentralized Optimization with Topology-Independent Communication</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14488">https://arxiv.org/abs/2509.14488</a><br>
<strong>作者</strong>: Ying Lin,Yao Kuang,Ahmet Alacaoglu,Michael P. Friedlander<br>
<strong>类目</strong>: Machine Learning (cs.LG); Optimization and Control (math.OC)<br>
*<strong>备注</strong>:  36 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Distributed optimization requires nodes to coordinate, yet full synchronization scales poorly. When  n  nodes collaborate through  m  pairwise regularizers, standard methods demand  \mathcalO(m)  communications per iteration. This paper proposes randomized local coordination: each node independently samples one regularizer uniformly and coordinates only with nodes sharing that term. This exploits partial separability, where each regularizer  G_j  depends on a subset  S_j \subseteq \1,\ldots,n\  of nodes. For graph-guided regularizers where  |S_j|=2 , expected communication drops to exactly 2 messages per iteration. This method achieves  \tilde\mathcalO(\varepsilon^-2)  iterations for convex objectives and under strong convexity,  \mathcalO(\varepsilon^-1)  to an  \varepsilon -solution and  \mathcalO(\log(1/\varepsilon))  to a neighborhood. Replacing the proximal map of the sum  \sum_j G_j  with the proximal map of a single randomly selected regularizer  G_j  preserves convergence while eliminating global coordination. Experiments validate both convergence rates and communication efficiency across synthetic and real-world datasets.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-53] H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14472">https://arxiv.org/abs/2509.14472</a><br>
<strong>作者</strong>: Mahsa Khazaei,Azim Ahmadzadeh,Alexei Pevtsov,Luca Bertello,Alexander Pevtsov<br>
<strong>类目</strong>: Machine Learning (cs.LG); Instrumentation and Methods for Astrophysics (<a target="_blank" rel="noopener" href="http://astro-ph.IM">astro-ph.IM</a>); Solar and Stellar Astrophysics (<a target="_blank" rel="noopener" href="http://astro-ph.SR">astro-ph.SR</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The plethora of space-borne and ground-based observatories has provided astrophysicists with an unprecedented volume of data, which can only be processed at scale using advanced computing algorithms. Consequently, ensuring the quality of data fed into machine learning (ML) models is critical. The H \alpha  observations from the GONG network represent one such data stream, producing several observations per minute, 24/7, since 2010. In this study, we introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha Anomalyzer, designed to identify anomalous observations based on user-defined criteria. Unlike many black-box algorithms, our approach highlights exactly which regions triggered the anomaly flag and quantifies the corresponding anomaly likelihood. For our comparative analysis, we also created and released a dataset of 2,000 observations, equally divided between anomalous and non-anomalous cases. Our results demonstrate that the proposed model not only outperforms existing methods but also provides explainability, enabling qualitative evaluation by domain experts.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-54] FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport <mark class="hl-label red">ICASSP</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14444">https://arxiv.org/abs/2509.14444</a><br>
<strong>作者</strong>: Herlock(SeyedAbolfazl)Rahimi,Dionysis Kalogerias<br>
<strong>类目</strong>: Machine Learning (cs.LG); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:  5 pages, 1 figure, ICASSP</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Federated Learning (FL) allows distributed model training without sharing raw data, but suffers when client participation is partial. In practice, the distribution of available users (\emphavailability distribution  q ) rarely aligns with the distribution defining the optimization objective (\emphimportance distribution  p ), leading to biased and unstable updates under classical FedAvg. We propose \textbfFereated AVerage with Optimal Transport (\textbfFedAVOT), which formulates aggregation as a masked optimal transport problem aligning  q  and  p . Using Sinkhorn scaling, \textbfFedAVOT computes transport-based aggregation weights with provable convergence guarantees. \textbfFedAVOT achieves a standard  \mathcalO(1/\sqrtT)  rate under a nonsmooth convex FL setting, independent of the number of participating users per round. Our experiments confirm drastically improved performance compared to FedAvg across heterogeneous, fairness-sensitive, and low-availability regimes, even when only two clients participate per round.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-55] Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14427">https://arxiv.org/abs/2509.14427</a><br>
<strong>作者</strong>: Ilyass Moummad,Kawtar Zaher,Lukas Rauch,Alexis Joly<br>
<strong>类目</strong>: Machine Learning (cs.LG); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Information retrieval with compact binary embeddings, also referred to as hashing, is crucial for scalable fast search applications, yet state-of-the-art hashing methods require expensive, scenario-specific training. In this work, we introduce Hashing-Baseline, a strong training-free hashing method leveraging powerful pretrained encoders that produce rich pretrained embeddings. We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization, to produce a strong baseline for hashing. Our approach combines these techniques with frozen embeddings from state-of-the-art vision and audio encoders to yield competitive retrieval performance without any additional learning or fine-tuning. To demonstrate the generality and effectiveness of this approach, we evaluate it on standard image retrieval benchmarks as well as a newly introduced benchmark for audio hashing.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-56] Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14386">https://arxiv.org/abs/2509.14386</a><br>
<strong>作者</strong>: Arjun S. Nair,Kristina P. Sinaga<br>
<strong>类目</strong>: Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  30 pages, 13 figures, 8 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We prove a fundamental impossibility theorem: neural networks cannot simultaneously learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision. Through rigorous mathematical analysis and comprehensive empirical evaluation spanning negative reward training, symmetric loss functions, and post-hoc calibration methods, we demonstrate this is an information-theoretic constraint, not a methodological failure. Our experiments reveal universal failure patterns: negative rewards produce extreme underconfidence (ECE greater than 0.8) while destroying confidence diversity (std less than 0.05), symmetric losses fail to escape binary signal averaging, and post-hoc methods achieve calibration (ECE less than 0.02) only by compressing the confidence distribution. We formalize this as an underspecified mapping problem where binary signals cannot distinguish between different confidence levels for correct predictions: a 60 percent confident correct answer receives identical supervision to a 90 percent confident one. Crucially, our real-world validation shows 100 percent failure rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while post-hoc calibration’s 33 percent success rate paradoxically confirms our theorem by achieving calibration through transformation rather than learning. This impossibility directly explains neural network hallucinations and establishes why post-hoc calibration is mathematically necessary, not merely convenient. We propose novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning that could overcome these fundamental limitations without requiring human confidence annotations.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-57] A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14384">https://arxiv.org/abs/2509.14384</a><br>
<strong>作者</strong>: Nishantak Panigrahi,Mayank Patwal<br>
<strong>类目</strong>: Machine Learning (cs.LG); Numerical Analysis (<a target="_blank" rel="noopener" href="http://math.NA">math.NA</a>)<br>
*<strong>备注</strong>:  6 pages, 10 figures. Presented at IEEE International Conference on Compute, Control, Network  Photonics (ICCCNP), 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we investigate the efficiency of Deep Neural Networks (DNNs) to approximate the solution of a nonlocal conservation law derived from the identical-oscillator Kuramoto model, focusing on the evaluation of an architectural choice and its impact on solution accuracy based on the energy norm and computation time. Through systematic experimentation, we demonstrate that network configuration parameters-specifically, activation function selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count)-significantly influence convergence characteristics. We observe that tanh activation yields stable convergence across configurations, whereas sine activation can attain marginally lower errors and training times in isolated cases, but occasionally produce nonphysical artefacts. Our comparative analysis with traditional numerical methods shows that optimally configured DNNs offer competitive accuracy with notably different computational trade-offs. Furthermore, we identify fundamental limitations of standard feed-forward architectures when handling singular or piecewise-constant solutions, providing empirical evidence that such networks inherently oversmooth sharp features due to the natural function space limitations of standard activation functions. This work contributes to the growing body of research on neural network-based scientific computing by providing practitioners with empirical guidelines for DNN implementation while illuminating fundamental theoretical constraints that must be overcome to expand their applicability to more challenging physical systems with discontinuities.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-58] Normalized Square Root: Sharper Matrix Factorization Bounds for Differentially Private Continual Counting</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14334">https://arxiv.org/abs/2509.14334</a><br>
<strong>作者</strong>: Monika Henzinger,Nikita P. Kalinin,Jalaj Upadhyay<br>
<strong>类目</strong>: Data Structures and Algorithms (cs.DS); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The factorization norms of the lower-triangular all-ones  n \times n  matrix,  \gamma_2(M_count)  and  \gamma_F(M_count) , play a central role in differential privacy as they are used to give theoretical justification of the accuracy of the only known production-level private training algorithm of deep neural networks by Google. Prior to this work, the best known upper bound on  \gamma_2(M_count)  was  1 + \frac\log n\pi  by Mathias (Linear Algebra and Applications, 1993), and the best known lower bound was  \frac1\pi(2 + \log(\frac2n+13)) \approx 0.507 + \frac\log n\pi  (Matoušek, Nikolov, Talwar, IMRN 2020), where  \log  denotes the natural logarithm. Recently, Henzinger and Upadhyay (SODA 2025) gave the first explicit factorization that meets the bound of Mathias (1993) and asked whether there exists an explicit factorization that improves on Mathias’ bound. We answer this question in the affirmative. Additionally, we improve the lower bound significantly. More specifically, we show that    0.701 + \frac\log n\pi + o(1) ;\leq; \gamma_2(M_count) ;\leq; 0.846 + \frac\log n\pi + o(1).    That is, we reduce the gap between the upper and lower bound to  0.14 + o(1) . We also show that our factors achieve a better upper bound for  \gamma_F(M_count)  compared to prior work, and we establish an improved lower bound:    0.701 + \frac\log n\pi + o(1) ;\leq; \gamma_F(M_count) ;\leq; 0.748 + \frac\log n\pi + o(1).    That is, the gap between the lower and upper bound provided by our explicit factorization is  0.047 + o(1) .         Subjects:  Data Structures and Algorithms (cs.DS); Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Machine Learning (cs.LG)  Cite as: arXiv:2509.14334 [cs.DS]    (or  arXiv:2509.14334v1 [cs.DS] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14334">https://doi.org/10.48550/arXiv.2509.14334</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)        Submission history From: Nikita P. Kalinin [view email]       [v1]         Wed, 17 Sep 2025 18:04:28 UTC (140 KB)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-59] Monitoring Machine Learning Systems: A Multivocal Literature Review</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14294">https://arxiv.org/abs/2509.14294</a><br>
<strong>作者</strong>: Hira Naveed,Scott Barnett,Chetan Arora,John Grundy,Hourieh Khalajzadeh,Omar Haggag<br>
<strong>类目</strong>: oftware Engineering (<a target="_blank" rel="noopener" href="http://cs.SE">cs.SE</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Context: Dynamic production environments make it challenging to maintain reliable machine learning (ML) systems. Runtime issues, such as changes in data patterns or operating contexts, that degrade model performance are a common occurrence in production settings. Monitoring enables early detection and mitigation of these runtime issues, helping maintain users’ trust and prevent unwanted consequences for organizations. Aim: This study aims to provide a comprehensive overview of the ML monitoring literature. Method: We conducted a multivocal literature review (MLR) following the well established guidelines by Garousi to investigate various aspects of ML monitoring approaches in 136 papers. Results: We analyzed selected studies based on four key areas: (1) the motivations, goals, and context; (2) the monitored aspects, specific techniques, metrics, and tools; (3) the contributions and benefits; and (4) the current limitations. We also discuss several insights found in the studies, their implications, and recommendations for future research and practice. Conclusion: Our MLR identifies and summarizes ML monitoring practices and gaps, emphasizing similarities and disconnects between formal and gray literature. Our study is valuable for both academics and practitioners, as it helps select appropriate solutions, highlights limitations in current approaches, and provides future directions for research and tool development.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-60] A Multi-<mark class="hl-label green">Agent</mark>  <mark class="hl-label green">LLM</mark>  Defense Pipeline Against Prompt Injection Attacks</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14285">https://arxiv.org/abs/2509.14285</a><br>
<strong>作者</strong>: S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Prompt injection attacks represent a major vulnerability in Large Language Model (LLM) deployments, where malicious instructions embedded in user inputs can override system prompts and induce unintended behaviors. This paper presents a novel multi-agent defense framework that employs specialized LLM agents in coordinated pipelines to detect and neutralize prompt injection attacks in real-time. We evaluate our approach using two distinct architectures: a sequential chain-of-agents pipeline and a hierarchical coordinator-based system. Our comprehensive evaluation on 55 unique prompt injection attacks, grouped into 8 categories and totaling 400 attack instances across two LLM platforms (ChatGLM and Llama2), demonstrates significant security improvements. Without defense mechanisms, baseline Attack Success Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent pipeline achieved 100% mitigation, reducing ASR to 0% across all tested scenarios. The framework demonstrates robustness across multiple attack categories including direct overrides, code execution attempts, data exfiltration, and obfuscation techniques, while maintaining system functionality for legitimate queries.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-61] Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of <mark class="hl-label green">GPT</mark> -3 and Contemporary Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14271">https://arxiv.org/abs/2509.14271</a><br>
<strong>作者</strong>: Gustavo Sandoval,Denys Fenchenko,Junyao Chen<br>
<strong>类目</strong>: Cryptography and Security (<a target="_blank" rel="noopener" href="http://cs.CR">cs.CR</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper documents early research conducted in 2022 on defending against prompt injection attacks in large language models, providing historical context for the evolution of this critical security domain. This research focuses on two adversarial attacks against Large Language Models (LLMs): prompt injection and goal hijacking. We examine how to construct these attacks, test them on various LLMs, and compare their effectiveness. We propose and evaluate a novel defense technique called Adversarial Fine-Tuning. Our results show that, without this defense, the attacks succeeded 31% of the time on GPT-3 series models. When using our Adversarial Fine-Tuning approach, attack success rates were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie), though we note that subsequent research has revealed limitations of fine-tuning-based defenses. We also find that more flexible models exhibit greater vulnerability to these attacks. Consequently, large models such as GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the specific models tested are now superseded, the core methodology and empirical findings contributed to the foundation of modern prompt injection defense research, including instruction hierarchy systems and constitutional AI approaches.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-62] Asymptotic Study of In-context Learning with Random Transformers through Equivalent Models</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15152">https://arxiv.org/abs/2509.15152</a><br>
<strong>作者</strong>: Samet Demir,Zafer Dogan<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  MLSP 2025, 6 pages 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We study the in-context learning (ICL) capabilities of pretrained Transformers in the setting of nonlinear regression. Specifically, we focus on a random Transformer with a nonlinear MLP head where the first layer is randomly initialized and fixed while the second layer is trained. Furthermore, we consider an asymptotic regime where the context length, input dimension, hidden dimension, number of training tasks, and number of training samples jointly grow. In this setting, we show that the random Transformer behaves equivalent to a finite-degree Hermite polynomial model in terms of ICL error. This equivalence is validated through simulations across varying activation functions, context lengths, hidden layer widths (revealing a double-descent phenomenon), and regularization settings. Our results offer theoretical and empirical insights into when and how MLP layers enhance ICL, and how nonlinearity and over-parameterization influence model performance.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-63] Next-Depth Lookahead Tree</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15143">https://arxiv.org/abs/2509.15143</a><br>
<strong>作者</strong>: Jaeho Lee,Kangjin Kim,Gyeong Taek Lee<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  25 pages, 2 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes the Next-Depth Lookahead Tree (NDLT), a single-tree model designed to improve performance by evaluating node splits not only at the node being optimized but also by evaluating the quality of the next depth level.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-64] Benefits of Online Tilted Empirical Risk Minimization: A Case Study of Outlier Detection and Robust Regression</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15141">https://arxiv.org/abs/2509.15141</a><br>
<strong>作者</strong>: Yigit E. Yildirim,Samet Demir,Zafer Dogan<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  MLSP 2025, 6 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Empirical Risk Minimization (ERM) is a foundational framework for supervised learning but primarily optimizes average-case performance, often neglecting fairness and robustness considerations. Tilted Empirical Risk Minimization (TERM) extends ERM by introducing an exponential tilt hyperparameter  t  to balance average-case accuracy with worst-case fairness and robustness. However, in online or streaming settings where data arrive one sample at a time, the classical TERM objective degenerates to standard ERM, losing tilt sensitivity. We address this limitation by proposing an online TERM formulation that removes the logarithm from the classical objective, preserving tilt effects without additional computational or memory overhead. This formulation enables a continuous trade-off controlled by  t , smoothly interpolating between ERM ( t \to 0 ), fairness emphasis ( t  0 ), and robustness to outliers ( t  0 ). We empirically validate online TERM on two representative streaming tasks: robust linear regression with adversarial outliers and minority-class detection in binary classification. Our results demonstrate that negative tilting effectively suppresses outlier influence, while positive tilting improves recall with minimal impact on precision, all at per-sample computational cost equivalent to ERM. Online TERM thus recovers the full robustness-fairness spectrum of classical TERM in an efficient single-sample learning regime.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-65] Learning Rate Should Scale Inversely with High-Order Data Moments in High-Dimensional Online Independent Component Analysis</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15127">https://arxiv.org/abs/2509.15127</a><br>
<strong>作者</strong>: M. Oguzhan Gultekin,Samet Demir,Zafer Dogan<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  MLSP 2025, 6 pages, 3 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We investigate the impact of high-order moments on the learning dynamics of an online Independent Component Analysis (ICA) algorithm under a high-dimensional data model composed of a weighted sum of two non-Gaussian random variables. This model allows precise control of the input moment structure via a weighting parameter. Building on an existing ordinary differential equation (ODE)-based analysis in the high-dimensional limit, we demonstrate that as the high-order moments increase, the algorithm exhibits slower convergence and demands both a lower learning rate and greater initial alignment to achieve informative solutions. Our findings highlight the algorithm’s sensitivity to the statistical structure of the input data, particularly its moment characteristics. Furthermore, the ODE framework reveals a critical learning rate threshold necessary for learning when moments approach their maximum. These insights motivate future directions in moment-aware initialization and adaptive learning rate strategies to counteract the degradation in learning speed caused by high non-Gaussianity, thereby enhancing the robustness and efficiency of ICA in complex, high-dimensional settings.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-66] Shedding Light on Dark Matter at the LHC with Machine Learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15121">https://arxiv.org/abs/2509.15121</a><br>
<strong>作者</strong>: Ernesto Arganda,Martín de los Rios,Andres D. Perez,Subhojit Roy,Rosa M. Sandá Seoane,Carlos E. M. Wagner<br>
<strong>类目</strong>: High Energy Physics - Phenomenology (hep-ph); Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)<br>
*<strong>备注</strong>:  24 pages + references, 5 figures, 8 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We investigate a WIMP dark matter (DM) candidate in the form of a singlino-dominated lightest supersymmetric particle (LSP) within the  Z_3 -symmetric Next-to-Minimal Supersymmetric Standard Model. This framework gives rise to regions of parameter space where DM is obtained via co-annihilation with nearby higgsino-like electroweakinos and DM direct detection~signals are suppressed, the so-called ``blind spots&quot;. On the other hand, collider signatures remain promising due to enhanced radiative decay modes of higgsinos into the singlino-dominated LSP and a photon, rather than into leptons or hadrons. This motivates searches for radiatively decaying neutralinos, however, these signals face substantial background challenges, as the decay products are typically soft due to the small mass-splits ( \Delta m ) between the LSP and the higgsino-like coannihilation partners. We apply a data-driven Machine Learning (ML) analysis that improves sensitivity to these subtle signals, offering a powerful complement to traditional search strategies to discover a new physics scenario. Using an LHC integrated luminosity of  100~\mathrmfb^-1  at  14~\mathrmTeV , the method achieves a  5\sigma  discovery reach for higgsino masses up to  225~\mathrmGeV  with  \Delta m!\lesssim!12~\mathrmGeV , and a  2\sigma  exclusion up to  285~\mathrmGeV  with  \Delta m!\lesssim!20~\mathrmGeV . These results highlight the power of collider searches to probe DM candidates that remain hidden from current direct detection experiments, and provide a motivation for a search by the LHC collaborations using ML methods.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-67] Real-Time Streaming Mel Vocoding with Generative Flow Matching</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15085">https://arxiv.org/abs/2509.15085</a><br>
<strong>作者</strong>: Simon Welker,Tal Peer,Timo Gerkmann<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Machine Learning (cs.LG); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Signal Processing (eess.SP)<br>
*<strong>备注</strong>:  © 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The task of Mel vocoding, i.e., the inversion of a Mel magnitude spectrogram to an audio waveform, is still a key component in many text-to-speech (TTS) systems today. Based on generative flow matching, our prior work on generative STFT phase retrieval (DiffPhase), and the pseudoinverse operator of the Mel filterbank, we develop MelFlow, a streaming-capable generative Mel vocoder for speech sampled at 16 kHz with an algorithmic latency of only 32 ms and a total latency of 48 ms. We show real-time streaming capability at this latency not only in theory, but in practice on a consumer laptop GPU. Furthermore, we show that our model achieves substantially better PESQ and SI-SDR values compared to well-established not streaming-capable baselines for Mel vocoding including HiFi-GAN.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-68] Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15029">https://arxiv.org/abs/2509.15029</a><br>
<strong>作者</strong>: Hamidreza Razavi,Nele Moelans<br>
<strong>类目</strong>: Materials Science (cond-mat.mtrl-sci); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper presents a physics-informed framework that integrates graph convolutional networks (GCN) with long short-term memory (LSTM) architecture to forecast microstructure evolution over long time horizons in both 2D and 3D with remarkable performance across varied metrics. The proposed framework is composition-aware, trained jointly on datasets with different compositions, and operates in latent graph space, which enables the model to capture compositions and morphological dynamics while remaining computationally efficient. Compressing and encoding phase-field simulation data with convolutional autoencoders and operating in Latent graph space facilitates efficient modeling of microstructural evolution across composition, dimensions, and long-term horizons. The framework captures the spatial and temporal patterns of evolving microstructures while enabling long-range forecasting at reduced computational cost after training.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-69] Undersampled Phase Retrieval with Image Priors</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15026">https://arxiv.org/abs/2509.15026</a><br>
<strong>作者</strong>: Stanislas Ducotterd,Zhiyuan Hu,Michael Unser,Jonathan Dong<br>
<strong>类目</strong>: Image and Video Processing (eess.IV); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Phase retrieval seeks to recover a complex signal from amplitude-only measurements, a challenging nonlinear inverse problem. Current theory and algorithms often ignore signal priors. By contrast, we evaluate here a variety of image priors in the context of severe undersampling with structured random Fourier measurements. Our results show that those priors significantly improve reconstruction, allowing accurate reconstruction even below the weak recovery threshold.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-70] BabyHuBERT: Multilingual Self-Supervised Learning for Segmenting Speakers in Child-Centered Long-Form Recordings</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15001">https://arxiv.org/abs/2509.15001</a><br>
<strong>作者</strong>: Théo Charlot,Tarek Kunze,Maxime Poli,Alejandrina Cristia,Emmanuel Dupoux,Marvin Lavechin<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Machine Learning (cs.LG); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>)<br>
*<strong>备注</strong>:  5 pages, 1 figure</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Child-centered long-form recordings are essential for studying early language development, but existing speech models trained on clean adult data perform poorly due to acoustic and linguistic differences. We introduce BabyHuBERT, the first self-supervised speech representation model trained on 13,000 hours of multilingual child-centered long-form recordings spanning over 40 languages. We evaluate BabyHuBERT on speaker segmentation, identifying when target children speak versus female adults, male adults, or other children – a fundamental preprocessing step for analyzing naturalistic language experiences. BabyHuBERT achieves F1-scores from 52.1% to 74.4% across six diverse datasets, consistently outperforming W2V2-LL4300 (trained on English long-forms) and standard HuBERT (trained on clean adult speech). Notable improvements include 13.2 absolute F1 points over HuBERT on Vanuatu and 15.9 points on Solomon Islands corpora, demonstrating effectiveness on underrepresented languages. By sharing code and models, BabyHuBERT serves as a foundation model for child speech research, enabling fine-tuning on diverse downstream tasks.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-71] owards universal property prediction in Cartesian space: TACE is all you need</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14961">https://arxiv.org/abs/2509.14961</a><br>
<strong>作者</strong>: Zemin Xu,Wenbo Xie,Daiqian Xie,P. Hu<br>
<strong>类目</strong>: Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>); Materials Science (cond-mat.mtrl-sci); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Machine learning has revolutionized atomistic simulations and materials science, yet current approaches often depend on spherical-harmonic representations. Here we introduce the Tensor Atomic Cluster Expansion and Tensor Moment Potential, the first unified framework formulated entirely in Cartesian space for the systematic prediction of arbitrary structure-determined tensorial properties. TACE achieves this by decomposing atomic environments into a complete hierarchy of (irreducible) Cartesian tensors, ensuring symmetry-consistent representations that naturally encode invariance and equivariance constraints. Beyond geometry, TACE incorporates universal embeddings that flexibly integrate diverse attributes including basis sets, charges, magnetic moments and field perturbations. This allows explicit control over external invariants and equivariants in the prediction process. Long-range interactions are also accurately described through the Latent Ewald Summation module within the short-range approximation, providing a rigorous yet computationally efficient treatment of electrostatic interactions. We demonstrate that TACE attains accuracy, stability, and efficiency on par with or surpassing leading equivariant frameworks across finite molecules and extended materials, including in-domain and out-of-domain benchmarks, spectra, hessians, external-field response, charged systems, magnetic systems, multi-fidelity training, and heterogeneous catalytic systems. Crucially, TACE bridges scalar and tensorial modeling and establishes a Cartesian-space paradigm that unifies and extends beyond the design space of spherical-harmonic-based methods. This work lays the foundation for a new generation of universal atomistic machine learning models capable of systematically capturing the rich interplay of geometry, fields and material properties within a single coherent framework.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-72] Mitigating data replication in text-to-audio generative diffusion models through anti-memorization guidance</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14934">https://arxiv.org/abs/2509.14934</a><br>
<strong>作者</strong>: Francisco Messina,Francesca Ronchini,Luca Comanducci,Paolo Bestagini,Fabio Antonacci<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Machine Learning (cs.LG); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>); Signal Processing (eess.SP)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:A persistent challenge in generative audio models is data replication, where the model unintentionally generates parts of its training data during inference. In this work, we address this issue in text-to-audio diffusion models by exploring the use of anti-memorization strategies. We adopt Anti-Memorization Guidance (AMG), a technique that modifies the sampling process of pre-trained diffusion models to discourage memorization. Our study explores three types of guidance within AMG, each designed to reduce replication while preserving generation quality. We use Stable Audio Open as our backbone, leveraging its fully open-source architecture and training dataset. Our comprehensive experimental analysis suggests that AMG significantly mitigates memorization in diffusion-based text-to-audio generation without compromising audio fidelity or semantic alignment.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-73] Inspired by machine learning optimization: can gradient-based optimizers solve cycle skipping in full waveform inversion given sufficient iterations?</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14919">https://arxiv.org/abs/2509.14919</a><br>
<strong>作者</strong>: Xinru Mu,Omar M. Saad,Shaowen Wang,Tariq Alkhalifah<br>
<strong>类目</strong>: Geophysics (physics.geo-ph); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  40 pages, 40 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Full waveform inversion (FWI) iteratively updates the velocity model by minimizing the difference between observed and simulated data. Due to the high computational cost and memory requirements associated with global optimization algorithms, FWI is typically implemented using local optimization methods. However, when the initial velocity model is inaccurate and low-frequency seismic data (e.g., below 3 Hz) are absent, the mismatch between simulated and observed data may exceed half a cycle, a phenomenon known as cycle skipping. In such cases, local optimization algorithms (e.g., gradient-based local optimizers) tend to converge to local minima, leading to inaccurate inversion results. In machine learning, neural network training is also an optimization problem prone to local minima. It often employs gradient-based optimizers with a relatively large learning rate (beyond the theoretical limits of local optimization that are usually determined numerically by a line search), which allows the optimization to behave like a quasi-global optimizer. Consequently, after training for several thousand iterations, we can obtain a neural network model with strong generative capability. In this study, we also employ gradient-based optimizers with a relatively large learning rate for FWI. Results from both synthetic and field data experiments show that FWI may initially converge to a local minimum; however, with sufficient additional iterations, the inversion can gradually approach the global minimum, slowly from shallow subsurface to deep, ultimately yielding an accurate velocity model. Furthermore, numerical examples indicate that, given sufficient iterations, reasonable velocity inversion results can still be achieved even when low-frequency data below 5 Hz are missing.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-74] Beyond Spherical geometry: Unraveling complex features of objects orbiting around stars from its transit light curve using deep learning</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14875">https://arxiv.org/abs/2509.14875</a><br>
<strong>作者</strong>: Ushasi Bhowmick,Shivam Kumaran<br>
<strong>类目</strong>: Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (<a target="_blank" rel="noopener" href="http://astro-ph.IM">astro-ph.IM</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  16 pages, 17 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Characterizing the geometry of an object orbiting around a star from its transit light curve is a powerful tool to uncover various complex phenomena. This problem is inherently ill-posed, since similar or identical light curves can be produced by multiple different shapes. In this study, we investigate the extent to which the features of a shape can be embedded in a transit light curve. We generate a library of two-dimensional random shapes and simulate their transit light curves with light curve simulator, Yuti. Each shape is decomposed into a series of elliptical components expressed in the form of Fourier coefficients that adds increasingly diminishing perturbations to an ideal ellipse. We train deep neural networks to predict these Fourier coefficients directly from simulated light curves. Our results demonstrate that the neural network can successfully reconstruct the low-order ellipses, which describe overall shape, orientation and large-scale perturbations. For higher order ellipses the scale is successfully determined but the inference of eccentricity and orientation is limited, demonstrating the extent of shape information in the light curve. We explore the impact of non-convex shape features in reconstruction, and show its dependence on shape orientation. The level of reconstruction achieved by the neural network underscores the utility of using light curves as a means to extract geometric information from transiting systems.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-75] Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14844">https://arxiv.org/abs/2509.14844</a><br>
<strong>作者</strong>: Francesco C. Mantegazza,Federica Caforio,Christoph Augustin,Matthias A.F. Gsell,Gundolf Haase,Elias Karabelas<br>
<strong>类目</strong>: Medical Physics (physics.med-ph); Machine Learning (cs.LG); Numerical Analysis (<a target="_blank" rel="noopener" href="http://math.NA">math.NA</a>)<br>
*<strong>备注</strong>:  42 pages, 12 figures, 6 tables</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots – no governing equations, assembly routines, or solver access – enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-76] Sampling Method for Generalized Graph Signals with Pre-selected Vertices via DC Optimization</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14836">https://arxiv.org/abs/2509.14836</a><br>
<strong>作者</strong>: Keitaro Yamashita,Kazuki Naganuma,Shunsuke Ono<br>
<strong>类目</strong>: ignal Processing (eess.SP); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Submitted to the IEEE Open Journal of Signal Processing</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:This paper proposes a method for vertex-wise flexible sampling of a broad class of graph signals, designed to attain the best possible recovery based on the generalized sampling theory. This is achieved by designing a sampling operator by an optimization problem, which is inherently non-convex, as the best possible recovery imposes a rank constraint. An existing method for vertex-wise flexible sampling is able to control the number of active vertices but cannot incorporate prior knowledge of mandatory or forbidden vertices. To address these challenges, we formulate the operator design as a problem that handles a constraint of the number of active vertices and prior knowledge on specific vertices for sampling, mandatory inclusion or exclusion. We transformed this constrained problem into a difference-of-convex (DC) optimization problem by using the nuclear norm and a DC penalty for vertex selection. To solve this, we develop a convergent solver based on the general double-proximal gradient DC algorithm. The effectiveness of our method is demonstrated through experiments on various graph signal models, including real-world data, showing superior performance in the recovery accuracy by comparing to existing methods.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-77] Aligning Audio Captions with Human Preferences <mark class="hl-label red">ICASSP2026</mark></p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14659">https://arxiv.org/abs/2509.14659</a><br>
<strong>作者</strong>: Kartik Hegde,Rehana Mahfuz,Yinyi Guo,Erik Visser<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Machine Learning (cs.LG); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>)<br>
*<strong>备注</strong>:  Submitted to ICASSP 2026</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Current audio captioning systems rely heavily on supervised learning with paired audio-caption datasets, which are expensive to curate and may not reflect human preferences in real-world scenarios. To address this limitation, we propose a preference-aligned audio captioning framework based on Reinforcement Learning from Human Feedback (RLHF). To effectively capture nuanced human preferences, we train a Contrastive Language-Audio Pretraining (CLAP)-based reward model using human-labeled pairwise preference data. This reward model is integrated into a reinforcement learning framework to fine-tune any baseline captioning system without relying on ground-truth caption annotations. Extensive human evaluations across multiple datasets show that our method produces captions preferred over those from baseline models, particularly in cases where the baseline models fail to provide correct and natural captions. Furthermore, our framework achieves performance comparable to supervised approaches with ground-truth data, demonstrating its effectiveness in aligning audio captioning with human preferences and its scalability in real-world scenarios.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-78] Radiolunadiff: Estimation of wireless network signal strength in lunar terrain</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14559">https://arxiv.org/abs/2509.14559</a><br>
<strong>作者</strong>: Paolo Torrado,Anders Pearson,Jason Klein,Alexander Moscibroda,Joshua Smith<br>
<strong>类目</strong>: ignal Processing (eess.SP); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we propose a novel physics-informed deep learning architecture for predicting radio maps over lunar terrain. Our approach integrates a physics-based lunar terrain generator, which produces realistic topography informed by publicly available NASA data, with a ray-tracing engine to create a high-fidelity dataset of radio propagation scenarios. Building on this dataset, we introduce a triplet-UNet architecture, consisting of two standard UNets and a diffusion network, to model complex propagation effects. Experimental results demonstrate that our method outperforms existing deep learning approaches on our terrain dataset across various metrics.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-79] Data coarse graining can improve model performance</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14498">https://arxiv.org/abs/2509.14498</a><br>
<strong>作者</strong>: Alex Nguyen,David J. Schwab,Vudtiwat Ngampruetikorn<br>
<strong>类目</strong>: atistical Mechanics (cond-mat.stat-mech); Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Neurons and Cognition (<a target="_blank" rel="noopener" href="http://q-bio.NC">q-bio.NC</a>); Machine Learning (<a target="_blank" rel="noopener" href="http://stat.ML">stat.ML</a>)<br>
*<strong>备注</strong>:  7 pages, 4 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Lossy data transformations by definition lose information. Yet, in modern machine learning, methods like data pruning and lossy data augmentation can help improve generalization performance. We study this paradox using a solvable model of high-dimensional, ridge-regularized linear regression under ‘data coarse graining.’ Inspired by the renormalization group in statistical physics, we analyze coarse-graining schemes that systematically discard features based on their relevance to the learning task. Our results reveal a nonmonotonic dependence of the prediction risk on the degree of coarse graining. A ‘high-pass’ scheme–which filters out less relevant, lower-signal features–can help models generalize better. By contrast, a ‘low-pass’ scheme that integrates out more relevant, higher-signal features is purely detrimental. Crucially, using optimal regularization, we demonstrate that this nonmonotonicity is a distinct effect of data coarse graining and not an artifact of double descent. Our framework offers a clear, analytical explanation for why careful data augmentation works: it strips away less relevant degrees of freedom and isolates more predictive signals. Our results highlight a complex, nonmonotonic risk landscape shaped by the structure of the data, and illustrate how ideas from statistical physics provide a principled lens for understanding modern machine learning phenomena.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-80] Efficiently learning depth-3 circuits via quantum agnostic boosting</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14461">https://arxiv.org/abs/2509.14461</a><br>
<strong>作者</strong>: Srinivasan Arunachalam,Arkopal Dutt,Alexandru Gheorghiu,Michael de Oliveira<br>
<strong>类目</strong>: Quantum Physics (quant-ph); Computational Complexity (<a target="_blank" rel="noopener" href="http://cs.CC">cs.CC</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  52 pages</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We initiate the study of quantum agnostic learning of phase states with respect to a function class  \mathsfC\subseteq \c:\0,1^n\rightarrow \0,1\ : given copies of an unknown  n -qubit state  |\psi\rangle  which has fidelity  \textsfopt  with a phase state  |\phi_c\rangle=\frac1\sqrt2^n\sum_x\in \0,1^n(-1)^c(x)|x\rangle  for some  c\in \mathsfC , output  |\phi\rangle  which has fidelity  |\langle \phi | \psi \rangle|^2 \geq \textsfopt-\varepsilon . To this end, we give agnostic learning protocols for the following classes: (i) Size- t  decision trees which runs in time  \textsfpoly(n,t,1/\varepsilon) . This also implies  k -juntas can be agnostically learned in time  \textsfpoly(n,2^k,1/\varepsilon) . (ii)  s -term DNF formulas in near-polynomial time  \textsfpoly(n,(s/\varepsilon)^\log \log s/\varepsilon) . Our main technical contribution is a quantum agnostic boosting protocol which converts a weak agnostic learner, which outputs a parity state  |\phi\rangle  such that  |\langle \phi|\psi\rangle|^2\geq \textsfopt/\textsfpoly(n) , into a strong learner which outputs a superposition of parity states  |\phi’\rangle  such that  |\langle \phi’|\psi\rangle|^2\geq \textsfopt - \varepsilon . Using quantum agnostic boosting, we obtain the first near-polynomial time  n^O(\log \log n)  algorithm for learning  \textsfpoly(n) -sized depth- 3  circuits (consisting of  \textsfAND ,  \textsfOR ,  \textsfNOT  gates) in the uniform quantum  \textsfPAC  model using quantum examples. Classically, the analogue of efficient learning depth- 3  circuits (and even depth- 2  circuits) in the uniform  \textsfPAC  model has been a longstanding open question in computational learning theory. Our work nearly settles this question, when the learner is given quantum examples.          Comments: 52 pages   Subjects:  Quantum Physics (quant-ph); Computational Complexity (<a target="_blank" rel="noopener" href="http://cs.CC">cs.CC</a>); Machine Learning (cs.LG)  Cite as: arXiv:2509.14461 [quant-ph]    (or  arXiv:2509.14461v1 [quant-ph] for this version)                <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2509.14461">https://doi.org/10.48550/arXiv.2509.14461</a>   Focus to learn more                      arXiv-issued DOI via DataCite (pending registration)</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-81] Indoor Airflow Imaging Using Physics-Informed Background-Oriented Schlieren Tomography</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14442">https://arxiv.org/abs/2509.14442</a><br>
<strong>作者</strong>: Arjun Teh,Wael H. Ali,Joshua Rapp,Hassan Mansour<br>
<strong>类目</strong>: ignal Processing (eess.SP); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Presented in ISCS25</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We develop a framework for non-invasive volumetric indoor airflow estimation from a single viewpoint using background-oriented schlieren (BOS) measurements and physics-informed reconstruction. Our framework utilizes a light projector that projects a pattern onto a target back-wall and a camera that observes small distortions in the light pattern. While the single-view BOS tomography problem is severely ill-posed, our proposed framework addresses this using: (1) improved ray tracing, (2) a physics-based light rendering approach and loss formulation, and (3) a physics-based regularization using a physics-informed neural network (PINN) to ensure that the reconstructed airflow is consistent with the governing equations for buoyancy-driven flows.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-82] Diffusion-Based Unsupervised Audio-Visual Speech Separation in Noisy Environments with Noise Prior</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14379">https://arxiv.org/abs/2509.14379</a><br>
<strong>作者</strong>: Yochai Yemini,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:In this paper, we address the problem of single-microphone speech separation in the presence of ambient noise. We propose a generative unsupervised technique that directly models both clean speech and structured noise components, training exclusively on these individual signals rather than noisy mixtures. Our approach leverages an audio-visual score model that incorporates visual cues to serve as a strong generative speech prior. By explicitly modelling the noise distribution alongside the speech distribution, we enable effective decomposition through the inverse problem paradigm. We perform speech separation by sampling from the posterior distributions via a reverse diffusion process, which directly estimates and removes the modelled noise component to recover clean constituent signals. Experimental results demonstrate promising performance, highlighting the effectiveness of our direct noise modelling approach in challenging acoustic environments.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-83] SpeechOp: Inference-Time Task Composition for Generative Speech Processing</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14298">https://arxiv.org/abs/2509.14298</a><br>
<strong>作者</strong>: Justin Lovelace,Rithesh Kumar,Jiaqi Su,Ke Chen,Kilian Q Weinberger,Zeyu Jin<br>
<strong>类目</strong>: Audio and Speech Processing (<a target="_blank" rel="noopener" href="http://eess.AS">eess.AS</a>); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:While generative Text-to-Speech (TTS) systems leverage vast ``in-the-wild&quot; data to achieve remarkable success, speech-to-speech processing tasks like enhancement face data limitations, which lead data-hungry generative approaches to distort speech content and speaker identity. To bridge this gap, we present SpeechOp, a multi-task latent diffusion model that transforms pre-trained TTS models into a universal speech processor capable of performing a wide range of speech tasks and composing them in novel ways at inference time. By adapting a pre-trained TTS model, SpeechOp inherits a rich understanding of natural speech, accelerating training and improving S2S task quality, while simultaneously enhancing core TTS performance. Finally, we introduce Implicit Task Composition (ITC), a novel pipeline where ASR-derived transcripts (e.g., from Whisper) guide SpeechOp’s enhancement via our principled inference-time task composition. ITC achieves state-of-the-art content preservation by robustly combining web-scale speech understanding with SpeechOp’s generative capabilities. Audio samples are available at this https URL</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-84] Artificial Intelligence-derived Cardiotocography Age as a Digital Biomarker for Predicting Future Adverse Pregnancy Outcomes</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14242">https://arxiv.org/abs/2509.14242</a><br>
<strong>作者</strong>: Jinshuai Gu,Zenghui Lin,Jingying Ma,Jingyu Wang,Linyan Zhang,Rui Bai,Zelin Tu,Youyou Jiang,Donglin Xie,Yuxi Zhou,Guoli Liu,Shenda Hong<br>
<strong>类目</strong>: ignal Processing (eess.SP); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Cardiotocography (CTG) is a low-cost, non-invasive fetal health assessment technique used globally, especially in underdeveloped countries. However, it is currently mainly used to identify the fetus’s current status (e.g., fetal acidosis or hypoxia), and the potential of CTG in predicting future adverse pregnancy outcomes has not been fully explored. We aim to develop an AI-based model that predicts biological age from CTG time series (named CTGage), then calculate the age gap between CTGage and actual age (named CTGage-gap), and use this gap as a new digital biomarker for future adverse pregnancy outcomes. The CTGage model is developed using 61,140 records from 11,385 pregnant women, collected at Peking University People’s Hospital between 2018 and 2022. For model training, a structurally designed 1D convolutional neural network is used, incorporating distribution-aligned augmented regression technology. The CTGage-gap is categorized into five groups:  -21 days (underestimation group), -21 to -7 days, -7 to 7 days (normal group), 7 to 21 days, and  21 days (overestimation group). We further defined the underestimation group and overestimation group together as the high-risk group. We then compare the incidence of adverse outcomes and maternal diseases across these groups. The average absolute error of the CTGage model is 10.91 days. When comparing the overestimation group with the normal group, premature infants incidence is 5.33% vs. 1.42% (p  0.05) and gestational diabetes mellitus (GDM) incidence is 31.93% vs. 20.86% (p  0.05). When comparing the underestimation group with the normal group, low birth weight incidence is 0.17% vs. 0.15% (p  0.05) and anaemia incidence is 37.51% vs. 34.74% (p  0.05). Artificial intelligence-derived CTGage can predict the future risk of adverse pregnancy outcomes and hold potential as a novel, non-invasive, and easily accessible digital biomarker.</p>
</div></div>
<div class="note pink no-icon flat"><p>[LG-85] Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14072">https://arxiv.org/abs/2509.14072</a><br>
<strong>作者</strong>: Vincent Lauinger,Lennart Schmitz,Patrick Matalla,Andrej Rode,Sebastian Randel,Laurent Schmalen<br>
<strong>类目</strong>: ignal Processing (eess.SP); Machine Learning (cs.LG)<br>
*<strong>备注</strong>:  Accepted and to be presented at the European Conference on Optical Communication (ECOC) 2025</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:We demonstrate the effectiveness of a novel phase-noise-tolerant, variational-autoencoder-based equalization scheme for space-division-multiplexed (SDM) transmission in an experiment over 150km of randomly-coupled multi-core fibers.</p>
</div></div>
<h3 id="信息检索">信息检索</h3>
<div class="note pink no-icon flat"><p>[IR-0] What Matters in <mark class="hl-label green">LLM</mark> -Based Feature Extractor for Recommender? A Systematic Analysis of Prompts Models and Adaptation</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14979">https://arxiv.org/abs/2509.14979</a><br>
<strong>作者</strong>: Kainan Shi(Xi’an Jiaotong University),Peilin Zhou(Hong Kong University of Science and Technology (Guangzhou)),Ge Wang(Xi’an Jiaotong University),Han Ding(Xi’an Jiaotong University),Fei Wang(Xi’an Jiaotong University)<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
*<strong>备注</strong>:  9 pages. Keywords: Recommender Systems, Large Language Models, Sequential Recommendation, Feature Extraction</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Using Large Language Models (LLMs) to generate semantic features has been demonstrated as a powerful paradigm for enhancing Sequential Recommender Systems (SRS). This typically involves three stages: processing item text, extracting features with LLMs, and adapting them for downstream models. However, existing methods vary widely in prompting, architecture, and adaptation strategies, making it difficult to fairly compare design choices and identify what truly drives performance. In this work, we propose RecXplore, a modular analytical framework that decomposes the LLM-as-feature-extractor pipeline into four modules: data processing, semantic feature extraction, feature adaptation, and sequential modeling. Instead of proposing new techniques, RecXplore revisits and organizes established methods, enabling systematic exploration of each module in isolation. Experiments on four public datasets show that simply combining the best designs from existing techniques without exhaustive search yields up to 18.7% relative improvement in NDCG@5 and 12.7% in HR@5 over strong baselines. These results underscore the utility of modular benchmarking for identifying effective design patterns and promoting standardized research in LLM-enhanced recommendation.</p>
</div></div>
<div class="note pink no-icon flat"><p>[IR-1] Music4All AA: A Multimodal Dataset for Music Information Retrieval Tasks</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14891">https://arxiv.org/abs/2509.14891</a><br>
<strong>作者</strong>: Jonas Geiger,Marta Moscati,Shah Nawaz,Markus Schedl<br>
<strong>类目</strong>: Multimedia (<a target="_blank" rel="noopener" href="http://cs.MM">cs.MM</a>); Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Sound (<a target="_blank" rel="noopener" href="http://cs.SD">cs.SD</a>)<br>
*<strong>备注</strong>:  7 pages, 6 tables, IEEE International Conference on Content-Based Multimedia Indexing (IEEE CBMI)</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Music is characterized by aspects related to different modalities, such as the audio signal, the lyrics, or the music video clips. This has motivated the development of multimodal datasets and methods for Music Information Retrieval (MIR) tasks such as genre classification or autotagging. Music can be described at different levels of granularity, for instance defining genres at the level of artists or music albums. However, most datasets for multimodal MIR neglect this aspect and provide data at the level of individual music tracks. We aim to fill this gap by providing Music4All Artist and Album (Music4All A+A), a dataset for multimodal MIR tasks based on music artists and albums. Music4All A+A is built on top of the Music4All-Onion dataset, an existing track-level dataset for MIR tasks. Music4All A+A provides metadata, genre labels, image representations, and textual descriptors for 6,741 artists and 19,511 albums. Furthermore, since Music4All A+A is built on top of Music4All-Onion, it allows access to other multimodal data at the track level, including user–item interaction data. This renders Music4All A+A suitable for a broad range of MIR tasks, including multimodal music recommendation, at several levels of granularity. To showcase the use of Music4All A+A, we carry out experiments on multimodal genre classification of artists and albums, including an analysis in missing-modality scenarios, and a quantitative comparison with genre classification in the movie domain. Our experiments show that images are more informative for classifying the genres of artists and albums, and that several multimodal models for genre classification struggle in generalizing across domains. We provide the code to reproduce our experiments at this https URL, the dataset is linked in the repository and provided open-source under a CC BY-NC-SA 4.0 license.</p>
</div></div>
<div class="note pink no-icon flat"><p>[IR-2] Keywords are not always the key: A metadata field analysis for natural language search on open data portals</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14457">https://arxiv.org/abs/2509.14457</a><br>
<strong>作者</strong>: Lisa-Yao Gan,Arunav Das,Johanna Walker,Elena Simperl<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>); Databases (cs.DB); Digital Libraries (cs.DL); Human-Computer Interaction (cs.HC)<br>
*<strong>备注</strong>:  Accepted to CHIRA 2025 as Full Paper</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:Open data portals are essential for providing public access to open datasets. However, their search interfaces typically rely on keyword-based mechanisms and a narrow set of metadata fields. This design makes it difficult for users to find datasets using natural language queries. The problem is worsened by metadata that is often incomplete or inconsistent, especially when users lack familiarity with domain-specific terminology. In this paper, we examine how individual metadata fields affect the success of conversational dataset retrieval and whether LLMs can help bridge the gap between natural queries and structured metadata. We conduct a controlled ablation study using simulated natural language queries over real-world datasets to evaluate retrieval performance under various metadata configurations. We also compare existing content of the metadata field ‘description’ with LLM-generated content, exploring how different prompting strategies influence quality and impact on search outcomes. Our findings suggest that dataset descriptions play a central role in aligning with user intent, and that LLM-generated descriptions can support effective retrieval. These results highlight both the limitations of current metadata practices and the potential of generative models to improve dataset discoverability in open data portals.</p>
</div></div>
<div class="note pink no-icon flat"><p>[IR-3] Overview of the TREC 2024 NeuCLIR Track</p>
</div>
<p><strong>链接</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14355">https://arxiv.org/abs/2509.14355</a><br>
<strong>作者</strong>: Dawn Lawrie,Sean MacAvaney,James Mayfield,Paul McNamee,Douglas W. Oard,Luca Soldaini,Eugene Yang<br>
<strong>类目</strong>: Information Retrieval (<a target="_blank" rel="noopener" href="http://cs.IR">cs.IR</a>)<br>
*<strong>备注</strong>:  28 pages, 13 figures</p>
<div class="hide-toggle" ><div class="hide-button toggle-title" style=""><i class="fas fa-caret-right fa-fw"></i><span>点击查看摘要</span></div>
    <div class="hide-content"><p>Abstract:The principal goal of the TREC Neural Cross-Language Information Retrieval (NeuCLIR) track is to study the effect of neural approaches on cross-language information access. The track has created test collections containing Chinese, Persian, and Russian news stories and Chinese academic abstracts. NeuCLIR includes four task types: Cross-Language Information Retrieval (CLIR) from news, Multilingual Information Retrieval (MLIR) from news, Report Generation from news, and CLIR from technical documents. A total of 274 runs were submitted by five participating teams (and as baselines by the track coordinators) for eight tasks across these four task types. Task descriptions and the available results are presented.</p>
</div></div>
<h3 id="附件下载">附件下载</h3>
<p><a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/lonePatient/lonePatient.github.io/master/arxiv/arxiv_papers_2025-09-19.txt">点击下载今日全部论文列表</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Weitang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lonepatient.top/2025/09/19/arxiv_papers_2025-09-19.html">http://lonepatient.top/2025/09/19/arxiv_papers_2025-09-19.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lonepatient.top" target="_blank">闲记算法</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Arxiv/">Arxiv</a></div><div class="post_share"><div class="social-share" data-image="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/09/22/arxiv_papers_2025-09-22.html"><img class="prev-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Arxiv今日论文 | 2025-09-22</div></div></a></div><div class="next-post pull-right"><a href="/2025/09/18/arxiv_papers_2025-09-18.html"><img class="next-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Arxiv今日论文 | 2025-09-18</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/10/ar5iv.html" title="ar5iv"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20231208121727.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-10</div><div class="title">ar5iv</div></div></a></div><div><a href="/2025/09/30/arxiv_papers_2025-09-30.html" title="Arxiv今日论文 | 2025-09-30"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-30</div><div class="title">Arxiv今日论文 | 2025-09-30</div></div></a></div><div><a href="/2025/08/11/arxiv_papers_2025-08-11.html" title="Arxiv今日论文 | 2025-08-11"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-11</div><div class="title">Arxiv今日论文 | 2025-08-11</div></div></a></div><div><a href="/2025/08/13/arxiv_papers_2025-08-13.html" title="Arxiv今日论文 | 2025-08-13"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-13</div><div class="title">Arxiv今日论文 | 2025-08-13</div></div></a></div><div><a href="/2025/08/14/arxiv_papers_2025-08-14.html" title="Arxiv今日论文 | 2025-08-14"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-14</div><div class="title">Arxiv今日论文 | 2025-08-14</div></div></a></div><div><a href="/2025/08/15/arxiv_papers_2025-08-15.html" title="Arxiv今日论文 | 2025-08-15"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-15</div><div class="title">Arxiv今日论文 | 2025-08-15</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Weitang Liu</div><div class="author-info__description">一个致力于记录技术的博客</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">266</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">306</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">70</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lonePatient"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lonePatient" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuweitangmath@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/277974397" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有任何问题可通过留言板或者微信公众号给我留言，谢谢！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">1.</span> <span class="toc-text">目录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%A7%88-2025-09-19"><span class="toc-number">1.1.</span> <span class="toc-text">概览 (2025-09-19)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">自然语言处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">1.3.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="toc-number">1.4.</span> <span class="toc-text">人工智能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2"><span class="toc-number">1.6.</span> <span class="toc-text">信息检索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%84%E4%BB%B6%E4%B8%8B%E8%BD%BD"><span class="toc-number">1.7.</span> <span class="toc-text">附件下载</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-31"/></a><div class="content"><a class="title" href="/2025/10/31/arxiv_papers_2025-10-31.html" title="Arxiv今日论文 | 2025-10-31">Arxiv今日论文 | 2025-10-31</a><time datetime="2025-10-31T10:30:00.000Z" title="发表于 2025-10-31 10:30:00">2025-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-30"/></a><div class="content"><a class="title" href="/2025/10/30/arxiv_papers_2025-10-30.html" title="Arxiv今日论文 | 2025-10-30">Arxiv今日论文 | 2025-10-30</a><time datetime="2025-10-30T10:30:00.000Z" title="发表于 2025-10-30 10:30:00">2025-10-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-29"/></a><div class="content"><a class="title" href="/2025/10/29/arxiv_papers_2025-10-29.html" title="Arxiv今日论文 | 2025-10-29">Arxiv今日论文 | 2025-10-29</a><time datetime="2025-10-29T10:30:00.000Z" title="发表于 2025-10-29 10:30:00">2025-10-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-28"/></a><div class="content"><a class="title" href="/2025/10/28/arxiv_papers_2025-10-28.html" title="Arxiv今日论文 | 2025-10-28">Arxiv今日论文 | 2025-10-28</a><time datetime="2025-10-28T10:30:00.000Z" title="发表于 2025-10-28 10:30:00">2025-10-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2025-10-27"/></a><div class="content"><a class="title" href="/2025/10/27/arxiv_papers_2025-10-27.html" title="Arxiv今日论文 | 2025-10-27">Arxiv今日论文 | 2025-10-27</a><time datetime="2025-10-27T10:30:00.000Z" title="发表于 2025-10-27 10:30:00">2025-10-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Weitang Liu</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'https://twikoo.lonepatient.top/',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.lonepatient.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo@1.4.11/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (95)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/计算机视觉/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 计算机视觉 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/知识图谱/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 知识图谱 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 深度学习 (135)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="http://lonepatient.top/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230219144729.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-18</span><a class="blog-slider__title" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">通向AGI之路：大型语言模型（LLM）技术精要</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/07/12/gaiic_2022_ner_top10.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20220712181905.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-07-12</span><a class="blog-slider__title" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">GAIIC2022商品标题识别二等奖获奖解决思路</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230807190424.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-01-20</span><a class="blog-slider__title" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">2025-03|高质量中文预训练模型集合</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>