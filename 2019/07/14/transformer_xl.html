<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Transformer-XL Attentive Language Models Beyond a Fixed-Length Context | 闲记算法</title><meta name="keywords" content="语言模型,Transformer-XL,长文本"><meta name="author" content="Weitang Liu"><meta name="copyright" content="Weitang Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="目前在自然语言处理领域，Transformer的编码能力超越了RNN，但是对长距离依赖的建模能力仍然不足。在基于LSTM的模型中，为了建模长距离依赖，提出了门控机制和梯度裁剪，目前可以编码的最长距离在200左右。在基于Transformer的模型中，允许词之间直接self-attention，能够更好地捕获长期依赖关系，但是还是有限制，本文将主要介绍Transformer-XL，并基于PyTorc">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer-XL Attentive Language Models Beyond a Fixed-Length Context">
<meta property="og:url" content="http://lonepatient.top/2019/07/14/transformer_xl.html">
<meta property="og:site_name" content="闲记算法">
<meta property="og:description" content="目前在自然语言处理领域，Transformer的编码能力超越了RNN，但是对长距离依赖的建模能力仍然不足。在基于LSTM的模型中，为了建模长距离依赖，提出了门控机制和梯度裁剪，目前可以编码的最长距离在200左右。在基于Transformer的模型中，允许词之间直接self-attention，能够更好地捕获长期依赖关系，但是还是有限制，本文将主要介绍Transformer-XL，并基于PyTorc">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223223929.jpg">
<meta property="article:published_time" content="2019-07-14T23:20:08.000Z">
<meta property="article:modified_time" content="2026-02-27T07:52:12.942Z">
<meta property="article:author" content="Weitang Liu">
<meta property="article:tag" content="语言模型">
<meta property="article:tag" content="Transformer-XL">
<meta property="article:tag" content="长文本">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223223929.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://lonepatient.top/2019/07/14/transformer_xl"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//s4.cnzz.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script async="async" data-pjax="data-pjax" src="https://s4.cnzz.com/z_stat.php?id=1273275888&amp;web_id=1273275888"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2026-02-27 07:52:12'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/backgroud.css"><script src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><script src="https://at.alicdn.com/t/c/font_3570527_dthoqrrv2tv.css"></script><script charset="UTF-8" id="LA_COLLECT" src="//sdk.51.la/js-sdk-pro.min.js"></script><script>LA.init({id:"JwRQtLKZggvJH4sJ",ck:"JwRQtLKZggvJH4sJ"})</script><link rel="stylesheet" href="/css/universe.css"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3536467946304280" crossorigin="anonymous"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">281</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">384</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">92</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223223929.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">闲记算法</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa-fw fa fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/charts/"><i class="fa-fw fa fa-archive"></i><span> 统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 网站</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://www.aitoolist.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集合</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.deepdh.com"><i class="fa-fw fa fa-star"></i><span> AI工具导航</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://www.ai-lib.club"><i class="fa-fw fa fa-star"></i><span> 人工智能工具</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://ai-bot.cn"><i class="fa-fw fa fa-star"></i><span> AI工具集</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 教程</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" target="_blank" rel="noopener" href="https://learningprompt.wiki/"><i class="fa-fw fa fa-star"></i><span> Prompt教程</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-tools"></i><span> 在线</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/md_editor/"><i class="fa-fw fa fa-star"></i><span> 在线markdown</span></a></li><li><a class="site-page" target="_blank" rel="noopener" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts"><i class="fa-fw fa fa-star"></i><span> 在线push</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/message/"><i class="fa-fw far fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user-circle"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer-XL Attentive Language Models Beyond a Fixed-Length Context<a class="post-edit-link" href="https://github.com/lonePatient/blog_source/tree/main/source/_posts/transformer_xl.md" title="编辑" target="_blank"><i class="fas fa-pencil-square"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-07-14T23:20:08.000Z" title="发表于 2019-07-14 23:20:08">2019-07-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-27T07:52:12.942Z" title="更新于 2026-02-27 07:52:12">2026-02-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>55分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2019/07/14/transformer_xl.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>目前在自然语言处理领域，Transformer的编码能力超越了RNN，但是对长距离依赖的建模能力仍然不足。在基于LSTM的模型中，为了建模长距离依赖，提出了门控机制和梯度裁剪，目前可以编码的最长距离在200左右。在基于Transformer的模型中，允许词之间直接self-attention，能够更好地捕获长期依赖关系，但是还是有限制，本文将主要介绍Transformer-XL，并基于PyTorch框架从头实现Transformer-XL。</p>
<span id="more"></span>
<h3 id="原始Transformer">原始Transformer</h3>
<p>细想一下，BERT在应用Transformer时，有一个参数sequence length，也就是BERT在训练和预测时，每次接受的输入是固定长度的。那么，怎么输入语料进行训练时最理想的呢？当然是将一个完整的段落一次性输入，进行特征提取了。但是现实是残酷的，这么大的Transformer，内存是消耗不起的。所以现有的做法是，对段落按照segment进行分隔。在训练时:</p>
<ul>
<li>当输入segment序列比sequence length短时，就做padding。</li>
<li>当输入segment序列比sequence length长时就做切割。</li>
</ul>
<p>这种做法显然是一种权宜之计，它有这么两个缺点：</p>
<ol>
<li>
<p>长句子切割必然会造成语义的残破，不利于模型的训练。</p>
</li>
<li>
<p>segment的切割没有考虑语义，也就是模型在训练当前segment时拿不到前面时刻segment的信息，造成了语义的分隔。</p>
</li>
</ol>
<p>那么，该如何解决上述问题呢？围绕建模长距离依赖，提出Transformer-XL【XL是extra long的意思】。</p>
<h3 id="Transformer-XL">Transformer-XL</h3>
<p>我们先想一下，如果要我们自己来解决Transformer上面的问题，会怎么处理呢？</p>
<p>熟悉NLP的同学，可能会想到RNN。在RNN中，为了获取序列中的历史记忆，采用了Recurrence机制，在计算该时刻的状态时，引入前一时刻的状态作为输入。那对Transformer来说，在计算当前序列的隐藏状态时，引入前一个序列的隐藏状态信息不就可以解决上面的问题了吗？</p>
<p>事情真的有这么简单吗？其实，基本上也就是这么简单，不过Transformer-XL在引入时做了一些巧妙的设计。下面我们看看，Transformer-XL是如何引入这种Recurrence机制来解决上述问题的。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223225224.webp" alt=""></p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223223904.jpg" alt=""></p>
<p>上图是传统的Transformer在训练和评估阶段采用的语料输入策略。在训练时，将整个语料库分割成可管理的大小的更短的片段，在每个片段中训练模型，忽略来自前一段的所有上下文信息；在评估阶段，传统的Transformer模型在每个步骤都消耗与训练期间相同长度的一个segment。然后，在下一步中，这个segment向右移动一个位置，并从头开始处理，只在最后一个位置进行一次预测。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223225301.webp" alt=""></p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223223929.jpg" alt=""></p>
<p>如上图所示，Transformer-XL采用了不同的策略，在训练过程中，对上一个segment计算的隐藏状态序列进行固定和缓存，并在模型处理下一个新的segment时对其进行利用。在评估阶段，可以重用前面部分的表示，而不是像传统模型那样从头开始计算，这样可以提高速度。</p>
<p>总的来说，相比Transformer，改进如下：</p>
<ol>
<li>
<p>片段级别的循环机制：增加Transformer处理文本的长度，而且解决文本碎片（指的是之前的Transformer最大处理长度为定长sequence length，超过sequence length则会截断，这样导致截断处文本信息断裂，连接不上上下文）的问题。相当于滑窗，窗口大小为sequence length。</p>
</li>
<li>
<p>相对位置编码：解决在不同片段中相同token，绝对位置编码可能相同，无法区分的问题。采用相对距离的方式得到相应的位置编码。</p>
</li>
</ol>
<h4 id="Recurrence机制">Recurrence机制</h4>
<p>事实上，问题的关键在于，在计算当前序列当前层的隐藏状态时，如何引入前一个序列上一层的隐藏状态。Transformer-XL的做法很简单，就是按照序列长度的维度将他们concate起来。如下的公式所示：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224005.jpg" alt=""></p>
<p>其中：</p>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>τ</mi><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">h_{\tau}^n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9414em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em;">τ</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span>是一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">L \times d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>的矩阵，表示的是第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span>个输入序列的第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>层的隐藏层的状态。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>表示序列长度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>表示嵌入维度。</p>
</li>
<li>
<p>SG(.)表示的Stop Gradient，这非常重要，避免了RNN会出现的一系列问题。</p>
</li>
</ul>
<p>从上述公式可以看出，Transformer-XL与传统的Transformer的差异主要在于隐藏层输入K和V的差异。Transformer-XL中引入了上一个序列前一个隐藏层的值，将他们concatenate起来，计算新的K和V。</p>
<p>具体以下图进行详细说明；</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224019.jpg" alt=""></p>
<p>所谓循环机制（其实就是滑窗），就是要重用之前的状态（上图中，橙色部分为初始的记忆单元memory，绿色部分表示segment）。比如在Layer1中，要计算第1个片段第1个位置seg1_1的结果，需要用到前一层的 “记忆单元（Embedding中的mem）和 seg1_1”。再比如在计算Layer1中seg2_1时，mem变成了Embedding中的seg1。就是说，窗口滑动是以片段为单位的。图中，箭头部分，表示当前需要计算的内容的attend来源。橙色箭头表示来自上一层的mem，绿色箭头表示来自上一层的对应的位置。</p>
<p>在计算attention时（比如Layer1的seg1_1，Q为Embedding的seg1_1，K=V为embedding中的mem+seg1_1），先计算 attn_weight =$ softmax(QK^T)$ ，表示要产生Layer1中seg1_1的attention，每个V要贡献的权重；再计算attn_weight$ *  V$ ，表示V加权求和的结果。需要注意的是，在计算反向传播时，mem部分是不进行梯度更新的。此外，这里可以很明显的看出与RNN等循环网络，“循环”的不同之处在于，RNN是在同一层传递的（ <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>step</mtext><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\text{step}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8592em;vertical-align:-0.2441em;"></span><span class="mord"><span class="mord text"><span class="mord">step</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span></span></span></span> 用到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>step</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\text{step}_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9176em;vertical-align:-0.3025em;"></span><span class="mord"><span class="mord text"><span class="mord">step</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3025em;"><span></span></span></span></span></span></span></span></span></span>及之前的记忆），Transformer-XL是在不同层之间传递的（  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>layer</mtext><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\text{layer}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9386em;vertical-align:-0.2441em;"></span><span class="mord"><span class="mord text"><span class="mord">layer</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span></span></span></span>用到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>layer</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\text{layer}_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9969em;vertical-align:-0.3025em;"></span><span class="mord"><span class="mord text"><span class="mord">layer</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3025em;"><span></span></span></span></span></span></span></span></span></span> 及以前的记忆）。</p>
<h4 id="Relative-Positional-Encodings">Relative Positional Encodings</h4>
<p>在传统的Transformer中，输入序列中的位置信息是怎么表示的？通过POS函数生成，它是位置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>和维度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>的函数，也就是不同输入segment在相同绝对位置中的位置表示是相同的。在传统的Transformer中，每个segment之间的表示是没有关联的，这当然就没有问题。但是在Transformer-XL中，因为引入了前一时刻segment的信息，就需要对不同时刻，同样是第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>个的词进行区分。</p>
<p>Transformer-XL引入了一种Relative Positional Encodings机制，会根据词之间的相对距离而非像传统的Transformer中的绝对位置进行编码。</p>
<p>在传统的Transformer中，计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和键<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>之间的attention分数的方式为</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi mathvariant="bold">A</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">s</mi></mrow></msubsup><mo>=</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">W</mi><mi>q</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">E</mi><msub><mi>x</mi><mi>i</mi></msub></msub><mo>+</mo><msub><mi mathvariant="bold">U</mi><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mi mathvariant="normal">⊤</mi></msup><mo stretchy="false">(</mo><msub><mi mathvariant="bold">W</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold">E</mi><msub><mi>x</mi><mi>j</mi></msub></msub><mo>+</mo><msub><mi mathvariant="bold">U</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{A}_{i, j}^{\mathrm{abs}} = (\mathbf{W}_q(\mathbf{E}_{x_i} + \mathbf{U}_i))^{\top} (\mathbf{W}_k(\mathbf{E}_{x_j} + \mathbf{U}_j))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2822em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathbf">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">abs</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2464em;vertical-align:-0.3473em;"></span><span class="mord"><span class="mord mathbf">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3473em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span></p>
<p>展开就是：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223233024.png" alt=""></p>
<p>其中:</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">E</mi><msub><mi>x</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">\mathbf{E}_{x_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9362em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>是词<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>的embedding</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">E</mi><msub><mi>x</mi><mi>j</mi></msub></msub></mrow><annotation encoding="application/x-tex">\mathbf{E}_{x_{j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0334em;vertical-align:-0.3473em;"></span><span class="mord"><span class="mord mathbf">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3473em;"><span></span></span></span></span></span></span></span></span></span>是词<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>的embedding</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">U_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">U_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 是位置向量。</li>
</ul>
<p>在Transformer-XL中，对上述的attention计算方式进行了变换，转为相对位置的计算，而且不仅仅在第一层这么计算，在每一层都是这样计算。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223233152.png" alt=""></p>
<p>对比来看，主要有三点变化：</p>
<ol>
<li>
<p>在b和d这两项中，将所有绝对位置向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">U_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">U_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>都转为相对位置向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{i−j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，与Transformer一样，这是一个固定的编码向量，不需要学习。</p>
</li>
<li>
<p>在c这一项中，将查询的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>U</mi><mi>i</mi><mi>T</mi></msubsup><msubsup><mi>W</mi><mi>q</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_i^TW_q^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span>向量转为一个需要学习的参数向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">u</span></span></span></span>，因为在考虑相对位置的时候，不需要query的绝对位置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>，因此对于任意的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span>，都可以采用同样的向量。同理，在d这一项中，也将query的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>U</mi><mi>i</mi><mi>T</mi></msubsup><msubsup><mi>W</mi><mi>q</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">U_i^TW_q^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span></span></span></span>向量转为另一个需要学习的参数向量v。</p>
</li>
<li>
<p>将K的权重变换矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>转为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{k,E}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{k,R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，分别作为content-based key vectors和location-based key vectors。</p>
</li>
</ol>
<p>四部分分别对应为:</p>
<ul>
<li>基于内容的“寻址”，即没有添加原始位置编码的原始分数。</li>
<li>基于内容的位置偏置，即相对于当前内容的位置偏差。</li>
<li>全局的内容偏置，用于衡量key的重要性。</li>
<li>全局的位置偏置，根据query和key之间的距离调整重要性。</li>
</ul>
<p>总的来说，Relative Positional Encodings就是在计算attention分数时，用相对位置<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>编码来代替原来的绝对位置编码<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">U_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>U</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">U_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>。并且学习了相对位置v和u用来调整不同距离和不同嵌入的得分。</p>
<h4 id="计算公式">计算公式</h4>
<p>结合上面两个创新点，将Transformer-XL模型的整体计算公式整理如下，这里考虑一个N层的只有一个注意力头的模型：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224145.jpg" alt=""></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span>代表第几段，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>代表第几层，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>τ</mi><mn>0</mn></msubsup><mo>:</mo><mo>=</mo><msub><mi>E</mi><msub><mi>s</mi><mi>τ</mi></msub></msub></mrow><annotation encoding="application/x-tex">h_\tau^0 := E_{s_\tau}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em;">τ</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9334em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em;">τ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span></span></span></span>定义为第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi></mrow><annotation encoding="application/x-tex">\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span></span></span></span>的词向量序列。值得一提的是，计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>矩阵的时候，需要对所有的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i-j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow><mi>n</mi></msubsup><msub><mi>R</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{k,R}^n R_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1025em;vertical-align:-0.4192em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-2.4169em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，如果直接按照公式计算的话，计算时间是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>l</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">O(length)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> ，而实际上<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i-j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>的范围只从0 ~ length，因此可以先计算好这length个向量，然后在实际计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>矩阵时直接取用即可。</p>
<p>具体的，设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>分别为memory和当前段序列的长度，则<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i−j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>的范围也就为0 ~ M + L - 1。下面的Q矩阵中的每一行都代表着<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub><msub><mi>R</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{k,R}R_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>中一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">i−j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span>的可能性，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>k</mi></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub><msub><mi>R</mi><mrow><mi>M</mi><mo>+</mo><mi>L</mi><mo>−</mo><mn>1</mn><mo>−</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">Q_k = W_{k, R} R_{M+L-1-k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">L</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">Q</mi><mo>:</mo><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msubsup><mi mathvariant="bold">R</mi><mrow><mi>M</mi><mo>+</mo><mi>L</mi><mo>−</mo><mn>1</mn></mrow><mi mathvariant="normal">⊤</mi></msubsup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msubsup><mi mathvariant="bold">R</mi><mrow><mi>M</mi><mo>+</mo><mi>L</mi><mo>−</mo><mn>2</mn></mrow><mi mathvariant="normal">⊤</mi></msubsup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msubsup><mi mathvariant="bold">R</mi><mn>1</mn><mi mathvariant="normal">⊤</mi></msubsup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msubsup><mi mathvariant="bold">R</mi><mn>0</mn><mi mathvariant="normal">⊤</mi></msubsup></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><msubsup><mi mathvariant="bold">W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow><mi mathvariant="normal">⊤</mi></msubsup><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mrow><mo fence="true">[</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub><msub><mi mathvariant="bold">R</mi><mrow><mi>M</mi><mo>+</mo><mi>L</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">]</mo></mrow><mi mathvariant="normal">⊤</mi></msup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mrow><mo fence="true">[</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub><msub><mi mathvariant="bold">R</mi><mn>1</mn></msub><mo fence="true">]</mo></mrow><mi mathvariant="normal">⊤</mi></msup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mrow><mo fence="true">[</mo><msub><mi mathvariant="bold">W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub><msub><mi mathvariant="bold">R</mi><mn>0</mn></msub><mo fence="true">]</mo></mrow><mi mathvariant="normal">⊤</mi></msup></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{Q} :=\left[ \begin{array}{c}{\mathbf{R}_{M+L-1}^{\top}} \\ {\mathbf{R}_{M+L-2}^{\top}} \\ {\vdots} \\ {\mathbf{R}_{1}^{\top}} \\ {\mathbf{R}_{0}^{\top}}\end{array}\right] \mathbf{W}_{k, R}^{\top}=\left[ \begin{array}{c}{\left[\mathbf{W}_{k, R} \mathbf{R}_{M+L-1}\right]^{\top}} \\ {\vdots} \\ {\vdots} \\ {\left[\mathbf{W}_{k, R} \mathbf{R}_{1}\right]^{\top}} \\ {\left[\mathbf{W}_{k, R} \mathbf{R}_{0}\right]^{\top}}\end{array}\right] \in \mathbb{R}^{(M+L) \times d}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8805em;vertical-align:-0.1944em;"></span><span class="mord mathbf">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:6.6964em;vertical-align:-3.0982em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.55em;"><span style="top:-2.611em;"><span class="pstrut" style="height:5.016em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-3.758em;"><span class="pstrut" style="height:5.016em;"></span><span style="height:3.016em;width:0.6667em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.6667em' height='3.016em' style='width:0.6667em' viewBox='0 0 666.67 3016' preserveAspectRatio='xMinYMin'><path d='M319 0 H403 V3016 H319z M319 0 H403 V3016 H319z'/></svg></span></span><span style="top:-7.4111em;"><span class="pstrut" style="height:5.016em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.05em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.5982em;"><span style="top:-6.4366em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.4247em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">L</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3337em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-5.2275em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.4247em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">L</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3337em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.3675em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em;"></span></span></span></span></span><span style="top:-2.1584em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-0.9493em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.0982em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.55em;"><span style="top:-2.611em;"><span class="pstrut" style="height:5.016em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-3.758em;"><span class="pstrut" style="height:5.016em;"></span><span style="height:3.016em;width:0.6667em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.6667em' height='3.016em' style='width:0.6667em' viewBox='0 0 666.67 3016' preserveAspectRatio='xMinYMin'><path d='M263 0 H347 V3016 H263z M263 0 H347 V3016 H263z'/></svg></span></span><span style="top:-7.4111em;"><span class="pstrut" style="height:5.016em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.05em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:7.8001em;vertical-align:-3.6501em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.1501em;"><span style="top:-3.211em;"><span class="pstrut" style="height:6.2161em;"></span><span class="delimsizinginner delim-size4"><span>⎣</span></span></span><span style="top:-4.358em;"><span class="pstrut" style="height:6.2161em;"></span><span style="height:4.2161em;width:0.6667em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.6667em' height='4.2161em' style='width:0.6667em' viewBox='0 0 666.67 4216' preserveAspectRatio='xMinYMin'><path d='M319 0 H403 V4216 H319z M319 0 H403 V4216 H319z'/></svg></span></span><span style="top:-9.2111em;"><span class="pstrut" style="height:6.2161em;"></span><span class="delimsizinginner delim-size4"><span>⎡</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.6501em;"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width:0.5em;"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.1335em;"><span style="top:-6.832em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">L</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">]</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.989em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-4.972em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em;"></span></span></span></span></span><span style="top:-3.112em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em;"></span></span></span></span></span><span style="top:-1.763em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">]</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.989em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-0.414em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">]</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.989em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">⊤</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.6335em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:0.5em;"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:4.1501em;"><span style="top:-3.211em;"><span class="pstrut" style="height:6.2161em;"></span><span class="delimsizinginner delim-size4"><span>⎦</span></span></span><span style="top:-4.358em;"><span class="pstrut" style="height:6.2161em;"></span><span style="height:4.2161em;width:0.6667em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.6667em' height='4.2161em' style='width:0.6667em' viewBox='0 0 666.67 4216' preserveAspectRatio='xMinYMin'><path d='M263 0 H347 V4216 H263z M263 0 H347 V4216 H263z'/></svg></span></span><span style="top:-9.2111em;"><span class="pstrut" style="height:6.2161em;"></span><span class="delimsizinginner delim-size4"><span>⎤</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:3.6501em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.938em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">L</span><span class="mclose mtight">)</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>则对于上面公式中的(b)项，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>q</mi><mi>i</mi><mi>T</mi></msubsup><msub><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub><msub><mi>R</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">q_i^T W_{k,R}R_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1274em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，其构成的所有可能向量的矩阵为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>矩阵，其形状为L * (M + L)。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223233152.png" alt=""></p>
<p>从上式中，这是我们最终需要的(b)项的attention结果。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223233641.png" alt=""></p>
<p>我们进一步定义<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>B</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9202em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span></p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223233710.png" alt=""></p>
<p>可见，需要的B矩阵的每一行只是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>B</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9202em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span>的向左shift而已。因此，可以直接利用矩阵乘法计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>B</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9202em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span>即可。设<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_{i-j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0077em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>R</mi></msub></mrow><annotation encoding="application/x-tex">d_R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">d_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>R</mi></mrow></msub></mrow><annotation encoding="application/x-tex">W_{k,R}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>矩阵的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>q</mi></msub><mo>×</mo><msub><mi>d</mi><mi>R</mi></msub></mrow><annotation encoding="application/x-tex">d_q \times d_R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，则直接计算矩阵B的时间复杂度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><msub><mi>d</mi><mi>q</mi></msub><mo>×</mo><msub><mi>d</mi><mi>R</mi></msub><mo>×</mo><mi>L</mi><mo>×</mo><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">2 \times d_q \times d_R \times L \times (M+L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mclose">)</span></span></span></span>，而计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>B</mi><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9202em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9202em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span><span style="top:-3.6023em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">~</span></span></span></span></span></span></span></span></span></span>的时间复杂度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>×</mo><msub><mi>d</mi><mi>q</mi></msub><mo>×</mo><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo><mo>+</mo><msub><mi>d</mi><mi>q</mi></msub><mo>×</mo><msub><mi>d</mi><mi>R</mi></msub><mo>×</mo><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L \times d_q \times (M + L) + d_q \times d_R \times (M + L)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mclose">)</span></span></span></span>，计算量明显不是一个量级（后者要快很多）。</p>
<p>我们以一个二维矩阵进行说明:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.linspace(<span class="number">1</span>, <span class="number">12</span>, <span class="number">12</span>).view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">        [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zero_pad = torch.zeros((x.size(<span class="number">0</span>), <span class="number">1</span>))</span><br><span class="line">x_padded = torch.cat([zero_pad, x], dim=<span class="number">1</span>)</span><br><span class="line">x_padded = x_padded.view(x.size(<span class="number">1</span>) + <span class="number">1</span>, x.size(<span class="number">0</span>))</span><br><span class="line">x = x_padded[<span class="number">1</span>:].view_as(x)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">tensor([[ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">0.</span>,  <span class="number">5.</span>],</span><br><span class="line">        [ <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>]])</span><br></pre></td></tr></table></figure>
<p>总的来说Transformer-XL对Transformer进行了一些调整，试图解决一些问题。按照论文的描述，Transformer-XL学习的依赖关系比RNN长80%，比传统Transformer长450%，在短序列和长序列上都获得了更好的性能，并且在评估阶段比传统Transformer快1800+倍。</p>
<h3 id="实验">实验</h3>
<p>接下来，我们将使用PyTorch框架从头实现Transformer-XL。真正理解某个模型的最好方法是从头开始构建。</p>
<h4 id="概述">概述</h4>
<p>由于Transformer-XL涉及到Transformer，因此让我们来回顾一下最初的Transformer结构。总体而言，Transformer结构是由多个MultiHeadAttention层堆叠在一起，并包含前馈层、残差层和层标准化层。如下图所示：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224238.png" alt=""></p>
<p>MultiHeadAttention层由多个attention head组成。每个attention head对其输入应用一个线性变换，并使用keys和querys计算其输入values上的attention。如下图所示:</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224251.png" alt=""></p>
<p>这种方法无法步抓到位置信息，因此Transformer将表示输入位置的embeddings与词embeddings进行相加。</p>
<p>现在，我们来看看Transformer-XL。为了更全面地了解整个数据流，看一下Transformer-XL的前向传递的简化版本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_</span>):</span><br><span class="line">    hidden_states = []</span><br><span class="line">    pos_embs = <span class="variable language_">self</span>.position_embedding(input_) <span class="comment"># 位置embedding</span></span><br><span class="line">    word_embs = <span class="variable language_">self</span>.word_embedding(input_) <span class="comment"># 词embedding</span></span><br><span class="line"></span><br><span class="line">    layer_out = word_embs</span><br><span class="line">    <span class="keyword">for</span> mem, layer <span class="keyword">in</span> <span class="built_in">zip</span>(memory, <span class="variable language_">self</span>.layers):</span><br><span class="line">        layer_out = layer(layer_out, pos_embs, mem)</span><br><span class="line">        hidden_states.append(layer_out)</span><br><span class="line">    logits = <span class="variable language_">self</span>.output_projection(<span class="variable language_">self</span>.drop(layer_out))        </span><br><span class="line">    new_memory = <span class="variable language_">self</span>.update_memory(memory, hidden_states)</span><br><span class="line">    <span class="keyword">return</span> logits, new_memory</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>
<p>memory： 这是Transformer XL的独特之处。正确地处理memory是使Transformer-XL正确运行的关键点之一。</p>
</li>
<li>
<p>layer： 这是Transformer-XL的核心部分。虽然这与MultiheadAttention层基本相同，但是有几个关键的变化，比如相对位置编码。</p>
</li>
</ul>
<p>接下来，我们将详细的实现每一个部分。</p>
<h4 id="单Attention-Head">单Attention Head</h4>
<p>我们将首先在一个MultiHeadAttention 层中实现一个attention head。以第一层为例，假设该层的收入为一个shape为(seq=7, batch_size=3, embedding_dim=32)的word embeddings。注意，Transformer-XL并不向输入添加位置embedding。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seq, batch_size, embedding_dim = <span class="number">7</span>, <span class="number">3</span>, <span class="number">32</span></span><br><span class="line">word_embs = torch.rand(seq, batch_size, embedding_dim)</span><br></pre></td></tr></table></figure>
<p>在Transformer-XL中，我们需要缓存之前的序列的输出。在第一层中，之前的序列输出定义为词embeddings。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224310.gif" alt=""></p>
<p>另外假设之前的序列长度为prev_seq=6，则：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prev_seq = <span class="number">6</span></span><br><span class="line">memory = torch.rand(prev_seq, batch_size, embedding_dim) <span class="comment"># hidden states from the previous sequence</span></span><br></pre></td></tr></table></figure>
<p>每个attention head以keys、queries和values作为输入。并进行下面的处理过程：:</p>
<ol>
<li>
<p>对每个keys、queries和、values进行不同的线性变换。</p>
</li>
<li>
<p>计算每个values的 attention scores。</p>
</li>
<li>
<p>对于每个query，计算values的attention-weighted sum。</p>
</li>
<li>
<p>进行残差连接和层标准化。</p>
</li>
</ol>
<p>我们从线性变换开始。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inner_dim = <span class="number">17</span> <span class="comment"># this will be the internal dimension</span></span><br><span class="line">linear_k = nn.Linear(embedding_dim, inner_dim)</span><br><span class="line">linear_v = nn.Linear(embedding_dim, inner_dim)</span><br><span class="line">linear_q = nn.Linear(embedding_dim, inner_dim)</span><br></pre></td></tr></table></figure>
<p>从Transformer-XL计算公式可知，keys和values与正常Transformer中的keys，values是不一样的。根据公式，将memory和输入在序列长度纬度进行拼接，并作为keys/values的输入。需要注意的是，query是不做该变化的，因为每个query表示一个我们想要预测的单词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_embs_w_memory = torch.cat([memory, word_embs], dim=<span class="number">0</span>)</span><br><span class="line">k_tfmd = linear_k(word_embs_w_memory)</span><br><span class="line">v_tfmd = linear_v(word_embs_w_memory)</span><br><span class="line">q_tfmd = linear_q(word_embs) <span class="comment"># No memory for the queries</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们类似正常的Transformer一样计算scaled dot product attention。scaled dot product attention通过计算query和key向量之间的点积作为attention score。为了防止values随着向量维数的增加而太大，我们将原始attention score除以embedding size的平方根。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="bold">s</mi><mi mathvariant="bold">o</mi><mi mathvariant="bold">f</mi><mi mathvariant="bold">t</mi><mi mathvariant="bold">m</mi><mi mathvariant="bold">a</mi><mi mathvariant="bold">x</mi></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q,K,V) = \mathbf{softmax}(\frac{QK^T}{\sqrt{d_k}})V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord"><span class="mord mathbf">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224332.png" alt=""></p>
<p>我们将在这里使用einsum符号进行编写，如果你不熟悉einsum的话，可以点击该<a href="https://lonepatient.top/2018/05/25/einsum.html">教程连接</a>。简而言之，einsum表示输入和输出的形状，使用一个字母表示每个维度。下面，输入的形状是’ (i, b, d) ‘和’ (j, b, d) ‘，输出的形状是’ (i, j, b) '，其中相同的字母表示相同的大小。einsum是通过对具有相同字符的维度进行点积来计算的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">content_attn = torch.einsum(<span class="string">&quot;ibd,jbd-&gt;ijb&quot;</span>, q_tfmd, k_tfmd) / (embedding_dim ** <span class="number">0.5</span>) <span class="comment"># scale</span></span><br></pre></td></tr></table></figure>
<p>注意，我们没有使用softmax激活函数，因为还要计算相对位置编码。</p>
<h4 id="相对位置编码">相对位置编码</h4>
<p>Transformer-XL中的一个关键点是相对位置编码。Transformer-XL计算一个表示任意两个token之间距离的embeddings，而不是使用每个token的绝对位置embeddings。</p>
<p>向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>计算公式如下:</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223233152.png" alt=""></p>
<p>这里<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">E_{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>的词embedding，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>是变换矩阵。a项是 content-based attention，我们已经在上面计算过了。b和d是基于相对位置嵌入的，并且依赖于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">k_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0315em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>之间的距离。u和v是表示对特定内容和特定位置的偏差的全局偏差术语。</p>
<p>下面让我们来看看b到d的具体实现。我们首先加入content bias (c项)，因为它是最容易计算的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.rand(<span class="number">17</span>).expand_as(q_tfmd)</span><br><span class="line">content_attn = content_attn + torch.einsum(<span class="string">&quot;ibd,jbd-&gt;ijb&quot;</span>, u, k_tfmd) / (embedding_dim ** <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，计算所需的相对位置嵌入。对于相对位置嵌入，Transformer-XL使用固定的正弦嵌入。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pos_idxs = torch.arange(seq + prev_seq - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1.0</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">pos_idxs</span><br><span class="line"><span class="comment">#output: tensor([12., 11., 10.,  9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.,  0.])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inv_freq = <span class="number">1</span> / (<span class="number">10000</span> ** (torch.arange(<span class="number">0.0</span>, embedding_dim, <span class="number">2.0</span>) / embedding_dim))</span><br><span class="line">sinusoid_inp = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, pos_idxs, inv_freq)</span><br><span class="line">plt.plot(sinusoid_inp[<span class="number">0</span>, :].detach().numpy())</span><br><span class="line">plt.plot(sinusoid_inp[<span class="number">6</span>, :].detach().numpy())</span><br></pre></td></tr></table></figure>
<p><img src="index.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pos_idxs = torch.arange(seq + prev_seq - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1.0</span>, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">inv_freq = <span class="number">1</span> / (<span class="number">10000</span> ** (torch.arange(<span class="number">0.0</span>, embedding_dim, <span class="number">2.0</span>) / embedding_dim))</span><br><span class="line">sinusoid_inp = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, pos_idxs, inv_freq)</span><br><span class="line"></span><br><span class="line">relative_positional_embeddings = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-<span class="number">1</span>)[:,<span class="literal">None</span>,:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">relative_positional_embeddings.shape</span><br><span class="line"><span class="comment">#output:torch.Size([13, 1, 32])</span></span><br></pre></td></tr></table></figure>
<p>将上述合在一起为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d = d</span><br><span class="line">        inv_freq = <span class="number">1</span> / (<span class="number">10000</span> ** (torch.arange(<span class="number">0.0</span>, d, <span class="number">2.0</span>) / d))</span><br><span class="line">                <span class="comment"># register buffer tells pytorch that this tensor is part of the modle</span></span><br><span class="line">        <span class="comment"># this means that it will be saved in the state_dict and moved to the GPU</span></span><br><span class="line">        <span class="comment"># along with the model</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, positions: torch.LongTensor, <span class="comment"># (seq, )</span></span></span><br><span class="line"><span class="params">               </span>):</span><br><span class="line">        <span class="comment"># outer product</span></span><br><span class="line">        sinusoid_inp = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, positions.<span class="built_in">float</span>(), <span class="variable language_">self</span>.inv_freq)</span><br><span class="line">        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> pos_emb[:,<span class="literal">None</span>,:]</span><br></pre></td></tr></table></figure>
<p>我们还需要对keys/values的位置嵌入纬度进行变换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">linear_p = nn.Linear(embedding_dim, inner_dim)</span><br><span class="line">pos_tfmd = linear_p(relative_positional_embeddings)</span><br></pre></td></tr></table></figure>
<p>因此，将位置偏差添加到attention计算过程中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.rand(<span class="number">17</span>) <span class="comment"># positional bias</span></span><br><span class="line">pos_attn = torch.einsum(<span class="string">&quot;ibd,jd-&gt;ijb&quot;</span>, q_tfmd + v, pos_tfmd[:,<span class="number">0</span>,:]) / (embedding_dim ** <span class="number">0.5</span>) <span class="comment"># scale</span></span><br><span class="line">pos_attn.shape</span><br><span class="line"><span class="comment">#output: torch.Size([7, 13, 3])</span></span><br></pre></td></tr></table></figure>
<p>由于我们需要为每个key-query对计算相对位置嵌入，所以上述中使用相对位置嵌入来实现注意力的简单实现在计算复杂度方面为O(n^2)。幸运的是，原作者提出了一个技巧，通过计算一个query的attention，然后为不同的query位置转移其嵌入，从而将时间减少到O(n)（具体可以见上述公式）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zero_pad = torch.zeros((seq, <span class="number">1</span>, batch_size), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># this padding + shifting efficiently computes the attention for all </span></span><br><span class="line">pos_attn = (torch.cat([zero_pad, pos_attn], dim=<span class="number">1</span>)</span><br><span class="line">                    .view(seq + prev_seq + <span class="number">1</span>, seq, batch_size)[<span class="number">1</span>:]</span><br><span class="line">                    .view_as(pos_attn)) </span><br></pre></td></tr></table></figure>
<p>因此，总的attention score为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_attn = content_attn + pos_attn</span><br></pre></td></tr></table></figure>
<p>当我们进行语言建模时，我们需要阻止模型查看它应该预测的单词。在Transformer中，我们通过将attention score设置为0来实现这一点。这将掩盖了我们不希望模型看到的字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mask = torch.triu(</span><br><span class="line">    torch.ones((seq, seq + prev_seq)),</span><br><span class="line">    diagonal=<span class="number">1</span> + prev_seq,</span><br><span class="line">).byte()[...,<span class="literal">None</span>]</span><br><span class="line">raw_attn = raw_attn.masked_fill(mask, -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>接下来计算value的加权和：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">attn = torch.softmax(raw_attn, dim=<span class="number">1</span>)</span><br><span class="line">attn_weighted_sum = torch.einsum(<span class="string">&quot;ijb,jbd-&gt;ibd&quot;</span>, attn, v_tfmd)</span><br><span class="line">attn_weighted_sum.shape</span><br><span class="line"><span class="comment">#output: torch.Size([7, 3, 17])</span></span><br></pre></td></tr></table></figure>
<p>最后，将attn_weighted_sum的纬度转换回原来纬度，并使用残差连接层和层标准化，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">linear_out = nn.Linear(inner_dim, embedding_dim)</span><br><span class="line">layer_norm = nn.LayerNorm(embedding_dim)</span><br><span class="line">output = layer_norm(word_embs + linear_out(attn_weighted_sum))</span><br><span class="line">output.shape</span><br><span class="line"><span class="comment">#output: torch.Size([7, 3, 32])</span></span><br></pre></td></tr></table></figure>
<h4 id="MultiHeadAttention模块">MultiHeadAttention模块</h4>
<p>结合上述代码模块，并增加dropout层，我们将得到一个MultiHeadAttention模块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_input: <span class="built_in">int</span>, d_inner: <span class="built_in">int</span>, n_heads: <span class="built_in">int</span>=<span class="number">4</span>, </span></span><br><span class="line"><span class="params">                 dropout: <span class="built_in">float</span>=<span class="number">0.1</span>, dropouta: <span class="built_in">float</span>=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_input = d_input</span><br><span class="line">        <span class="variable language_">self</span>.d_inner = d_inner</span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># this layer applies the linear transformation required</span></span><br><span class="line">        <span class="comment"># for the keys and values for all heads at once for efficiency</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_kv = nn.Linear(</span><br><span class="line">            d_input, </span><br><span class="line">            (d_inner * n_heads * <span class="number">2</span>), <span class="comment"># 2 is for keys and values</span></span><br><span class="line">            bias=<span class="literal">False</span>, <span class="comment"># we don&#x27;t apply bias, making this a simple matrix multiplication</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># for queries (will not be concatenated with memorized states so separate)</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_q = nn.Linear(</span><br><span class="line">            d_input, d_inner * n_heads,</span><br><span class="line">            bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># for positional embeddings</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_p = nn.Linear(</span><br><span class="line">            d_input, d_inner * n_heads,</span><br><span class="line">            bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.scale = <span class="number">1</span> / (d_inner ** <span class="number">0.5</span>) <span class="comment"># for scaled dot product attention</span></span><br><span class="line">        <span class="variable language_">self</span>.dropa = nn.Dropout(dropouta)</span><br><span class="line">        <span class="comment"># we will use this to project back to the input dimension</span></span><br><span class="line">        <span class="variable language_">self</span>.lout = nn.Linear(<span class="variable language_">self</span>.d_inner * <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_input, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.LayerNorm(<span class="variable language_">self</span>.d_input)</span><br><span class="line">        <span class="variable language_">self</span>.dropo = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_rel_shift</span>(<span class="params">self, x</span>):</span><br><span class="line">        zero_pad = torch.zeros((x.size(<span class="number">0</span>), <span class="number">1</span>, *x.size()[<span class="number">2</span>:]),</span><br><span class="line">                               device=x.device, dtype=x.dtype)</span><br><span class="line">        <span class="keyword">return</span> (torch.cat([zero_pad, x], dim=<span class="number">1</span>)</span><br><span class="line">                    .view(x.size(<span class="number">1</span>) + <span class="number">1</span>, x.size(<span class="number">0</span>), *x.size()[<span class="number">2</span>:])[<span class="number">1</span>:]</span><br><span class="line">                    .view_as(x)) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.FloatTensor, <span class="comment"># (cur_seq, b, d_in)</span></span></span><br><span class="line"><span class="params">                pos_embs: torch.FloatTensor, <span class="comment"># (cur_seq + prev_seq, d_in)</span></span></span><br><span class="line"><span class="params">                memory: torch.FloatTensor, <span class="comment"># (prev_seq, b, d_in)</span></span></span><br><span class="line"><span class="params">                u: torch.FloatTensor, <span class="comment"># (H, d)</span></span></span><br><span class="line"><span class="params">                v: torch.FloatTensor, <span class="comment"># (H, d)</span></span></span><br><span class="line"><span class="params">                mask: <span class="type">Optional</span>[torch.FloatTensor]=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        pos_embs: we pass the positional embeddings in separately</span></span><br><span class="line"><span class="string">            because we need to handle relative positions</span></span><br><span class="line"><span class="string">        input shape: (seq, bs, self.d_input)</span></span><br><span class="line"><span class="string">        pos_embs shape: (seq + prev_seq, bs, self.d_input)</span></span><br><span class="line"><span class="string">        output shape: (seq, bs, self.d_input)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        cur_seq = input_.shape[<span class="number">0</span>] <span class="comment">#  sequence length of current segment</span></span><br><span class="line">        prev_seq = memory.shape[<span class="number">0</span>] <span class="comment"># sequence length of previous segment</span></span><br><span class="line">        H, d = <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_inner</span><br><span class="line">        input_with_memory = torch.cat([memory, input_], dim=<span class="number">0</span>) <span class="comment"># concatenate recurrent memory</span></span><br><span class="line">                                                               <span class="comment"># across sequence dimension</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># we will use the following symbols to represent the shape of the tensors</span></span><br><span class="line">        <span class="comment"># cs: current sequence length, b: batch, H: number of heads</span></span><br><span class="line">        <span class="comment"># d: inner dimension, ps: previous sequence length</span></span><br><span class="line">        <span class="comment"># The key and value are now conditioned on the preceding context</span></span><br><span class="line">        k_tfmd, v_tfmd = \</span><br><span class="line">            torch.chunk(<span class="variable language_">self</span>.linear_kv(input_with_memory), <span class="number">2</span>, dim=-<span class="number">1</span>) <span class="comment"># (cs + ps, b, H * d)</span></span><br><span class="line">        q_tfmd = <span class="variable language_">self</span>.linear_q(input_) <span class="comment"># (cs, b, H * d)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply scaled dot product attention</span></span><br><span class="line">        <span class="comment"># look at the following dimensions carefully, since this is the key operation</span></span><br><span class="line">        <span class="comment"># in the Transformer/Transformer XL architecture</span></span><br><span class="line">        </span><br><span class="line">        _, bs, _ = q_tfmd.shape</span><br><span class="line">        <span class="keyword">assert</span> bs == k_tfmd.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># content-based attention term ((a) + (c) in the paper)</span></span><br><span class="line">        <span class="comment"># this is the standard attention term in the original Transformer, except without positional embeddings</span></span><br><span class="line">        <span class="comment"># which are handled separately in the Transformer XL (see below)</span></span><br><span class="line">        <span class="comment"># here, i corresponds to the number of queries = number of current inputs/targets (seq-wise)</span></span><br><span class="line">        <span class="comment"># j corresponds to the number of key/values = number of vectors that we can use to compute the </span></span><br><span class="line">        <span class="comment"># vector for each query</span></span><br><span class="line">        content_attn = torch.einsum(<span class="string">&quot;ibhd,jbhd-&gt;ijbh&quot;</span>, (</span><br><span class="line">                (q_tfmd.view(cur_seq, bs, H, d) + <span class="comment"># (a)</span></span><br><span class="line">                 u), <span class="comment"># (c): u represents the global (independent of the query)</span></span><br><span class="line">                     <span class="comment"># bias towards certain key/values = words</span></span><br><span class="line">                     <span class="comment"># Note: maybe this could be a per-attention head parameter?</span></span><br><span class="line">                 k_tfmd.view(cur_seq + prev_seq, bs, H, d) <span class="comment"># There is no positional information to be found here</span></span><br><span class="line">        )) <span class="comment"># (cs, cs + ps, b, H)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># position-based attention term ((b) + (d) in the paper)</span></span><br><span class="line">        <span class="comment"># this attention is solely based on the position of the key/values</span></span><br><span class="line">        <span class="comment"># (i.e. it does not take the content of the key/values into account)</span></span><br><span class="line">        p_tfmd = <span class="variable language_">self</span>.linear_p(pos_embs) <span class="comment"># (cs + ps, b, H * d)</span></span><br><span class="line">        position_attn = torch.einsum(<span class="string">&quot;ibhd,jhd-&gt;ijbh&quot;</span>, (</span><br><span class="line">                (q_tfmd.view(cur_seq, bs, H, d) + <span class="comment"># (b)</span></span><br><span class="line">                 v), <span class="comment"># (d): v represents the global (independent of the query)</span></span><br><span class="line">                     <span class="comment"># bias towards certain positions</span></span><br><span class="line">                 p_tfmd.view(cur_seq + prev_seq, H, d) <span class="comment"># Notice there is not content information</span></span><br><span class="line">                                                        <span class="comment"># regarding keys and values here!</span></span><br><span class="line">        )) <span class="comment"># (cs, cs + ps, b, H)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  Compute positional attention efficiently</span></span><br><span class="line">        position_attn = <span class="variable language_">self</span>._rel_shift(position_attn)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># the attention is the sum of content-based and position-based attention</span></span><br><span class="line">        attn = content_attn + position_attn</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> mask.<span class="built_in">any</span>().item():</span><br><span class="line">            attn = attn.masked_fill(</span><br><span class="line">                mask[...,<span class="literal">None</span>], -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">        attn = torch.softmax(attn * <span class="variable language_">self</span>.scale, <span class="comment"># rescale to prevent values from exploding</span></span><br><span class="line">                             dim=<span class="number">1</span>) <span class="comment"># normalize across the value sequence dimension</span></span><br><span class="line">        attn = <span class="variable language_">self</span>.dropa(attn)</span><br><span class="line"></span><br><span class="line">        attn_weighted_values = (torch.einsum(<span class="string">&quot;ijbh,jbhd-&gt;ibhd&quot;</span>,</span><br><span class="line">                                           (attn, <span class="comment"># (cs, cs + ps, b, H)</span></span><br><span class="line">                                            v_tfmd.view(cur_seq + prev_seq, bs, H, d), <span class="comment"># (cs + ps, b, H, d)</span></span><br><span class="line">                                           )) <span class="comment"># (cs, b, H, d)</span></span><br><span class="line">                                .contiguous() <span class="comment"># we need to change the memory layout to make `view` work</span></span><br><span class="line">                                .view(cur_seq, bs, H * d)) <span class="comment"># (cs, b, H * d)</span></span><br><span class="line"></span><br><span class="line">         <span class="comment"># Project back to input dimension and add residual connection</span></span><br><span class="line">        output = input_ + <span class="variable language_">self</span>.dropo(<span class="variable language_">self</span>.lout(attn_weighted_values))</span><br><span class="line">        output = <span class="variable language_">self</span>.norm(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>我们使用一个随机数进行测试是否正确，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mha = MultiHeadAttention(<span class="number">32</span>, <span class="number">17</span>, n_heads=<span class="number">4</span>)</span><br><span class="line">inpt = torch.rand(<span class="number">7</span>, <span class="number">3</span>, <span class="number">32</span>)</span><br><span class="line">pos = torch.rand(<span class="number">13</span>, <span class="number">32</span>)</span><br><span class="line">mem = torch.rand(<span class="number">6</span>, <span class="number">3</span>, <span class="number">32</span>)</span><br><span class="line">u, v = torch.rand(<span class="number">4</span>, <span class="number">17</span>), torch.rand(<span class="number">4</span>, <span class="number">17</span>)</span><br><span class="line">x1 = mha(inpt, pos, mem, u, v)</span><br><span class="line">x1.shape</span><br><span class="line"><span class="comment">#output: torch.Size([7, 3, 32])</span></span><br><span class="line">x1[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">tensor([[ <span class="number">0.6264</span>,  <span class="number">0.3405</span>, -<span class="number">1.9065</span>,  <span class="number">0.1543</span>,  <span class="number">0.0389</span>, -<span class="number">1.6033</span>,  <span class="number">1.4415</span>,  <span class="number">0.4983</span>,</span><br><span class="line">          <span class="number">0.7548</span>,  <span class="number">1.0990</span>, -<span class="number">1.1783</span>, -<span class="number">1.3847</span>, -<span class="number">1.7358</span>,  <span class="number">1.4651</span>,  <span class="number">1.0633</span>,  <span class="number">0.2168</span>,</span><br><span class="line">         -<span class="number">0.3323</span>,  <span class="number">1.1270</span>,  <span class="number">0.1614</span>,  <span class="number">1.0170</span>,  <span class="number">1.0459</span>, -<span class="number">0.7286</span>,  <span class="number">0.5064</span>, -<span class="number">1.4765</span>,</span><br><span class="line">          <span class="number">0.0448</span>, -<span class="number">1.2500</span>,  <span class="number">0.3132</span>, -<span class="number">0.8007</span>,  <span class="number">0.4089</span>,  <span class="number">0.7325</span>, -<span class="number">1.2740</span>,  <span class="number">0.6147</span>],</span><br><span class="line">        [ <span class="number">0.9541</span>,  <span class="number">0.3682</span>, -<span class="number">0.8096</span>,  <span class="number">0.1357</span>, -<span class="number">0.9159</span>, -<span class="number">1.4382</span>, -<span class="number">1.3385</span>,  <span class="number">0.8269</span>,</span><br><span class="line">          <span class="number">0.2721</span>, -<span class="number">0.4982</span>,  <span class="number">1.3105</span>, -<span class="number">0.0236</span>, -<span class="number">1.0547</span>, -<span class="number">1.3076</span>,  <span class="number">1.8884</span>, -<span class="number">0.2891</span>,</span><br><span class="line">          <span class="number">1.5231</span>,  <span class="number">0.5507</span>, -<span class="number">0.6423</span>,  <span class="number">0.4412</span>,  <span class="number">1.3656</span>,  <span class="number">0.7858</span>, -<span class="number">0.9425</span>, -<span class="number">0.3198</span>,</span><br><span class="line">         -<span class="number">0.3162</span>, -<span class="number">0.0086</span>,  <span class="number">1.5257</span>, -<span class="number">1.3216</span>,  <span class="number">1.4492</span>, -<span class="number">0.1750</span>, -<span class="number">0.1669</span>, -<span class="number">1.8291</span>],</span><br><span class="line">        [ <span class="number">1.0132</span>,  <span class="number">0.7205</span>, -<span class="number">0.4221</span>,  <span class="number">0.2952</span>, -<span class="number">1.4117</span>, -<span class="number">0.6182</span>, -<span class="number">1.7520</span>, -<span class="number">1.7426</span>,</span><br><span class="line">         -<span class="number">0.4648</span>, -<span class="number">0.2122</span>,  <span class="number">2.0889</span>, -<span class="number">1.3544</span>, -<span class="number">0.1611</span>, -<span class="number">1.0696</span>,  <span class="number">1.3492</span>, -<span class="number">1.0179</span>,</span><br><span class="line">          <span class="number">1.2820</span>,  <span class="number">0.8990</span>,  <span class="number">0.7411</span>,  <span class="number">0.8052</span>, -<span class="number">0.5322</span>,  <span class="number">0.6277</span>, -<span class="number">0.2733</span>, -<span class="number">1.0738</span>,</span><br><span class="line">         -<span class="number">0.8435</span>,  <span class="number">1.5357</span>,  <span class="number">0.8260</span>, -<span class="number">0.3422</span>, -<span class="number">0.6204</span>,  <span class="number">1.0091</span>,  <span class="number">0.1011</span>,  <span class="number">0.6182</span>]],</span><br><span class="line">       grad_fn=&lt;SelectBackward&gt;)</span><br></pre></td></tr></table></figure>
<h4 id="Decoder">Decoder</h4>
<p>在deocder模块中，除了MultiHeadAttention 层外，还需要FFN。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_input, d_inner, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_input = d_input</span><br><span class="line">        <span class="variable language_">self</span>.d_inner = d_inner</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="variable language_">self</span>.ff = nn.Sequential(</span><br><span class="line">            nn.Linear(d_input, d_inner), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(d_inner, d_input),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(d_input)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.FloatTensor, <span class="comment"># (cur_seq, bs, d_input)</span></span></span><br><span class="line"><span class="params">               </span>) -&gt; torch.FloatTensor: <span class="comment"># (cur_seq, bs, d_input)</span></span><br><span class="line">        ff_out = <span class="variable language_">self</span>.ff(input_)</span><br><span class="line">        output = <span class="variable language_">self</span>.layer_norm(input_ + ff_out)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>则Decoder模块如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_heads, d_input, </span></span><br><span class="line"><span class="params">                 d_head_inner, d_ff_inner,</span></span><br><span class="line"><span class="params">                 dropout, dropouta=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.mha = MultiHeadAttention(d_input, d_head_inner, n_heads=n_heads, </span><br><span class="line">                                      dropout=dropout, dropouta=dropouta)</span><br><span class="line">        <span class="variable language_">self</span>.ff = PositionwiseFF(d_input, d_ff_inner, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.FloatTensor, <span class="comment"># (cur_seq, bs, d_input)</span></span></span><br><span class="line"><span class="params">                pos_embs: torch.FloatTensor, <span class="comment"># (cur_seq + prev_seq, d_input),</span></span></span><br><span class="line"><span class="params">                u: torch.FloatTensor, <span class="comment"># (H, d_input), </span></span></span><br><span class="line"><span class="params">                v: torch.FloatTensor, <span class="comment"># (H, d_input),</span></span></span><br><span class="line"><span class="params">                mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                mems=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               </span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.ff(<span class="variable language_">self</span>.mha(input_, pos_embs, mems, u, v, mask=mask))</span><br></pre></td></tr></table></figure>
<p>现在有了上述模块，我们就可以构建完整的Transformer-XL模型了。</p>
<p>除了上面提到的，我们还没有涉及到的语言建模的一个常见技巧是将输入嵌入矩阵E和输出投影矩阵P绑定在一起。请记住，语言模型预测序列中的下一个token，因此它的输出维度是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{|V|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mord mtight">∣</span></span></span></span></span></span></span></span></span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|V|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord">∣</span></span></span></span>是vocab的大小。如果我们将倒数第二层的输出约束为与嵌入层<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>相同的维度，则嵌入矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span>的shape为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{|V| \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mord mtight">∣</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>，输出投影矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{d \times |V|}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mbin mtight">×</span><span class="mord mtight">∣</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span><span class="mord mtight">∣</span></span></span></span></span></span></span></span></span></span></span></span>。</p>
<p>将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><msup><mi>E</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">P = E^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>，可以提高性能，同时大大减少模型的总参数(从而减少内存使用量!)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StandardWordEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, embedding_dim,</span></span><br><span class="line"><span class="params">                div_val=<span class="number">1</span>, sample_softmax=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_embeddings = num_embeddings</span><br><span class="line">        <span class="variable language_">self</span>.embedding_dim = embedding_dim</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.scale = embedding_dim ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.LongTensor</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.embedding(input_) * <span class="variable language_">self</span>.scale</span><br></pre></td></tr></table></figure>
<p>因此完整的Transformer-XL代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerXL</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, n_layers, n_heads, </span></span><br><span class="line"><span class="params">                 d_model, d_head_inner, d_ff_inner,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0.1</span>, dropouta=<span class="number">0.</span>, </span></span><br><span class="line"><span class="params">                 seq_len: <span class="built_in">int</span>=<span class="number">0</span>, mem_len: <span class="built_in">int</span>=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_layers,<span class="variable language_">self</span>.n_heads,<span class="variable language_">self</span>.d_model,<span class="variable language_">self</span>.d_head_inner,<span class="variable language_">self</span>.d_ff_inner = \</span><br><span class="line">            n_layers,n_heads,d_model,d_head_inner,d_ff_inner</span><br><span class="line">        <span class="comment"># Embedding layers</span></span><br><span class="line">        <span class="variable language_">self</span>.word_embs = StandardWordEmbedding(num_embeddings, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.pos_embs = PositionalEmbedding(d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Core transformer</span></span><br><span class="line">        <span class="variable language_">self</span>.drop = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([DecoderBlock(n_heads, d_model, d_head_inner=d_head_inner,</span><br><span class="line">                                                  d_ff_inner=d_ff_inner,</span><br><span class="line">                                                  dropout=dropout, dropouta=dropouta)</span><br><span class="line">                                     <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tie weights</span></span><br><span class="line">        <span class="variable language_">self</span>.output_projection = nn.Linear(d_model, num_embeddings)</span><br><span class="line">        <span class="variable language_">self</span>.output_projection.weight = <span class="variable language_">self</span>.word_embs.embedding.weight</span><br><span class="line">        <span class="variable language_">self</span>.loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.seq_len, <span class="variable language_">self</span>.mem_len = seq_len, mem_len</span><br><span class="line"></span><br><span class="line">        <span class="comment"># u and v are global parameters: maybe changing these to per-head parameters</span></span><br><span class="line">        <span class="comment"># might help performance?</span></span><br><span class="line">        <span class="variable language_">self</span>.u, <span class="variable language_">self</span>.v = (nn.Parameter(torch.Tensor(<span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_head_inner)),</span><br><span class="line">                          nn.Parameter(torch.Tensor(<span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_head_inner)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_memory</span>(<span class="params">self, device=torch.device(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span>) -&gt; torch.FloatTensor:</span><br><span class="line">        <span class="keyword">return</span> [torch.empty(<span class="number">0</span>, dtype=torch.<span class="built_in">float</span>).to(device) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_layers+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_memory</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">            previous_memory: <span class="type">List</span>[torch.FloatTensor], </span></span><br><span class="line"><span class="params">            hidden_states: <span class="type">List</span>[torch.FloatTensor],</span></span><br><span class="line"><span class="params">        </span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(hidden_states) == <span class="built_in">len</span>(previous_memory)</span><br><span class="line">        mem_len, seq_len = previous_memory[<span class="number">0</span>].size(<span class="number">0</span>), hidden_states[<span class="number">0</span>].size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For the updated memory, we use the most recent `self.mem_len`</span></span><br><span class="line">        <span class="comment"># states, including the previous memory</span></span><br><span class="line">        <span class="comment"># In other words, if `seq_len` &lt; `self.mem_len` some of the previous memory</span></span><br><span class="line">        <span class="comment"># will carry over to the next memory</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            new_memory = []</span><br><span class="line">            end_idx = mem_len + seq_len</span><br><span class="line">            beg_idx = <span class="built_in">max</span>(<span class="number">0</span>, end_idx - <span class="variable language_">self</span>.mem_len)</span><br><span class="line">            <span class="keyword">for</span> m, h <span class="keyword">in</span> <span class="built_in">zip</span>(previous_memory, hidden_states):</span><br><span class="line">                cat = torch.cat([m, h], dim=<span class="number">0</span>) <span class="comment"># (mem_len + seq_len, bs, d)</span></span><br><span class="line">                new_memory.append(cat[beg_idx:end_idx].detach()) <span class="comment"># (self.mem_len, bs, d)</span></span><br><span class="line">        <span class="keyword">return</span> new_memory</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_length</span>(<span class="params">self, seq_len, ext_len, mem_len</span>):</span><br><span class="line">        <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line">        <span class="variable language_">self</span>.mem_len = mem_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idxs: torch.LongTensor, <span class="comment"># (cs, bs)</span></span></span><br><span class="line"><span class="params">                target: torch.LongTensor, <span class="comment"># (cs, bs)</span></span></span><br><span class="line"><span class="params">                memory: <span class="type">Optional</span>[<span class="type">List</span>[torch.FloatTensor]]=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">               </span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">            memory: <span class="type">List</span>[torch.FloatTensor] = <span class="variable language_">self</span>.init_memory(idxs.device)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(memory) == <span class="built_in">len</span>(<span class="variable language_">self</span>.layers) + <span class="number">1</span></span><br><span class="line">        cur_seq, bs = idxs.size()</span><br><span class="line">        prev_seq = memory[<span class="number">0</span>].size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct attention mask</span></span><br><span class="line">        dec_attn_mask = torch.triu(</span><br><span class="line">            torch.ones((cur_seq, cur_seq + prev_seq)),</span><br><span class="line">            diagonal=<span class="number">1</span> + prev_seq,</span><br><span class="line">        ).byte()[...,<span class="literal">None</span>].to(idxs.device)</span><br><span class="line"></span><br><span class="line">        word_embs = <span class="variable language_">self</span>.drop(<span class="variable language_">self</span>.word_embs(idxs))</span><br><span class="line">        pos_idxs = torch.arange(cur_seq + prev_seq - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1.0</span>, dtype=torch.<span class="built_in">float</span>).to(word_embs.device)</span><br><span class="line">        pos_embs = <span class="variable language_">self</span>.drop(<span class="variable language_">self</span>.pos_embs(pos_idxs))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Main part of forward pass</span></span><br><span class="line">        hidden_states = [word_embs]</span><br><span class="line">        layer_out = word_embs</span><br><span class="line">        <span class="keyword">for</span> mem, layer <span class="keyword">in</span> <span class="built_in">zip</span>(memory, <span class="variable language_">self</span>.layers):</span><br><span class="line">            layer_out = layer(layer_out, pos_embs, <span class="variable language_">self</span>.u, <span class="variable language_">self</span>.v, </span><br><span class="line">                              mask=dec_attn_mask, mems=mem)</span><br><span class="line">            hidden_states.append(layer_out)</span><br><span class="line"></span><br><span class="line">        logits = <span class="variable language_">self</span>.output_projection(<span class="variable language_">self</span>.drop(layer_out))        </span><br><span class="line">        loss = <span class="variable language_">self</span>.loss_fn(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), target.view(-<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update memory </span></span><br><span class="line">        <span class="comment"># Ensure the memory is treated as a constant</span></span><br><span class="line">        <span class="comment"># and we do not back propagate through them</span></span><br><span class="line">        new_memory = <span class="variable language_">self</span>.update_memory(memory, hidden_states)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;logits&quot;</span>: logits, <span class="string">&quot;memory&quot;</span>: new_memory&#125;</span><br></pre></td></tr></table></figure>
<p>同样使用一个随机数进行测试，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transformer = TransformerXL(<span class="number">1000</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">17</span>, <span class="number">71</span>, mem_len=<span class="number">5</span>)</span><br><span class="line">idxs = torch.randint(<span class="number">1000</span>, (<span class="number">5</span>, <span class="number">9</span>))</span><br><span class="line">tgts = torch.randint(<span class="number">1000</span>, (<span class="number">5</span>, <span class="number">9</span>))</span><br><span class="line">transformer(idxs, tgts)</span><br></pre></td></tr></table></figure>
<h4 id="数据加载">数据加载</h4>
<p>Transformer-XL的数据加载类似于基于rnn的语言模型的数据加载，但与标准的数据加载有很大的不同。</p>
<p>假设我们将输入分成4个单词的序列输入到模型中。请记住Transformer-XL是有状态的，这意味着每个mini-batch的计算将被转移到下一个mini-batch。对于mini-batch为1的情况，处理起来很简单。我们只是把输入分成块，然后像这样输入到模型中:</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224434.png" alt=""></p>
<p>如果批大小是2会发生什么?我们不能像这样拆分句子，否则，我们将打破片段之间的依赖关系。</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224453.png" alt=""></p>
<p>处理batch size为2的语料库的正确方法，应为：</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224518.png" alt=""></p>
<p>在此基础上，我们首先将语料库划分成batch size的长度片段，然后将每个片段逐块输入到模型中。让我们来看一个例子。假设batch size 为4，我们的整个语料库是这样的:</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pytorch is an amazing deep learning framework that makes nlp really easy</span><br></pre></td></tr></table></figure>
<p>我们想要确保前一batch包含在相同位置上的前一段。换句话说，假设我们一次向模型输入一个单词，我们希望像这样迭代这个句子</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Batch 1: pytorch amazing framework nlp</span><br><span class="line">Batch 2: is deep that really</span><br><span class="line">Batch 3: an learning makes easy</span><br></pre></td></tr></table></figure>
<p>注意，这意味着你通过从上到下，从左到右，而不是从左到右，从上到下来重新构造原句子。实际上，每个batch中的单词序列的长度通常为bptt(时间反向传播)长度，因为这是梯度沿序列方向传播的最大长度。例如，当bptt长度为2时，batch的shape为(batch_size, bptt)：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Batch 1: pytorch amazing framework nlp</span><br><span class="line">         is deep that really</span><br><span class="line">Batch 2: an learning makes easy</span><br></pre></td></tr></table></figure>
<p>我们可以实现这在一个数据加载这样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LMDataLoader</span>(data.DataLoader):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data: torch.LongTensor, batch_size: <span class="built_in">int</span>, bptt: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 device=torch.device(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span>):</span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="variable language_">self</span>.bptt = bptt</span><br><span class="line">        <span class="variable language_">self</span>.n_steps = data.size(<span class="number">0</span>) // batch_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># we reshape the data here so that we can index</span></span><br><span class="line">        <span class="comment"># efficiently into it while training</span></span><br><span class="line">        <span class="variable language_">self</span>.data = (data[:<span class="variable language_">self</span>.n_steps * batch_size] <span class="comment"># trim off any elements that don&#x27;t fit cleanly</span></span><br><span class="line">                     .view(batch_size, <span class="variable language_">self</span>.n_steps) <span class="comment"># </span></span><br><span class="line">                     .transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment"># </span></span><br><span class="line">                     .contiguous().to(device) <span class="comment"># put on device as contiguous tensor</span></span><br><span class="line">                     )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> batch_start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="variable language_">self</span>.data.size(<span class="number">0</span>) - <span class="number">1</span>, <span class="variable language_">self</span>.bptt):</span><br><span class="line">            batch_end_idx = <span class="built_in">min</span>(batch_start_idx + <span class="variable language_">self</span>.bptt, <span class="variable language_">self</span>.data.size(<span class="number">0</span>) - <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> What is `self.ext_len` in the original code?</span></span><br><span class="line">            batch_data = <span class="variable language_">self</span>.data[batch_start_idx:batch_end_idx]</span><br><span class="line">            target = <span class="variable language_">self</span>.data[batch_start_idx+<span class="number">1</span>:batch_end_idx+<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># we generate the sequence length as well for loss calculation later</span></span><br><span class="line">            <span class="keyword">yield</span> batch_data, target, batch_end_idx - batch_start_idx</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> math.ceil(<span class="variable language_">self</span>.data.size(<span class="number">0</span>) / <span class="variable language_">self</span>.bptt)</span><br></pre></td></tr></table></figure>
<p>测试结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">test_corpus = torch.arange(<span class="number">1000</span>)</span><br><span class="line">BS = <span class="number">16</span></span><br><span class="line">BPTT = <span class="number">10</span></span><br><span class="line">test_corpus[:BPTT]</span><br><span class="line"><span class="comment">#output: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line">loader = LMDataLoader(test_corpus, BS, BPTT)</span><br><span class="line">b1, *_ = <span class="built_in">next</span>(<span class="built_in">iter</span>(loader))</span><br><span class="line">b1.shape</span><br><span class="line"><span class="comment">#output: torch.Size([10, 16])</span></span><br></pre></td></tr></table></figure>
<h3 id="完整代码">完整代码</h3>
<p>下载数据集，新建一个名为download_data.sh脚本文件，并写入以下内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo &quot;- Downloading Penn Treebank (PTB)&quot;</span><br><span class="line">wget --quiet --continue http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</span><br><span class="line">tar -xzf simple-examples.tgz</span><br><span class="line">mkdir -p penn</span><br><span class="line">cd penn</span><br><span class="line">mv ../simple-examples/data/ptb.train.txt train.txt</span><br><span class="line">mv ../simple-examples/data/ptb.test.txt test.txt</span><br><span class="line">mv ../simple-examples/data/ptb.valid.txt valid.txt</span><br><span class="line">cd ..</span><br><span class="line">echo &quot;- Downloading Penn Treebank (Character)&quot;</span><br><span class="line">mkdir -p pennchar</span><br><span class="line">cd pennchar</span><br><span class="line">mv ../simple-examples/data/ptb.char.train.txt train.txt</span><br><span class="line">mv ../simple-examples/data/ptb.char.test.txt test.txt</span><br><span class="line">mv ../simple-examples/data/ptb.char.valid.txt valid.txt</span><br><span class="line">cd ..</span><br><span class="line">rm -rf simple-examples/</span><br></pre></td></tr></table></figure>
<p>运行<code>sh download_data.sh</code>命令进行自动下载数据集。</p>
<p>新建一个名为vocabulary.py文件，并写入以下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter, OrderedDict</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Borrowed from the Transformer XL repository&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, special=[], min_freq=<span class="number">0</span>, max_size=<span class="literal">None</span>, lower_case=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 delimiter=<span class="literal">None</span>, vocab_file=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.counter = Counter()</span><br><span class="line">        <span class="variable language_">self</span>.special = special</span><br><span class="line">        <span class="variable language_">self</span>.min_freq = min_freq</span><br><span class="line">        <span class="variable language_">self</span>.max_size = max_size</span><br><span class="line">        <span class="variable language_">self</span>.lower_case = lower_case</span><br><span class="line">        <span class="variable language_">self</span>.delimiter = delimiter</span><br><span class="line">        <span class="variable language_">self</span>.vocab_file = vocab_file</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">self, line, add_eos=<span class="literal">False</span>, add_double_eos=<span class="literal">False</span></span>):</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="comment"># convert to lower case</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.lower_case:</span><br><span class="line">            line = line.lower()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># empty delimiter &#x27;&#x27; will evaluate False</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.delimiter == <span class="string">&#x27;&#x27;</span>:</span><br><span class="line">            symbols = line</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            symbols = line.split(<span class="variable language_">self</span>.delimiter)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> add_double_eos: <span class="comment"># lm1b</span></span><br><span class="line">            <span class="keyword">return</span> [<span class="string">&#x27;&lt;S&gt;&#x27;</span>] + symbols + [<span class="string">&#x27;&lt;S&gt;&#x27;</span>]</span><br><span class="line">        <span class="keyword">elif</span> add_eos:</span><br><span class="line">            <span class="keyword">return</span> symbols + [<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> symbols</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_file</span>(<span class="params">self, path, verbose=<span class="literal">False</span>, add_eos=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">&#x27;counting file &#123;&#125; ...&#x27;</span>.<span class="built_in">format</span>(path))</span><br><span class="line">        <span class="keyword">assert</span> os.path.exists(path)</span><br><span class="line">        sents = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> idx, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(f):</span><br><span class="line">                <span class="keyword">if</span> verbose <span class="keyword">and</span> idx &gt; <span class="number">0</span> <span class="keyword">and</span> idx % <span class="number">500000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;    line &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(idx))</span><br><span class="line">                symbols = <span class="variable language_">self</span>.tokenize(line, add_eos=add_eos)</span><br><span class="line">                <span class="variable language_">self</span>.counter.update(symbols)</span><br><span class="line">                sents.append(symbols)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sents</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">count_sents</span>(<span class="params">self, sents, verbose=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            sents : a list of sentences, each a list of tokenized symbols</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">&#x27;counting &#123;&#125; sents ...&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(sents)))</span><br><span class="line">        <span class="keyword">for</span> idx, symbols <span class="keyword">in</span> <span class="built_in">enumerate</span>(sents):</span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> idx &gt; <span class="number">0</span> <span class="keyword">and</span> idx % <span class="number">500000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;    line &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(idx))</span><br><span class="line">            <span class="variable language_">self</span>.counter.update(symbols)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_build_from_file</span>(<span class="params">self, vocab_file</span>):</span><br><span class="line">        <span class="variable language_">self</span>.idx2sym = []</span><br><span class="line">        <span class="variable language_">self</span>.sym2idx = OrderedDict()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(vocab_file, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                symb = line.strip().split()[<span class="number">0</span>]</span><br><span class="line">                <span class="variable language_">self</span>.add_symbol(symb)</span><br><span class="line">        <span class="variable language_">self</span>.unk_idx = <span class="variable language_">self</span>.sym2idx[<span class="string">&#x27;&lt;UNK&gt;&#x27;</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_vocab</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.vocab_file:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;building vocab from &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="variable language_">self</span>.vocab_file))</span><br><span class="line">            <span class="variable language_">self</span>._build_from_file(<span class="variable language_">self</span>.vocab_file)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;final vocab size &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(<span class="variable language_">self</span>)))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;building vocab with min_freq=&#123;&#125;, max_size=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                <span class="variable language_">self</span>.min_freq, <span class="variable language_">self</span>.max_size))</span><br><span class="line">            <span class="variable language_">self</span>.idx2sym = []</span><br><span class="line">            <span class="variable language_">self</span>.sym2idx = OrderedDict()</span><br><span class="line">            <span class="keyword">for</span> sym <span class="keyword">in</span> <span class="variable language_">self</span>.special:</span><br><span class="line">                <span class="variable language_">self</span>.add_special(sym)</span><br><span class="line">            <span class="keyword">for</span> sym, cnt <span class="keyword">in</span> <span class="variable language_">self</span>.counter.most_common(<span class="variable language_">self</span>.max_size):</span><br><span class="line">                <span class="keyword">if</span> cnt &lt; <span class="variable language_">self</span>.min_freq: <span class="keyword">break</span></span><br><span class="line">                <span class="variable language_">self</span>.add_symbol(sym)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;final vocab size &#123;&#125; from &#123;&#125; unique tokens&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                <span class="built_in">len</span>(<span class="variable language_">self</span>), <span class="built_in">len</span>(<span class="variable language_">self</span>.counter)))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_file</span>(<span class="params">self, path, ordered=<span class="literal">False</span>, verbose=<span class="literal">False</span>, add_eos=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">            add_double_eos=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">&#x27;encoding file &#123;&#125; ...&#x27;</span>.<span class="built_in">format</span>(path))</span><br><span class="line">        <span class="keyword">assert</span> os.path.exists(path)</span><br><span class="line">        encoded = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> idx, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(f):</span><br><span class="line">                <span class="keyword">if</span> verbose <span class="keyword">and</span> idx &gt; <span class="number">0</span> <span class="keyword">and</span> idx % <span class="number">500000</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;    line &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(idx))</span><br><span class="line">                symbols = <span class="variable language_">self</span>.tokenize(line, add_eos=add_eos,</span><br><span class="line">                    add_double_eos=add_double_eos)</span><br><span class="line">                encoded.append(<span class="variable language_">self</span>.convert_to_tensor(symbols))</span><br><span class="line">        <span class="keyword">if</span> ordered:</span><br><span class="line">            encoded = torch.cat(encoded)</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode_sents</span>(<span class="params">self, sents, ordered=<span class="literal">False</span>, verbose=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">if</span> verbose: <span class="built_in">print</span>(<span class="string">&#x27;encoding &#123;&#125; sents ...&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(sents)))</span><br><span class="line">        encoded = []</span><br><span class="line">        <span class="keyword">for</span> idx, symbols <span class="keyword">in</span> <span class="built_in">enumerate</span>(sents):</span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> idx &gt; <span class="number">0</span> <span class="keyword">and</span> idx % <span class="number">500000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;    line &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(idx))</span><br><span class="line">            encoded.append(<span class="variable language_">self</span>.convert_to_tensor(symbols))</span><br><span class="line">        <span class="keyword">if</span> ordered:</span><br><span class="line">            encoded = torch.cat(encoded)</span><br><span class="line">        <span class="keyword">return</span> encoded</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_special</span>(<span class="params">self, sym</span>):</span><br><span class="line">        <span class="keyword">if</span> sym <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.sym2idx:</span><br><span class="line">            <span class="variable language_">self</span>.idx2sym.append(sym)</span><br><span class="line">            <span class="variable language_">self</span>.sym2idx[sym] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2sym) - <span class="number">1</span></span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, <span class="string">&#x27;&#123;&#125;_idx&#x27;</span>.<span class="built_in">format</span>(sym.strip(<span class="string">&#x27;&lt;&gt;&#x27;</span>)), <span class="variable language_">self</span>.sym2idx[sym])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_symbol</span>(<span class="params">self, sym</span>):</span><br><span class="line">        <span class="keyword">if</span> sym <span class="keyword">not</span> <span class="keyword">in</span> <span class="variable language_">self</span>.sym2idx:</span><br><span class="line">            <span class="variable language_">self</span>.idx2sym.append(sym)</span><br><span class="line">            <span class="variable language_">self</span>.sym2idx[sym] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2sym) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_sym</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= idx &lt; <span class="built_in">len</span>(<span class="variable language_">self</span>), <span class="string">&#x27;Index &#123;&#125; out of range&#x27;</span>.<span class="built_in">format</span>(idx)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.idx2sym[idx]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_idx</span>(<span class="params">self, sym</span>):</span><br><span class="line">        <span class="keyword">if</span> sym <span class="keyword">in</span> <span class="variable language_">self</span>.sym2idx:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.sym2idx[sym]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># print(&#x27;encounter unk &#123;&#125;&#x27;.format(sym))</span></span><br><span class="line">            <span class="keyword">assert</span> <span class="string">&#x27;&lt;eos&gt;&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> sym</span><br><span class="line">            <span class="keyword">assert</span> <span class="built_in">hasattr</span>(<span class="variable language_">self</span>, <span class="string">&#x27;unk_idx&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.sym2idx.get(sym, <span class="variable language_">self</span>.unk_idx)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_symbols</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.get_sym(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> indices]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_indices</span>(<span class="params">self, symbols</span>):</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.get_idx(sym) <span class="keyword">for</span> sym <span class="keyword">in</span> symbols]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert_to_tensor</span>(<span class="params">self, symbols</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.LongTensor(<span class="variable language_">self</span>.get_indices(symbols))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert_to_sent</span>(<span class="params">self, indices, exclude=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> exclude <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join([<span class="variable language_">self</span>.get_sym(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> indices])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join([<span class="variable language_">self</span>.get_sym(idx) <span class="keyword">for</span> idx <span class="keyword">in</span> indices <span class="keyword">if</span> idx <span class="keyword">not</span> <span class="keyword">in</span> exclude])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx2sym)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>改脚本主要处理语料数据，接下里新建一个名为trainsformer_xl.py文件，并写入以下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> vocabulary <span class="keyword">import</span> Vocab</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d = d</span><br><span class="line">        inv_freq = <span class="number">1</span> / (<span class="number">10000</span> ** (torch.arange(<span class="number">0.0</span>, d, <span class="number">2.0</span>) / d))</span><br><span class="line">        <span class="comment"># register buffer tells pytorch that this tensor is part of the modle</span></span><br><span class="line">        <span class="comment"># this means that it will be saved in the state_dict and moved to the GPU</span></span><br><span class="line">        <span class="comment"># along with the model</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&quot;inv_freq&quot;</span>, inv_freq)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, positions: torch.LongTensor</span>):</span><br><span class="line">        <span class="comment"># outer product</span></span><br><span class="line">        sinusoid_inp = torch.einsum(<span class="string">&quot;i,j-&gt;ij&quot;</span>, positions.<span class="built_in">float</span>(), <span class="variable language_">self</span>.inv_freq)</span><br><span class="line">        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> pos_emb[:, <span class="literal">None</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_input: <span class="built_in">int</span>, d_inner: <span class="built_in">int</span>, n_heads: <span class="built_in">int</span> = <span class="number">4</span>,</span></span><br><span class="line"><span class="params">                 dropout: <span class="built_in">float</span> = <span class="number">0.1</span>, dropouta: <span class="built_in">float</span> = <span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_input = d_input</span><br><span class="line">        <span class="variable language_">self</span>.d_inner = d_inner</span><br><span class="line">        <span class="variable language_">self</span>.n_heads = n_heads</span><br><span class="line">        <span class="comment"># this layer applies the linear transformation required</span></span><br><span class="line">        <span class="comment"># for the keys and values for all heads at once for efficiency</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_kv = nn.Linear(</span><br><span class="line">            d_input,</span><br><span class="line">            (d_inner * n_heads * <span class="number">2</span>),  <span class="comment"># 2 is for keys and values</span></span><br><span class="line">            bias=<span class="literal">False</span>,  <span class="comment"># we don&#x27;t apply bias, making this a simple matrix multiplication</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># for queries (will not be concatenated with memorized states so separate)</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_q = nn.Linear(</span><br><span class="line">            d_input, d_inner * n_heads,</span><br><span class="line">            bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># for positional embeddings</span></span><br><span class="line">        <span class="variable language_">self</span>.linear_p = nn.Linear(</span><br><span class="line">            d_input, d_inner * n_heads,</span><br><span class="line">            bias=<span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.scale = <span class="number">1</span> / (d_inner ** <span class="number">0.5</span>)  <span class="comment"># for scaled dot product attention</span></span><br><span class="line">        <span class="variable language_">self</span>.dropa = nn.Dropout(dropouta)</span><br><span class="line">        <span class="comment"># we will use this to project back to the input dimension</span></span><br><span class="line">        <span class="variable language_">self</span>.lout = nn.Linear(<span class="variable language_">self</span>.d_inner * <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_input, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.LayerNorm(<span class="variable language_">self</span>.d_input)</span><br><span class="line">        <span class="variable language_">self</span>.dropo = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_rel_shift</span>(<span class="params">self, x</span>):</span><br><span class="line">        zero_pad = torch.zeros((x.size(<span class="number">0</span>), <span class="number">1</span>, *x.size()[<span class="number">2</span>:]),</span><br><span class="line">                               device=x.device, dtype=x.dtype)</span><br><span class="line">        <span class="keyword">return</span> (torch.cat([zero_pad, x], dim=<span class="number">1</span>)</span><br><span class="line">                .view(x.size(<span class="number">1</span>) + <span class="number">1</span>, x.size(<span class="number">0</span>), *x.size()[<span class="number">2</span>:])[<span class="number">1</span>:]</span><br><span class="line">                .view_as(x))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.FloatTensor,  <span class="comment"># (cur_seq, b, d_in)</span></span></span><br><span class="line"><span class="params">                pos_embs: torch.FloatTensor,  <span class="comment"># (cur_seq + prev_seq, d_in)</span></span></span><br><span class="line"><span class="params">                memory: torch.FloatTensor,  <span class="comment"># (prev_seq, b, d_in)</span></span></span><br><span class="line"><span class="params">                u: torch.FloatTensor,  <span class="comment"># (H, d)</span></span></span><br><span class="line"><span class="params">                v: torch.FloatTensor,  <span class="comment"># (H, d)</span></span></span><br><span class="line"><span class="params">                mask: <span class="type">Optional</span>[torch.FloatTensor] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        pos_embs: we pass the positional embeddings in separately</span></span><br><span class="line"><span class="string">            because we need to handle relative positions</span></span><br><span class="line"><span class="string">        input shape: (seq, bs, self.d_input)</span></span><br><span class="line"><span class="string">        pos_embs shape: (seq + prev_seq, bs, self.d_input)</span></span><br><span class="line"><span class="string">        output shape: (seq, bs, self.d_input)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        cur_seq = input_.shape[<span class="number">0</span>]  <span class="comment"># sequence length of current segment</span></span><br><span class="line">        prev_seq = memory.shape[<span class="number">0</span>]  <span class="comment"># sequence length of previous segment</span></span><br><span class="line">        H, d = <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_inner</span><br><span class="line">        input_with_memory = torch.cat([memory, input_], dim=<span class="number">0</span>)  <span class="comment"># concatenate recurrent memory</span></span><br><span class="line">        <span class="comment"># across sequence dimension</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># we will use the following symbols to represent the shape of the tensors</span></span><br><span class="line">        <span class="comment"># cs: current sequence length, b: batch, H: number of heads</span></span><br><span class="line">        <span class="comment"># d: inner dimension, ps: previous sequence length</span></span><br><span class="line">        <span class="comment"># The key and value are now conditioned on the preceding context</span></span><br><span class="line">        k_tfmd, v_tfmd = \</span><br><span class="line">            torch.chunk(<span class="variable language_">self</span>.linear_kv(input_with_memory), <span class="number">2</span>, dim=-<span class="number">1</span>)  <span class="comment"># (cs + ps, b, H * d)</span></span><br><span class="line">        q_tfmd = <span class="variable language_">self</span>.linear_q(input_)  <span class="comment"># (cs, b, H * d)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># apply scaled dot product attention</span></span><br><span class="line">        <span class="comment"># look at the following dimensions carefully, since this is the key operation</span></span><br><span class="line">        <span class="comment"># in the Transformer/Transformer XL architecture</span></span><br><span class="line"></span><br><span class="line">        _, bs, _ = q_tfmd.shape</span><br><span class="line">        <span class="keyword">assert</span> bs == k_tfmd.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># content-based attention term ((a) + (c) in the paper)</span></span><br><span class="line">        <span class="comment"># this is the standard attention term in the original Transformer, except without positional embeddings</span></span><br><span class="line">        <span class="comment"># which are handled separately in the Transformer XL (see below)</span></span><br><span class="line">        <span class="comment"># here, i corresponds to the number of queries = number of current inputs/targets (seq-wise)</span></span><br><span class="line">        <span class="comment"># j corresponds to the number of key/values = number of vectors that we can use to compute the</span></span><br><span class="line">        <span class="comment"># vector for each query</span></span><br><span class="line">        content_attn = torch.einsum(<span class="string">&quot;ibhd,jbhd-&gt;ijbh&quot;</span>, (</span><br><span class="line">            (q_tfmd.view(cur_seq, bs, H, d) +  <span class="comment"># (a)</span></span><br><span class="line">             u),  <span class="comment"># (c): u represents the global (independent of the query)</span></span><br><span class="line">            <span class="comment"># bias towards certain key/values = words</span></span><br><span class="line">            <span class="comment"># Note: maybe this could be a per-attention head parameter?</span></span><br><span class="line">            k_tfmd.view(cur_seq + prev_seq, bs, H, d)  <span class="comment"># There is no positional information to be found here</span></span><br><span class="line">        ))  <span class="comment"># (cs, cs + ps, b, H)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># position-based attention term ((b) + (d) in the paper)</span></span><br><span class="line">        <span class="comment"># this attention is solely based on the position of the key/values</span></span><br><span class="line">        <span class="comment"># (i.e. it does not take the content of the key/values into account)</span></span><br><span class="line">        p_tfmd = <span class="variable language_">self</span>.linear_p(pos_embs)  <span class="comment"># (cs + ps, b, H * d)</span></span><br><span class="line">        position_attn = torch.einsum(<span class="string">&quot;ibhd,jhd-&gt;ijbh&quot;</span>, (</span><br><span class="line">            (q_tfmd.view(cur_seq, bs, H, d) +  <span class="comment"># (b)</span></span><br><span class="line">             v),  <span class="comment"># (d): v represents the global (independent of the query)</span></span><br><span class="line">            <span class="comment"># bias towards certain positions</span></span><br><span class="line">            p_tfmd.view(cur_seq + prev_seq, H, d)  <span class="comment"># Notice there is not content information</span></span><br><span class="line">            <span class="comment"># regarding keys and values here!</span></span><br><span class="line">        ))  <span class="comment"># (cs, cs + ps, b, H)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#  Compute positional attention efficiently</span></span><br><span class="line">        position_attn = <span class="variable language_">self</span>._rel_shift(position_attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># the attention is the sum of content-based and position-based attention</span></span><br><span class="line">        attn = content_attn + position_attn</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> mask.<span class="built_in">any</span>().item():</span><br><span class="line">            attn = attn.masked_fill(</span><br><span class="line">                mask[..., <span class="literal">None</span>], -<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">        attn = torch.softmax(attn * <span class="variable language_">self</span>.scale,  <span class="comment"># rescale to prevent values from exploding</span></span><br><span class="line">                             dim=<span class="number">1</span>)  <span class="comment"># normalize across the value sequence dimension</span></span><br><span class="line">        attn = <span class="variable language_">self</span>.dropa(attn)</span><br><span class="line"></span><br><span class="line">        attn_weighted_values = (torch.einsum(<span class="string">&quot;ijbh,jbhd-&gt;ibhd&quot;</span>,</span><br><span class="line">                                             (attn,  <span class="comment"># (cs, cs + ps, b, H)</span></span><br><span class="line">                                              v_tfmd.view(cur_seq + prev_seq, bs, H, d),  <span class="comment"># (cs + ps, b, H, d)</span></span><br><span class="line">                                              ))  <span class="comment"># (cs, b, H, d)</span></span><br><span class="line">                                .contiguous()  <span class="comment"># we need to change the memory layout to make `view` work</span></span><br><span class="line">                                .view(cur_seq, bs, H * d))  <span class="comment"># (cs, b, H * d)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Project back to input dimension and add residual connection</span></span><br><span class="line">        output = input_ + <span class="variable language_">self</span>.dropo(<span class="variable language_">self</span>.lout(attn_weighted_values))</span><br><span class="line">        output = <span class="variable language_">self</span>.norm(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFF</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_input, d_inner, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.d_input = d_input</span><br><span class="line">        <span class="variable language_">self</span>.d_inner = d_inner</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="variable language_">self</span>.ff = nn.Sequential(</span><br><span class="line">            nn.Linear(d_input, d_inner), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(d_inner, d_input),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.layer_norm = nn.LayerNorm(d_input)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.FloatTensor, <span class="comment"># (cur_seq, bs, d_input)</span></span></span><br><span class="line"><span class="params">               </span>) -&gt; torch.FloatTensor: <span class="comment"># (cur_seq, bs, d_input)</span></span><br><span class="line">        ff_out = <span class="variable language_">self</span>.ff(input_)</span><br><span class="line">        output = <span class="variable language_">self</span>.layer_norm(input_ + ff_out)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_heads, d_input,</span></span><br><span class="line"><span class="params">                 d_head_inner, d_ff_inner,</span></span><br><span class="line"><span class="params">                 dropout, dropouta=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.mha = MultiHeadAttention(d_input, d_head_inner, n_heads=n_heads,</span><br><span class="line">                                      dropout=dropout, dropouta=dropouta)</span><br><span class="line">        <span class="variable language_">self</span>.ff = PositionwiseFF(d_input, d_ff_inner, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.FloatTensor,  <span class="comment"># (cur_seq, bs, d_input)</span></span></span><br><span class="line"><span class="params">                pos_embs: torch.FloatTensor,  <span class="comment"># (cur_seq + prev_seq, d_input),</span></span></span><br><span class="line"><span class="params">                u: torch.FloatTensor,  <span class="comment"># (H, d_input),</span></span></span><br><span class="line"><span class="params">                v: torch.FloatTensor,  <span class="comment"># (H, d_input),</span></span></span><br><span class="line"><span class="params">                mask=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                mems=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.ff(<span class="variable language_">self</span>.mha(input_, pos_embs, mems, u, v, mask=mask))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StandardWordEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, embedding_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_embeddings = num_embeddings</span><br><span class="line">        <span class="variable language_">self</span>.embedding_dim = embedding_dim</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.scale = embedding_dim ** <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_: torch.LongTensor</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.embedding(input_) * <span class="variable language_">self</span>.scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerXL</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_embeddings, n_layers, n_heads,</span></span><br><span class="line"><span class="params">                 d_model, d_head_inner, d_ff_inner,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0.1</span>, dropouta=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 seq_len: <span class="built_in">int</span> = <span class="number">0</span>, mem_len: <span class="built_in">int</span> = <span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_layers, <span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_model, <span class="variable language_">self</span>.d_head_inner, <span class="variable language_">self</span>.d_ff_inner = \</span><br><span class="line">            n_layers, n_heads, d_model, d_head_inner, d_ff_inner</span><br><span class="line">        <span class="comment"># Embedding layers</span></span><br><span class="line">        <span class="variable language_">self</span>.word_embs = StandardWordEmbedding(num_embeddings, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.pos_embs = PositionalEmbedding(d_model)</span><br><span class="line">        <span class="comment"># Core transformer</span></span><br><span class="line">        <span class="variable language_">self</span>.drop = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([DecoderBlock(n_heads, d_model, d_head_inner=d_head_inner,</span><br><span class="line">                                                  d_ff_inner=d_ff_inner,</span><br><span class="line">                                                  dropout=dropout, dropouta=dropouta)</span><br><span class="line">                                     <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tie weights</span></span><br><span class="line">        <span class="variable language_">self</span>.output_projection = nn.Linear(d_model, num_embeddings)</span><br><span class="line">        <span class="variable language_">self</span>.output_projection.weight = <span class="variable language_">self</span>.word_embs.embedding.weight</span><br><span class="line">        <span class="variable language_">self</span>.loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.seq_len, <span class="variable language_">self</span>.mem_len = seq_len, mem_len</span><br><span class="line"></span><br><span class="line">        <span class="comment"># u and v are global parameters: maybe changing these to per-head parameters</span></span><br><span class="line">        <span class="comment"># might help performance?</span></span><br><span class="line">        <span class="variable language_">self</span>.u, <span class="variable language_">self</span>.v = (nn.Parameter(torch.Tensor(<span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_head_inner)),</span><br><span class="line">                          nn.Parameter(torch.Tensor(<span class="variable language_">self</span>.n_heads, <span class="variable language_">self</span>.d_head_inner)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_memory</span>(<span class="params">self, device=torch.device(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span>) -&gt; torch.FloatTensor:</span><br><span class="line">        <span class="keyword">return</span> [torch.empty(<span class="number">0</span>, dtype=torch.<span class="built_in">float</span>).to(device) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_layers + <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_memory</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                      previous_memory: <span class="type">List</span>[torch.FloatTensor],</span></span><br><span class="line"><span class="params">                      hidden_states: <span class="type">List</span>[torch.FloatTensor],</span></span><br><span class="line"><span class="params">                      </span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(hidden_states) == <span class="built_in">len</span>(previous_memory)</span><br><span class="line">        mem_len, seq_len = previous_memory[<span class="number">0</span>].size(<span class="number">0</span>), hidden_states[<span class="number">0</span>].size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For the updated memory, we use the most recent `self.mem_len`</span></span><br><span class="line">        <span class="comment"># states, including the previous memory</span></span><br><span class="line">        <span class="comment"># In other words, if `seq_len` &lt; `self.mem_len` some of the previous memory</span></span><br><span class="line">        <span class="comment"># will carry over to the next memory</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            new_memory = []</span><br><span class="line">            end_idx = mem_len + seq_len</span><br><span class="line">            beg_idx = <span class="built_in">max</span>(<span class="number">0</span>, end_idx - <span class="variable language_">self</span>.mem_len)</span><br><span class="line">            <span class="keyword">for</span> m, h <span class="keyword">in</span> <span class="built_in">zip</span>(previous_memory, hidden_states):</span><br><span class="line">                cat = torch.cat([m, h], dim=<span class="number">0</span>)  <span class="comment"># (mem_len + seq_len, bs, d)</span></span><br><span class="line">                new_memory.append(cat[beg_idx:end_idx].detach())  <span class="comment"># (self.mem_len, bs, d)</span></span><br><span class="line">        <span class="keyword">return</span> new_memory</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset_length</span>(<span class="params">self, seq_len, ext_len, mem_len</span>):</span><br><span class="line">        <span class="variable language_">self</span>.seq_len = seq_len</span><br><span class="line">        <span class="variable language_">self</span>.mem_len = mem_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, idxs: torch.LongTensor,  <span class="comment"># (cs, bs)</span></span></span><br><span class="line"><span class="params">                target: torch.LongTensor,  <span class="comment"># (cs, bs)</span></span></span><br><span class="line"><span class="params">                memory: <span class="type">Optional</span>[<span class="type">List</span>[torch.FloatTensor]] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                </span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">if</span> memory <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            memory: <span class="type">List</span>[torch.FloatTensor] = <span class="variable language_">self</span>.init_memory(idxs.device)</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(memory) == <span class="built_in">len</span>(<span class="variable language_">self</span>.layers) + <span class="number">1</span></span><br><span class="line">        cur_seq, bs = idxs.size()</span><br><span class="line">        prev_seq = memory[<span class="number">0</span>].size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Construct attention mask</span></span><br><span class="line">        dec_attn_mask = torch.triu(</span><br><span class="line">            torch.ones((cur_seq, cur_seq + prev_seq)),</span><br><span class="line">            diagonal=<span class="number">1</span> + prev_seq,</span><br><span class="line">        ).byte()[..., <span class="literal">None</span>].to(idxs.device)</span><br><span class="line"></span><br><span class="line">        word_embs = <span class="variable language_">self</span>.drop(<span class="variable language_">self</span>.word_embs(idxs))</span><br><span class="line">        pos_idxs = torch.arange(cur_seq + prev_seq - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1.0</span>, dtype=torch.<span class="built_in">float</span>).to(word_embs.device)</span><br><span class="line">        pos_embs = <span class="variable language_">self</span>.drop(<span class="variable language_">self</span>.pos_embs(pos_idxs))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Main part of forward pass</span></span><br><span class="line">        hidden_states = [word_embs]</span><br><span class="line">        layer_out = word_embs</span><br><span class="line">        <span class="keyword">for</span> mem, layer <span class="keyword">in</span> <span class="built_in">zip</span>(memory, <span class="variable language_">self</span>.layers):</span><br><span class="line">            layer_out = layer(layer_out, pos_embs, <span class="variable language_">self</span>.u, <span class="variable language_">self</span>.v,</span><br><span class="line">                              mask=dec_attn_mask, mems=mem)</span><br><span class="line">            hidden_states.append(layer_out)</span><br><span class="line"></span><br><span class="line">        logits = <span class="variable language_">self</span>.output_projection(<span class="variable language_">self</span>.drop(layer_out))</span><br><span class="line">        loss = <span class="variable language_">self</span>.loss_fn(logits.view(-<span class="number">1</span>, logits.size(-<span class="number">1</span>)), target.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update memory</span></span><br><span class="line">        <span class="comment"># Ensure the memory is treated as a constant</span></span><br><span class="line">        <span class="comment"># and we do not back propagate through them</span></span><br><span class="line">        new_memory = <span class="variable language_">self</span>.update_memory(memory, hidden_states)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;logits&quot;</span>: logits, <span class="string">&quot;memory&quot;</span>: new_memory&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>(<span class="title class_ inherited__">dict</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">            <span class="built_in">setattr</span>(<span class="variable language_">self</span>, k, v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set</span>(<span class="params">self, key, val</span>):</span><br><span class="line">        <span class="variable language_">self</span>[key] = val</span><br><span class="line">        <span class="built_in">setattr</span>(<span class="variable language_">self</span>, key, val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, dct</span>):</span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> dct.items():</span><br><span class="line">            <span class="variable language_">self</span>.<span class="built_in">set</span>(k, v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LMDataLoader</span>(data.DataLoader):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data: torch.LongTensor, batch_size: <span class="built_in">int</span>, bptt: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">                 device=torch.device(<span class="params"><span class="string">&quot;cpu&quot;</span></span>)</span>):</span><br><span class="line">        <span class="variable language_">self</span>.batch_size = batch_size</span><br><span class="line">        <span class="variable language_">self</span>.bptt = bptt</span><br><span class="line">        <span class="variable language_">self</span>.n_steps = data.size(<span class="number">0</span>) // batch_size</span><br><span class="line">        <span class="comment"># we reshape the data here so that we can index</span></span><br><span class="line">        <span class="comment"># efficiently into it while training</span></span><br><span class="line">        <span class="variable language_">self</span>.data = (data[:<span class="variable language_">self</span>.n_steps * batch_size] <span class="comment"># trim off any elements that don&#x27;t fit cleanly</span></span><br><span class="line">                     .view(batch_size, <span class="variable language_">self</span>.n_steps) <span class="comment">#</span></span><br><span class="line">                     .transpose(<span class="number">0</span>, <span class="number">1</span>) <span class="comment">#</span></span><br><span class="line">                     .contiguous().to(device) <span class="comment"># put on device as contiguous tensor</span></span><br><span class="line">                     )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> batch_start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="variable language_">self</span>.data.size(<span class="number">0</span>) - <span class="number">1</span>, <span class="variable language_">self</span>.bptt):</span><br><span class="line">            batch_end_idx = <span class="built_in">min</span>(batch_start_idx + <span class="variable language_">self</span>.bptt, <span class="variable language_">self</span>.data.size(<span class="number">0</span>) - <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> What is `self.ext_len` in the original code?</span></span><br><span class="line">            batch_data = <span class="variable language_">self</span>.data[batch_start_idx:batch_end_idx]</span><br><span class="line">            target = <span class="variable language_">self</span>.data[batch_start_idx +<span class="number">1</span>:batch_end_idx +<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># we generate the sequence length as well for loss calculation later</span></span><br><span class="line">            <span class="keyword">yield</span> batch_data, target, batch_end_idx - batch_start_idx</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> math.ceil(<span class="variable language_">self</span>.data.size(<span class="number">0</span>) / <span class="variable language_">self</span>.bptt)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weight</span>(<span class="params">weight</span>):</span><br><span class="line">    nn.init.normal_(weight, <span class="number">0.0</span>, <span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_bias</span>(<span class="params">bias</span>):</span><br><span class="line">    nn.init.constant_(bias, <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Borrowed from the transformer XL repo</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weights_init</span>(<span class="params">m</span>):</span><br><span class="line">    classname = m.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> classname.find(<span class="string">&#x27;Linear&#x27;</span>) != -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;weight&#x27;</span>) <span class="keyword">and</span> m.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            init_weight(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;bias&#x27;</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            init_bias(m.bias)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">&#x27;Embedding&#x27;</span>) != -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;weight&#x27;</span>):</span><br><span class="line">            init_weight(m.weight)</span><br><span class="line">    <span class="keyword">elif</span> classname.find(<span class="string">&#x27;LayerNorm&#x27;</span>) != -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;weight&#x27;</span>):</span><br><span class="line">            nn.init.normal_(m.weight, <span class="number">1.0</span>, <span class="number">0.02</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;bias&#x27;</span>) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            init_bias(m.bias)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;u&#x27;</span>):</span><br><span class="line">            init_weight(m.u)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(m, <span class="string">&#x27;v&#x27;</span>):</span><br><span class="line">            init_weight(m.v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">        epoch: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">        model: nn.Module, train_loader: data.DataLoader,</span></span><br><span class="line"><span class="params">        val_loader: data.DataLoader,</span></span><br><span class="line"><span class="params">        optimizer: optim.Optimizer,</span></span><br><span class="line"><span class="params">        scheduler,</span></span><br><span class="line"><span class="params">        train_step_start=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># Turn on training mode which enables dropout.</span></span><br><span class="line">    model.train()</span><br><span class="line">    mems = <span class="literal">None</span></span><br><span class="line">    train_step = train_step_start</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    log_start_time = time.time()</span><br><span class="line">    best_val_loss = <span class="built_in">float</span>(<span class="string">&quot;inf&quot;</span>)</span><br><span class="line"></span><br><span class="line">    pbar = tqdm(train_loader, total=<span class="built_in">min</span>(config.max_step - train_step_start, <span class="built_in">len</span>(train_loader)))</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span>(pbar):</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        out_dict = model(data, target, memory=mems)</span><br><span class="line">        loss, mems = out_dict[<span class="string">&quot;loss&quot;</span>], out_dict[<span class="string">&quot;memory&quot;</span>]</span><br><span class="line"></span><br><span class="line">        loss.backward()</span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        loss_change.append(loss.item())</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip)</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step-wise learning rate annealing</span></span><br><span class="line">        train_step += <span class="number">1</span></span><br><span class="line">        <span class="comment"># linear warmup stage</span></span><br><span class="line">        <span class="keyword">if</span> train_step &lt; config.warmup_step:</span><br><span class="line">            curr_lr = config.lr * train_step / config.warmup_step</span><br><span class="line">            optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>] = curr_lr</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scheduler.step(train_step)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_step % config.log_interval == <span class="number">0</span>:</span><br><span class="line">            cur_loss = train_loss / config.log_interval</span><br><span class="line">            elapsed = time.time() - log_start_time</span><br><span class="line">            log_str = <span class="string">&#x27;| epoch &#123;:3d&#125; step &#123;:&gt;8d&#125; | lr &#123;:.3g&#125; &#x27;</span> \</span><br><span class="line">                      <span class="string">&#x27;| loss &#123;:5.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, train_step, optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>], cur_loss)</span><br><span class="line">            log_str += <span class="string">&#x27; | ppl &#123;:9.3f&#125;&#x27;</span>.<span class="built_in">format</span>(math.exp(cur_loss))</span><br><span class="line">            pbar.set_description(log_str)</span><br><span class="line">            train_loss = <span class="number">0</span></span><br><span class="line">            log_start_time = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_step % config.eval_interval == <span class="number">0</span>:</span><br><span class="line">            val_loss = evaluate(model, val_loader)</span><br><span class="line">            val_loss_change.append(val_loss)</span><br><span class="line">            eval_start_time = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> train_step == config.max_step:</span><br><span class="line">            <span class="keyword">return</span> train_step</span><br><span class="line">    <span class="keyword">return</span> train_step</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">model, train_loader, valid_loader</span>):</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=config.lr)</span><br><span class="line">    total_steps = <span class="built_in">min</span>(config.max_step, <span class="built_in">len</span>(train_loader) * config.epochs)</span><br><span class="line">    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,</span><br><span class="line">                    total_steps, eta_min=config.min_lr)</span><br><span class="line">    train_step_start = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(config.epochs):</span><br><span class="line">        <span class="keyword">if</span> train_step_start &gt;= config.max_step:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        train_step_start = train_epoch(</span><br><span class="line">            epoch,</span><br><span class="line">            model,</span><br><span class="line">            train_iter,</span><br><span class="line">            valid_iter,</span><br><span class="line">            optimizer,</span><br><span class="line">            scheduler,</span><br><span class="line">            train_step_start,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model: nn.Module, val_loader: data.DataLoader</span>):</span><br><span class="line">    <span class="comment"># Turn on evaluation mode which disables dropout.</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    model.reset_length(config.eval_bptt,</span><br><span class="line">        <span class="number">0</span>, config.eval_mem_len+config.train_bptt-config.eval_bptt)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluation</span></span><br><span class="line">    total_len, total_loss = <span class="number">0</span>, <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        mems = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i, (data, target, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_loader):</span><br><span class="line">            out_dict = model(data, target, memory=mems)</span><br><span class="line">            loss, mems = out_dict[<span class="string">&quot;loss&quot;</span>], out_dict[<span class="string">&quot;memory&quot;</span>]</span><br><span class="line">            total_loss += seq_len * loss.<span class="built_in">float</span>().item()</span><br><span class="line">            total_len += seq_len</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Switch back to the training mode</span></span><br><span class="line">    model.reset_length(config.train_bptt, <span class="number">0</span>, config.mem_len)</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_len</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_final</span>(<span class="params">model, val_loader</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_len, total_loss = <span class="number">0</span>, <span class="number">0.</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    model.reset_length(config.eval_bptt, <span class="number">0</span>, config.eval_mem_len + config.train_bptt - config.eval_bptt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        mems = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i, (data, target, seq_len) <span class="keyword">in</span> <span class="built_in">enumerate</span>(val_loader):</span><br><span class="line">            out_dict = model(data, target, memory=mems)</span><br><span class="line">            loss, mems = out_dict[<span class="string">&quot;loss&quot;</span>], out_dict[<span class="string">&quot;memory&quot;</span>]</span><br><span class="line">            total_loss += seq_len * loss.item()</span><br><span class="line">            total_len += seq_len</span><br><span class="line">        total_time = time.time() - start_time</span><br><span class="line"></span><br><span class="line">    model.reset_length(config.train_bptt, <span class="number">0</span>, config.mem_len)</span><br><span class="line">    loss_val = total_loss / total_len</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;loss&quot;</span>: loss_val, <span class="string">&quot;ppl&quot;</span>: math.exp(loss_val)&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    TESTING = <span class="literal">True</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cpu&quot;</span>) <span class="keyword">if</span> <span class="keyword">not</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">    <span class="comment"># We will use prime numbers to ensure our implementation</span></span><br><span class="line">    <span class="comment"># is actually correct</span></span><br><span class="line">    config = Config(</span><br><span class="line">        seed=<span class="number">101</span>,</span><br><span class="line">        debug=<span class="literal">False</span>,</span><br><span class="line">        warmup_step=<span class="number">0</span>,</span><br><span class="line">        <span class="comment"># Check default params</span></span><br><span class="line">        min_lr=<span class="number">0.</span>,</span><br><span class="line">        dropouta=<span class="number">0.</span>,</span><br><span class="line">        clip=<span class="number">0.25</span>,</span><br><span class="line">        log_interval=<span class="number">200</span>,</span><br><span class="line">        eval_interval=<span class="number">100</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> TESTING:</span><br><span class="line">        config.update(<span class="built_in">dict</span>(</span><br><span class="line">            debug=<span class="literal">True</span>,</span><br><span class="line">            lr=<span class="number">0.00025</span>,</span><br><span class="line">            bs=<span class="number">8</span>,</span><br><span class="line">            epochs=<span class="number">2</span>,</span><br><span class="line">            max_step=<span class="number">10000</span>, <span class="comment"># shorten for testing</span></span><br><span class="line">            n_layers=<span class="number">4</span>,</span><br><span class="line">            n_heads=<span class="number">3</span>,</span><br><span class="line">            d_model=<span class="number">32</span>,</span><br><span class="line">            d_head_inner=<span class="number">17</span>,</span><br><span class="line">            d_ff_inner=<span class="number">71</span>,</span><br><span class="line">            dropout=<span class="number">0.1</span>,</span><br><span class="line">            train_bptt=<span class="number">33</span>,</span><br><span class="line">            eval_bptt=<span class="number">41</span>,</span><br><span class="line">            mem_len=<span class="number">41</span>,</span><br><span class="line">            eval_mem_len=<span class="number">63</span>,</span><br><span class="line">        ))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        config.update(<span class="built_in">dict</span>(</span><br><span class="line">            lr=<span class="number">0.0025</span>,</span><br><span class="line">            bs=<span class="number">64</span>,</span><br><span class="line">            epochs=<span class="number">4</span>,</span><br><span class="line">            max_step=<span class="number">400000</span>,</span><br><span class="line">            n_layers=<span class="number">12</span>,</span><br><span class="line">            n_heads=<span class="number">8</span>,</span><br><span class="line">            d_model=<span class="number">512</span>,</span><br><span class="line">            d_head_inner=<span class="number">64</span>,</span><br><span class="line">            d_ff_inner=<span class="number">2048</span>,</span><br><span class="line">            dropout=<span class="number">0.1</span>,</span><br><span class="line">            train_bptt=<span class="number">512</span>,</span><br><span class="line">            eval_bptt=<span class="number">128</span>,</span><br><span class="line">            mem_len=<span class="number">512</span>,</span><br><span class="line">            eval_mem_len=<span class="number">2100</span>,</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">    DATASET = <span class="string">&quot;penn&quot;</span></span><br><span class="line">    DATA_DIR = Path(<span class="string">&quot;.&quot;</span>) / DATASET</span><br><span class="line"></span><br><span class="line">    vocab = Vocab(special=[<span class="string">&quot;&lt;eos&gt;&quot;</span>], lower_case=<span class="literal">True</span>)</span><br><span class="line">    vocab.count_file(DATA_DIR / <span class="string">&quot;train.txt&quot;</span>)</span><br><span class="line">    vocab.count_file(DATA_DIR / <span class="string">&quot;valid.txt&quot;</span>)</span><br><span class="line">    vocab.count_file(DATA_DIR / <span class="string">&quot;test.txt&quot;</span>)</span><br><span class="line">    vocab.build_vocab()</span><br><span class="line">    train_dataset = vocab.encode_file(DATA_DIR / <span class="string">&quot;train.txt&quot;</span>, ordered=<span class="literal">True</span>, add_eos=<span class="literal">True</span>)</span><br><span class="line">    valid_dataset = vocab.encode_file(DATA_DIR / <span class="string">&quot;valid.txt&quot;</span>, ordered=<span class="literal">True</span>, add_eos=<span class="literal">True</span>)</span><br><span class="line">    test_dataset = vocab.encode_file(DATA_DIR / <span class="string">&quot;test.txt&quot;</span>, ordered=<span class="literal">True</span>, add_eos=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_iter = LMDataLoader(train_dataset, config.bs, config.train_bptt, device=device)</span><br><span class="line">    valid_iter = LMDataLoader(valid_dataset, config.bs, config.eval_bptt, device=device)</span><br><span class="line">    test_iter = LMDataLoader(test_dataset, config.bs, config.eval_bptt, device=device)</span><br><span class="line"></span><br><span class="line">    loss_change = []</span><br><span class="line">    val_loss_change = []</span><br><span class="line"></span><br><span class="line">    transformer_xl = TransformerXL(</span><br><span class="line">        num_embeddings=<span class="built_in">len</span>(vocab), n_layers=config.n_layers,</span><br><span class="line">        n_heads=config.n_heads, d_model=config.d_model,</span><br><span class="line">        d_head_inner=config.d_head_inner,</span><br><span class="line">        d_ff_inner=config.d_ff_inner,</span><br><span class="line">        dropout=config.dropout,</span><br><span class="line">        dropouta=config.dropouta,</span><br><span class="line">        seq_len=config.train_bptt,</span><br><span class="line">        mem_len=config.mem_len,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        transformer_xl.cuda()</span><br><span class="line">    transformer_xl.apply(weights_init)</span><br><span class="line">    train(</span><br><span class="line">        transformer_xl,</span><br><span class="line">        train_iter,</span><br><span class="line">        valid_iter,</span><br><span class="line">    )</span><br><span class="line">    eval_result = evaluate_final(transformer_xl, valid_iter)</span><br><span class="line">    <span class="built_in">print</span>(eval_result)</span><br><span class="line">    plt.plot(loss_change)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;train_loss.png&#x27;</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">    plt.close()</span><br><span class="line">    plt.plot(val_loss_change)</span><br><span class="line">    plt.savefig(<span class="string">&#x27;val_loss.png&#x27;</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">    plt.close()</span><br></pre></td></tr></table></figure>
<p>运行<code>python trainsformer_xl.py</code>命令，将得到以下结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;loss&#x27;: 6.048809673745043, &#x27;ppl&#x27;: 423.6084975087664&#125;</span><br></pre></td></tr></table></figure>
<p>训练loss变化</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224645.png" alt=""></p>
<p>验证loss变化</p>
<p><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223224737.png" alt=""></p>
<h3 id="参考">参考</h3>
<ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1901.02860.pdf">Transformer-XL：Attentive Language Models Beyond a Fixed-Length Context</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Weitang Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lonepatient.top/2019/07/14/transformer_xl.html">http://lonepatient.top/2019/07/14/transformer_xl.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lonepatient.top" target="_blank">闲记算法</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">语言模型</a><a class="post-meta__tags" href="/tags/Transformer-XL/">Transformer-XL</a><a class="post-meta__tags" href="/tags/%E9%95%BF%E6%96%87%E6%9C%AC/">长文本</a></div><div class="post_share"><div class="social-share" data-image="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20191223223929.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/08/11/MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation.html"><img class="prev-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20200922222831.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">MASS-Masked Sequence to Sequence Pre-training for Language Generation</div></div></a></div><div class="next-post pull-right"><a href="/2019/06/20/ubuntu_ssr_vpn.html"><img class="next-cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20200219144306.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Ubuntu 18.04环境下搭建SSR</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/30/Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.html" title="Chain of Thought Prompting Elicits Reasoning in Large Language Models"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230222150222.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-30</div><div class="title">Chain of Thought Prompting Elicits Reasoning in Large Language Models</div></div></a></div><div><a href="/2019/02/12/Contextual-String-Embeddings-for-Sequence-Labeling.html" title="Contextual String Embeddings for Sequence Labeling"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/20190225224900.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-02-12</div><div class="title">Contextual String Embeddings for Sequence Labeling</div></div></a></div><div><a href="/2021/11/18/Language_Models_are_Few-Shot_Learners.html" title="Language Models are Few-Shot Learners"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230220200030.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-18</div><div class="title">Language Models are Few-Shot Learners</div></div></a></div><div><a href="/2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" title="通向AGI之路：大型语言模型（LLM）技术精要"><img class="cover" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230219144729.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-18</div><div class="title">通向AGI之路：大型语言模型（LLM）技术精要</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/touxiang.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Weitang Liu</div><div class="author-info__description">一个致力于记录技术的博客</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">281</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">384</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">92</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lonePatient"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/lonePatient" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuweitangmath@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/277974397" target="_blank" title="Weibo"><i class="fab fa-weibo"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">记录和分享一些学习和开源内容，若有任何问题可通过留言板或者微信公众号给我留言，谢谢！<img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/blog_picgo/202602201201102.jpg"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8BTransformer"><span class="toc-number">1.</span> <span class="toc-text">原始Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer-XL"><span class="toc-number">2.</span> <span class="toc-text">Transformer-XL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Recurrence%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.</span> <span class="toc-text">Recurrence机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Relative-Positional-Encodings"><span class="toc-number">2.2.</span> <span class="toc-text">Relative Positional Encodings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="toc-number">2.3.</span> <span class="toc-text">计算公式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">3.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">3.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95Attention-Head"><span class="toc-number">3.2.</span> <span class="toc-text">单Attention Head</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text">相对位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MultiHeadAttention%E6%A8%A1%E5%9D%97"><span class="toc-number">3.4.</span> <span class="toc-text">MultiHeadAttention模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Decoder"><span class="toc-number">3.5.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD"><span class="toc-number">3.6.</span> <span class="toc-text">数据加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">4.</span> <span class="toc-text">完整代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/02/27/arxiv_papers_2026-02-27.html" title="Arxiv今日论文 | 2026-02-27"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2026-02-27"/></a><div class="content"><a class="title" href="/2026/02/27/arxiv_papers_2026-02-27.html" title="Arxiv今日论文 | 2026-02-27">Arxiv今日论文 | 2026-02-27</a><time datetime="2026-02-27T12:30:00.000Z" title="发表于 2026-02-27 12:30:00">2026-02-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/26/arxiv_papers_2026-02-26.html" title="Arxiv今日论文 | 2026-02-26"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2026-02-26"/></a><div class="content"><a class="title" href="/2026/02/26/arxiv_papers_2026-02-26.html" title="Arxiv今日论文 | 2026-02-26">Arxiv今日论文 | 2026-02-26</a><time datetime="2026-02-26T12:30:00.000Z" title="发表于 2026-02-26 12:30:00">2026-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/25/arxiv_papers_2026-02-25.html" title="Arxiv今日论文 | 2026-02-25"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2026-02-25"/></a><div class="content"><a class="title" href="/2026/02/25/arxiv_papers_2026-02-25.html" title="Arxiv今日论文 | 2026-02-25">Arxiv今日论文 | 2026-02-25</a><time datetime="2026-02-25T12:30:00.000Z" title="发表于 2026-02-25 12:30:00">2026-02-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/25/2026-02-25-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs.html" title="大模型数据合成新范式：2K样本打败30万，从特征空间精准狙击任务短板"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/blog_picgo/20260225222513891.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大模型数据合成新范式：2K样本打败30万，从特征空间精准狙击任务短板"/></a><div class="content"><a class="title" href="/2026/02/25/2026-02-25-Less-is-Enough-Synthesizing-Diverse-Data-in-Feature-Space-of-LLMs.html" title="大模型数据合成新范式：2K样本打败30万，从特征空间精准狙击任务短板">大模型数据合成新范式：2K样本打败30万，从特征空间精准狙击任务短板</a><time datetime="2026-02-25T12:00:00.000Z" title="发表于 2026-02-25 12:00:00">2026-02-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/25/2026-02-25-Midtraining-%E4%B8%AD%E9%97%B4%E8%AE%AD%E7%BB%83-%E6%9E%84%E5%BB%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%90%8E%E8%AE%AD%E7%BB%83%E4%B9%8B%E9%97%B4%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%A5%E6%A2%81.html" title="mid-training：构建预训练与后训练之间的分布式桥梁"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/blog_picgo/20260225123005910.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="mid-training：构建预训练与后训练之间的分布式桥梁"/></a><div class="content"><a class="title" href="/2026/02/25/2026-02-25-Midtraining-%E4%B8%AD%E9%97%B4%E8%AE%AD%E7%BB%83-%E6%9E%84%E5%BB%BA%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%90%8E%E8%AE%AD%E7%BB%83%E4%B9%8B%E9%97%B4%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%A5%E6%A2%81.html" title="mid-training：构建预训练与后训练之间的分布式桥梁">mid-training：构建预训练与后训练之间的分布式桥梁</a><time datetime="2026-02-25T00:00:00.000Z" title="发表于 2026-02-25 00:00:00">2026-02-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/24/arxiv_papers_2026-02-24.html" title="Arxiv今日论文 | 2026-02-24"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2026-02-24"/></a><div class="content"><a class="title" href="/2026/02/24/arxiv_papers_2026-02-24.html" title="Arxiv今日论文 | 2026-02-24">Arxiv今日论文 | 2026-02-24</a><time datetime="2026-02-24T12:30:00.000Z" title="发表于 2026-02-24 12:30:00">2026-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/23/arxiv_papers_2026-02-23.html" title="Arxiv今日论文 | 2026-02-23"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2026-02-23"/></a><div class="content"><a class="title" href="/2026/02/23/arxiv_papers_2026-02-23.html" title="Arxiv今日论文 | 2026-02-23">Arxiv今日论文 | 2026-02-23</a><time datetime="2026-02-23T12:30:00.000Z" title="发表于 2026-02-23 12:30:00">2026-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/23/Think-Deep,-Not-Just-Long-Measuring-LLM-Reasoning-Effort-via-Deep-Thinking-Tokens.html" title="用&quot;深度思考率&quot;精准度量LLM推理质量"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/blog_picgo/20260223165943195.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="用&quot;深度思考率&quot;精准度量LLM推理质量"/></a><div class="content"><a class="title" href="/2026/02/23/Think-Deep,-Not-Just-Long-Measuring-LLM-Reasoning-Effort-via-Deep-Thinking-Tokens.html" title="用&quot;深度思考率&quot;精准度量LLM推理质量">用&quot;深度思考率&quot;精准度量LLM推理质量</a><time datetime="2026-02-23T12:00:00.000Z" title="发表于 2026-02-23 12:00:00">2026-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/20/arxiv_papers_2026-02-20.html" title="Arxiv今日论文 | 2026-02-20"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20210911210134.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Arxiv今日论文 | 2026-02-20"/></a><div class="content"><a class="title" href="/2026/02/20/arxiv_papers_2026-02-20.html" title="Arxiv今日论文 | 2026-02-20">Arxiv今日论文 | 2026-02-20</a><time datetime="2026-02-20T12:30:00.000Z" title="发表于 2026-02-20 12:30:00">2026-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2026/02/20/frontier_training_cn.html" title="前沿大模型训练方法：深度解析与实践指南"><img src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/blog_picgo/202602201606857.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="前沿大模型训练方法：深度解析与实践指南"/></a><div class="content"><a class="title" href="/2026/02/20/frontier_training_cn.html" title="前沿大模型训练方法：深度解析与实践指南">前沿大模型训练方法：深度解析与实践指南</a><time datetime="2026-02-20T10:30:00.000Z" title="发表于 2026-02-20 10:30:00">2026-02-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2026 By Weitang Liu</div><div class="footer_custom_text"><p><a style="margin-inline:5px"target="_blank"href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo"title="博客框架为Hexo"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender"title="主题采用butterfly"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr"title="本站使用JsDelivr为静态资源提供CDN加速"alt="img"></a><a style="margin-inline:5px"target="_blank"href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub"title="本站项目由Gtihub托管"alt="img"></a><a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris"alt="img"title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></br></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    let initData = {
      el: '#twikoo-wrap',
      envId: 'https://twikoo.lonepatient.top/',
      region: ''
    }

    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    twikoo.init(initData)
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo.lonepatient.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo@1.4.11/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (10)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/自然语言处理/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 自然语言处理 (99)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/计算机视觉/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🎮 计算机视觉 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/知识图谱/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🐱 知识图谱 (13)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="http://lonepatient.top/categories/深度学习/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 深度学习 (139)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="http://lonepatient.top/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>function electric_clock_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-clock/clock/images/weather/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>';
                console.log('已挂载electric_clock')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            electric_clock_injector_config()
        } </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax  src="https://cdn.jsdelivr.net/gh/Zfour/hexo-electric-clock@1.0.6/clock.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230219144729.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-02-18</span><a class="blog-slider__title" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">通向AGI之路：大型语言模型（LLM）技术精要</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2023/02/18/The_Road_to_AGI_Larg_Language_Model_Technical_Essentials.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/07/12/gaiic_2022_ner_top10.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20220712181905.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-07-12</span><a class="blog-slider__title" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">GAIIC2022商品标题识别二等奖获奖解决思路</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2022/07/12/gaiic_2022_ner_top10.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt=""><img width="48" height="48" src="https://lonepatient-1257945978.cos.ap-chengdu.myqcloud.com/images/20230807190424.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2021-01-20</span><a class="blog-slider__title" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">2025-03|高质量中文预训练模型集合</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="2021/01/20/awesome-pretrained-chinese-nlp-models.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --></body></html>